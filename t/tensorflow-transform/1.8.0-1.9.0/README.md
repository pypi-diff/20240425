# Comparing `tmp/tensorflow_transform-1.8.0-py3-none-any.whl.zip` & `tmp/tensorflow_transform-1.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,108 +1,108 @@
-Zip file size: 435273 bytes, number of entries: 106
--rw-rw-r--  2.0 unx     1631 b- defN 22-May-13 07:04 tensorflow_transform/__init__.py
--rw-rw-r--  2.0 unx    39852 b- defN 22-May-13 07:04 tensorflow_transform/analyzer_nodes.py
--rw-rw-r--  2.0 unx   109671 b- defN 22-May-13 07:04 tensorflow_transform/analyzers.py
--rw-rw-r--  2.0 unx    23461 b- defN 22-May-13 07:04 tensorflow_transform/analyzers_test.py
--rw-rw-r--  2.0 unx     9081 b- defN 22-May-13 07:04 tensorflow_transform/annotators.py
--rw-rw-r--  2.0 unx     2416 b- defN 22-May-13 07:04 tensorflow_transform/annotators_test.py
--rw-rw-r--  2.0 unx     2869 b- defN 22-May-13 07:04 tensorflow_transform/common.py
--rw-rw-r--  2.0 unx     2031 b- defN 22-May-13 07:04 tensorflow_transform/common_test.py
--rw-rw-r--  2.0 unx     2906 b- defN 22-May-13 07:04 tensorflow_transform/common_types.py
--rw-rw-r--  2.0 unx    13792 b- defN 22-May-13 07:04 tensorflow_transform/gaussianization.py
--rw-rw-r--  2.0 unx     9430 b- defN 22-May-13 07:04 tensorflow_transform/gaussianization_test.py
--rw-rw-r--  2.0 unx     4653 b- defN 22-May-13 07:04 tensorflow_transform/graph_context.py
--rw-rw-r--  2.0 unx    39030 b- defN 22-May-13 07:04 tensorflow_transform/graph_tools.py
--rw-rw-r--  2.0 unx    53081 b- defN 22-May-13 07:04 tensorflow_transform/graph_tools_test.py
--rw-rw-r--  2.0 unx    41078 b- defN 22-May-13 07:04 tensorflow_transform/impl_helper.py
--rw-rw-r--  2.0 unx    39285 b- defN 22-May-13 07:04 tensorflow_transform/impl_helper_test.py
--rw-rw-r--  2.0 unx     5011 b- defN 22-May-13 07:04 tensorflow_transform/info_theory.py
--rw-rw-r--  2.0 unx     4597 b- defN 22-May-13 07:04 tensorflow_transform/info_theory_test.py
--rw-rw-r--  2.0 unx     4318 b- defN 22-May-13 07:04 tensorflow_transform/inspect_preprocessing_fn.py
--rw-rw-r--  2.0 unx     6096 b- defN 22-May-13 07:04 tensorflow_transform/inspect_preprocessing_fn_test.py
--rw-rw-r--  2.0 unx    95662 b- defN 22-May-13 07:04 tensorflow_transform/mappers.py
--rw-rw-r--  2.0 unx    37123 b- defN 22-May-13 07:04 tensorflow_transform/mappers_test.py
--rw-rw-r--  2.0 unx    12912 b- defN 22-May-13 07:04 tensorflow_transform/nodes.py
--rw-rw-r--  2.0 unx     9531 b- defN 22-May-13 07:04 tensorflow_transform/nodes_test.py
--rw-rw-r--  2.0 unx    23233 b- defN 22-May-13 07:04 tensorflow_transform/output_wrapper.py
--rw-rw-r--  2.0 unx     2133 b- defN 22-May-13 07:04 tensorflow_transform/pickle_helper.py
--rw-rw-r--  2.0 unx    11375 b- defN 22-May-13 07:04 tensorflow_transform/pretrained_models.py
--rw-rw-r--  2.0 unx     7278 b- defN 22-May-13 07:04 tensorflow_transform/pretrained_models_test.py
--rw-rw-r--  2.0 unx    31652 b- defN 22-May-13 07:04 tensorflow_transform/schema_inference.py
--rw-rw-r--  2.0 unx    16745 b- defN 22-May-13 07:04 tensorflow_transform/schema_inference_test.py
--rw-rw-r--  2.0 unx    13104 b- defN 22-May-13 07:04 tensorflow_transform/test_case.py
--rw-rw-r--  2.0 unx     3131 b- defN 22-May-13 07:04 tensorflow_transform/test_case_test.py
--rw-rw-r--  2.0 unx     8703 b- defN 22-May-13 07:04 tensorflow_transform/tf2_utils.py
--rw-rw-r--  2.0 unx     4241 b- defN 22-May-13 07:04 tensorflow_transform/tf2_utils_test.py
--rw-rw-r--  2.0 unx    68763 b- defN 22-May-13 07:04 tensorflow_transform/tf_utils.py
--rw-rw-r--  2.0 unx    99904 b- defN 22-May-13 07:04 tensorflow_transform/tf_utils_test.py
--rw-rw-r--  2.0 unx      710 b- defN 22-May-13 07:04 tensorflow_transform/version.py
--rw-rw-r--  2.0 unx     1732 b- defN 22-May-13 07:04 tensorflow_transform/beam/__init__.py
--rw-rw-r--  2.0 unx    26760 b- defN 22-May-13 07:04 tensorflow_transform/beam/analysis_graph_builder.py
--rw-rw-r--  2.0 unx    44955 b- defN 22-May-13 07:04 tensorflow_transform/beam/analysis_graph_builder_test.py
--rw-rw-r--  2.0 unx    11354 b- defN 22-May-13 07:04 tensorflow_transform/beam/analyzer_cache.py
--rw-rw-r--  2.0 unx     9786 b- defN 22-May-13 07:04 tensorflow_transform/beam/analyzer_cache_test.py
--rw-rw-r--  2.0 unx    55415 b- defN 22-May-13 07:04 tensorflow_transform/beam/analyzer_impls.py
--rw-rw-r--  2.0 unx     5820 b- defN 22-May-13 07:04 tensorflow_transform/beam/analyzer_impls_test.py
--rw-rw-r--  2.0 unx     7649 b- defN 22-May-13 07:04 tensorflow_transform/beam/beam_nodes.py
--rw-rw-r--  2.0 unx    32841 b- defN 22-May-13 07:04 tensorflow_transform/beam/bucketize_integration_test.py
--rw-rw-r--  2.0 unx    75768 b- defN 22-May-13 07:04 tensorflow_transform/beam/cached_impl_test.py
--rw-rw-r--  2.0 unx    25939 b- defN 22-May-13 07:04 tensorflow_transform/beam/combiner_packing_util.py
--rw-rw-r--  2.0 unx    53092 b- defN 22-May-13 07:04 tensorflow_transform/beam/combiner_packing_util_test.py
--rw-rw-r--  2.0 unx     7881 b- defN 22-May-13 07:04 tensorflow_transform/beam/common.py
--rw-rw-r--  2.0 unx     7168 b- defN 22-May-13 07:04 tensorflow_transform/beam/context.py
--rw-rw-r--  2.0 unx     1596 b- defN 22-May-13 07:04 tensorflow_transform/beam/context_test.py
--rw-rw-r--  2.0 unx    10333 b- defN 22-May-13 07:04 tensorflow_transform/beam/deep_copy.py
--rw-rw-r--  2.0 unx    13697 b- defN 22-May-13 07:04 tensorflow_transform/beam/deep_copy_test.py
--rw-rw-r--  2.0 unx    66916 b- defN 22-May-13 07:04 tensorflow_transform/beam/impl.py
--rw-rw-r--  2.0 unx     5193 b- defN 22-May-13 07:04 tensorflow_transform/beam/impl_output_record_batches_test.py
--rw-rw-r--  2.0 unx   153451 b- defN 22-May-13 07:04 tensorflow_transform/beam/impl_test.py
--rw-rw-r--  2.0 unx      807 b- defN 22-May-13 07:04 tensorflow_transform/beam/test_helpers.py
--rw-rw-r--  2.0 unx    17953 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_unit.py
--rw-rw-r--  2.0 unx    24237 b- defN 22-May-13 07:04 tensorflow_transform/beam/tukey_hh_params_integration_test.py
--rw-rw-r--  2.0 unx    79775 b- defN 22-May-13 07:04 tensorflow_transform/beam/vocabulary_integration_test.py
--rw-rw-r--  2.0 unx     2075 b- defN 22-May-13 07:04 tensorflow_transform/beam/vocabulary_tfrecord_gzip_integration_test.py
--rw-rw-r--  2.0 unx      736 b- defN 22-May-13 07:04 tensorflow_transform/beam/experimental/__init__.py
--rw-rw-r--  2.0 unx     1037 b- defN 22-May-13 07:04 tensorflow_transform/beam/experimental/analyzer_impls.py
--rw-rw-r--  2.0 unx      913 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/__init__.py
--rw-rw-r--  2.0 unx     3680 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/beam_metadata_io.py
--rw-rw-r--  2.0 unx     3866 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/beam_metadata_io_test.py
--rw-rw-r--  2.0 unx     1284 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/test_metadata.py
--rw-rw-r--  2.0 unx     5467 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/transform_fn_io.py
--rw-rw-r--  2.0 unx     7134 b- defN 22-May-13 07:04 tensorflow_transform/beam/tft_beam_io/transform_fn_io_test.py
--rw-rw-r--  2.0 unx      794 b- defN 22-May-13 07:04 tensorflow_transform/coders/__init__.py
--rw-rw-r--  2.0 unx     8922 b- defN 22-May-13 07:04 tensorflow_transform/coders/csv_coder.py
--rw-rw-r--  2.0 unx     9868 b- defN 22-May-13 07:04 tensorflow_transform/coders/csv_coder_test.py
--rw-rw-r--  2.0 unx     9676 b- defN 22-May-13 07:04 tensorflow_transform/coders/example_proto_coder.py
--rw-rw-r--  2.0 unx    17310 b- defN 22-May-13 07:04 tensorflow_transform/coders/example_proto_coder_test.py
--rw-rw-r--  2.0 unx      836 b- defN 22-May-13 07:04 tensorflow_transform/experimental/__init__.py
--rw-rw-r--  2.0 unx    24226 b- defN 22-May-13 07:04 tensorflow_transform/experimental/analyzers.py
--rw-rw-r--  2.0 unx     2902 b- defN 22-May-13 07:04 tensorflow_transform/experimental/annotators.py
--rw-rw-r--  2.0 unx     4777 b- defN 22-May-13 07:04 tensorflow_transform/experimental/mappers.py
--rw-rw-r--  2.0 unx      809 b- defN 22-May-13 07:04 tensorflow_transform/py_func/__init__.py
--rw-rw-r--  2.0 unx     3263 b- defN 22-May-13 07:04 tensorflow_transform/py_func/api.py
--rw-rw-r--  2.0 unx     6983 b- defN 22-May-13 07:04 tensorflow_transform/py_func/pyfunc_helper.py
--rw-rw-r--  2.0 unx      655 b- defN 22-May-13 07:04 tensorflow_transform/saved/__init__.py
--rw-rw-r--  2.0 unx      797 b- defN 22-May-13 07:04 tensorflow_transform/saved/constants.py
--rw-rw-r--  2.0 unx     2898 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_model_loader.py
--rw-rw-r--  2.0 unx     1510 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_model_loader_test.py
--rw-rw-r--  2.0 unx    19956 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_transform_io.py
--rw-rw-r--  2.0 unx    13690 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_transform_io_test.py
--rw-rw-r--  2.0 unx    22590 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_transform_io_v2.py
--rw-rw-r--  2.0 unx    26397 b- defN 22-May-13 07:04 tensorflow_transform/saved/saved_transform_io_v2_test.py
--rw-rw-r--  2.0 unx      596 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/__init__.py
--rw-rw-r--  2.0 unx     2338 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/dataset_metadata.py
--rw-rw-r--  2.0 unx     1060 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/dataset_metadata_test.py
--rw-rw-r--  2.0 unx     4828 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/metadata_io.py
--rw-rw-r--  2.0 unx     3523 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/metadata_io_test.py
--rw-rw-r--  2.0 unx    30036 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/schema_utils.py
--rw-rw-r--  2.0 unx     1266 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/schema_utils_legacy.py
--rw-rw-r--  2.0 unx     3685 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/schema_utils_test.py
--rw-rw-r--  2.0 unx    24266 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/schema_utils_test_cases.py
--rw-rw-r--  2.0 unx     1415 b- defN 22-May-13 07:04 tensorflow_transform/tf_metadata/test_common.py
--rw-rw-r--  2.0 unx    11420 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx    12083 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx        1 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/namespace_packages.txt
--rw-rw-r--  2.0 unx       21 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    10714 b- defN 22-May-13 07:04 tensorflow_transform-1.8.0.dist-info/RECORD
-106 files, 1913002 bytes uncompressed, 417799 bytes compressed:  78.2%
+Zip file size: 436838 bytes, number of entries: 106
+-rw-rw-r--  2.0 unx     1631 b- defN 22-Jun-29 07:26 tensorflow_transform/__init__.py
+-rw-rw-r--  2.0 unx    39852 b- defN 22-Jun-29 07:26 tensorflow_transform/analyzer_nodes.py
+-rw-rw-r--  2.0 unx   109964 b- defN 22-Jun-29 07:26 tensorflow_transform/analyzers.py
+-rw-rw-r--  2.0 unx    23461 b- defN 22-Jun-29 07:26 tensorflow_transform/analyzers_test.py
+-rw-rw-r--  2.0 unx     9244 b- defN 22-Jun-29 07:26 tensorflow_transform/annotators.py
+-rw-rw-r--  2.0 unx     2605 b- defN 22-Jun-29 07:26 tensorflow_transform/annotators_test.py
+-rw-rw-r--  2.0 unx     2869 b- defN 22-Jun-29 07:26 tensorflow_transform/common.py
+-rw-rw-r--  2.0 unx     2031 b- defN 22-Jun-29 07:26 tensorflow_transform/common_test.py
+-rw-rw-r--  2.0 unx     2906 b- defN 22-Jun-29 07:26 tensorflow_transform/common_types.py
+-rw-rw-r--  2.0 unx    13792 b- defN 22-Jun-29 07:26 tensorflow_transform/gaussianization.py
+-rw-rw-r--  2.0 unx     9430 b- defN 22-Jun-29 07:26 tensorflow_transform/gaussianization_test.py
+-rw-rw-r--  2.0 unx     4653 b- defN 22-Jun-29 07:26 tensorflow_transform/graph_context.py
+-rw-rw-r--  2.0 unx    39030 b- defN 22-Jun-29 07:26 tensorflow_transform/graph_tools.py
+-rw-rw-r--  2.0 unx    53081 b- defN 22-Jun-29 07:26 tensorflow_transform/graph_tools_test.py
+-rw-rw-r--  2.0 unx    41078 b- defN 22-Jun-29 07:26 tensorflow_transform/impl_helper.py
+-rw-rw-r--  2.0 unx    39285 b- defN 22-Jun-29 07:26 tensorflow_transform/impl_helper_test.py
+-rw-rw-r--  2.0 unx     5035 b- defN 22-Jun-29 07:26 tensorflow_transform/info_theory.py
+-rw-rw-r--  2.0 unx     5165 b- defN 22-Jun-29 07:26 tensorflow_transform/info_theory_test.py
+-rw-rw-r--  2.0 unx     4318 b- defN 22-Jun-29 07:26 tensorflow_transform/inspect_preprocessing_fn.py
+-rw-rw-r--  2.0 unx     6096 b- defN 22-Jun-29 07:26 tensorflow_transform/inspect_preprocessing_fn_test.py
+-rw-rw-r--  2.0 unx    96187 b- defN 22-Jun-29 07:26 tensorflow_transform/mappers.py
+-rw-rw-r--  2.0 unx    37123 b- defN 22-Jun-29 07:26 tensorflow_transform/mappers_test.py
+-rw-rw-r--  2.0 unx    12912 b- defN 22-Jun-29 07:26 tensorflow_transform/nodes.py
+-rw-rw-r--  2.0 unx     9531 b- defN 22-Jun-29 07:26 tensorflow_transform/nodes_test.py
+-rw-rw-r--  2.0 unx    23233 b- defN 22-Jun-29 07:26 tensorflow_transform/output_wrapper.py
+-rw-rw-r--  2.0 unx     2133 b- defN 22-Jun-29 07:26 tensorflow_transform/pickle_helper.py
+-rw-rw-r--  2.0 unx    11375 b- defN 22-Jun-29 07:26 tensorflow_transform/pretrained_models.py
+-rw-rw-r--  2.0 unx     7278 b- defN 22-Jun-29 07:26 tensorflow_transform/pretrained_models_test.py
+-rw-rw-r--  2.0 unx    31652 b- defN 22-Jun-29 07:26 tensorflow_transform/schema_inference.py
+-rw-rw-r--  2.0 unx    16745 b- defN 22-Jun-29 07:26 tensorflow_transform/schema_inference_test.py
+-rw-rw-r--  2.0 unx    13104 b- defN 22-Jun-29 07:26 tensorflow_transform/test_case.py
+-rw-rw-r--  2.0 unx     3131 b- defN 22-Jun-29 07:26 tensorflow_transform/test_case_test.py
+-rw-rw-r--  2.0 unx     8703 b- defN 22-Jun-29 07:26 tensorflow_transform/tf2_utils.py
+-rw-rw-r--  2.0 unx     4241 b- defN 22-Jun-29 07:26 tensorflow_transform/tf2_utils_test.py
+-rw-rw-r--  2.0 unx    69571 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_utils.py
+-rw-rw-r--  2.0 unx   102376 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_utils_test.py
+-rw-rw-r--  2.0 unx      710 b- defN 22-Jun-29 07:26 tensorflow_transform/version.py
+-rw-rw-r--  2.0 unx     1732 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/__init__.py
+-rw-rw-r--  2.0 unx    26760 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analysis_graph_builder.py
+-rw-rw-r--  2.0 unx    44955 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analysis_graph_builder_test.py
+-rw-rw-r--  2.0 unx    11354 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analyzer_cache.py
+-rw-rw-r--  2.0 unx     9786 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analyzer_cache_test.py
+-rw-rw-r--  2.0 unx    55415 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analyzer_impls.py
+-rw-rw-r--  2.0 unx     5820 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/analyzer_impls_test.py
+-rw-rw-r--  2.0 unx     7649 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/beam_nodes.py
+-rw-rw-r--  2.0 unx    32841 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/bucketize_integration_test.py
+-rw-rw-r--  2.0 unx    75889 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/cached_impl_test.py
+-rw-rw-r--  2.0 unx    25939 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/combiner_packing_util.py
+-rw-rw-r--  2.0 unx    53092 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/combiner_packing_util_test.py
+-rw-rw-r--  2.0 unx     7997 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/common.py
+-rw-rw-r--  2.0 unx     7168 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/context.py
+-rw-rw-r--  2.0 unx     1596 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/context_test.py
+-rw-rw-r--  2.0 unx    10333 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/deep_copy.py
+-rw-rw-r--  2.0 unx    13697 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/deep_copy_test.py
+-rw-rw-r--  2.0 unx    66916 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/impl.py
+-rw-rw-r--  2.0 unx     5193 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/impl_output_record_batches_test.py
+-rw-rw-r--  2.0 unx   164905 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/impl_test.py
+-rw-rw-r--  2.0 unx      807 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/test_helpers.py
+-rw-rw-r--  2.0 unx    17953 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_unit.py
+-rw-rw-r--  2.0 unx    24237 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tukey_hh_params_integration_test.py
+-rw-rw-r--  2.0 unx    79775 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/vocabulary_integration_test.py
+-rw-rw-r--  2.0 unx     2075 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/vocabulary_tfrecord_gzip_integration_test.py
+-rw-rw-r--  2.0 unx      736 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/experimental/__init__.py
+-rw-rw-r--  2.0 unx     1037 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/experimental/analyzer_impls.py
+-rw-rw-r--  2.0 unx      913 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/__init__.py
+-rw-rw-r--  2.0 unx     3680 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/beam_metadata_io.py
+-rw-rw-r--  2.0 unx     3866 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/beam_metadata_io_test.py
+-rw-rw-r--  2.0 unx     1284 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/test_metadata.py
+-rw-rw-r--  2.0 unx     5467 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/transform_fn_io.py
+-rw-rw-r--  2.0 unx     7134 b- defN 22-Jun-29 07:26 tensorflow_transform/beam/tft_beam_io/transform_fn_io_test.py
+-rw-rw-r--  2.0 unx      794 b- defN 22-Jun-29 07:26 tensorflow_transform/coders/__init__.py
+-rw-rw-r--  2.0 unx     8922 b- defN 22-Jun-29 07:26 tensorflow_transform/coders/csv_coder.py
+-rw-rw-r--  2.0 unx     9868 b- defN 22-Jun-29 07:26 tensorflow_transform/coders/csv_coder_test.py
+-rw-rw-r--  2.0 unx     9676 b- defN 22-Jun-29 07:26 tensorflow_transform/coders/example_proto_coder.py
+-rw-rw-r--  2.0 unx    17310 b- defN 22-Jun-29 07:26 tensorflow_transform/coders/example_proto_coder_test.py
+-rw-rw-r--  2.0 unx      836 b- defN 22-Jun-29 07:26 tensorflow_transform/experimental/__init__.py
+-rw-rw-r--  2.0 unx    24226 b- defN 22-Jun-29 07:26 tensorflow_transform/experimental/analyzers.py
+-rw-rw-r--  2.0 unx     2902 b- defN 22-Jun-29 07:26 tensorflow_transform/experimental/annotators.py
+-rw-rw-r--  2.0 unx     4777 b- defN 22-Jun-29 07:26 tensorflow_transform/experimental/mappers.py
+-rw-rw-r--  2.0 unx      809 b- defN 22-Jun-29 07:26 tensorflow_transform/py_func/__init__.py
+-rw-rw-r--  2.0 unx     3263 b- defN 22-Jun-29 07:26 tensorflow_transform/py_func/api.py
+-rw-rw-r--  2.0 unx     6983 b- defN 22-Jun-29 07:26 tensorflow_transform/py_func/pyfunc_helper.py
+-rw-rw-r--  2.0 unx      655 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/__init__.py
+-rw-rw-r--  2.0 unx      797 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/constants.py
+-rw-rw-r--  2.0 unx     2898 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_model_loader.py
+-rw-rw-r--  2.0 unx     1510 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_model_loader_test.py
+-rw-rw-r--  2.0 unx    19956 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_transform_io.py
+-rw-rw-r--  2.0 unx    13690 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_transform_io_test.py
+-rw-rw-r--  2.0 unx    22771 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_transform_io_v2.py
+-rw-rw-r--  2.0 unx    26397 b- defN 22-Jun-29 07:26 tensorflow_transform/saved/saved_transform_io_v2_test.py
+-rw-rw-r--  2.0 unx      596 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/__init__.py
+-rw-rw-r--  2.0 unx     2338 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/dataset_metadata.py
+-rw-rw-r--  2.0 unx     1060 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/dataset_metadata_test.py
+-rw-rw-r--  2.0 unx     4828 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/metadata_io.py
+-rw-rw-r--  2.0 unx     3523 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/metadata_io_test.py
+-rw-rw-r--  2.0 unx    30062 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/schema_utils.py
+-rw-rw-r--  2.0 unx     1266 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/schema_utils_legacy.py
+-rw-rw-r--  2.0 unx     3685 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/schema_utils_test.py
+-rw-rw-r--  2.0 unx    24266 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/schema_utils_test_cases.py
+-rw-rw-r--  2.0 unx     1415 b- defN 22-Jun-29 07:26 tensorflow_transform/tf_metadata/test_common.py
+-rw-rw-r--  2.0 unx    11420 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    12257 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        1 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/namespace_packages.txt
+-rw-rw-r--  2.0 unx       21 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    10715 b- defN 22-Jun-29 07:26 tensorflow_transform-1.9.0.dist-info/RECORD
+106 files, 1930117 bytes uncompressed, 419364 bytes compressed:  78.3%
```

## zipnote {}

```diff
@@ -294,26 +294,26 @@
 
 Filename: tensorflow_transform/tf_metadata/schema_utils_test_cases.py
 Comment: 
 
 Filename: tensorflow_transform/tf_metadata/test_common.py
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/LICENSE
+Filename: tensorflow_transform-1.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/METADATA
+Filename: tensorflow_transform-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/WHEEL
+Filename: tensorflow_transform-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/namespace_packages.txt
+Filename: tensorflow_transform-1.9.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/top_level.txt
+Filename: tensorflow_transform-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: tensorflow_transform-1.8.0.dist-info/RECORD
+Filename: tensorflow_transform-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensorflow_transform/analyzers.py

```diff
@@ -301,17 +301,14 @@
     else:
       return [
           self._fn((sub_accumulator, batch_value), axis=0)
           for sub_accumulator, batch_value in zip(accumulator, batch_values)
       ]
 
   def merge_accumulators(self, accumulators):
-    # TODO(b/422923883): Operate in place on accumulators[0] or batch values
-    # internally for vectorization benefits after AccumulateFn is in use.
-
     # If the first subaccumulator is default, then the accumulator is default
     # and can be discarded.
     non_default_accumulators = [
         accumulator for accumulator in accumulators
         if not self._is_default_sub_accumulator(accumulator[0])
     ]
     if non_default_accumulators:
@@ -347,14 +344,20 @@
   # When reducing over batch dimensions, with known shape, the result will be
   # the same shape as the input, but without the batch.
   if x.shape.rank is not None:
     return x.shape.as_list()[1:]
   return (None,)
 
 
+def _get_elementwise_per_key_output_shape(
+    x: tf.Tensor, key: Optional[tf.Tensor]) -> Optional[Tuple[int]]:
+  shape = x.get_shape() if key is None else x.get_shape()[1:]
+  return tuple(shape) if shape.is_fully_defined() else None
+
+
 # TODO(b/112414577): Go back to accepting only a single input.
 # Currently we accept multiple inputs so that we can implement min and max
 # with a single combiner. Once this is done, add a return pytype as well.
 def _numeric_combine(inputs: List[tf.Tensor],
                      fn: Callable[[np.ndarray], np.ndarray],
                      default_accumulator_value: Union[float, int],
                      reduce_instance_dims: bool = True,
@@ -400,29 +403,28 @@
     output_dtypes = [x.dtype for x in inputs]
   if reduce_instance_dims:
     # If reducing over all dimensions, result is scalar.
     output_shapes = [() for _ in inputs]
   else:
     # Reducing over batch dimensions.
     output_shapes = [
-        (tuple(x.get_shape()) if x.get_shape().is_fully_defined() else None)
-        for x in inputs
+        _get_elementwise_per_key_output_shape(x, key) for x in inputs
     ]
   combiner = NumPyCombiner(fn, default_accumulator_value,
                            [dtype.as_numpy_dtype for dtype in output_dtypes],
                            output_shapes)
   if key is None:
     return _apply_cacheable_combiner(combiner, *inputs)
 
   if key_vocabulary_filename is None:
     return _apply_cacheable_combiner_per_key(combiner, key, *inputs)
 
   return _apply_cacheable_combiner_per_key_large(
-      combiner, _maybe_get_per_key_vocab_filename(key_vocabulary_filename),
-      key, *inputs)
+      combiner, _maybe_get_per_key_vocab_filename(key_vocabulary_filename), key,
+      *inputs)
 
 
 @common.log_api_use(common.ANALYZER_COLLECTION)
 def min(  # pylint: disable=redefined-builtin
     x: common_types.TensorType,
     reduce_instance_dims: bool = True,
     name: Optional[str] = None) -> tf.Tensor:
@@ -564,16 +566,18 @@
 
   Raises:
     TypeError: If the type of `x` is not supported.
   """
   if key is None:
     raise ValueError('A key is required for _min_and_max_per_key')
 
-  if not reduce_instance_dims:
-    raise NotImplementedError('Per-key elementwise reduction not supported')
+  if not reduce_instance_dims and isinstance(
+      x, (tf.SparseTensor, tf.RaggedTensor)):
+    raise NotImplementedError(
+        'Per-key elementwise reduction of Composite Tensors not supported ')
 
   with tf.compat.v1.name_scope(name, 'min_and_max_per_key'):
     output_dtype = x.dtype
     if (not reduce_instance_dims and
         isinstance(x,
                    (tf.SparseTensor, tf.RaggedTensor)) and x.dtype.is_floating):
       combine_fn = np.nanmax
@@ -581,15 +585,16 @@
                                    -output_dtype.max)
     else:
       combine_fn = np.max
       default_accumulator_value = (-np.inf if x.dtype.is_floating else
                                    -output_dtype.max)
 
     key_vocab, x_batch_minus_min, x_batch_max = (
-        tf_utils.reduce_batch_minus_min_and_max_per_key(x, key))
+        tf_utils.reduce_batch_minus_min_and_max_per_key(x, key,
+                                                        reduce_instance_dims))
 
     key_values = _numeric_combine(  # pylint: disable=unbalanced-tuple-unpacking
         inputs=[x_batch_minus_min, x_batch_max],
         fn=combine_fn,
         default_accumulator_value=default_accumulator_value,
         reduce_instance_dims=reduce_instance_dims,
         key=key_vocab,
@@ -1097,24 +1102,26 @@
     output_dtype = _FLOAT_OUTPUT_DTYPE_MAP.get(x.dtype)
     if output_dtype is None:
       raise TypeError('Tensor type %r is not supported' % x.dtype)
 
   if key is None:
     raise ValueError('A non-None key is required for _mean_and_var_per_key')
 
-  if not reduce_instance_dims:
-    raise NotImplementedError('Per-key elementwise reduction not supported')
+  if not reduce_instance_dims and isinstance(
+      x, (tf.SparseTensor, tf.RaggedTensor)):
+    raise NotImplementedError(
+        'Per-key elementwise reduction of Composite Tensors not supported ')
 
   with tf.compat.v1.name_scope('mean_and_var_per_key'):
     x = tf.cast(x, output_dtype)
 
     key_vocab, key_counts, key_means, key_variances = (
         tf_utils.reduce_batch_count_mean_and_var_per_key(
             x, key, reduce_instance_dims=reduce_instance_dims))
-    output_shape = ()
+    output_shape = () if reduce_instance_dims else x.get_shape()[1:]
 
     combine_inputs = _WeightedMeanAndVarAccumulator(
         count=key_counts,
         mean=key_means,
         variance=key_variances,
         weight=tf.zeros_like(key_means, tf.float32))
 
@@ -2418,15 +2425,14 @@
     return [
         sum_product + batch_cross_terms, sum_vectors + batch_sum,
         count + batch_count
     ]
 
   def merge_accumulators(self, accumulators):
     """Sums values in each accumulator entry."""
-    # TODO(b/215378946): Consider updating accumulators[0] in place.
     products, vectors, counts = zip(*accumulators)
     return [
         np.sum(products, axis=0),
         np.sum(vectors, axis=0),
         np.sum(counts, axis=0)
     ]
```

## tensorflow_transform/annotators.py

```diff
@@ -23,16 +23,21 @@
 
 import tensorflow as tf
 from tensorflow_transform.graph_context import TFGraphContext
 
 # pylint: disable=g-direct-tensorflow-import
 from tensorflow.python.framework import func_graph
 from tensorflow.python.framework import ops
-from tensorflow.python.training.tracking import base
-# pylint: enable=g-direct-tensorflow-import
+# pylint: disable=g-import-not-at-top
+try:
+  # Moved in TensorFlow 2.10.
+  from tensorflow.python.trackable import base
+except ImportError:
+  from tensorflow.python.training.tracking import base
+# pylint: enable=g-direct-tensorflow-import, g-import-not-at-top
 
 __all__ = ['annotate_asset', 'make_and_track_object']
 
 _ASSET_KEY_COLLECTION = 'tft_asset_key_collection'
 _ASSET_FILENAME_COLLECTION = 'tft_asset_filename_collection'
 # Thread-Hostile
 _OBJECT_TRACKER = None
```

## tensorflow_transform/annotators_test.py

```diff
@@ -13,15 +13,21 @@
 # limitations under the License.
 """Tests for tensorflow_transform.annotators."""
 
 import tensorflow as tf
 from tensorflow_transform import annotators
 from tensorflow_transform import test_case
 
-from tensorflow.python.training.tracking import base  # pylint: disable=g-direct-tensorflow-import
+# pylint: disable=g-direct-tensorflow-import, g-import-not-at-top
+try:
+  # Moved in TensorFlow 2.10.
+  from tensorflow.python.trackable import base
+except ImportError:
+  from tensorflow.python.training.tracking import base
+# pylint: enable=g-direct-tensorflow-import, g-import-not-at-top
 
 
 class AnnotatorsTest(test_case.TransformTestCase):
 
   @test_case.named_parameters(
       dict(testcase_name='tf_compat_v1', use_tf_compat_v1=True),
       dict(testcase_name='tf2', use_tf_compat_v1=False))
```

## tensorflow_transform/info_theory.py

```diff
@@ -80,15 +80,15 @@
     x_i: The frequency of x=i.
     y_j: The frequency of y=j.
     n: The total # observations
 
   Returns:
     Mutual information for the cell x=i, y=j.
   """
-  if n_ij == 0:
+  if n_ij == 0 or x_i == 0 or y_j == 0:
     return 0
   return n_ij * ((log2(n_ij) + log2(n)) -
                  (log2(x_i) + log2(y_j)))
 
 
 def _hypergeometric_pmf(n, x_i, y_j):
   """Probablity for expectation computation under hypergeometric distribution.
```

## tensorflow_transform/info_theory_test.py

```diff
@@ -133,14 +133,35 @@
       dict(
           testcase_name='zero_mi',
           cell_count=4,
           row_count=8,
           col_count=8,
           total_count=16,
           expected_mi=0),
+      dict(
+          testcase_name='invalid_input_zero_cell_count',
+          cell_count=4,
+          row_count=0,
+          col_count=8,
+          total_count=8,
+          expected_mi=0),
+      dict(
+          testcase_name='invalid_input_zero_row_count',
+          cell_count=4,
+          row_count=0,
+          col_count=8,
+          total_count=8,
+          expected_mi=0),
+      dict(
+          testcase_name='invalid_input_zero_col_count',
+          cell_count=4,
+          row_count=8,
+          col_count=0,
+          total_count=8,
+          expected_mi=0),
   )
   def test_mutual_information(self, cell_count, row_count, col_count,
                               total_count, expected_mi):
     per_cell_mi = info_theory.calculate_partial_mutual_information(
         cell_count, row_count, col_count, total_count)
     self.assertNear(per_cell_mi, expected_mi, EPSILON)
```

## tensorflow_transform/mappers.py

```diff
@@ -363,32 +363,36 @@
 
   x = tf.cast(x, tf.float32)
   if key is None:
     min_x_value, max_x_value = analyzers._min_and_max(  # pylint: disable=protected-access
         x,
         reduce_instance_dims=not elementwise)
   else:
-    if elementwise:
-      raise NotImplementedError('Per-key elementwise reduction not supported')
+    if elementwise and isinstance(x, (tf.SparseTensor, tf.RaggedTensor)):
+      raise NotImplementedError(
+          'Per-key elementwise reduction of Composite Tensors not supported')
     key_values = analyzers._min_and_max_per_key(  # pylint: disable=protected-access
         x,
         key,
-        reduce_instance_dims=True,
+        reduce_instance_dims=not elementwise,
         key_vocabulary_filename=key_vocabulary_filename)
     if key_vocabulary_filename is None:
       key_vocab, min_x_value, max_x_value = key_values
       # Missing keys will translate to 0 for both min and max which will be
       # ignored below in the tf.where.
       min_x_value, max_x_value = tf_utils.map_per_key_reductions(
-          (min_x_value, max_x_value), key, key_vocab, x)
+          (min_x_value, max_x_value), key, key_vocab, x, not elementwise)
     else:
+      if elementwise:
+        raise NotImplementedError(
+            'Elementwise scaling does not support key_vocabulary_filename')
       minus_min_max_for_key = tf_utils.apply_per_key_vocabulary(
           key_values, key, target_ndims=x.get_shape().ndims)
-      min_x_value, max_x_value = (
-          -minus_min_max_for_key[:, 0], minus_min_max_for_key[:, 1])
+      min_x_value, max_x_value = (-minus_min_max_for_key[:, 0],
+                                  minus_min_max_for_key[:, 1])
 
   compose_result_fn = _make_composite_tensor_wrapper_if_composite(x)
   x_values = tf_utils.get_values(x)
   if isinstance(x, tf.SparseTensor):
     if elementwise:
       min_x_value = tf.gather_nd(
           tf.broadcast_to(min_x_value, x.dense_shape), x.indices)
@@ -611,28 +615,35 @@
   # x_mean will be float16, float32, or float64, depending on type of x
   if key is None:
     x_mean, x_var = analyzers._mean_and_var(  # pylint: disable=protected-access
         x,
         reduce_instance_dims=not elementwise,
         output_dtype=output_dtype)
   else:
-    if elementwise:
-      raise NotImplementedError('Per-key elementwise reduction not supported')
+    if elementwise and isinstance(x, (tf.SparseTensor, tf.RaggedTensor)):
+      raise NotImplementedError(
+          'Per-key elementwise reduction of Composite Tensors not supported')
 
     mean_and_var_per_key_result = analyzers._mean_and_var_per_key(  # pylint: disable=protected-access
-        x, key, key_vocabulary_filename=key_vocabulary_filename,
+        x,
+        key,
+        reduce_instance_dims=not elementwise,
+        key_vocabulary_filename=key_vocabulary_filename,
         output_dtype=output_dtype)
 
     if key_vocabulary_filename is None:
       # Missing keys will translate to 0 for both mean and var which will be
       # ignored below in the tf.where.
       key_vocab, key_means, key_vars = mean_and_var_per_key_result
-      x_mean, x_var = tf_utils.map_per_key_reductions((key_means, key_vars),
-                                                      key, key_vocab, x)
+      x_mean, x_var = tf_utils.map_per_key_reductions(
+          (key_means, key_vars), key, key_vocab, x, not elementwise)
     else:
+      if elementwise:
+        raise NotImplementedError(
+            'Elementwise scaling does not support key_vocabulary_filename')
       mean_var_for_key = tf_utils.apply_per_key_vocabulary(
           mean_and_var_per_key_result, key, target_ndims=x.get_shape().ndims)
       x_mean, x_var = (mean_var_for_key[:, 0], mean_var_for_key[:, 1])
 
   compose_result_fn = _make_composite_tensor_wrapper_if_composite(x)
   x_values = tf_utils.get_values(x)
```

## tensorflow_transform/tf_utils.py

```diff
@@ -1230,26 +1230,28 @@
   if target_ndims is None or target_ndims <= tensor.get_shape().ndims:
     return tensor
   for _ in range(target_ndims - tensor.get_shape().ndims):
     tensor = tf.expand_dims(tensor, -1)
   return tensor
 
 
-def map_per_key_reductions(
-    tensors_to_map: Tuple[tf.Tensor, ...], key: common_types.TensorType,
-    key_vocab: tf.Tensor,
-    original_input: common_types.TensorType) -> Tuple[tf.Tensor, ...]:
+def map_per_key_reductions(tensors_to_map: Tuple[tf.Tensor, ...],
+                           key: common_types.TensorType, key_vocab: tf.Tensor,
+                           original_input: common_types.TensorType,
+                           reduce_instance_dims: bool) -> Tuple[tf.Tensor, ...]:
   """Rearrange the reduced per-key result to correspond to the original keys.
 
   Args:
     tensors_to_map: A tuple of 1-D `Tensor`s that are same shape as key_vocab,
-        to be mapped to respective key.
+      to be mapped to respective key.
     key: A `Tensor` or `CompositeTensor`.
     key_vocab: A 1-D `Tensor`.
     original_input: A `Tensor` or `CompositeTensor`.
+    reduce_instance_dims: A `bool`. True if tensors_to_map are reduced in
+      dimension, else False.
 
   Returns:
     A tuple same length as tensors_to_map, of `Tensor`s the same dimension as
     original_input. We are mapping using the key for each original_input,
     but output rank needs to match original_input in the dense case.
     For the sparse case, it is enough for output to match original_input.values.
     Any missing key would result in a mapping to 0.
@@ -1258,25 +1260,30 @@
   _, key = _validate_and_get_dense_value_key_inputs(original_input, key)
   key_indices = lookup_key(key, key_vocab)
 
   ndims = (None if isinstance(original_input,
                               (tf.SparseTensor, tf.RaggedTensor)) else
            original_input.get_shape().ndims)
 
-  # Append a 0 to allow mapping OOVs to it.
-  tensors_to_map = [tf.concat([t, [0]], axis=0) for t in tensors_to_map]
+  # Append 0s to allow mapping OOVs to it.
+  tensors_to_map = [
+      tf.concat([t, tf.expand_dims(tf.zeros_like(t[0]), 0)], axis=0)
+      for t in tensors_to_map
+  ]
 
   # Replace `-1`s due to OOV with size of key_vocab.
   adjusted_indices = tf.where(
       key_indices >= 0, key_indices,
       tf.cast(
           tf.fill(tf.shape(key_indices), tf.size(key_vocab)), dtype=tf.int64))
-
-  mapped_result = [_align_dims(tf.gather(t, adjusted_indices, axis=-1), ndims)
-                   for t in tensors_to_map]
+  axis = -1 if reduce_instance_dims else 0
+  mapped_result = [
+      _align_dims(tf.gather(t, adjusted_indices, axis=axis), ndims)
+      for t in tensors_to_map
+  ]
 
   return tuple(mapped_result)
 
 
 def reduce_batch_count_mean_and_var_per_key(
     x: common_types.TensorType, key: common_types.TensorType,
     reduce_instance_dims: bool
@@ -1536,39 +1543,51 @@
     # TODO(iindyk): switch to `tf.math.negative` when analyzer cache will get
     # invalidated next time.
     return (tf.reduce_max(input_tensor=0 - x, axis=0), x_batch_max)
 
 
 def reduce_batch_minus_min_and_max_per_key(
     x: common_types.TensorType,
-    key: common_types.TensorType) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
+    key: common_types.TensorType,
+    reduce_instance_dims: bool = True
+) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
   """Computes the -min and max of a tensor x.
 
   Args:
     x: A `Tensor` or `CompositeTensor`.
     key: A `Tensor` or `CompositeTensor`.
         Must meet one of the following conditions:
         1. Both x and key are dense,
         2. Both x and key are composite and `key` must exactly match `x` in
         everything except values,
         3. The axis=1 index of each element of sparse x matches its index of
         dense key.
+    reduce_instance_dims: A bool indicating whether this should collapse the
+      batch and instance dimensions to arrive at a single scalar output, or only
+      collapse the batch dimension and outputs a vector of the same shape as the
+      input.
   Returns:
     A 3-tuple containing the `Tensor`s (key_vocab, min_per_key, max_per_key).
   """
   if x.dtype == tf.uint8 or x.dtype == tf.uint16:
     x = tf.cast(x, tf.int32)
 
   elif x.dtype == tf.uint32 or x.dtype == tf.uint64:
     raise TypeError('Tensor type %r is not supported' % x.dtype)
 
+  if not reduce_instance_dims and isinstance(
+      x, (tf.SparseTensor, tf.RaggedTensor)):
+    raise NotImplementedError(
+        'Elementwise reduction of composite tensors is not supported'
+    )
+
   x, key = _validate_and_get_dense_value_key_inputs(x, key)
 
   def get_batch_max_per_key(tensor, key_uniques):  # pylint: disable=missing-docstring
-    if tensor.get_shape().ndims < 2:
+    if not reduce_instance_dims or tensor.get_shape().ndims < 2:
       row_maxes = tensor
     else:
       row_maxes = tf.reduce_max(
           tensor, axis=tf.range(1, tensor.get_shape().ndims))
     return tf.math.unsorted_segment_max(row_maxes, key_uniques.idx,
                                         tf.size(input=key_uniques.y))
```

## tensorflow_transform/tf_utils_test.py

```diff
@@ -1623,45 +1623,73 @@
           dict(
               testcase_name='sparse',
               x=tf.compat.v1.SparseTensorValue(
                   indices=[[0, 0], [1, 1], [2, 2], [3, 1]],
                   values=[3, 2, -1, 3],
                   dense_shape=[4, 5]),
               key=['a', 'a', 'a', 'b'],
+              reduce_instance_dims=True,
               expected_key_vocab=[b'a', b'b'],
               expected_x_minus_min=[1, -3],
               expected_x_max=[3, 3],
               input_signature=[
                   tf.SparseTensorSpec([None, None], tf.int64),
                   tf.TensorSpec([None], tf.string)
               ]),
           dict(
               testcase_name='float',
               x=[[1], [5], [2], [3]],
               key=['a', 'a', 'a', 'b'],
+              reduce_instance_dims=True,
               expected_key_vocab=[b'a', b'b'],
               expected_x_minus_min=[-1, -3],
               expected_x_max=[5, 3],
               input_signature=[
                   tf.TensorSpec([None, None], tf.float32),
                   tf.TensorSpec([None], tf.string)
               ]),
           dict(
+              testcase_name='float_elementwise',
+              x=[[1], [5], [2], [3]],
+              key=['a', 'a', 'a', 'b'],
+              reduce_instance_dims=False,
+              expected_key_vocab=[b'a', b'b'],
+              expected_x_minus_min=[[-1], [-3]],
+              expected_x_max=[[5], [3]],
+              input_signature=[
+                  tf.TensorSpec([None, None], tf.float32),
+                  tf.TensorSpec([None], tf.string)
+              ]),
+          dict(
               testcase_name='float3dims',
               x=[[[1, 5], [1, 1]], [[5, 1], [5, 5]], [[2, 2], [2, 5]],
                  [[3, -3], [3, 3]]],
               key=['a', 'a', 'a', 'b'],
+              reduce_instance_dims=True,
               expected_key_vocab=[b'a', b'b'],
               expected_x_minus_min=[-1, 3],
               expected_x_max=[5, 3],
               input_signature=[
                   tf.TensorSpec([None, None, None], tf.float32),
                   tf.TensorSpec([None], tf.string)
               ]),
           dict(
+              testcase_name='float3dims_elementwise',
+              x=[[[1, 5], [1, 1]], [[5, 1], [5, 5]], [[2, 2], [2, 5]],
+                 [[3, -3], [3, 3]]],
+              key=['a', 'a', 'a', 'b'],
+              reduce_instance_dims=False,
+              expected_key_vocab=[b'a', b'b'],
+              expected_x_minus_min=[[[-1, -1], [-1, -1]], [[-3, 3], [-3, -3]]],
+              expected_x_max=[[[5, 5], [5, 5]], [[3, -3], [3, 3]]],
+              input_signature=[
+                  tf.TensorSpec([None, None, None], tf.float32),
+                  tf.TensorSpec([None], tf.string)
+              ]),
+          dict(
               testcase_name='ragged',
               x=tf.compat.v1.ragged.RaggedTensorValue(
                   values=tf.compat.v1.ragged.RaggedTensorValue(
                       values=tf.compat.v1.ragged.RaggedTensorValue(
                           values=np.array([3., 2., 3., 4., 5.], np.float32),
                           row_splits=np.array([0, 2, 3, 4, 5])),
                       row_splits=np.array([0, 2, 3, 4])),
@@ -1669,29 +1697,31 @@
               key=tf.compat.v1.ragged.RaggedTensorValue(
                   values=tf.compat.v1.ragged.RaggedTensorValue(
                       values=tf.compat.v1.ragged.RaggedTensorValue(
                           values=np.array(['a', 'a', 'b', 'a', 'b']),
                           row_splits=np.array([0, 2, 3, 4, 5])),
                       row_splits=np.array([0, 2, 3, 4])),
                   row_splits=np.array([0, 2, 3])),
+              reduce_instance_dims=True,
               expected_key_vocab=[b'a', b'b'],
               expected_x_minus_min=[-2., -3.],
               expected_x_max=[4., 5.],
               input_signature=[
                   tf.RaggedTensorSpec([None, None, None, None], tf.float32),
                   tf.RaggedTensorSpec([None, None, None, None], tf.string)
               ]),
       ]))
   def test_reduce_batch_minus_min_and_max_per_key(
-      self, x, key, expected_key_vocab, expected_x_minus_min, expected_x_max,
-      input_signature, function_handler):
+      self, x, key, reduce_instance_dims, expected_key_vocab,
+      expected_x_minus_min, expected_x_max, input_signature, function_handler):
 
     @function_handler(input_signature=input_signature)
     def _reduce_batch_minus_min_and_max_per_key(x, key):
-      return tf_utils.reduce_batch_minus_min_and_max_per_key(x, key)
+      return tf_utils.reduce_batch_minus_min_and_max_per_key(
+          x, key, reduce_instance_dims=reduce_instance_dims)
 
     key_vocab, x_minus_min, x_max = _reduce_batch_minus_min_and_max_per_key(
         x, key)
 
     self.assertAllEqual(key_vocab, expected_key_vocab)
     self.assertAllEqual(x_minus_min, expected_x_minus_min)
     self.assertAllEqual(x_max, expected_x_max)
@@ -1908,48 +1938,52 @@
   @test_case.named_parameters(
       dict(
           testcase_name='dense_tensor',
           key=['b', 'a', 'b'],
           key_vocab=['a', 'b'],
           reductions=([1, 2], [3, 4]),
           x=[5, 6, 7],
+          reduce_instance_dims=True,
           expected_results=([2, 1, 2], [4, 3, 4])),
       dict(
           testcase_name='sparse_tensor_dense_key',
           key=['b', 'a', 'b'],
           key_vocab=['a', 'b'],
           reductions=([1, 2], [3, 4]),
           x=tf.compat.v1.SparseTensorValue(
               indices=[[0, 0], [1, 2], [2, 2], [2, 3]],
               values=[3, 2, -1, 3],
               dense_shape=[3, 5]),
+          reduce_instance_dims=True,
           expected_results=([2, 1, 2, 2], [4, 3, 4, 4])),
       dict(
           testcase_name='sparse_tensor_sparse_key',
           key=tf.compat.v1.SparseTensorValue(
               indices=[[0, 0], [1, 2], [2, 2], [2, 3]],
               values=['b', 'a', 'b', 'b'],
               dense_shape=[3, 5]),
           key_vocab=['a', 'b'],
           reductions=([1, 2], [3, 4]),
           x=tf.compat.v1.SparseTensorValue(
               indices=[[0, 0], [1, 2], [2, 2], [2, 3]],
               values=[3, 2, -1, 3],
               dense_shape=[3, 5]),
+          reduce_instance_dims=True,
           expected_results=([2, 1, 2, 2], [4, 3, 4, 4])),
       dict(
           testcase_name='ragged_tensor_dense_key',
           key=['a', 'b', 'a'],
           key_vocab=['a', 'b'],
           reductions=([1, 2], [3, 4]),
           x=tf.compat.v1.ragged.RaggedTensorValue(
               values=tf.compat.v1.ragged.RaggedTensorValue(
                   values=np.array([1.2, 1., 1.2, 1.]),
                   row_splits=np.array([0, 2, 4])),
               row_splits=np.array([0, 1, 2, 2])),
+          reduce_instance_dims=True,
           expected_results=([1, 1, 2, 2], [3, 3, 4, 4])),
       dict(
           testcase_name='ragged_tensor_ragged_key',
           key=tf.compat.v1.ragged.RaggedTensorValue(
               values=tf.compat.v1.ragged.RaggedTensorValue(
                   values=np.array(['a', 'b', 'b', 'a']),
                   row_splits=np.array([0, 2, 4])),
@@ -1957,32 +1991,53 @@
           key_vocab=['a', 'b'],
           reductions=([1, 2], [3, 4]),
           x=tf.compat.v1.ragged.RaggedTensorValue(
               values=tf.compat.v1.ragged.RaggedTensorValue(
                   values=np.array([1.2, 1., 1.2, 1.]),
                   row_splits=np.array([0, 2, 4])),
               row_splits=np.array([0, 2])),
+          reduce_instance_dims=True,
           expected_results=([1, 2, 2, 1], [3, 4, 4, 3])),
       dict(
           testcase_name='missing_key',
           key=['b', 'a', 'c'],
           key_vocab=['z', 'a', 'b'],
           reductions=([-77, 1, 2], [-99, 3, 4]),
           x=[5, 6, 7],
+          reduce_instance_dims=True,
           expected_results=([2, 1, 0], [4, 3, 0])),
+      dict(
+          testcase_name='_dense_tensor_2d_elementwise',
+          key=['a'],
+          key_vocab=['a', 'b'],
+          reductions=([[1, 5], [-2, 0]], [[5, 9], [2, 4]]),
+          x=[[4, 8]],
+          reduce_instance_dims=False,
+          expected_results=([[1, 5]], [[5, 9]])),
+      dict(
+          testcase_name='_dense_tensor_3d_elementwise',
+          key=['a'],
+          key_vocab=['a', 'b'],
+          reductions=([[[1, 1], [1, 1]], [[3, -3], [3, 3]]], [[[5, 5], [5, 5]],
+                                                              [[3, -3], [3,
+                                                                         3]]]),
+          x=[[[1, 5], [1, 1]]],
+          reduce_instance_dims=False,
+          expected_results=([[[1, 1], [1, 1]]], [[[5, 5], [5, 5]]])),
   )
-  def test_map_per_key_reductions(
-      self, key, key_vocab, reductions, x, expected_results):
+  def test_map_per_key_reductions(self, key, key_vocab, reductions, x,
+                                  reduce_instance_dims, expected_results):
     with tf.compat.v1.Graph().as_default():
       key = _value_to_tensor(key)
       key_vocab = tf.constant(key_vocab)
       reductions = tuple([tf.constant(t) for t in reductions])
       x = _value_to_tensor(x)
       expected_results = tuple(tf.constant(t) for t in expected_results)
-      results = tf_utils.map_per_key_reductions(reductions, key, key_vocab, x)
+      results = tf_utils.map_per_key_reductions(reductions, key, key_vocab, x,
+                                                reduce_instance_dims)
       with tf.compat.v1.Session() as sess:
         sess.run(tf.compat.v1.tables_initializer())
         output = sess.run(results)
         for result, expected_result in zip(output, expected_results):
           self.assertAllEqual(result, expected_result)
 
   @test_case.named_parameters(test_case.cross_with_function_handlers([
```

## tensorflow_transform/version.py

```diff
@@ -10,8 +10,8 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Contains the version string of TF.Transform."""
 
 # Note that setup.py uses this version.
-__version__ = '1.8.0'
+__version__ = '1.9.0'
```

## tensorflow_transform/beam/cached_impl_test.py

```diff
@@ -1100,18 +1100,21 @@
   def test_cached_ptransform_analyzer(self, use_tf_compat_v1):
     if not use_tf_compat_v1:
       tft_unit.skip_if_not_tf2('Tensorflow 2.x required.')
 
     class _AnalyzerMakeAccumulators(beam.PTransform):
 
       def expand(self, pcoll):
+        # TODO(b/237367328): Use sum directly when beam>=2.40 allows it.
+        def _sum(x):
+          return sum(x)
         input_sum = pcoll | beam.FlatMap(
-            sum) | 'ReduceSum' >> beam.CombineGlobally(sum)
+            _sum) | 'ReduceSum' >> beam.CombineGlobally(_sum)
         size = pcoll | beam.Map(
-            np.size) | 'ReduceCount' >> beam.CombineGlobally(sum)
+            np.size) | 'ReduceCount' >> beam.CombineGlobally(_sum)
 
         return (pcoll.pipeline
                 | beam.Create([None])
                 | beam.Map(
                     lambda _, a, b: (a, b),  # pyformat: disable
                     beam.pvalue.AsSingleton(input_sum),
                     beam.pvalue.AsSingleton(size)))
```

## tensorflow_transform/beam/common.py

```diff
@@ -186,16 +186,19 @@
 
     # TODO(zoyahav): Consider extracting a single PCollection before passing to
     # ptransform if len(inputs) == 1.
     if tag is None:
       tagged_label = operation.label
     else:
       tagged_label = '{label}[{tag}]'.format(label=operation.label, tag=tag)
-    outputs = ((inputs or beam.pvalue.PBegin(self._extra_args.pipeline))
-               | tagged_label >> ptransform(operation, self._extra_args))
+    try:
+      outputs = ((inputs or beam.pvalue.PBegin(self._extra_args.pipeline))
+                 | tagged_label >> ptransform(operation, self._extra_args))
+    except Exception as e:
+      raise RuntimeError('Failed to apply: {}'.format(tagged_label)) from e
 
     if isinstance(outputs, beam.pvalue.PCollection):
       return (outputs,)
     else:
       return outputs
 
   def validate_value(self, value):
```

## tensorflow_transform/beam/impl_test.py

```diff
@@ -784,21 +784,22 @@
         'x_scaled': tf.io.FixedLenFeature([], tf.float32),
         'y_scaled': tf.io.FixedLenFeature([], tf.float32)
     })
     self.assertAnalyzeAndTransformResults(input_data, input_metadata,
                                           preprocessing_fn, expected_data,
                                           expected_metadata)
 
-  def testScaleUnitIntervalPerKey(self):
+  @tft_unit.parameters((True,), (False,))
+  def testScaleUnitIntervalPerKey(self, elementwise):
 
     def preprocessing_fn(inputs):
       outputs = {}
       stacked_input = tf.stack([inputs['x'], inputs['y']], axis=1)
       result = tft.scale_to_0_1_per_key(
-          stacked_input, inputs['key'], elementwise=False)
+          stacked_input, inputs['key'], elementwise)
       outputs['x_scaled'], outputs['y_scaled'] = tf.unstack(result, axis=1)
       return outputs
 
     input_data = [{
         'x': 4,
         'y': 5,
         'key': 'a'
@@ -824,33 +825,54 @@
         'key': 'b'
     }]
     input_metadata = tft.DatasetMetadata.from_feature_spec({
         'x': tf.io.FixedLenFeature([], tf.float32),
         'y': tf.io.FixedLenFeature([], tf.float32),
         'key': tf.io.FixedLenFeature([], tf.string)
     })
-    expected_data = [{
-        'x_scaled': 0.6,
-        'y_scaled': 0.8
-    }, {
-        'x_scaled': 0.0,
-        'y_scaled': 0.2
-    }, {
-        'x_scaled': 0.8,
-        'y_scaled': 1.0
-    }, {
-        'x_scaled': 0.2,
-        'y_scaled': 0.4
-    }, {
-        'x_scaled': 1.0,
-        'y_scaled': 0.0
-    }, {
-        'x_scaled': 0.6,
-        'y_scaled': 0.5
-    }]
+    if elementwise:
+      expected_data = [{
+          'x_scaled': 0.75,
+          'y_scaled': 0.75
+      }, {
+          'x_scaled': 0.0,
+          'y_scaled': 0.0
+      }, {
+          'x_scaled': 1.0,
+          'y_scaled': 1.0
+      }, {
+          'x_scaled': 0.25,
+          'y_scaled': 0.25
+      }, {
+          'x_scaled': 1.0,
+          'y_scaled': 0.0
+      }, {
+          'x_scaled': 0.0,
+          'y_scaled': 1.0
+      }]
+    else:
+      expected_data = [{
+          'x_scaled': 0.6,
+          'y_scaled': 0.8
+      }, {
+          'x_scaled': 0.0,
+          'y_scaled': 0.2
+      }, {
+          'x_scaled': 0.8,
+          'y_scaled': 1.0
+      }, {
+          'x_scaled': 0.2,
+          'y_scaled': 0.4
+      }, {
+          'x_scaled': 1.0,
+          'y_scaled': 0.0
+      }, {
+          'x_scaled': 0.6,
+          'y_scaled': 0.5
+      }]
     expected_metadata = tft.DatasetMetadata.from_feature_spec({
         'x_scaled': tf.io.FixedLenFeature([], tf.float32),
         'y_scaled': tf.io.FixedLenFeature([], tf.float32)
     })
     self.assertAnalyzeAndTransformResults(input_data, input_metadata,
                                           preprocessing_fn, expected_data,
                                           expected_metadata)
@@ -915,31 +937,41 @@
         'y_scaled': tf.io.FixedLenFeature([], tf.float32)
     })
     self.assertAnalyzeAndTransformResults(input_data, input_metadata,
                                           preprocessing_fn, expected_data,
                                           expected_metadata)
 
   @tft_unit.named_parameters(
-      dict(testcase_name='_empty_filename',
-           key_vocabulary_filename=''),
-      dict(testcase_name='_nonempty_filename',
-           key_vocabulary_filename='per_key'),
-      dict(testcase_name='_none_filename',
-           key_vocabulary_filename=None)
-  )
-  def testScaleMinMaxPerKey(self, key_vocabulary_filename):
+      dict(
+          testcase_name='_empty_filename',
+          elementwise=False,
+          key_vocabulary_filename=''),
+      dict(
+          testcase_name='_nonempty_filename',
+          elementwise=False,
+          key_vocabulary_filename='per_key'),
+      dict(
+          testcase_name='_none_filename',
+          elementwise=False,
+          key_vocabulary_filename=None),
+      dict(
+          testcase_name='_elementwise_none_filename',
+          elementwise=True,
+          key_vocabulary_filename=None))
+  def testScaleMinMaxPerKey(self, elementwise, key_vocabulary_filename):
+
     def preprocessing_fn(inputs):
       outputs = {}
       stacked_input = tf.stack([inputs['x'], inputs['y']], axis=1)
       result = tft.scale_by_min_max_per_key(
           stacked_input,
           inputs['key'],
           output_min=-1,
           output_max=1,
-          elementwise=False,
+          elementwise=elementwise,
           key_vocabulary_filename=key_vocabulary_filename)
       outputs['x_scaled'], outputs['y_scaled'] = tf.unstack(result, axis=1)
       return outputs
 
     input_data = [{
         'x': 4,
         'y': 8,
@@ -966,45 +998,69 @@
         'key': 'b'
     }]
     input_metadata = tft.DatasetMetadata.from_feature_spec({
         'x': tf.io.FixedLenFeature([], tf.float32),
         'y': tf.io.FixedLenFeature([], tf.float32),
         'key': tf.io.FixedLenFeature([], tf.string)
     })
-
-    expected_data = [{
-        'x_scaled': -0.25,
-        'y_scaled': 0.75
-    }, {
-        'x_scaled': -1.0,
-        'y_scaled': 0.0
-    }, {
-        'x_scaled': 0.0,
-        'y_scaled': 1.0
-    }, {
-        'x_scaled': -0.75,
-        'y_scaled': 0.25
-    }, {
-        'x_scaled': -1.0,
-        'y_scaled': 0.0
-    }, {
-        'x_scaled': 0.0,
-        'y_scaled': 1.0
-    }]
+    if elementwise:
+      expected_data = [{
+          'x_scaled': 0.5,
+          'y_scaled': 0.5
+      }, {
+          'x_scaled': -1.0,
+          'y_scaled': -1.0
+      }, {
+          'x_scaled': 1.0,
+          'y_scaled': 1.0
+      }, {
+          'x_scaled': -0.5,
+          'y_scaled': -0.5
+      }, {
+          'x_scaled': -1.0,
+          'y_scaled': -1.0
+      }, {
+          'x_scaled': 1.0,
+          'y_scaled': 1.0
+      }]
+    else:
+      expected_data = [{
+          'x_scaled': -0.25,
+          'y_scaled': 0.75
+      }, {
+          'x_scaled': -1.0,
+          'y_scaled': 0.0
+      }, {
+          'x_scaled': 0.0,
+          'y_scaled': 1.0
+      }, {
+          'x_scaled': -0.75,
+          'y_scaled': 0.25
+      }, {
+          'x_scaled': -1.0,
+          'y_scaled': 0.0
+      }, {
+          'x_scaled': 0.0,
+          'y_scaled': 1.0
+      }]
     expected_metadata = tft.DatasetMetadata.from_feature_spec({
         'x_scaled': tf.io.FixedLenFeature([], tf.float32),
         'y_scaled': tf.io.FixedLenFeature([], tf.float32)
     })
     if key_vocabulary_filename:
-      per_key_vocab_contents = {key_vocabulary_filename:
-                                    [(b'a', [-1.0, 9.0]), (b'b', [2.0, 2.0])]}
+      per_key_vocab_contents = {
+          key_vocabulary_filename: [(b'a', [-1.0, 9.0]), (b'b', [2.0, 2.0])]
+      }
     else:
       per_key_vocab_contents = None
     self.assertAnalyzeAndTransformResults(
-        input_data, input_metadata, preprocessing_fn, expected_data,
+        input_data,
+        input_metadata,
+        preprocessing_fn,
+        expected_data,
         expected_metadata,
         expected_vocab_file_contents=per_key_vocab_contents)
 
   def testScalePerKeySparse(self):
     def preprocessing_fn(inputs):
       return {
           'scaled_by_min_max':
@@ -1749,14 +1805,147 @@
         'y_scaled': tf.io.FixedLenFeature([1], tf.float32),
         's_scaled': tf.io.FixedLenFeature([], tf.float32),
     })
     self.assertAnalyzeAndTransformResults(input_data, input_metadata,
                                           preprocessing_fn, expected_data,
                                           expected_metadata)
 
+  @tft_unit.named_parameters(
+      dict(
+          testcase_name='_float',
+          input_data=[
+              {
+                  'x': [-4, 0],
+                  'key': 'a',
+              },
+              {
+                  'x': [10, 0],
+                  'key': 'a',
+              },
+              {
+                  'x': [2, 0],
+                  'key': 'a',
+              },
+              {
+                  'x': [4, 0],
+                  'key': 'a',
+              },
+              {
+                  'x': [1, 0],
+                  'key': 'b',
+              },
+              {
+                  'x': [-1, 0],
+                  'key': 'b',
+              },
+              {
+                  'x': [np.nan, np.nan],
+                  'key': 'b',
+              },
+          ],
+          # Elementwise = True
+          # Mean      [a, b] = [[ 3.0, 0.0], [0.0, 0.0]]
+          # Variance  [a, b] = [[25.0, 0.0], [1.0, 0.0]]
+          # StdDev    [a, b] = [[ 5.0, 0.0], [1.0, 0.0]]
+          expected_data=[
+              {
+                  'x_scaled': [-1.4, 0.0],  # [(-4 - 3) / 5, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [1.4, 0.0]  # [(10 - 3) / 5, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [-0.2, 0.0]  # [(2 - 3) / 5, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [0.2, 0.0],  # [(4 - 3) / 5, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [1.0, 0.0]  # [(1 - 0) / 1, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [-1.0, 0.0]  # [(-1 - 0) / 1, (0 - 0) / 0]
+              },
+              {
+                  'x_scaled': [np.nan, np.nan]
+              },
+          ],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          expected_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x_scaled': tf.io.FixedLenFeature([2], tf.float32),
+          })),
+      dict(
+          testcase_name='float_3dims',
+          input_data=[
+              {
+                  'x': [[-4, -8], [-12, -16]],
+                  'key': 'a',
+              },
+              {
+                  'x': [[10, 20], [30, 40]],
+                  'key': 'a',
+              },
+              {
+                  'x': [[2, 4], [6, 8]],
+                  'key': 'a',
+              },
+              {
+                  'x': [[4, 8], [12, 16]],
+                  'key': 'a',
+              },
+              {
+                  'x': [[1, 2], [3, 4]],
+                  'key': 'b',
+              },
+          ],
+          expected_data=[
+              {
+                  'x_scaled': [[-1.4, -1.4], [-1.4, -1.4]],
+              },
+              {
+                  'x_scaled': [[1.4, 1.4], [1.4, 1.4]],
+              },
+              {
+                  'x_scaled': [[-0.2, -0.2], [-0.2, -0.2]],
+              },
+              {
+                  'x_scaled': [[0.2, 0.2], [0.2, 0.2]],
+              },
+              {
+                  'x_scaled': [[0.0, 0.0], [0.0, 0.0]],
+              },
+          ],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2, 2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          expected_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x_scaled': tf.io.FixedLenFeature([2, 2], tf.float32),
+          })),
+  )
+  def testScaleToZScorePerKeyElementwise(self, input_data, expected_data,
+                                         input_metadata, expected_metadata):
+
+    def preprocessing_fn(inputs):
+      outputs = {}
+      outputs['x_scaled'] = tft.scale_to_z_score_per_key(
+          tf.cast(inputs['x'], tf.float32),
+          key=inputs['key'],
+          elementwise=True,
+          key_vocabulary_filename=None)
+      self.assertEqual(outputs['x_scaled'].dtype, tf.float32)
+      return outputs
+
+    self.assertAnalyzeAndTransformResults(input_data, input_metadata,
+                                          preprocessing_fn, expected_data,
+                                          expected_metadata)
+
   @tft_unit.parameters(
       (tf.int16,),
       (tf.int32,),
       (tf.int64,),
       (tf.float32,),
       (tf.float64,),
   )
@@ -1915,14 +2104,207 @@
     self.assertAnalyzerOutputs(
         input_data,
         input_metadata,
         analyzer_fn,
         expected_outputs,
         desired_batch_size=10)
 
+  def testMeanAndVarPerKeyElementwise(self):
+
+    def analyzer_fn(inputs):
+      key_vocab, mean, var = analyzers._mean_and_var_per_key(
+          inputs['x'], inputs['key'], reduce_instance_dims=False)
+      return {
+          'key_vocab': key_vocab,
+          'mean': mean,
+          'var': tf.round(100 * var) / 100.0
+      }
+
+    input_data = input_data = [{
+        'x': [-4, -1],
+        'key': 'a',
+    }, {
+        'x': [10, 0],
+        'key': 'a',
+    }, {
+        'x': [2, 0],
+        'key': 'a',
+    }, {
+        'x': [4, -1],
+        'key': 'a',
+    }, {
+        'x': [10, 0],
+        'key': 'b',
+    }, {
+        'x': [0, 10],
+        'key': 'b',
+    }]
+    input_metadata = tft.DatasetMetadata.from_feature_spec({
+        'x': tf.io.FixedLenFeature([2], tf.float32),
+        'key': tf.io.FixedLenFeature([], tf.string)
+    })
+    expected_outputs = {
+        'key_vocab': np.array([b'a', b'b'], np.object),
+        'mean': np.array([[3.0, -0.5], [5.0, 5.0]], np.float32),
+        'var': np.array([[25.0, 0.25], [25.0, 25.0]], np.float32)
+    }
+    self.assertAnalyzerOutputs(input_data, input_metadata, analyzer_fn,
+                               expected_outputs)
+
+  @tft_unit.named_parameters(
+      dict(
+          testcase_name='_dense_2d',
+          input_data=[{
+              'x': [4, 8],
+              'key': 'a'
+          }, {
+              'x': [1, 5],
+              'key': 'a'
+          }, {
+              'x': [5, 9],
+              'key': 'a'
+          }, {
+              'x': [2, 6],
+              'key': 'a'
+          }, {
+              'x': [-2, 0],
+              'key': 'b'
+          }, {
+              'x': [0, 2],
+              'key': 'b'
+          }, {
+              'x': [2, 4],
+              'key': 'b'
+          }],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          reduce_instance_dims=True,
+          expected_outputs={
+              'key_vocab': np.array([b'a', b'b'], np.object),
+              'min_x_value': np.array([1, -2], np.float32),
+              'max_x_value': np.array([9, 4], np.float32),
+          }),
+      dict(
+          testcase_name='_dense_2d_elementwise',
+          input_data=[{
+              'x': [4, 8],
+              'key': 'a'
+          }, {
+              'x': [1, 5],
+              'key': 'a'
+          }, {
+              'x': [5, 9],
+              'key': 'a'
+          }, {
+              'x': [2, 6],
+              'key': 'a'
+          }, {
+              'x': [-2, 0],
+              'key': 'b'
+          }, {
+              'x': [0, 2],
+              'key': 'b'
+          }, {
+              'x': [2, 4],
+              'key': 'b'
+          }],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          reduce_instance_dims=False,
+          expected_outputs={
+              'key_vocab': np.array([b'a', b'b'], np.object),
+              'min_x_value': np.array([[1, 5], [-2, 0]], np.float32),
+              'max_x_value': np.array([[5, 9], [2, 4]], np.float32),
+          }),
+      dict(
+          testcase_name='_dense_3d',
+          input_data=[
+              {
+                  'x': [[1, 5], [1, 1]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[5, 1], [5, 5]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[2, 2], [2, 5]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[3, -3], [3, 3]],
+                  'key': 'b'
+              },
+          ],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2, 2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          reduce_instance_dims=True,
+          expected_outputs={
+              'key_vocab': np.array([b'a', b'b'], np.object),
+              'min_x_value': np.array([1, -3], np.float32),
+              'max_x_value': np.array([5, 3], np.float32),
+          }),
+      dict(
+          testcase_name='_dense_3d_elementwise',
+          input_data=[
+              {
+                  'x': [[1, 5], [1, 1]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[5, 1], [5, 5]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[2, 2], [2, 5]],
+                  'key': 'a'
+              },
+              {
+                  'x': [[3, -3], [3, 3]],
+                  'key': 'b'
+              },
+          ],
+          input_metadata=tft.DatasetMetadata.from_feature_spec({
+              'x': tf.io.FixedLenFeature([2, 2], tf.float32),
+              'key': tf.io.FixedLenFeature([], tf.string),
+          }),
+          reduce_instance_dims=False,
+          expected_outputs={
+              'key_vocab':
+                  np.array([b'a', b'b'], np.object),
+              'min_x_value':
+                  np.array([[[1, 1], [1, 1]], [[3, -3], [3, 3]]], np.float32),
+              'max_x_value':
+                  np.array([[[5, 5], [5, 5]], [[3, -3], [3, 3]]], np.float32),
+          }),
+  )
+  def testMinAndMaxPerKey(self, input_data, input_metadata,
+                          reduce_instance_dims, expected_outputs):
+    self._SkipIfOutputRecordBatches()
+
+    def analyzer_fn(inputs):
+      key_vocab, min_x_value, max_x_value = analyzers._min_and_max_per_key(
+          x=inputs['x'],
+          key=inputs['key'],
+          reduce_instance_dims=reduce_instance_dims)
+      return {
+          'key_vocab': key_vocab,
+          'min_x_value': min_x_value,
+          'max_x_value': max_x_value,
+      }
+
+    self.assertAnalyzerOutputs(input_data, input_metadata, analyzer_fn,
+                               expected_outputs)
+
   @tft_unit.parameters((True,), (False,))
   def testPerKeyWithOOVKeys(self, use_vocabulary):
     def preprocessing_fn(inputs):
       result = {}
       result['x_scaled'] = tft.scale_to_0_1_per_key(
           inputs['x'],
           inputs['key'],
```

## tensorflow_transform/saved/saved_transform_io_v2.py

```diff
@@ -26,17 +26,24 @@
 from tensorflow_transform.saved import saved_model_loader
 from tensorflow_transform.saved import saved_transform_io
 # pylint: disable=g-direct-tensorflow-import
 from tensorflow.python.eager import function
 from tensorflow.python.eager import wrap_function
 from tensorflow.python.framework import composite_tensor
 from tensorflow.python.ops import lookup_ops
-from tensorflow.python.training.tracking import tracking
 from tensorflow.python.util import object_identity
-# pylint: enable=g-direct-tensorflow-import
+
+# pylint: disable=g-import-not-at-top
+try:
+  # Moved in TensorFlow 2.10.
+  from tensorflow.python.trackable import resource as tracking
+except ImportError:
+  from tensorflow.python.training.tracking import tracking
+
+# pylint: enable=g-direct-tensorflow-import, g-import-not-at-top
 
 
 def _restore_from_v1_saved_model(
     restored_function: function.ConcreteFunction, saved_model_dir: str
 ) -> Tuple[function.ConcreteFunction, Mapping[str, Any], Mapping[
     str, common_types.TensorType]]:
   """Restores an exported TF1 compat SavedModel."""
```

## tensorflow_transform/tf_metadata/schema_utils.py

```diff
@@ -513,23 +513,23 @@
   """Returns a representation of a FixedShape as a tensorflow shape."""
   # TODO(b/120869660): Remove the cast to int.  Casting to int is currently
   # needed as some TF code explicitly checks for `int` and does not allow `long`
   # in tensor shapes.
   return [int(dim.size) for dim in fixed_shape.dim]
 
 
-_DEPRECATED_LIFECYCLE_STAGES = [
+_IGNORED_LIFECYCLE_STAGES = [
     schema_pb2.DEPRECATED, schema_pb2.DISABLED, schema_pb2.PLANNED,
-    schema_pb2.ALPHA, schema_pb2.DEBUG_ONLY
+    schema_pb2.ALPHA, schema_pb2.DEBUG_ONLY, schema_pb2.VALIDATION_DERIVED,
 ]
 
 
 def _include_in_parsing_spec(feature):
   return not (schema_utils_legacy.get_deprecated(feature) or
-              feature.lifecycle_stage in _DEPRECATED_LIFECYCLE_STAGES)
+              feature.lifecycle_stage in _IGNORED_LIFECYCLE_STAGES)
 
 
 def _legacy_schema_from_feature_spec(feature_spec, domains=None):
   """Infer a Schema from a feature spec, using the legacy feature spec logic.
 
   Infers a Schema proto that with generate_legacy_feature_spec set to true,
   which will result in the given feature spec and domains when
```

## Comparing `tensorflow_transform-1.8.0.dist-info/LICENSE` & `tensorflow_transform-1.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensorflow_transform-1.8.0.dist-info/METADATA` & `tensorflow_transform-1.9.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensorflow-transform
-Version: 1.8.0
+Version: 1.9.0
 Summary: A library for data preprocessing with TensorFlow
 Home-page: https://www.tensorflow.org/tfx/transform/get_started
 Download-URL: https://github.com/tensorflow/transform/tags
 Author: Google Inc.
 Author-email: tensorflow-extended-dev@googlegroups.com
 License: Apache 2.0
 Keywords: tensorflow transform tfx
@@ -32,17 +32,17 @@
 License-File: LICENSE
 Requires-Dist: absl-py (<2.0.0,>=0.9)
 Requires-Dist: apache-beam[gcp] (<3,>=2.38)
 Requires-Dist: numpy (<2,>=1.16)
 Requires-Dist: protobuf (<4,>=3.13)
 Requires-Dist: pyarrow (<6,>=1)
 Requires-Dist: pydot (<2,>=1.2)
-Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5)
-Requires-Dist: tensorflow-metadata (<1.9.0,>=1.8.0)
-Requires-Dist: tfx-bsl (<1.9.0,>=1.8.0)
+Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5)
+Requires-Dist: tensorflow-metadata (<1.10.0,>=1.9.0)
+Requires-Dist: tfx-bsl (<1.10.0,>=1.9.0)
 
 <!-- See: www.tensorflow.org/tfx/transform/ -->
 
 # TensorFlow Transform
 
 [![Python](https://img.shields.io/pypi/pyversions/tensorflow-transform.svg?style=plastic)](https://github.com/tensorflow/transform)
 [![PyPI](https://badge.fury.io/py/tensorflow-transform.svg)](https://badge.fury.io/py/tensorflow-transform)
@@ -134,15 +134,16 @@
 
 The following table is the `tf.Transform` package versions that are
 compatible with each other. This is determined by our testing framework, but
 other *untested* combinations may also work.
 
 tensorflow-transform                                                            | apache-beam[gcp] | pyarrow | tensorflow        | tensorflow-metadata | tfx-bsl |
 ------------------------------------------------------------------------------- | -----------------| --------|-------------------|---------------------|---------|
-[GitHub master](https://github.com/tensorflow/transform/blob/master/RELEASE.md) | 2.38.0           | 5.0.0   | nightly (1.x/2.x) | 1.8.0               | 1.8.0   |
+[GitHub master](https://github.com/tensorflow/transform/blob/master/RELEASE.md) | 2.38.0           | 5.0.0   | nightly (1.x/2.x) | 1.9.0               | 1.9.0   |
+[1.9.0](https://github.com/tensorflow/transform/blob/v1.9.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15.5 / 2.9      | 1.9.0               | 1.9.0   |
 [1.8.0](https://github.com/tensorflow/transform/blob/v1.8.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15.5 / 2.8      | 1.8.0               | 1.8.0   |
 [1.7.0](https://github.com/tensorflow/transform/blob/v1.7.0/RELEASE.md)         | 2.36.0           | 5.0.0   | 1.15.5 / 2.8      | 1.7.0               | 1.7.0   |
 [1.6.1](https://github.com/tensorflow/transform/blob/v1.6.1/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15.5 / 2.8      | 1.6.0               | 1.6.0   |
 [1.6.0](https://github.com/tensorflow/transform/blob/v1.6.0/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15.5 / 2.7      | 1.6.0               | 1.6.0   |
 [1.5.0](https://github.com/tensorflow/transform/blob/v1.5.0/RELEASE.md)         | 2.34.0           | 5.0.0   | 1.15.2 / 2.7      | 1.5.0               | 1.5.0   |
 [1.4.1](https://github.com/tensorflow/transform/blob/v1.4.1/RELEASE.md)         | 2.33.0           | 4.0.1   | 1.15.2 / 2.6      | 1.4.0               | 1.4.0   |
 [1.4.0](https://github.com/tensorflow/transform/blob/v1.4.0/RELEASE.md)         | 2.33.0           | 4.0.1   | 1.15.2 / 2.6      | 1.4.0               | 1.4.0   |
```

## Comparing `tensorflow_transform-1.8.0.dist-info/RECORD` & `tensorflow_transform-1.9.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,64 +1,64 @@
 tensorflow_transform/__init__.py,sha256=6W_-kY8AFjnsncnH1RL_y1HI1j76RXkSzwv4B3_atQo,1631
 tensorflow_transform/analyzer_nodes.py,sha256=ZUZPJ6oo648S1iq8UR6WTn_iMNXGiNTZ8sjaKapzYTY,39852
-tensorflow_transform/analyzers.py,sha256=uUX7F74JkgvVFw9U6QQPH_SUfY-QhM03n_1Wk8C-IAY,109671
+tensorflow_transform/analyzers.py,sha256=4uU_NWHT5dt93wtnkS6gJItyCQrhkmTDameVfvOpMik,109964
 tensorflow_transform/analyzers_test.py,sha256=8xLNGxgi6wG1A--qhEHWlvjT5v3-uUf1NtuifS-QZCU,23461
-tensorflow_transform/annotators.py,sha256=HlLlBgkEgOXAsYrraOY6nnrrvpCfVthg3tMpmwbJ_9A,9081
-tensorflow_transform/annotators_test.py,sha256=5_G5WTE2iv2pAlIz8JPVXRu3LGiIMISBt9C0otNjQK0,2416
+tensorflow_transform/annotators.py,sha256=WEP_a4D9C5DizEq9RPL-MxFi6ZY8LuHoxZjT55y9kQQ,9244
+tensorflow_transform/annotators_test.py,sha256=26y5ms-g3EK61pbXFdAgMtG_qrEN12iWSHJy3aLHX30,2605
 tensorflow_transform/common.py,sha256=yavlJIGfU_q5ABvvSo38hdWxpW-ZstyJ1GmykShWC_c,2869
 tensorflow_transform/common_test.py,sha256=ABdXt0a0peTI2LZ8KPfTalVwHXw8ccWeBJnYH5wLGUw,2031
 tensorflow_transform/common_types.py,sha256=AsIooh-qLFcnFDv2J9jc42DVgLTdGjwwcJkwhF5YQuo,2906
 tensorflow_transform/gaussianization.py,sha256=ObOKFcVuZvcmklIPbm5DfcGeIx0_hyzGYSRzgNZOHH0,13792
 tensorflow_transform/gaussianization_test.py,sha256=xOFpixYGvd8yq4FL584CyMBBvCcGLVqrD6YYwBZAyUQ,9430
 tensorflow_transform/graph_context.py,sha256=if4i2apPzxLnpHphkEwS2cjACvQ0bfWxZbUWmbt4L2w,4653
 tensorflow_transform/graph_tools.py,sha256=J00My6HiRdMjxW5u7LTdPCcCHVHwSBJG8e788_JNcz8,39030
 tensorflow_transform/graph_tools_test.py,sha256=eb5qYbZ1u9VngJG12pqjSJaJjBVhENPKUJL34Ad0mVo,53081
 tensorflow_transform/impl_helper.py,sha256=TQpzeey0R_v0hvlDD9A7mH-RJDTvyIpNIHdubUIJU70,41078
 tensorflow_transform/impl_helper_test.py,sha256=j0ekDkk0aQ2v7tEGDHnwHMt5R9rSh5n2K-K64mgNlZ4,39285
-tensorflow_transform/info_theory.py,sha256=2VXke4NR9Cd61omdaK7I5XcbC3pj-G1SwHgKqgYNdPQ,5011
-tensorflow_transform/info_theory_test.py,sha256=Nv9qZO7xVWBy44RI3f1J2AIP2kD_vPaaGoPBHWHMl2A,4597
+tensorflow_transform/info_theory.py,sha256=DEdPOhj4yw5Nmey4pRL8oDhnL9bMlSw1nWa1aPldfwQ,5035
+tensorflow_transform/info_theory_test.py,sha256=x24XpR6PXr5DGyii0LALY47MDY_JUAEbR72T3SyPQ-M,5165
 tensorflow_transform/inspect_preprocessing_fn.py,sha256=PFb38wP-6zlMGr-C27I4XARzPWKYaaP42xduq8RRIEs,4318
 tensorflow_transform/inspect_preprocessing_fn_test.py,sha256=_X4q3y88XtdgNn7e_K0OXvtOEaPfbFYrVF6BX-orcwE,6096
-tensorflow_transform/mappers.py,sha256=WiA5E_uD7Z4JAGtVpDCAG6uFzn9FI3E-yUPS_C03Efw,95662
+tensorflow_transform/mappers.py,sha256=OCw4ryp5S1s_0__PMdU8tBR7IykMxlXdjs8g_aSOAAI,96187
 tensorflow_transform/mappers_test.py,sha256=ZSohlXUspL7-pAOIOSr5IENFFST25cdfFojq3qYnXzo,37123
 tensorflow_transform/nodes.py,sha256=VIssVEICXDmniKCYBac1osj6fcGN1pFxaOp_h9kJIR8,12912
 tensorflow_transform/nodes_test.py,sha256=IJWVRR5KvSiCeWuFMoxHxLz_lUhy0AmwPG4WFqG_lsc,9531
 tensorflow_transform/output_wrapper.py,sha256=Hxmk-dr7We8tJUPgf6UqNdvfZHN6o5TSB6rb4osBKiY,23233
 tensorflow_transform/pickle_helper.py,sha256=Ad6P_7Mw-V53Pdv31pBQ9hTyT8yo9dXJaJOQ1_eHzSA,2133
 tensorflow_transform/pretrained_models.py,sha256=2cDkqdCqhwBY2yQ0QhgSTHcDAZi3qQLN-FvXm-icTg8,11375
 tensorflow_transform/pretrained_models_test.py,sha256=5_34Hhbeockhs7ss8iPaSYxwiMSx_vvND-a6rj0_Vzg,7278
 tensorflow_transform/schema_inference.py,sha256=Z6KwArCV1wt5hAEZenju1yrys_hVgBn0CtZpca3z7Y4,31652
 tensorflow_transform/schema_inference_test.py,sha256=FOUBgp7M7-gdSfG6otmdPeBqfwYUHij09Xq-gkGF0BA,16745
 tensorflow_transform/test_case.py,sha256=aeb2LBIwbsvsJSR1eUtk7GJcZt9uaXy3LtLWdqjRDBo,13104
 tensorflow_transform/test_case_test.py,sha256=WLxfzyjx1cXi47JZwUQSXQFJsFR2RuMfu7KFoHx1sbE,3131
 tensorflow_transform/tf2_utils.py,sha256=hXoxgn0kUukp_q6EMHIuPOIlr38inpPWHC6m5Sfj5i8,8703
 tensorflow_transform/tf2_utils_test.py,sha256=rN4Hh1_kWA21eZu12GQDuMNDuOreQiM3t9rFSZit9Xg,4241
-tensorflow_transform/tf_utils.py,sha256=And7dTD1gbI21qr4qEx0XReE68U_4QMFdkfgG9EjIvw,68763
-tensorflow_transform/tf_utils_test.py,sha256=cR3XCXx99lb5v0pvPw_y7kUITlEuG7usl0mX0uwPRUg,99904
-tensorflow_transform/version.py,sha256=XhZxLdN50HXW1mraaHniW2Iu_YErKXymn_EcCkda4vM,710
+tensorflow_transform/tf_utils.py,sha256=-WNhsOjlU7Ie335aVzl0KHDA0w_LAo8ltSUS-9thwxI,69571
+tensorflow_transform/tf_utils_test.py,sha256=rUg0gM1G_76PWroGFWfRWXm8I61uTzrq0DDV54_Hx3A,102376
+tensorflow_transform/version.py,sha256=4UV4QJOMizYeDl_ulkSkRVk-DSm0NCVGMeyGkyLGCYg,710
 tensorflow_transform/beam/__init__.py,sha256=rH7DntkQk3T8JRmpr8V5qyuQJHDj96PTKA4MtkgHjfw,1732
 tensorflow_transform/beam/analysis_graph_builder.py,sha256=JFHqeZYGTKuLfRoFgyIwjHa9ZRD2Mjbr6LPaxo4PDwA,26760
 tensorflow_transform/beam/analysis_graph_builder_test.py,sha256=LX68A_NAYIe_zTvOlNMy4KiBIC_U-s6yuxitQp9-sOw,44955
 tensorflow_transform/beam/analyzer_cache.py,sha256=rFoEjFQJQbpm4wPhC6DzK8p99ijhPWIIe2OEZzbowHE,11354
 tensorflow_transform/beam/analyzer_cache_test.py,sha256=InjLsFvotdYhds_5UKPqRNSGDyLUmdTqK5XcKUpoz-c,9786
 tensorflow_transform/beam/analyzer_impls.py,sha256=fpzTKbcGa-0Npv9eeC5HdfUwHfgrjc47tzFi6J2PlXY,55415
 tensorflow_transform/beam/analyzer_impls_test.py,sha256=SmJnNw6xoQwIuKwNhqKZ90fzC004zDgTNbIaumym9Ig,5820
 tensorflow_transform/beam/beam_nodes.py,sha256=A7gBfKJ5_42pHK8UToGazjhimuNSmlH30nBNKxwsO2Q,7649
 tensorflow_transform/beam/bucketize_integration_test.py,sha256=L89JX6fxFJFZOiBHnNdycVAXcBjUsJ5jQHYCB84W2k0,32841
-tensorflow_transform/beam/cached_impl_test.py,sha256=iYSDK7fZyXMXkY1MM4G2gm0MB_af7wKF-TkT9PLbTBk,75768
+tensorflow_transform/beam/cached_impl_test.py,sha256=YSBLDgUPdT6d588ZoFCYD8kqyHZzipGg_a8YkFZhtsQ,75889
 tensorflow_transform/beam/combiner_packing_util.py,sha256=M57OA5a4mZ0Venk_ASXObBabYyPSPSpwRLeUzA-bFvs,25939
 tensorflow_transform/beam/combiner_packing_util_test.py,sha256=JK6XioNW2BFB8fqQN1LNYZkxq1xU3UBnt7vMXXRBcY4,53092
-tensorflow_transform/beam/common.py,sha256=LHMCr7WbHx1erfj4MzhuMMIIJ-WePNzf9VC4Gr5dI7c,7881
+tensorflow_transform/beam/common.py,sha256=fCYWkxItWR1cAejNfHxDdYtSqAJiKNdqGP2fR1Ww3_c,7997
 tensorflow_transform/beam/context.py,sha256=S-H4vYLzwsnSClcIF9FyIbQPqsZwe3zVTWHvHvP_Uqk,7168
 tensorflow_transform/beam/context_test.py,sha256=0q6Uxlzlt-YcOcjNlhFlYGF7NM6WHEaJM60Q84SThAI,1596
 tensorflow_transform/beam/deep_copy.py,sha256=UMT_qYW4VfWhRVcVWBt5jYkbo79ZUuyq8LtiUZ3XOLI,10333
 tensorflow_transform/beam/deep_copy_test.py,sha256=oKpsgiIVcYm0oHMMUFdws_Sz6RM_xzGnWIPf0uxcPE8,13697
 tensorflow_transform/beam/impl.py,sha256=_gnxBMwT_CucTxKFTW3M2WibnLaQsNjuVUSTHU9eeH8,66916
 tensorflow_transform/beam/impl_output_record_batches_test.py,sha256=YsqAiROAwlfNu1DwyyHiBcuNO1WTaGSVYTauG2FdqiE,5193
-tensorflow_transform/beam/impl_test.py,sha256=fLELW7E81lcavufOdNRdZ1ApVtLzwUgx8K3U9p50uyE,153451
+tensorflow_transform/beam/impl_test.py,sha256=UMeeb7U8MzC2uDz5ZyNm-9ZOod4pbTov01RQmVJ9_B4,164905
 tensorflow_transform/beam/test_helpers.py,sha256=t3b7XG4CYezWxe_4JCpsvOj1c6XGRJkcOqDDzYNPqf4,807
 tensorflow_transform/beam/tft_unit.py,sha256=Vqg_HWi2D59JC8N9flRzL2_NOn8r_G6h_O2SpTuea1U,17953
 tensorflow_transform/beam/tukey_hh_params_integration_test.py,sha256=xSimhgAuSa9nUl7GUC6gpsfd-snS0I5CIXJ9qICvqL8,24237
 tensorflow_transform/beam/vocabulary_integration_test.py,sha256=sLsso49yFjhiZ6XF95moOmq02MK-J0voMMt6jjtdDb8,79775
 tensorflow_transform/beam/vocabulary_tfrecord_gzip_integration_test.py,sha256=wNXqsrMdCZ57cVxbZRJ5DBTlczGGVkFDk27r_2O7nrE,2075
 tensorflow_transform/beam/experimental/__init__.py,sha256=cANt-fPpXEl-lJLo-TLkKp9jm58LlHQgzqzsZ_tiE7c,736
 tensorflow_transform/beam/experimental/analyzer_impls.py,sha256=g0r5i3avU8-kNE6XwOZIGNIHOOE64bnUTJbaSND9znk,1037
@@ -82,25 +82,25 @@
 tensorflow_transform/py_func/pyfunc_helper.py,sha256=cqsE8kJ4yWfESFcp97UMfr42iBSgf02sRhCO70ttfMg,6983
 tensorflow_transform/saved/__init__.py,sha256=YqVihVnD6oPj4Xyuyng6Jo1_qfOfbsZme1w5QwKOEe8,655
 tensorflow_transform/saved/constants.py,sha256=4odfGkyO5iS3Prgs35KCJ6kfsWlvvoLj3KlzVHfJP9o,797
 tensorflow_transform/saved/saved_model_loader.py,sha256=zjCpAHOAAPyu-Y_gqK6N58BUsTskerJPgGmtGZxGSq0,2898
 tensorflow_transform/saved/saved_model_loader_test.py,sha256=kzPQJ8ExWh5SjfpFY1k2kE94ayFy-VvQT1bP_Xl9ge8,1510
 tensorflow_transform/saved/saved_transform_io.py,sha256=2EmTFkhGy5Ak2zZbGPYiano72fZ2bJkiOSfORXJTuOY,19956
 tensorflow_transform/saved/saved_transform_io_test.py,sha256=qVSf-9F4B1nfedJHNVZRfZVkdWLNndi0AC2snWbomhg,13690
-tensorflow_transform/saved/saved_transform_io_v2.py,sha256=Q_WwP4LOjG6kNuVty3DeLc9oPmxsl01bLdGhW6zb-tA,22590
+tensorflow_transform/saved/saved_transform_io_v2.py,sha256=7dpDWRU3dSy-_kPVrKkdWesfcf75eox8cDKc4MEYGaw,22771
 tensorflow_transform/saved/saved_transform_io_v2_test.py,sha256=lqPvv32U7AE_UlTNwIdLE6w9-zbmTKHUuQLhxryhES4,26397
 tensorflow_transform/tf_metadata/__init__.py,sha256=blj8UsSrxAk_EittU9b-RW2H3LJrhG7g4CA6PEo0bgY,596
 tensorflow_transform/tf_metadata/dataset_metadata.py,sha256=B73eY98_o6KSQuAmzpN7tOa-tYkPRV5scGx2aAqrun8,2338
 tensorflow_transform/tf_metadata/dataset_metadata_test.py,sha256=BDbmvDkrA2wBakUNUt_AtvG9gYImw7A-CvQ0FbyNNyM,1060
 tensorflow_transform/tf_metadata/metadata_io.py,sha256=vv5oe0oE9ugFPG3fYq9Eqy4rGznbi7ieEh5ZoNJYUyU,4828
 tensorflow_transform/tf_metadata/metadata_io_test.py,sha256=8798WdHOuSJx8zL4J5vpNWtwk6SWqe3NHQF1D3cRXvA,3523
-tensorflow_transform/tf_metadata/schema_utils.py,sha256=cXYXEz17aytP-wNTLd6OhjI8wMoDmQiiILRmMjgnITM,30036
+tensorflow_transform/tf_metadata/schema_utils.py,sha256=VTQFQOi3wHjon_MVKrYH452KIZQm7legt0HPDuQKa1E,30062
 tensorflow_transform/tf_metadata/schema_utils_legacy.py,sha256=dThpCoHnoKNxGjEd9Rmvs8H6u8hwyA-oaQnSA6RbcNw,1266
 tensorflow_transform/tf_metadata/schema_utils_test.py,sha256=MPCSYtCCCtCnarp9gK5WiJcjAT1lVwK1jxKv0-C3yI8,3685
 tensorflow_transform/tf_metadata/schema_utils_test_cases.py,sha256=O9CZ15TzedA_cBKT3eNNbTX8jL6WWAxEHThvUQkZ2b0,24266
 tensorflow_transform/tf_metadata/test_common.py,sha256=zcIi20lnDXOJPojvfhnHSaHqNfgtRYibAQ_HJHe8bgE,1415
-tensorflow_transform-1.8.0.dist-info/LICENSE,sha256=QaDauV7vxjKm9KvRKmGXkI9qmGgq69bo6HBmawpFWVM,11420
-tensorflow_transform-1.8.0.dist-info/METADATA,sha256=9G9PbvN_n0E_sZWWxIDLdek68iIRCWopsg00zdCdYDE,12083
-tensorflow_transform-1.8.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-tensorflow_transform-1.8.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-tensorflow_transform-1.8.0.dist-info/top_level.txt,sha256=fJpiFAzXm4liUMmGPBdt3gxGHmATzvRzlNItKjuYNCE,21
-tensorflow_transform-1.8.0.dist-info/RECORD,,
+tensorflow_transform-1.9.0.dist-info/LICENSE,sha256=QaDauV7vxjKm9KvRKmGXkI9qmGgq69bo6HBmawpFWVM,11420
+tensorflow_transform-1.9.0.dist-info/METADATA,sha256=SQsenarAitE5wBeeUTH84FGLciWrXEZ32BYVNo_sjLU,12257
+tensorflow_transform-1.9.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+tensorflow_transform-1.9.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+tensorflow_transform-1.9.0.dist-info/top_level.txt,sha256=fJpiFAzXm4liUMmGPBdt3gxGHmATzvRzlNItKjuYNCE,21
+tensorflow_transform-1.9.0.dist-info/RECORD,,
```

