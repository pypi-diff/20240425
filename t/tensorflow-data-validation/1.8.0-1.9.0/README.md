# Comparing `tmp/tensorflow_data_validation-1.8.0-cp39-cp39-win_amd64.whl.zip` & `tmp/tensorflow_data_validation-1.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,120 +1,121 @@
-Zip file size: 1213745 bytes, number of entries: 118
--rw-rw-rw-  2.0 fat     4427 b- defN 22-May-16 07:49 tensorflow_data_validation/__init__.py
--rw-rw-rw-  2.0 fat     1457 b- defN 22-May-16 07:49 tensorflow_data_validation/constants.py
--rw-rw-rw-  2.0 fat     4888 b- defN 22-May-16 07:49 tensorflow_data_validation/types.py
--rw-rw-rw-  2.0 fat     3231 b- defN 22-May-16 07:49 tensorflow_data_validation/types_test.py
--rw-rw-rw-  2.0 fat      700 b- defN 22-May-16 07:49 tensorflow_data_validation/version.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/anomalies/__init__.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/anomalies/proto/__init__.py
--rw-rw-rw-  2.0 fat     5623 b- defN 22-May-16 07:51 tensorflow_data_validation/anomalies/proto/validation_config_pb2.py
--rw-rw-rw-  2.0 fat     6710 b- defN 22-May-16 07:51 tensorflow_data_validation/anomalies/proto/validation_metadata_pb2.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/api/__init__.py
--rw-rw-rw-  2.0 fat     8665 b- defN 22-May-16 07:49 tensorflow_data_validation/api/stats_api.py
--rw-rw-rw-  2.0 fat    22724 b- defN 22-May-16 07:49 tensorflow_data_validation/api/stats_api_test.py
--rw-rw-rw-  2.0 fat    39017 b- defN 22-May-16 07:49 tensorflow_data_validation/api/validation_api.py
--rw-rw-rw-  2.0 fat   111489 b- defN 22-May-16 07:49 tensorflow_data_validation/api/validation_api_test.py
--rw-rw-rw-  2.0 fat     2488 b- defN 22-May-16 07:49 tensorflow_data_validation/api/validation_options.py
--rw-rw-rw-  2.0 fat     1893 b- defN 22-May-16 07:49 tensorflow_data_validation/api/validation_options_test.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-16 07:49 tensorflow_data_validation/arrow/__init__.py
--rw-rw-rw-  2.0 fat    17118 b- defN 22-May-16 07:49 tensorflow_data_validation/arrow/arrow_util.py
--rw-rw-rw-  2.0 fat    23028 b- defN 22-May-16 07:49 tensorflow_data_validation/arrow/arrow_util_test.py
--rw-rw-rw-  2.0 fat     3668 b- defN 22-May-16 07:49 tensorflow_data_validation/arrow/decoded_examples_to_arrow.py
--rw-rw-rw-  2.0 fat     8005 b- defN 22-May-16 07:49 tensorflow_data_validation/arrow/decoded_examples_to_arrow_test.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/coders/__init__.py
--rw-rw-rw-  2.0 fat     3827 b- defN 22-May-16 07:49 tensorflow_data_validation/coders/csv_decoder.py
--rw-rw-rw-  2.0 fat    16744 b- defN 22-May-16 07:49 tensorflow_data_validation/coders/csv_decoder_test.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/pywrap/__init__.py
--rw-rw-rw-  2.0 fat  2199552 b- defN 22-May-16 07:51 tensorflow_data_validation/pywrap/tensorflow_data_validation_extension.pyd
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-16 07:49 tensorflow_data_validation/skew/__init__.py
--rw-rw-rw-  2.0 fat    17185 b- defN 22-May-16 07:49 tensorflow_data_validation/skew/feature_skew_detector.py
--rw-rw-rw-  2.0 fat    20037 b- defN 22-May-16 07:49 tensorflow_data_validation/skew/feature_skew_detector_test.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-May-16 07:49 tensorflow_data_validation/skew/protos/__init__.py
--rw-rw-rw-  2.0 fat     9944 b- defN 22-May-16 07:51 tensorflow_data_validation/skew/protos/feature_skew_results_pb2.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/__init__.py
--rw-rw-rw-  2.0 fat    37899 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/stats_impl.py
--rw-rw-rw-  2.0 fat   127501 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/stats_impl_test.py
--rw-rw-rw-  2.0 fat    24773 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/stats_options.py
--rw-rw-rw-  2.0 fat    16262 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/stats_options_test.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/__init__.py
--rw-rw-rw-  2.0 fat    55459 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/basic_stats_generator.py
--rw-rw-rw-  2.0 fat   114071 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/basic_stats_generator_test.py
--rw-rw-rw-  2.0 fat    10048 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator.py
--rw-rw-rw-  2.0 fat     6100 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator_test.py
--rw-rw-rw-  2.0 fat    14195 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/image_stats_generator.py
--rw-rw-rw-  2.0 fat    14315 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/image_stats_generator_test.py
--rw-rw-rw-  2.0 fat     5199 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/input_batch.py
--rw-rw-rw-  2.0 fat     6902 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/input_batch_test.py
--rw-rw-rw-  2.0 fat    43483 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/lift_stats_generator.py
--rw-rw-rw-  2.0 fat    76379 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/lift_stats_generator_test.py
--rw-rw-rw-  2.0 fat    27733 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/mutual_information.py
--rw-rw-rw-  2.0 fat    55105 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/mutual_information_test.py
--rw-rw-rw-  2.0 fat     9572 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator.py
--rw-rw-rw-  2.0 fat     8979 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator_test.py
--rw-rw-rw-  2.0 fat    27926 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/natural_language_stats_generator.py
--rw-rw-rw-  2.0 fat    18426 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/natural_language_stats_generator_test.py
--rw-rw-rw-  2.0 fat    21944 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/partitioned_stats_generator.py
--rw-rw-rw-  2.0 fat    23760 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/partitioned_stats_generator_test.py
--rw-rw-rw-  2.0 fat    18653 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/sklearn_mutual_information.py
--rw-rw-rw-  2.0 fat    25782 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/sklearn_mutual_information_test.py
--rw-rw-rw-  2.0 fat     7988 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/sparse_feature_stats_generator.py
--rw-rw-rw-  2.0 fat    31108 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/sparse_feature_stats_generator_test.py
--rw-rw-rw-  2.0 fat    17759 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/stats_generator.py
--rw-rw-rw-  2.0 fat    14681 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/time_stats_generator.py
--rw-rw-rw-  2.0 fat    14815 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/time_stats_generator_test.py
--rw-rw-rw-  2.0 fat    12299 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/top_k_uniques_sketch_stats_generator.py
--rw-rw-rw-  2.0 fat    45015 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/top_k_uniques_sketch_stats_generator_test.py
--rw-rw-rw-  2.0 fat    14330 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/top_k_uniques_stats_generator.py
--rw-rw-rw-  2.0 fat    43317 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/top_k_uniques_stats_generator_test.py
--rw-rw-rw-  2.0 fat     4738 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/weighted_feature_stats_generator.py
--rw-rw-rw-  2.0 fat     9777 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/weighted_feature_stats_generator_test.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/constituents/__init__.py
--rw-rw-rw-  2.0 fat     4247 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/constituents/count_missing_generator.py
--rw-rw-rw-  2.0 fat     3272 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/constituents/count_missing_generator_test.py
--rw-rw-rw-  2.0 fat     5738 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/constituents/length_diff_generator.py
--rw-rw-rw-  2.0 fat     5921 b- defN 22-May-16 07:49 tensorflow_data_validation/statistics/generators/constituents/length_diff_generator_test.py
--rw-rw-rw-  2.0 fat      590 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/__init__.py
--rw-rw-rw-  2.0 fat     6029 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/anomalies_util.py
--rw-rw-rw-  2.0 fat    22706 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/anomalies_util_test.py
--rw-rw-rw-  2.0 fat     2001 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/batch_util.py
--rw-rw-rw-  2.0 fat     2549 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/batch_util_test.py
--rw-rw-rw-  2.0 fat     3923 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/bin_util.py
--rw-rw-rw-  2.0 fat     2014 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/bin_util_test.py
--rw-rw-rw-  2.0 fat    23064 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/display_util.py
--rw-rw-rw-  2.0 fat    27977 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/display_util_test.py
--rw-rw-rw-  2.0 fat     2328 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/example_weight_map.py
--rw-rw-rw-  2.0 fat     2230 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/example_weight_map_test.py
--rw-rw-rw-  2.0 fat     6874 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/feature_partition_util.py
--rw-rw-rw-  2.0 fat    10048 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/feature_partition_util_test.py
--rw-rw-rw-  2.0 fat     4211 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/io_util.py
--rw-rw-rw-  2.0 fat     2212 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/io_util_test.py
--rw-rw-rw-  2.0 fat    26635 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/mutual_information_util.py
--rw-rw-rw-  2.0 fat    17807 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/mutual_information_util_test.py
--rw-rw-rw-  2.0 fat    13141 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/quantiles_util.py
--rw-rw-rw-  2.0 fat    12699 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/quantiles_util_test.py
--rw-rw-rw-  2.0 fat    12874 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/schema_util.py
--rw-rw-rw-  2.0 fat    19682 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/schema_util_test.py
--rw-rw-rw-  2.0 fat    12322 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/slicing_util.py
--rw-rw-rw-  2.0 fat    11529 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/slicing_util_test.py
--rw-rw-rw-  2.0 fat     3587 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/statistics_io_impl.py
--rw-rw-rw-  2.0 fat     1610 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/statistics_io_impl_test.py
--rw-rw-rw-  2.0 fat    14740 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/stats_gen_lib.py
--rw-rw-rw-  2.0 fat    24602 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/stats_gen_lib_test.py
--rw-rw-rw-  2.0 fat    23949 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/stats_util.py
--rw-rw-rw-  2.0 fat    19199 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/stats_util_test.py
--rw-rw-rw-  2.0 fat    19302 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/test_util.py
--rw-rw-rw-  2.0 fat     9096 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/test_util_test.py
--rw-rw-rw-  2.0 fat    12271 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/top_k_uniques_stats_util.py
--rw-rw-rw-  2.0 fat    11756 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/top_k_uniques_stats_util_test.py
--rw-rw-rw-  2.0 fat    13362 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/validation_lib.py
--rw-rw-rw-  2.0 fat    18748 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/validation_lib_test.py
--rw-rw-rw-  2.0 fat     5350 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/variance_util.py
--rw-rw-rw-  2.0 fat     7666 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/variance_util_test.py
--rw-rw-rw-  2.0 fat     2144 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/vocab_util.py
--rw-rw-rw-  2.0 fat     1719 b- defN 22-May-16 07:49 tensorflow_data_validation/utils/vocab_util_test.py
--rw-rw-rw-  2.0 fat    11573 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    17090 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        1 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/namespace_packages.txt
--rw-rw-rw-  2.0 fat       27 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    13908 b- defN 22-May-16 07:51 tensorflow_data_validation-1.8.0.dist-info/RECORD
-118 files, 4041030 bytes uncompressed, 1190369 bytes compressed:  70.5%
+Zip file size: 1303356 bytes, number of entries: 119
+-rw-rw-rw-  2.0 fat     4598 b- defN 22-Jun-29 17:42 tensorflow_data_validation/__init__.py
+-rw-rw-rw-  2.0 fat     1457 b- defN 22-Jun-29 17:42 tensorflow_data_validation/constants.py
+-rw-rw-rw-  2.0 fat     4888 b- defN 22-Jun-29 17:42 tensorflow_data_validation/types.py
+-rw-rw-rw-  2.0 fat     3231 b- defN 22-Jun-29 17:42 tensorflow_data_validation/types_test.py
+-rw-rw-rw-  2.0 fat      700 b- defN 22-Jun-29 17:42 tensorflow_data_validation/version.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/anomalies/__init__.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/anomalies/proto/__init__.py
+-rw-rw-rw-  2.0 fat     5623 b- defN 22-Jun-29 17:45 tensorflow_data_validation/anomalies/proto/validation_config_pb2.py
+-rw-rw-rw-  2.0 fat     6710 b- defN 22-Jun-29 17:45 tensorflow_data_validation/anomalies/proto/validation_metadata_pb2.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/__init__.py
+-rw-rw-rw-  2.0 fat     8827 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/stats_api.py
+-rw-rw-rw-  2.0 fat    22724 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/stats_api_test.py
+-rw-rw-rw-  2.0 fat    38909 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/validation_api.py
+-rw-rw-rw-  2.0 fat   111439 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/validation_api_test.py
+-rw-rw-rw-  2.0 fat     2488 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/validation_options.py
+-rw-rw-rw-  2.0 fat     1893 b- defN 22-Jun-29 17:42 tensorflow_data_validation/api/validation_options_test.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-29 17:42 tensorflow_data_validation/arrow/__init__.py
+-rw-rw-rw-  2.0 fat    17117 b- defN 22-Jun-29 17:42 tensorflow_data_validation/arrow/arrow_util.py
+-rw-rw-rw-  2.0 fat    23028 b- defN 22-Jun-29 17:42 tensorflow_data_validation/arrow/arrow_util_test.py
+-rw-rw-rw-  2.0 fat     3668 b- defN 22-Jun-29 17:42 tensorflow_data_validation/arrow/decoded_examples_to_arrow.py
+-rw-rw-rw-  2.0 fat     8005 b- defN 22-Jun-29 17:42 tensorflow_data_validation/arrow/decoded_examples_to_arrow_test.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/coders/__init__.py
+-rw-rw-rw-  2.0 fat     3827 b- defN 22-Jun-29 17:42 tensorflow_data_validation/coders/csv_decoder.py
+-rw-rw-rw-  2.0 fat    16744 b- defN 22-Jun-29 17:42 tensorflow_data_validation/coders/csv_decoder_test.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/pywrap/__init__.py
+-rw-rw-rw-  2.0 fat  2421760 b- defN 22-Jun-29 17:45 tensorflow_data_validation/pywrap/tensorflow_data_validation_extension.pyd
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-29 17:42 tensorflow_data_validation/skew/__init__.py
+-rw-rw-rw-  2.0 fat    17351 b- defN 22-Jun-29 17:42 tensorflow_data_validation/skew/feature_skew_detector.py
+-rw-rw-rw-  2.0 fat    23489 b- defN 22-Jun-29 17:42 tensorflow_data_validation/skew/feature_skew_detector_test.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Jun-29 17:42 tensorflow_data_validation/skew/protos/__init__.py
+-rw-rw-rw-  2.0 fat     9861 b- defN 22-Jun-29 17:45 tensorflow_data_validation/skew/protos/feature_skew_results_pb2.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/__init__.py
+-rw-rw-rw-  2.0 fat    38460 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/stats_impl.py
+-rw-rw-rw-  2.0 fat   127501 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/stats_impl_test.py
+-rw-rw-rw-  2.0 fat    24773 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/stats_options.py
+-rw-rw-rw-  2.0 fat    16269 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/stats_options_test.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/__init__.py
+-rw-rw-rw-  2.0 fat    55459 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/basic_stats_generator.py
+-rw-rw-rw-  2.0 fat   114071 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/basic_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    10048 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator.py
+-rw-rw-rw-  2.0 fat     6100 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    14195 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/image_stats_generator.py
+-rw-rw-rw-  2.0 fat    14315 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/image_stats_generator_test.py
+-rw-rw-rw-  2.0 fat     5199 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/input_batch.py
+-rw-rw-rw-  2.0 fat     6902 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/input_batch_test.py
+-rw-rw-rw-  2.0 fat    46166 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/lift_stats_generator.py
+-rw-rw-rw-  2.0 fat    76379 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/lift_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    27733 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/mutual_information.py
+-rw-rw-rw-  2.0 fat    55105 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/mutual_information_test.py
+-rw-rw-rw-  2.0 fat     9572 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator.py
+-rw-rw-rw-  2.0 fat     8979 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    27926 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/natural_language_stats_generator.py
+-rw-rw-rw-  2.0 fat    18426 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/natural_language_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    21944 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/partitioned_stats_generator.py
+-rw-rw-rw-  2.0 fat    23760 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/partitioned_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    18653 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/sklearn_mutual_information.py
+-rw-rw-rw-  2.0 fat    25782 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/sklearn_mutual_information_test.py
+-rw-rw-rw-  2.0 fat     7988 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/sparse_feature_stats_generator.py
+-rw-rw-rw-  2.0 fat    31108 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/sparse_feature_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    17759 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/stats_generator.py
+-rw-rw-rw-  2.0 fat    14681 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/time_stats_generator.py
+-rw-rw-rw-  2.0 fat    14815 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/time_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    12299 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/top_k_uniques_sketch_stats_generator.py
+-rw-rw-rw-  2.0 fat    45015 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/top_k_uniques_sketch_stats_generator_test.py
+-rw-rw-rw-  2.0 fat    14330 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/top_k_uniques_stats_generator.py
+-rw-rw-rw-  2.0 fat    43317 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/top_k_uniques_stats_generator_test.py
+-rw-rw-rw-  2.0 fat     4738 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/weighted_feature_stats_generator.py
+-rw-rw-rw-  2.0 fat     9777 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/weighted_feature_stats_generator_test.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/constituents/__init__.py
+-rw-rw-rw-  2.0 fat     4247 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/constituents/count_missing_generator.py
+-rw-rw-rw-  2.0 fat     3272 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/constituents/count_missing_generator_test.py
+-rw-rw-rw-  2.0 fat     5738 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/constituents/length_diff_generator.py
+-rw-rw-rw-  2.0 fat     5921 b- defN 22-Jun-29 17:42 tensorflow_data_validation/statistics/generators/constituents/length_diff_generator_test.py
+-rw-rw-rw-  2.0 fat      590 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6029 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/anomalies_util.py
+-rw-rw-rw-  2.0 fat    22706 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/anomalies_util_test.py
+-rw-rw-rw-  2.0 fat     2001 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/batch_util.py
+-rw-rw-rw-  2.0 fat     2549 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/batch_util_test.py
+-rw-rw-rw-  2.0 fat     3923 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/bin_util.py
+-rw-rw-rw-  2.0 fat     2014 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/bin_util_test.py
+-rw-rw-rw-  2.0 fat    23064 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/display_util.py
+-rw-rw-rw-  2.0 fat    27977 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/display_util_test.py
+-rw-rw-rw-  2.0 fat     2328 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/example_weight_map.py
+-rw-rw-rw-  2.0 fat     2230 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/example_weight_map_test.py
+-rw-rw-rw-  2.0 fat     7020 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/feature_partition_util.py
+-rw-rw-rw-  2.0 fat    10594 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/feature_partition_util_test.py
+-rw-rw-rw-  2.0 fat     4211 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/io_util.py
+-rw-rw-rw-  2.0 fat     2212 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/io_util_test.py
+-rw-rw-rw-  2.0 fat    26635 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/mutual_information_util.py
+-rw-rw-rw-  2.0 fat    17807 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/mutual_information_util_test.py
+-rw-rw-rw-  2.0 fat      806 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/preprocessing_util.py
+-rw-rw-rw-  2.0 fat    13141 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/quantiles_util.py
+-rw-rw-rw-  2.0 fat    12699 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/quantiles_util_test.py
+-rw-rw-rw-  2.0 fat    12874 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/schema_util.py
+-rw-rw-rw-  2.0 fat    19682 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/schema_util_test.py
+-rw-rw-rw-  2.0 fat    12322 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/slicing_util.py
+-rw-rw-rw-  2.0 fat    11529 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/slicing_util_test.py
+-rw-rw-rw-  2.0 fat     3396 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/statistics_io_impl.py
+-rw-rw-rw-  2.0 fat     1610 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/statistics_io_impl_test.py
+-rw-rw-rw-  2.0 fat    14740 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/stats_gen_lib.py
+-rw-rw-rw-  2.0 fat    24602 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/stats_gen_lib_test.py
+-rw-rw-rw-  2.0 fat    24096 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/stats_util.py
+-rw-rw-rw-  2.0 fat    19199 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/stats_util_test.py
+-rw-rw-rw-  2.0 fat    19302 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/test_util.py
+-rw-rw-rw-  2.0 fat     9096 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/test_util_test.py
+-rw-rw-rw-  2.0 fat    12271 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/top_k_uniques_stats_util.py
+-rw-rw-rw-  2.0 fat    11756 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/top_k_uniques_stats_util_test.py
+-rw-rw-rw-  2.0 fat    13362 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/validation_lib.py
+-rw-rw-rw-  2.0 fat    18748 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/validation_lib_test.py
+-rw-rw-rw-  2.0 fat     7711 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/variance_util.py
+-rw-rw-rw-  2.0 fat    15746 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/variance_util_test.py
+-rw-rw-rw-  2.0 fat     2144 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/vocab_util.py
+-rw-rw-rw-  2.0 fat     1719 b- defN 22-Jun-29 17:42 tensorflow_data_validation/utils/vocab_util_test.py
+-rw-rw-rw-  2.0 fat    11573 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    17288 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        1 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/namespace_packages.txt
+-rw-rw-rw-  2.0 fat       27 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    14019 b- defN 22-Jun-29 17:45 tensorflow_data_validation-1.9.0.dist-info/RECORD
+119 files, 4282402 bytes uncompressed, 1279796 bytes compressed:  70.1%
```

## zipnote {}

```diff
@@ -264,14 +264,17 @@
 
 Filename: tensorflow_data_validation/utils/mutual_information_util.py
 Comment: 
 
 Filename: tensorflow_data_validation/utils/mutual_information_util_test.py
 Comment: 
 
+Filename: tensorflow_data_validation/utils/preprocessing_util.py
+Comment: 
+
 Filename: tensorflow_data_validation/utils/quantiles_util.py
 Comment: 
 
 Filename: tensorflow_data_validation/utils/quantiles_util_test.py
 Comment: 
 
 Filename: tensorflow_data_validation/utils/schema_util.py
@@ -330,26 +333,26 @@
 
 Filename: tensorflow_data_validation/utils/vocab_util.py
 Comment: 
 
 Filename: tensorflow_data_validation/utils/vocab_util_test.py
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/LICENSE
+Filename: tensorflow_data_validation-1.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/METADATA
+Filename: tensorflow_data_validation-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/WHEEL
+Filename: tensorflow_data_validation-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/namespace_packages.txt
+Filename: tensorflow_data_validation-1.9.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/top_level.txt
+Filename: tensorflow_data_validation-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: tensorflow_data_validation-1.8.0.dist-info/RECORD
+Filename: tensorflow_data_validation-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensorflow_data_validation/__init__.py

```diff
@@ -11,14 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Init module for TensorFlow Data Validation."""
 
 # Import stats API.
+from tensorflow_data_validation.api.stats_api import default_sharded_output_suffix
+from tensorflow_data_validation.api.stats_api import default_sharded_output_supported
 from tensorflow_data_validation.api.stats_api import GenerateStatistics
 from tensorflow_data_validation.api.stats_api import MergeDatasetFeatureStatisticsList
 from tensorflow_data_validation.api.stats_api import WriteStatisticsToBinaryFile
 from tensorflow_data_validation.api.stats_api import WriteStatisticsToRecordsAndBinaryFile
 from tensorflow_data_validation.api.stats_api import WriteStatisticsToTFRecord
 
 # Import validation API.
```

## tensorflow_data_validation/version.py

```diff
@@ -11,8 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Contains the version string of TFDV."""
 
 # Note that setup.py uses this version.
-__version__ = '1.8.0'
+__version__ = '1.9.0'
```

## tensorflow_data_validation/api/stats_api.py

```diff
@@ -104,15 +104,14 @@
                    ) -> Generator[pa.RecordBatch, None, None]:
   """Sample examples at input sampling rate."""
   if random.random() <= sample_rate:
     yield example
 
 
 @beam.typehints.with_input_types(statistics_pb2.DatasetFeatureStatisticsList)
-@beam.typehints.with_output_types(beam.pvalue.PDone)
 class WriteStatisticsToBinaryFile(beam.PTransform):
   """API for writing serialized data statistics to a binary file."""
 
   def __init__(self, output_path: Text) -> None:
     """Initializes the transform.
 
     Args:
@@ -129,15 +128,14 @@
                 shard_name_template='',
                 append_trailing_newlines=False,
                 coder=beam.coders.ProtoCoder(
                     statistics_pb2.DatasetFeatureStatisticsList)))
 
 
 @beam.typehints.with_input_types(statistics_pb2.DatasetFeatureStatisticsList)
-@beam.typehints.with_output_types(beam.pvalue.PDone)
 class WriteStatisticsToTFRecord(beam.PTransform):
   """API for writing serialized data statistics to TFRecord file."""
 
   def __init__(self, output_path: Text, sharded_output=False) -> None:
     """Initializes the transform.
 
     Args:
@@ -165,15 +163,14 @@
 
   def expand(self, stats: beam.PCollection):
     return stats | 'MergeDatasetFeatureStatisticsProtos' >> beam.CombineGlobally(
                 merge_util.merge_dataset_feature_statistics_list)
 
 
 @beam.typehints.with_input_types(statistics_pb2.DatasetFeatureStatisticsList)
-@beam.typehints.with_output_types(beam.pvalue.PDone)
 class WriteStatisticsToRecordsAndBinaryFile(beam.PTransform):
   """API for writing statistics to both sharded records and binary pb.
 
   This PTransform assumes that input represents sharded statistics, which are
   written directly. These statistics are also merged and written to a binary
   proto.
 
@@ -209,7 +206,17 @@
         stats | 'WriteShardedStats' >> self._io_provider.record_sink_impl(
             output_path_prefix=self._records_path_prefix))
     return (stats
             | 'MergeDatasetFeatureStatisticsProtos' >> beam.CombineGlobally(
                 merge_util.merge_dataset_feature_statistics_list)
             | 'WriteBinaryStats' >> WriteStatisticsToBinaryFile(
                 self._binary_proto_path))
+
+
+def default_sharded_output_supported() -> bool:
+  """True if sharded output is supported by default."""
+  return statistics_io_impl.should_write_sharded()
+
+
+def default_sharded_output_suffix() -> str:
+  """Returns the default sharded output suffix."""
+  return statistics_io_impl.get_io_provider().file_suffix()
```

## tensorflow_data_validation/api/validation_api.py

```diff
@@ -680,15 +680,14 @@
             'DetectFeatureSkew' >> feature_skew_detector.DetectFeatureSkewImpl(
                 self._identifier_features, self._features_to_ignore,
                 self._sample_size, self._float_round_ndigits,
                 self._allow_duplicate_identifiers))
 
 
 @beam.typehints.with_input_types(feature_skew_results_pb2.FeatureSkew)
-@beam.typehints.with_output_types(beam.pvalue.PDone)
 class WriteFeatureSkewResultsToTFRecord(beam.PTransform):
   """API for writing serialized feature skew results to a TFRecord file."""
 
   def __init__(self, output_path: str) -> None:
     """Initializes the transform.
 
     Args:
@@ -702,15 +701,14 @@
                 self._output_path,
                 shard_name_template='',
                 coder=beam.coders.ProtoCoder(
                     feature_skew_results_pb2.FeatureSkew)))
 
 
 @beam.typehints.with_input_types(feature_skew_results_pb2.SkewPair)
-@beam.typehints.with_output_types(beam.pvalue.PDone)
 class WriteSkewPairsToTFRecord(beam.PTransform):
   """API for writing serialized skew pairs to a TFRecord file."""
 
   def __init__(self, output_path: str) -> None:
     """Initializes the transform.
 
     Args:
```

## tensorflow_data_validation/api/validation_api_test.py

```diff
@@ -3034,24 +3034,24 @@
        """, tf.train.Example())
     ]
 
     expected_feature_skew_result = [
         text_format.Parse(
             """
         feature_name: 'feature_a'
-        training_count: 2
-        serving_count: 1
+        base_count: 2
+        test_count: 1
         match_count: 1
-        training_only: 1
+        base_only: 1
         diff_count: 1""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'feature_b'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 1
         mismatch_count: 1
         diff_count: 1""", feature_skew_results_pb2.FeatureSkew())
     ]
 
     with beam.Pipeline() as p:
       training_data = p | 'CreateTraining' >> beam.Create(training_data)
@@ -3068,23 +3068,23 @@
       util.assert_that(skew_sample, util.is_not_empty(), 'CheckSkewSample')
 
   def test_write_feature_skew_results_to_tf_record(self):
     feature_skew_results = [
         text_format.Parse(
             """
         feature_name: 'skewed'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         mismatch_count: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'no_skew'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 2""", feature_skew_results_pb2.FeatureSkew())
     ]
     output_path = os.path.join(tempfile.mkdtemp(), 'feature_skew')
     with beam.Pipeline() as p:
       _ = (
           p | beam.Create(feature_skew_results)
           | validation_api.WriteFeatureSkewResultsToTFRecord(output_path))
@@ -3096,27 +3096,27 @@
     self._assert_feature_skew_results_protos_equal(skew_results_from_file,
                                                    feature_skew_results)
 
   def test_write_skew_pairs_to_tf_record(self):
     skew_pairs = [
         text_format.Parse(
             """
-            training {
+            base {
               features {
                 feature {
                   key: 'id'
                   value { bytes_list { value: [ 'id_feature' ] } }
                 }
               feature {
                   key: 'feature_a'
                   value { float_list { value: [ 10.0 ] } }
               }
              }
             }
-            serving {
+            test {
               features {
                 feature {
                   key: 'id'
                   value { bytes_list { value: [ 'id_feature' ] } }
                 }
               feature {
                   key: 'feature_a'
@@ -3124,35 +3124,35 @@
                 }
              }
            }
             mismatched_features : [ 'feature_a' ]
             """, feature_skew_results_pb2.SkewPair()),
         text_format.Parse(
             """
-            training {
+            base {
               features {
                 feature {
                   key: 'id'
                   value { bytes_list { value: [ 'id_feature' ] } }
                   }
                 feature {
                     key: 'feature_a'
                     value { int64_list { value: [ 5 ] } }
                   }
                }
            }
-           serving {
+           test {
              features {
                 feature {
                   key: 'id'
                   value { bytes_list { value: [ 'id_feature' ] } }
                 }
              }
            }
-           training_only_features: [ 'feature_a' ]
+           base_only_features: [ 'feature_a' ]
                """, feature_skew_results_pb2.SkewPair())
     ]
     output_path = os.path.join(tempfile.mkdtemp(), 'skew_pairs')
     with beam.Pipeline() as p:
       _ = (
           p | beam.Create(skew_pairs)
           | validation_api.WriteSkewPairsToTFRecord(output_path))
```

## tensorflow_data_validation/arrow/arrow_util.py

```diff
@@ -142,15 +142,15 @@
   """Retrieve a nested array (and optionally example indices) from RecordBatch.
 
   This function has the same assumption over `record_batch` as
   `enumerate_arrays()` does.
 
   If the provided path refers to a leaf in the `record_batch`, then a
   "nested_list" will be returned. If the provided path does not refer to a leaf,
-  a "struct" with be returned.
+  a "struct" will be returned.
 
   See `enumerate_arrays()` for definition of "nested_list" and "struct".
 
   Args:
     record_batch: The RecordBatch whose arrays to be visited.
     query_path: The FeaturePath to lookup in the record_batch.
     return_example_indices: Whether to return an additional array containing the
@@ -159,15 +159,15 @@
     wrap_flat_struct_in_list: if True, and if the query_path leads to a
       struct<[Ts]> array, it will be wrapped in a list array, where each
       sub-list contains one element. Caller can make use of this option to
       assume this function always returns a list<inner_type>.
 
   Returns:
     A tuple. The first term is the feature array and the second term is the
-    example_indeices array for the feature array (i.e. array[i] came from the
+    example_indices array for the feature array (i.e. array[i] came from the
     example at row example_indices[i] in the record_batch.).
 
   Raises:
     KeyError: When the query_path is empty, or cannot be found in the
     record_batch and its nested struct arrays.
   """
```

## tensorflow_data_validation/skew/feature_skew_detector.py

```diff
@@ -7,28 +7,28 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Finds feature skew between training and serving examples.
+"""Finds feature skew between baseline and test examples.
 
-Feature skew is detected by joining training and serving examples on a
+Feature skew is detected by joining baseline and test examples on a
 fingerprint computed based on the provided identifier features. For each pair,
-the feature skew detector compares the fingerprint of each training feature
-value to the fingerprint of the corresponding serving feature value.
+the feature skew detector compares the fingerprint of each baseline feature
+value to the fingerprint of the corresponding test feature value.
 
-If there is a mismatch in feature values, if the feature is only in the training
-example, or if the feature is only in the serving example, feature skew is
+If there is a mismatch in feature values, if the feature is only in the baseline
+example, or if the feature is only in the test example, feature skew is
 reported in the skew results and (optionally) a skew sample is output with
-training-serving example pairs that exhibit the feature skew.
+baseline-test example pairs that exhibit the feature skew.
 
 For example, given the following examples with an identifier feature of 'id':
-Training
+Baseline
   features {
     feature {
       key: "id"
       value { bytes_list {
         value: "id_1"
       }
     }
@@ -37,15 +37,15 @@
       value { float_list {
         value: 1.0
         value: 2.0
       }}
     }
   }
 
-Serving
+Test
   features {
     feature {
       key: "id"
       value { bytes_list {
         value: "id_1"
       }
     }
@@ -56,32 +56,32 @@
         value: 3.0
       }}
     }
   }
 
 The following feature skew will be detected:
   feature_name: "float_values"
-  training_count: 1
-  serving_count: 1
+  baseline_count: 1
+  test_count: 1
   mismatch_count: 1
   diff_count: 1
 """
 
 from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
 
 import apache_beam as beam
 import farmhash
 import tensorflow as tf
 from tensorflow_data_validation import constants
 from tensorflow_data_validation import types
 from tensorflow_data_validation.skew.protos import feature_skew_results_pb2
 
 
-_TRAINING_KEY = "training"
-_SERVING_KEY = "serving"
+_BASELINE_KEY = "base"
+_TEST_KEY = "test"
 
 _EXAMPLES_WITH_MISSING_IDENTIFIER_COUNTER = beam.metrics.Metrics.counter(
     constants.METRICS_NAMESPACE, "examples_with_missing_identifier_features")
 
 _PerFeatureSkew = List[Tuple[str, feature_skew_results_pb2.FeatureSkew]]
 _PairOrFeatureSkew = Union[feature_skew_results_pb2.SkewPair,
                            Tuple[str, feature_skew_results_pb2.FeatureSkew]]
@@ -112,86 +112,89 @@
             round(value, float_round_ndigits))
       return str(rounded_feature.SerializePartialToString(deterministic=True))
   else:
     raise ValueError("Unknown feature type detected: %s" % kind)
 
 
 def _compute_skew_for_features(
-    training_feature: tf.train.Feature, serving_feature: tf.train.Feature,
+    base_feature: tf.train.Feature, test_feature: tf.train.Feature,
     float_round_ndigits: Optional[int],
     feature_name: str) -> feature_skew_results_pb2.FeatureSkew:
-  """Computes feature skew for a pair of training and serving features.
+  """Computes feature skew for a pair of baseline and test features.
 
   Args:
-    training_feature: The feature to compare from the training example.
-    serving_feature: The feature to compare from the serving example.
+    base_feature: The feature to compare from the baseline example.
+    test_feature: The feature to compare from the test example.
     float_round_ndigits: Number of digits precision after the decimal point to
       which to round float values before comparison.
     feature_name: The name of the feature for which to compute skew between the
       examples.
 
   Returns:
     A FeatureSkew proto containing information about skew for the specified
       feature.
   """
   skew_results = feature_skew_results_pb2.FeatureSkew()
   skew_results.feature_name = feature_name
-  if training_feature is not None and serving_feature is not None:
-    skew_results.training_count = 1
-    skew_results.serving_count = 1
+  if not _empty_or_null(base_feature) and not _empty_or_null(test_feature):
+    skew_results.base_count = 1
+    skew_results.test_count = 1
     if (farmhash.fingerprint64(
-        _get_serialized_feature(training_feature,
+        _get_serialized_feature(base_feature,
                                 float_round_ndigits)) == farmhash.fingerprint64(
                                     _get_serialized_feature(
-                                        serving_feature, float_round_ndigits))):
+                                        test_feature, float_round_ndigits))):
       skew_results.match_count = 1
     else:
       skew_results.mismatch_count = 1
-  elif training_feature is not None:
-    skew_results.training_count = 1
-    skew_results.training_only = 1
-  elif serving_feature is not None:
-    skew_results.serving_count = 1
-    skew_results.serving_only = 1
+  elif not _empty_or_null(base_feature):
+    skew_results.base_count = 1
+    skew_results.base_only = 1
+  elif not _empty_or_null(test_feature):
+    skew_results.test_count = 1
+    skew_results.test_only = 1
+  elif (test_feature is None) == (base_feature is None):
+    # Both features are None, or present with zero values.
+    skew_results.match_count = 1
   return skew_results
 
 
 def _compute_skew_for_examples(
-    training_example: tf.train.Example, serving_example: tf.train.Example,
+    base_example: tf.train.Example, test_example: tf.train.Example,
     features_to_ignore: List[tf.train.Feature],
     float_round_ndigits: Optional[int]) -> Tuple[_PerFeatureSkew, bool]:
-  """Computes feature skew for a pair of training and serving examples.
+  """Computes feature skew for a pair of baseline and test examples.
 
   Args:
-    training_example: The training example to compare.
-    serving_example: The serving example to compare.
+    base_example: The baseline example to compare.
+    test_example: The test example to compare.
     features_to_ignore: The features not to compare.
     float_round_ndigits: Number of digits precision after the decimal point to
       which to round float values before comparison.
 
   Returns:
     A tuple containing a list of the skew information for each feature
     and a boolean indicating whether skew was found in any feature, in which
     case the examples are considered skewed.
   """
   all_feature_names = set()
-  all_feature_names.update(training_example.features.feature.keys())
-  all_feature_names.update(serving_example.features.feature.keys())
+  all_feature_names.update(base_example.features.feature.keys())
+  all_feature_names.update(test_example.features.feature.keys())
   feature_names = all_feature_names.difference(set(features_to_ignore))
 
   result = list()
   is_skewed = False
   for name in feature_names:
-    training_feature = training_example.features.feature.get(name)
-    serving_feature = serving_example.features.feature.get(name)
-    skew = _compute_skew_for_features(training_feature, serving_feature,
+    base_feature = base_example.features.feature.get(name)
+    test_feature = test_example.features.feature.get(name)
+    skew = _compute_skew_for_features(base_feature, test_feature,
                                       float_round_ndigits, name)
     if skew.match_count == 0:
-      # If any features have a mismatch or are found only in the training or
-      # serving example, the examples are considered skewed.
+      # If any features have a mismatch or are found only in the baseline or
+      # test example, the examples are considered skewed.
       is_skewed = True
     result.append((name, skew))
   return result, is_skewed
 
 
 def _merge_feature_skew_results(
     skew_results: Iterable[feature_skew_results_pb2.FeatureSkew]
@@ -206,56 +209,66 @@
   """
   result = feature_skew_results_pb2.FeatureSkew()
   for skew_result in skew_results:
     if not result.feature_name:
       result.feature_name = skew_result.feature_name
     elif result.feature_name != skew_result.feature_name:
       raise ValueError("Attempting to merge skew results with different names.")
-    result.training_count += skew_result.training_count
-    result.serving_count += skew_result.serving_count
+    result.base_count += skew_result.base_count
+    result.test_count += skew_result.test_count
     result.match_count += skew_result.match_count
-    result.training_only += skew_result.training_only
-    result.serving_only += skew_result.serving_only
+    result.base_only += skew_result.base_only
+    result.test_only += skew_result.test_only
     result.mismatch_count += skew_result.mismatch_count
   result.diff_count = (
-      result.training_only + result.serving_only + result.mismatch_count)
+      result.base_only + result.test_only + result.mismatch_count)
   return result
 
 
 def _construct_skew_pair(
     per_feature_skew: List[Tuple[str, feature_skew_results_pb2.FeatureSkew]],
-    training_example: tf.train.Example,
-    serving_example: tf.train.Example) -> feature_skew_results_pb2.SkewPair:
-  """Constructs a SkewPair from training and serving examples.
+    base_example: tf.train.Example,
+    test_example: tf.train.Example) -> feature_skew_results_pb2.SkewPair:
+  """Constructs a SkewPair from baseline and test examples.
 
   Args:
     per_feature_skew: Skew results for each feature in the input examples.
-    training_example: The training example to include.
-    serving_example: The serving example to include.
+    base_example: The baseline example to include.
+    test_example: The test example to include.
 
   Returns:
     A SkewPair containing examples that exhibit some skew.
   """
   skew_pair = feature_skew_results_pb2.SkewPair()
-  skew_pair.training.CopyFrom(training_example)
-  skew_pair.serving.CopyFrom(serving_example)
+  skew_pair.base.CopyFrom(base_example)
+  skew_pair.test.CopyFrom(test_example)
 
   for feature_name, skew_result in per_feature_skew:
     if skew_result.match_count == 1:
       skew_pair.matched_features.append(feature_name)
-    elif skew_result.training_only == 1:
-      skew_pair.training_only_features.append(feature_name)
-    elif skew_result.serving_only == 1:
-      skew_pair.serving_only_features.append(feature_name)
+    elif skew_result.base_only == 1:
+      skew_pair.base_only_features.append(feature_name)
+    elif skew_result.test_only == 1:
+      skew_pair.test_only_features.append(feature_name)
     elif skew_result.mismatch_count == 1:
       skew_pair.mismatched_features.append(feature_name)
 
   return skew_pair
 
 
+def _empty_or_null(feature: Optional[tf.train.Feature]) -> bool:
+  """True if feature is None or holds no values."""
+  if feature is None:
+    return True
+  if len(feature.bytes_list.value) + len(feature.int64_list.value) + len(
+      feature.float_list.value) == 0:
+    return True
+  return False
+
+
 class _ExtractIdentifiers(beam.DoFn):
   """DoFn that extracts a unique fingerprint for each example.
 
   This class computes fingerprints by combining the identifier features.
   """
 
   def __init__(self, identifier_features: List[types.FeatureName],
@@ -273,15 +286,15 @@
 
   def process(
       self,
       example: tf.train.Example) -> Iterable[Tuple[str, tf.train.Example]]:
     serialized_feature_values = []
     for identifier_feature in self._identifier_features:
       feature = example.features.feature.get(identifier_feature)
-      if feature is None:
+      if _empty_or_null(feature):
         _EXAMPLES_WITH_MISSING_IDENTIFIER_COUNTER.inc()
         return
       else:
         serialized_feature_values.append(
             _get_serialized_feature(feature, self._float_round_ndigits))
     yield (str(farmhash.fingerprint64("".join(serialized_feature_values))),
            example)
@@ -298,54 +311,54 @@
     Args:
       features_to_ignore: Names of features that are ignored in skew detection.
       float_round_ndigits: Number of digits precision after the decimal point to
         which to round float values before detecting skew.
       allow_duplicate_identifiers: If set, skew detection will be done on
         examples for which there are duplicate identifier feature values. In
         this case, the counts in the FeatureSkew result are based on each
-        training-serving example pair analyzed. Examples with given identifier
+        baseline-test example pair analyzed. Examples with given identifier
         feature values must all fit in memory.
     """
     self._features_to_ignore = features_to_ignore
     self._float_round_ndigits = float_round_ndigits
     self._allow_duplicate_identifiers = allow_duplicate_identifiers
     self._skipped_duplicate_identifiers = beam.metrics.Metrics.counter(
         constants.METRICS_NAMESPACE, "skipped_duplicate_identifier")
 
   def process(
       self, element: Tuple[str,
                            Dict[str,
                                 List[Any]]]) -> Iterable[_PairOrFeatureSkew]:
     (_, examples) = element
-    training_examples = examples.get(_TRAINING_KEY)
-    serving_examples = examples.get(_SERVING_KEY)
+    base_examples = examples.get(_BASELINE_KEY)
+    test_examples = examples.get(_TEST_KEY)
     if not self._allow_duplicate_identifiers:
-      if len(training_examples) > 1 or len(serving_examples) > 1:
+      if len(base_examples) > 1 or len(test_examples) > 1:
         self._skipped_duplicate_identifiers.inc(1)
         return
-    if training_examples and serving_examples:
-      for training_example in training_examples:
-        for serving_example in serving_examples:
+    if base_examples and test_examples:
+      for base_example in base_examples:
+        for test_example in test_examples:
           result, is_skewed = _compute_skew_for_examples(
-              training_example, serving_example, self._features_to_ignore,
+              base_example, test_example, self._features_to_ignore,
               self._float_round_ndigits)
           if is_skewed:
-            skew_pair = _construct_skew_pair(result, training_example,
-                                             serving_example)
+            skew_pair = _construct_skew_pair(result, base_example,
+                                             test_example)
             yield beam.pvalue.TaggedOutput("skew_pairs", skew_pair)
           for each in result:
             yield beam.pvalue.TaggedOutput("skew_results", each)
 
 
 class DetectFeatureSkewImpl(beam.PTransform):
-  """Identifies feature skew in training and serving examples.
+  """Identifies feature skew in baseline and test examples.
 
   This PTransform returns a tuple of PCollections containing:
-    1. Aggregated skew statistics (containing, e.g., mismatch count, training
-       only, serving only) for each feature; and
+    1. Aggregated skew statistics (containing, e.g., mismatch count, baseline
+       only, test only) for each feature; and
     2. A sample of skewed example pairs (if sample_size is > 0).
   """
 
   def __init__(self,
                identifier_features: List[types.FeatureName],
                features_to_ignore: Optional[List[types.FeatureName]] = None,
                sample_size: int = 0,
@@ -354,22 +367,22 @@
     """Initializes DetectFeatureSkewImpl.
 
     Args:
       identifier_features: The names of the features to use to identify an
         example.
       features_to_ignore: The names of the features for which skew detection is
         not done.
-      sample_size: Size of the sample of training-serving example pairs that
+      sample_size: Size of the sample of baseline-test example pairs that
         exhibit skew to include in the skew results.
       float_round_ndigits: Number of digits of precision after the decimal point
         to which to round float values before detecting skew.
       allow_duplicate_identifiers: If set, skew detection will be done on
         examples for which there are duplicate identifier feature values. In
         this case, the counts in the FeatureSkew result are based on each
-        training-serving example pair analyzed. Examples with given identifier
+        baseline-test example pair analyzed. Examples with given identifier
         feature values must all fit in memory.
     """
     if not identifier_features:
       raise ValueError("At least one feature name must be specified in "
                        "identifier_features.")
     self._identifier_features = identifier_features
     self._sample_size = sample_size
@@ -380,27 +393,27 @@
       self._features_to_ignore = identifier_features
     self._allow_duplicate_identifiers = allow_duplicate_identifiers
 
   def expand(
       self, pcollections: Tuple[beam.pvalue.PCollection,
                                 beam.pvalue.PCollection]
   ) -> Tuple[beam.pvalue.PCollection, beam.pvalue.PCollection]:
-    training_examples, serving_examples = pcollections
-    keyed_training_examples = (
-        training_examples | "ExtractTrainingIdentifiers" >> beam.ParDo(
+    base_examples, test_examples = pcollections
+    keyed_base_examples = (
+        base_examples | "ExtractBaseIdentifiers" >> beam.ParDo(
             _ExtractIdentifiers(self._identifier_features,
                                 self._float_round_ndigits)))
-    keyed_serving_examples = (
-        serving_examples | "ExtractServingIdentifiers" >> beam.ParDo(
+    keyed_test_examples = (
+        test_examples | "ExtractTestIdentifiers" >> beam.ParDo(
             _ExtractIdentifiers(self._identifier_features,
                                 self._float_round_ndigits)))
     results = (
         {
-            "training": keyed_training_examples,
-            "serving": keyed_serving_examples
+            "base": keyed_base_examples,
+            "test": keyed_test_examples
         } | "JoinExamples" >> beam.CoGroupByKey()
         | "ComputeSkew" >> beam.ParDo(
             _ComputeSkew(self._features_to_ignore, self._float_round_ndigits,
                          self._allow_duplicate_identifiers)).with_outputs(
                              "skew_results", "skew_pairs"))
     skew_results = (
         results.skew_results | "MergeSkewResultsPerFeature" >>  # pytype: disable=attribute-error
```

## tensorflow_data_validation/skew/feature_skew_detector_test.py

```diff
@@ -28,23 +28,23 @@
 # Ranges of values for identifier features.
 _IDENTIFIER_RANGE = 2
 # Names of identifier features.
 _IDENTIFIER1 = 'id1'
 _IDENTIFIER2 = 'id2'
 # Name of feature that is skewed in the test data.
 _SKEW_FEATURE = 'skewed'
-# Name of feature that appears only in the training data and not serving.
-_TRAINING_ONLY_FEATURE = 'training_only'
-# Name of feature that appears only in the serving data and not training.
-_SERVING_ONLY_FEATURE = 'serving_only'
-# Name of feature that has the same value in both training and serving data.
+# Name of feature that appears only in the base data and not test.
+_BASE_ONLY_FEATURE = 'base_only'
+# Name of feature that appears only in the test data and not base.
+_TEST_ONLY_FEATURE = 'test_only'
+# Name of feature that has the same value in both base and test data.
 _NO_SKEW_FEATURE = 'no_skew'
 # Name of feature that has skew but should be ignored.
 _IGNORE_FEATURE = 'ignore'
-# Name of float feature that has values that are close in training and serving
+# Name of float feature that has values that are close in base and test
 # data.
 _CLOSE_FLOAT_FEATURE = 'close_float'
 
 
 def make_sample_equal_fn(test, expected_size, potential_samples):
   """Makes a matcher function for checking SkewPair results."""
 
@@ -56,341 +56,333 @@
     except AssertionError:
       raise util.BeamAssertException(traceback.format_exc())
 
   return _matcher
 
 
 def get_test_input(include_skewed_features, include_close_floats):
-  training_examples = list()
-  serving_examples = list()
+  baseline_examples = list()
+  test_examples = list()
   skew_pairs = list()
   for i in range(_IDENTIFIER_RANGE):
     for j in range(_IDENTIFIER_RANGE):
-      base_example = tf.train.Example()
-      base_example.features.feature[_IDENTIFIER1].int64_list.value.append(i)
-      base_example.features.feature[_IDENTIFIER2].int64_list.value.append(j)
-      base_example.features.feature[_NO_SKEW_FEATURE].int64_list.value.append(1)
-
-      training_example = tf.train.Example()
-      training_example.CopyFrom(base_example)
-      serving_example = tf.train.Example()
-      serving_example.CopyFrom(base_example)
-
-      training_example.features.feature[
-          _IGNORE_FEATURE].int64_list.value.append(0)
-      serving_example.features.feature[_IGNORE_FEATURE].int64_list.value.append(
+      shared_example = tf.train.Example()
+      shared_example.features.feature[_IDENTIFIER1].int64_list.value.append(i)
+      shared_example.features.feature[_IDENTIFIER2].int64_list.value.append(j)
+      shared_example.features.feature[_NO_SKEW_FEATURE].int64_list.value.append(
           1)
 
+      base_example = tf.train.Example()
+      base_example.CopyFrom(shared_example)
+      test_example = tf.train.Example()
+      test_example.CopyFrom(shared_example)
+
+      base_example.features.feature[_IGNORE_FEATURE].int64_list.value.append(0)
+      test_example.features.feature[_IGNORE_FEATURE].int64_list.value.append(1)
+
     if include_close_floats:
-      training_example.features.feature[
+      base_example.features.feature[
           _CLOSE_FLOAT_FEATURE].float_list.value.append(1.12345)
-      serving_example.features.feature[
+      test_example.features.feature[
           _CLOSE_FLOAT_FEATURE].float_list.value.append(1.12456)
 
     if include_skewed_features:
       # Add three different kinds of skew: value mismatch, appears only in
-      # training, and appears only in serving.
-      training_example.features.feature[_SKEW_FEATURE].int64_list.value.append(
+      # base, and appears only in test.
+      base_example.features.feature[_SKEW_FEATURE].int64_list.value.append(0)
+      test_example.features.feature[_SKEW_FEATURE].int64_list.value.append(1)
+      base_example.features.feature[_BASE_ONLY_FEATURE].int64_list.value.append(
           0)
-      serving_example.features.feature[_SKEW_FEATURE].int64_list.value.append(1)
-      training_example.features.feature[
-          _TRAINING_ONLY_FEATURE].int64_list.value.append(0)
-      serving_example.features.feature[
-          _SERVING_ONLY_FEATURE].int64_list.value.append(1)
+      test_example.features.feature[_TEST_ONLY_FEATURE].int64_list.value.append(
+          1)
 
       skew_pair = feature_skew_results_pb2.SkewPair()
-      skew_pair.training.CopyFrom(training_example)
-      skew_pair.serving.CopyFrom(serving_example)
+      skew_pair.base.CopyFrom(base_example)
+      skew_pair.test.CopyFrom(test_example)
       skew_pair.matched_features.append(_NO_SKEW_FEATURE)
       skew_pair.mismatched_features.append(_SKEW_FEATURE)
-      skew_pair.training_only_features.append(_TRAINING_ONLY_FEATURE)
-      skew_pair.serving_only_features.append(_SERVING_ONLY_FEATURE)
+      skew_pair.base_only_features.append(_BASE_ONLY_FEATURE)
+      skew_pair.test_only_features.append(_TEST_ONLY_FEATURE)
       skew_pairs.append(skew_pair)
 
-    training_examples.append(training_example)
-    serving_examples.append(serving_example)
-  return (training_examples, serving_examples, skew_pairs)
+    baseline_examples.append(base_example)
+    test_examples.append(test_example)
+  return (baseline_examples, test_examples, skew_pairs)
 
 
 class FeatureSkewDetectorTest(absltest.TestCase):
 
   def test_detect_feature_skew(self):
-    training_examples, serving_examples, _ = get_test_input(
+    baseline_examples, test_examples, _ = get_test_input(
         include_skewed_features=True, include_close_floats=True)
 
     expected_result = [
         text_format.Parse(
             """
         feature_name: 'close_float'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         mismatch_count: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'skewed'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         mismatch_count: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
-        feature_name: 'training_only'
-        training_count: 2
-        training_only: 2
+        feature_name: 'base_only'
+        base_count: 2
+        base_only: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
-        feature_name: 'serving_only'
-        serving_count: 2
-        serving_only: 2
+        feature_name: 'test_only'
+        test_count: 2
+        test_only: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'no_skew'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 2
         diff_count: 0""", feature_skew_results_pb2.FeatureSkew()),
     ]
 
     with beam.Pipeline() as p:
-      training_examples = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples = p | 'Create Serving' >> beam.Create(serving_examples)
-      skew_result, _ = ((training_examples, serving_examples)
+      baseline_examples = p | 'Create Base' >> beam.Create(baseline_examples)
+      test_examples = p | 'Create Test' >> beam.Create(test_examples)
+      skew_result, _ = ((baseline_examples, test_examples)
                         | feature_skew_detector.DetectFeatureSkewImpl(
                             [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE]))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self, expected_result))
 
   def test_detect_no_skew(self):
-    training_examples, serving_examples, _ = get_test_input(
+    baseline_examples, test_examples, _ = get_test_input(
         include_skewed_features=False, include_close_floats=False)
 
     expected_result = [
         text_format.Parse(
             """
         feature_name: 'no_skew'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 2
         diff_count: 0""", feature_skew_results_pb2.FeatureSkew()),
     ]
 
     with beam.Pipeline() as p:
-      training_examples = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples = p | 'Create Serving' >> beam.Create(serving_examples)
+      baseline_examples = p | 'Create Baseline' >> beam.Create(
+          baseline_examples)
+      test_examples = p | 'Create Test' >> beam.Create(test_examples)
       skew_result, skew_sample = (
-          (training_examples, serving_examples)
+          (baseline_examples, test_examples)
           | feature_skew_detector.DetectFeatureSkewImpl(
               [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE], sample_size=2))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self, expected_result),
           'CheckSkewResult')
       util.assert_that(skew_sample, make_sample_equal_fn(self, 0, []),
                        'CheckSkewSample')
 
   def test_obtain_skew_sample(self):
-    training_examples, serving_examples, skew_pairs = get_test_input(
+    baseline_examples, test_examples, skew_pairs = get_test_input(
         include_skewed_features=True, include_close_floats=False)
 
     sample_size = 1
     potential_samples = skew_pairs
     with beam.Pipeline() as p:
-      training_examples = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples = p | 'Create Serving' >> beam.Create(serving_examples)
+      baseline_examples = p | 'Create Base' >> beam.Create(baseline_examples)
+      test_examples = p | 'Create Test' >> beam.Create(test_examples)
       _, skew_sample = (
-          (training_examples, serving_examples)
+          (baseline_examples, test_examples)
           | feature_skew_detector.DetectFeatureSkewImpl(
               [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE], sample_size))
       util.assert_that(
           skew_sample, make_sample_equal_fn(self, sample_size,
                                             potential_samples))
 
   def test_empty_inputs(self):
-    training_examples, serving_examples, _ = get_test_input(
+    baseline_examples, test_examples, _ = get_test_input(
         include_skewed_features=True, include_close_floats=True)
 
     # Expect no skew results or sample in each case.
     expected_result = list()
 
-    # Empty training collection.
+    # Empty base collection.
     with beam.Pipeline() as p:
-      training_examples_1 = p | 'Create Training' >> beam.Create([])
-      serving_examples_1 = p | 'Create Serving' >> beam.Create(serving_examples)
+      baseline_examples_1 = p | 'Create Base' >> beam.Create([])
+      test_examples_1 = p | 'Create Test' >> beam.Create(test_examples)
       skew_result_1, skew_sample_1 = (
-          (training_examples_1, serving_examples_1)
+          (baseline_examples_1, test_examples_1)
           | feature_skew_detector.DetectFeatureSkewImpl(
               [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE], sample_size=1))
       util.assert_that(
           skew_result_1,
           test_util.make_skew_result_equal_fn(self, expected_result),
           'CheckSkewResult')
       util.assert_that(skew_sample_1,
                        make_sample_equal_fn(self, 0, expected_result),
                        'CheckSkewSample')
 
-    # Empty serving collection.
+    # Empty test collection.
     with beam.Pipeline() as p:
-      training_examples_2 = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples_2 = p | 'Create Serving' >> beam.Create([])
+      baseline_examples_2 = p | 'Create Base' >> beam.Create(baseline_examples)
+      test_examples_2 = p | 'Create Test' >> beam.Create([])
       skew_result_2, skew_sample_2 = (
-          (training_examples_2, serving_examples_2)
+          (baseline_examples_2, test_examples_2)
           | feature_skew_detector.DetectFeatureSkewImpl(
               [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE], sample_size=1))
       util.assert_that(
           skew_result_2,
           test_util.make_skew_result_equal_fn(self, expected_result),
           'CheckSkewResult')
       util.assert_that(skew_sample_2,
                        make_sample_equal_fn(self, 0, expected_result),
                        'CheckSkewSample')
 
-    # Empty training and serving collections.
+    # Empty base and test collections.
     with beam.Pipeline() as p:
-      training_examples_3 = p | 'Create Training' >> beam.Create([])
-      serving_examples_3 = p | 'Create Serving' >> beam.Create([])
+      baseline_examples_3 = p | 'Create Base' >> beam.Create([])
+      test_examples_3 = p | 'Create Test' >> beam.Create([])
       skew_result_3, skew_sample_3 = (
-          (training_examples_3, serving_examples_3)
+          (baseline_examples_3, test_examples_3)
           | feature_skew_detector.DetectFeatureSkewImpl(
               [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE], sample_size=1))
       util.assert_that(
           skew_result_3,
           test_util.make_skew_result_equal_fn(self, expected_result),
           'CheckSkewResult')
       util.assert_that(skew_sample_3,
                        make_sample_equal_fn(self, 0, expected_result),
                        'CheckSkewSample')
 
   def test_float_precision_configuration(self):
-    training_examples, serving_examples, _ = get_test_input(
+    baseline_examples, test_examples, _ = get_test_input(
         include_skewed_features=True, include_close_floats=True)
 
     expected_result = [
         text_format.Parse(
             """
         feature_name: 'skewed'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         mismatch_count: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
-        feature_name: 'training_only'
-        training_count: 2
-        training_only: 2
+        feature_name: 'base_only'
+        base_count: 2
+        base_only: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
-        feature_name: 'serving_only'
-        serving_count: 2
-        serving_only: 2
+        feature_name: 'test_only'
+        test_count: 2
+        test_only: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'no_skew'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 2""", feature_skew_results_pb2.FeatureSkew()),
     ]
 
     expected_with_float = expected_result + [
         text_format.Parse(
             """
         feature_name: 'close_float'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         mismatch_count: 2
         diff_count: 2""", feature_skew_results_pb2.FeatureSkew())
     ]
 
     # Do not set a float_round_ndigits.
     with beam.Pipeline() as p:
-      training_examples_1 = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples_1 = p | 'Create Serving' >> beam.Create(serving_examples)
-      skew_result, _ = ((training_examples_1, serving_examples_1)
+      baseline_examples_1 = p | 'Create Base' >> beam.Create(baseline_examples)
+      test_examples_1 = p | 'Create Test' >> beam.Create(test_examples)
+      skew_result, _ = ((baseline_examples_1, test_examples_1)
                         | feature_skew_detector.DetectFeatureSkewImpl(
                             [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE],
                             sample_size=1))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self, expected_with_float))
 
     expected_with_float_and_option = expected_result + [
         text_format.Parse(
             """
               feature_name: 'close_float'
-              training_count: 2
-              serving_count: 2
+              base_count: 2
+              test_count: 2
               match_count: 2
               """, feature_skew_results_pb2.FeatureSkew())
     ]
 
     # Set float_round_ndigits
     with beam.Pipeline() as p:
-      training_examples_2 = p | 'Create Training' >> beam.Create(
-          training_examples)
-      serving_examples_2 = p | 'Create Serving' >> beam.Create(serving_examples)
-      skew_result, _ = ((training_examples_2, serving_examples_2)
+      baseline_examples_2 = p | 'Create Base' >> beam.Create(baseline_examples)
+      test_examples_2 = p | 'Create Test' >> beam.Create(test_examples)
+      skew_result, _ = ((baseline_examples_2, test_examples_2)
                         | feature_skew_detector.DetectFeatureSkewImpl(
                             [_IDENTIFIER1, _IDENTIFIER2], [_IGNORE_FEATURE],
                             sample_size=1,
                             float_round_ndigits=2))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self,
                                               expected_with_float_and_option))
 
   def test_no_identifier_features(self):
-    training_examples, serving_examples, _ = get_test_input(
+    baseline_examples, test_examples, _ = get_test_input(
         include_skewed_features=False, include_close_floats=False)
     with self.assertRaisesRegex(ValueError,
                                 'At least one feature name must be specified'):
       with beam.Pipeline() as p:
-        training_examples = p | 'Create Training' >> beam.Create(
-            training_examples)
-        serving_examples = p | 'Create Serving' >> beam.Create(serving_examples)
-        _ = ((training_examples, serving_examples)
+        baseline_examples = p | 'Create Base' >> beam.Create(baseline_examples)
+        test_examples = p | 'Create Test' >> beam.Create(test_examples)
+        _ = ((baseline_examples, test_examples)
              | feature_skew_detector.DetectFeatureSkewImpl([]))
 
   def test_duplicate_identifiers_allowed_with_duplicates(self):
-    training_example_1 = text_format.Parse(
+    base_example_1 = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
             key: "val"
             value { int64_list { value: 100 } }
           }
         }
         """, tf.train.Example())
-    training_example_2 = text_format.Parse(
+    base_example_2 = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
             key: "val"
             value { int64_list { value: 50 } }
           }
         }
         """, tf.train.Example())
-    serving_example = text_format.Parse(
+    test_example = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
@@ -403,65 +395,66 @@
           }
         }
         """, tf.train.Example())
     expected_result = [
         text_format.Parse(
             """
         feature_name: 'val'
-        training_count: 2
-        serving_count: 2
+        base_count: 2
+        test_count: 2
         match_count: 1
         mismatch_count: 1
         diff_count: 1""", feature_skew_results_pb2.FeatureSkew()),
         text_format.Parse(
             """
         feature_name: 'val2'
-        training_count: 0
-        serving_count: 2
-        serving_only: 2
-        diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),]
-    with beam.Pipeline() as p:
-      training_examples = p | 'Create Training' >> beam.Create(
-          [training_example_1, training_example_2])
-      serving_examples = p | 'Create Serving' >> beam.Create([serving_example])
-      skew_result, _ = ((training_examples, serving_examples)
+        base_count: 0
+        test_count: 2
+        test_only: 2
+        diff_count: 2""", feature_skew_results_pb2.FeatureSkew()),
+    ]
+    with beam.Pipeline() as p:
+      baseline_examples = p | 'Create Base' >> beam.Create(
+          [base_example_1, base_example_2])
+      test_examples = p | 'Create Test' >> beam.Create([test_example])
+      skew_result, _ = ((baseline_examples, test_examples)
                         | feature_skew_detector.DetectFeatureSkewImpl(
                             ['id'], [], allow_duplicate_identifiers=True))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self, expected_result))
 
   def test_duplicate_identifiers_not_allowed_with_duplicates(self):
-    training_example_1 = text_format.Parse(
+    base_example_1 = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
             key: "val"
             value { int64_list { value: 100 } }
           }
         }
         """, tf.train.Example())
-    training_example_2 = text_format.Parse(
+    base_example_2 = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
             key: "val"
             value { int64_list { value: 50 } }
           }
         }
         """, tf.train.Example())
-    serving_example = text_format.Parse(
+    test_example = text_format.Parse(
         """
         features {
           feature {
             key: "id"
             value { int64_list { value: 1 } }
           }
           feature {
@@ -471,18 +464,18 @@
           feature {
             key: "val2"
             value { int64_list { value: 100 } }
           }
         }
         """, tf.train.Example())
     with beam.Pipeline() as p:
-      training_examples = p | 'Create Training' >> beam.Create(
-          [training_example_1, training_example_2])
-      serving_examples = p | 'Create Serving' >> beam.Create([serving_example])
-      skew_result, _ = ((training_examples, serving_examples)
+      baseline_examples = p | 'Create Base' >> beam.Create(
+          [base_example_1, base_example_2])
+      test_examples = p | 'Create Test' >> beam.Create([test_example])
+      skew_result, _ = ((baseline_examples, test_examples)
                         | feature_skew_detector.DetectFeatureSkewImpl(
                             ['id'], [], allow_duplicate_identifiers=False))
       util.assert_that(
           skew_result,
           test_util.make_skew_result_equal_fn(self, []))
 
     runner = p.run()
@@ -490,37 +483,168 @@
     result_metrics = runner.metrics()
     actual_counter = result_metrics.query(
         beam.metrics.metric.MetricsFilter().with_name(
             'skipped_duplicate_identifier'))['counters']
     self.assertLen(actual_counter, 1)
     self.assertEqual(actual_counter[0].committed, 1)
 
+  def test_skips_missing_identifier_example(self):
+    base_example_1 = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { }
+          }
+          feature {
+            key: "val"
+            value { int64_list { value: 100 } }
+          }
+        }
+        """, tf.train.Example())
+    test_example = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { int64_list { value: 1 } }
+          }
+          feature {
+            key: "val"
+            value { int64_list { value: 100 } }
+          }
+        }
+        """, tf.train.Example())
+    with beam.Pipeline() as p:
+      baseline_examples = p | 'Create Base' >> beam.Create([base_example_1])
+      test_examples = p | 'Create Test' >> beam.Create([test_example])
+      skew_result, _ = ((baseline_examples, test_examples)
+                        | feature_skew_detector.DetectFeatureSkewImpl(
+                            ['id'], [], allow_duplicate_identifiers=True))
+      util.assert_that(skew_result,
+                       test_util.make_skew_result_equal_fn(self, []))
+
+    runner = p.run()
+    runner.wait_until_finish()
+
+  def test_empty_features_equivalent(self):
+    base_example_1 = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { int64_list { value: 1 } }
+          }
+          feature {
+            key: "val"
+            value {}
+          }
+        }
+        """, tf.train.Example())
+    test_example = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { int64_list { value: 1 } }
+          }
+          feature {
+            key: "val"
+            value {}
+          }
+        }
+        """, tf.train.Example())
+    with beam.Pipeline() as p:
+      baseline_examples = p | 'Create Base' >> beam.Create([base_example_1])
+      test_examples = p | 'Create Test' >> beam.Create([test_example])
+      skew_result, skew_pairs = (
+          (baseline_examples, test_examples)
+          | feature_skew_detector.DetectFeatureSkewImpl(
+              ['id'], [], allow_duplicate_identifiers=True, sample_size=10))
+      expected_result = [
+          text_format.Parse(
+              """
+        feature_name: 'val'
+        match_count: 1
+        """, feature_skew_results_pb2.FeatureSkew()),
+      ]
+      util.assert_that(
+          skew_result,
+          test_util.make_skew_result_equal_fn(self, expected_result))
+      util.assert_that(skew_pairs, self.assertEmpty, label='assert_pairs_empty')
+
+    runner = p.run()
+    runner.wait_until_finish()
+
+  def test_empty_features_not_equivalent_to_missing(self):
+    base_example_1 = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { int64_list { value: 1 } }
+          }
+          feature {
+            key: "val"
+            value {}
+          }
+        }
+        """, tf.train.Example())
+    test_example = text_format.Parse(
+        """
+        features {
+          feature {
+            key: "id"
+            value { int64_list { value: 1 } }
+          }
+        }
+        """, tf.train.Example())
+    with beam.Pipeline() as p:
+      baseline_examples = p | 'Create Base' >> beam.Create([base_example_1])
+      test_examples = p | 'Create Test' >> beam.Create([test_example])
+      skew_result, _ = (
+          (baseline_examples, test_examples)
+          | feature_skew_detector.DetectFeatureSkewImpl(
+              ['id'], [], allow_duplicate_identifiers=True, sample_size=10))
+      expected_result = [
+          text_format.Parse(
+              """
+        feature_name: 'val'
+        """, feature_skew_results_pb2.FeatureSkew()),
+      ]
+      util.assert_that(
+          skew_result,
+          test_util.make_skew_result_equal_fn(self, expected_result))
+
+    runner = p.run()
+    runner.wait_until_finish()
+
   def test_telemetry(self):
-    base_example = tf.train.Example()
-    base_example.features.feature[_IDENTIFIER1].int64_list.value.append(1)
+    shared_example = tf.train.Example()
+    shared_example.features.feature[_IDENTIFIER1].int64_list.value.append(1)
 
-    training_example = tf.train.Example()
-    training_example.CopyFrom(base_example)
-    serving_example = tf.train.Example()
-    serving_example.CopyFrom(base_example)
+    base_example = tf.train.Example()
+    base_example.CopyFrom(shared_example)
+    test_example = tf.train.Example()
+    test_example.CopyFrom(base_example)
 
-    # Add Identifier 2 to training example only.
-    training_example.features.feature[_IDENTIFIER2].int64_list.value.append(2)
+    # Add Identifier 2 to base example only.
+    base_example.features.feature[_IDENTIFIER2].int64_list.value.append(2)
 
     p = beam.Pipeline()
-    training_data = p | 'Create Training' >> beam.Create([training_example])
-    serving_data = p | 'Create Serving' >> beam.Create([serving_example])
-    _, _ = ((training_data, serving_data)
+    baseline_data = p | 'Create Base' >> beam.Create([base_example])
+    test_data = p | 'Create Test' >> beam.Create([test_example])
+    _, _ = ((baseline_data, test_data)
             | feature_skew_detector.DetectFeatureSkewImpl(
                 [_IDENTIFIER1, _IDENTIFIER2]))
     runner = p.run()
     runner.wait_until_finish()
     result_metrics = runner.metrics()
 
-    # Serving example does not have Identifier 2.
+    # Test example does not have Identifier 2.
     actual_counter = result_metrics.query(
         beam.metrics.metric.MetricsFilter().with_name(
             'examples_with_missing_identifier_features'))['counters']
     self.assertLen(actual_counter, 1)
     self.assertEqual(actual_counter[0].committed, 1)
```

## tensorflow_data_validation/skew/protos/feature_skew_results_pb2.py

```diff
@@ -16,15 +16,15 @@
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='tensorflow_data_validation/skew/protos/feature_skew_results.proto',
   package='tensorflow.data_validation',
   syntax='proto3',
   serialized_options=None,
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\nAtensorflow_data_validation/skew/protos/feature_skew_results.proto\x12\x1atensorflow.data_validation\x1a%tensorflow/core/example/example.proto\"\xc0\x01\n\x0b\x46\x65\x61tureSkew\x12\x14\n\x0c\x66\x65\x61ture_name\x18\x01 \x01(\t\x12\x16\n\x0etraining_count\x18\x02 \x01(\x04\x12\x15\n\rserving_count\x18\x03 \x01(\x04\x12\x13\n\x0bmatch_count\x18\x04 \x01(\x04\x12\x15\n\rtraining_only\x18\x05 \x01(\x04\x12\x14\n\x0cserving_only\x18\x06 \x01(\x04\x12\x16\n\x0emismatch_count\x18\x07 \x01(\x04\x12\x12\n\ndiff_count\x18\x08 \x01(\x04\"\xcd\x01\n\x08SkewPair\x12%\n\x08training\x18\x01 \x01(\x0b\x32\x13.tensorflow.Example\x12$\n\x07serving\x18\x02 \x01(\x0b\x32\x13.tensorflow.Example\x12\x1e\n\x16training_only_features\x18\x03 \x03(\t\x12\x1d\n\x15serving_only_features\x18\x04 \x03(\t\x12\x18\n\x10matched_features\x18\x05 \x03(\t\x12\x1b\n\x13mismatched_features\x18\x06 \x03(\tb\x06proto3'
+  serialized_pb=b'\nAtensorflow_data_validation/skew/protos/feature_skew_results.proto\x12\x1atensorflow.data_validation\x1a%tensorflow/core/example/example.proto\"\xb2\x01\n\x0b\x46\x65\x61tureSkew\x12\x14\n\x0c\x66\x65\x61ture_name\x18\x01 \x01(\t\x12\x12\n\nbase_count\x18\x02 \x01(\x04\x12\x12\n\ntest_count\x18\x03 \x01(\x04\x12\x13\n\x0bmatch_count\x18\x04 \x01(\x04\x12\x11\n\tbase_only\x18\x05 \x01(\x04\x12\x11\n\ttest_only\x18\x06 \x01(\x04\x12\x16\n\x0emismatch_count\x18\x07 \x01(\x04\x12\x12\n\ndiff_count\x18\x08 \x01(\x04\"\xbf\x01\n\x08SkewPair\x12!\n\x04\x62\x61se\x18\x01 \x01(\x0b\x32\x13.tensorflow.Example\x12!\n\x04test\x18\x02 \x01(\x0b\x32\x13.tensorflow.Example\x12\x1a\n\x12\x62\x61se_only_features\x18\x03 \x03(\t\x12\x1a\n\x12test_only_features\x18\x04 \x03(\t\x12\x18\n\x10matched_features\x18\x05 \x03(\t\x12\x1b\n\x13mismatched_features\x18\x06 \x03(\tb\x06proto3'
   ,
   dependencies=[tensorflow_dot_core_dot_example_dot_example__pb2.DESCRIPTOR,])
 
 
 
 
 _FEATURESKEW = _descriptor.Descriptor(
@@ -39,43 +39,43 @@
       name='feature_name', full_name='tensorflow.data_validation.FeatureSkew.feature_name', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=b"".decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='training_count', full_name='tensorflow.data_validation.FeatureSkew.training_count', index=1,
+      name='base_count', full_name='tensorflow.data_validation.FeatureSkew.base_count', index=1,
       number=2, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='serving_count', full_name='tensorflow.data_validation.FeatureSkew.serving_count', index=2,
+      name='test_count', full_name='tensorflow.data_validation.FeatureSkew.test_count', index=2,
       number=3, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='match_count', full_name='tensorflow.data_validation.FeatureSkew.match_count', index=3,
       number=4, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='training_only', full_name='tensorflow.data_validation.FeatureSkew.training_only', index=4,
+      name='base_only', full_name='tensorflow.data_validation.FeatureSkew.base_only', index=4,
       number=5, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='serving_only', full_name='tensorflow.data_validation.FeatureSkew.serving_only', index=5,
+      name='test_only', full_name='tensorflow.data_validation.FeatureSkew.test_only', index=5,
       number=6, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='mismatch_count', full_name='tensorflow.data_validation.FeatureSkew.mismatch_count', index=6,
@@ -100,49 +100,49 @@
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
   serialized_start=137,
-  serialized_end=329,
+  serialized_end=315,
 )
 
 
 _SKEWPAIR = _descriptor.Descriptor(
   name='SkewPair',
   full_name='tensorflow.data_validation.SkewPair',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   create_key=_descriptor._internal_create_key,
   fields=[
     _descriptor.FieldDescriptor(
-      name='training', full_name='tensorflow.data_validation.SkewPair.training', index=0,
+      name='base', full_name='tensorflow.data_validation.SkewPair.base', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='serving', full_name='tensorflow.data_validation.SkewPair.serving', index=1,
+      name='test', full_name='tensorflow.data_validation.SkewPair.test', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='training_only_features', full_name='tensorflow.data_validation.SkewPair.training_only_features', index=2,
+      name='base_only_features', full_name='tensorflow.data_validation.SkewPair.base_only_features', index=2,
       number=3, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
-      name='serving_only_features', full_name='tensorflow.data_validation.SkewPair.serving_only_features', index=3,
+      name='test_only_features', full_name='tensorflow.data_validation.SkewPair.test_only_features', index=3,
       number=4, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='matched_features', full_name='tensorflow.data_validation.SkewPair.matched_features', index=4,
@@ -166,20 +166,20 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=332,
-  serialized_end=537,
+  serialized_start=318,
+  serialized_end=509,
 )
 
-_SKEWPAIR.fields_by_name['training'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE
-_SKEWPAIR.fields_by_name['serving'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE
+_SKEWPAIR.fields_by_name['base'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE
+_SKEWPAIR.fields_by_name['test'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE
 DESCRIPTOR.message_types_by_name['FeatureSkew'] = _FEATURESKEW
 DESCRIPTOR.message_types_by_name['SkewPair'] = _SKEWPAIR
 _sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 FeatureSkew = _reflection.GeneratedProtocolMessageType('FeatureSkew', (_message.Message,), {
   'DESCRIPTOR' : _FEATURESKEW,
   '__module__' : 'tensorflow_data_validation.skew.protos.feature_skew_results_pb2'
```

## tensorflow_data_validation/statistics/stats_impl.py

```diff
@@ -19,28 +19,30 @@
 from typing import Any, Callable, cast, Dict, Iterable, List, Optional, Text, Tuple
 
 import apache_beam as beam
 import pyarrow as pa
 from tensorflow_data_validation import constants
 from tensorflow_data_validation import types
 from tensorflow_data_validation.arrow import arrow_util
+from tensorflow_data_validation.utils import preprocessing_util
 from tensorflow_data_validation.statistics import stats_options
 from tensorflow_data_validation.statistics.generators import basic_stats_generator
 from tensorflow_data_validation.statistics.generators import image_stats_generator
 from tensorflow_data_validation.statistics.generators import lift_stats_generator
 from tensorflow_data_validation.statistics.generators import natural_language_domain_inferring_stats_generator
 from tensorflow_data_validation.statistics.generators import natural_language_stats_generator
 from tensorflow_data_validation.statistics.generators import sparse_feature_stats_generator
 from tensorflow_data_validation.statistics.generators import stats_generator
 from tensorflow_data_validation.statistics.generators import time_stats_generator
 from tensorflow_data_validation.statistics.generators import top_k_uniques_sketch_stats_generator
 from tensorflow_data_validation.statistics.generators import top_k_uniques_stats_generator
 from tensorflow_data_validation.statistics.generators import weighted_feature_stats_generator
 from tensorflow_data_validation.utils import feature_partition_util
 from tensorflow_data_validation.utils import slicing_util
+
 from tfx_bsl.arrow import table_util
 from tfx_bsl.statistics import merge_util
 from tfx_bsl.telemetry import collection
 
 from tensorflow_metadata.proto.v0 import schema_pb2
 from tensorflow_metadata.proto.v0 import statistics_pb2
 
@@ -57,14 +59,24 @@
       options: stats_options.StatsOptions = stats_options.StatsOptions()
       ) -> None:
     self._options = options
 
   def expand(
       self, dataset: beam.PCollection[pa.RecordBatch]
   ) -> beam.PCollection[statistics_pb2.DatasetFeatureStatisticsList]:
+    # Generate derived features, if applicable.
+    if self._options.schema is not None:
+      dataset, derivers_configured = preprocessing_util.add_derived_features(
+          dataset, self._options.schema)
+      if derivers_configured:
+        metadata_generator = preprocessing_util.get_metadata_generator()
+        assert metadata_generator is not None
+        self._options.generators = self._options.generators or []
+        self._options.generators.append(metadata_generator)
+
     # If a set of allowed features are provided, keep only those features.
     if self._options.feature_allowlist:
       dataset |= ('FilterFeaturesByAllowList' >> beam.Map(
           _filter_features, feature_allowlist=self._options.feature_allowlist))
 
     _ = dataset | 'TrackTotalBytes' >> collection.TrackRecordBatchBytes(
         constants.METRICS_NAMESPACE, 'record_batch_input_bytes')
```

## tensorflow_data_validation/statistics/stats_options.py

```diff
@@ -177,16 +177,16 @@
       experimental_result_partitions: The number of feature partitions to
         combine output DatasetFeatureStatisticsLists into. If set to 1 (default)
         output is globally combined. If set to value greater than one, up to
         that many shards are returned, each containing a subset of features.
       experimental_num_feature_partitions: If > 1, partitions computations by
         supported generators to act on this many bundles of features. For best
         results this should be set to at least several times less than the
-        number of features in a dataset, and never more than the available
-        beam parallelism.
+        number of features in a dataset, and never more than the available beam
+        parallelism.
     """
     self.generators = generators
     self.feature_allowlist = feature_allowlist
     self.schema = schema
     self.label_feature = label_feature
     self.weight_feature = weight_feature
     if slice_functions is not None and experimental_slice_functions is not None:
```

## tensorflow_data_validation/statistics/stats_options_test.py

```diff
@@ -297,15 +297,16 @@
         infer_type_from_schema=infer_type_from_schema,
         desired_batch_size=desired_batch_size,
         enable_semantic_domain_stats=enable_semantic_domain_stats,
         semantic_domain_stats_sample_rate=semantic_domain_stats_sample_rate,
         per_feature_weight_override=per_feature_weight_override,
         add_default_generators=add_default_generators,
         experimental_use_sketch_based_topk_uniques=use_sketch_based_topk_uniques,
-        experimental_result_partitions=experimental_result_partitions)
+        experimental_result_partitions=experimental_result_partitions,
+    )
 
     options_json = options.to_json()
     options = stats_options.StatsOptions.from_json(options_json)
 
     self.assertEqual(feature_allowlist, options.feature_allowlist)
     compare.assertProtoEqual(self, schema, options.schema)
     self.assertEqual(vocab_paths, options.vocab_paths)
```

## tensorflow_data_validation/statistics/generators/lift_stats_generator.py

```diff
@@ -7,36 +7,15 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-r"""Computes lifts between one feature and a set of categorical features.
-
-We define the feature value lift(x_i, y_i) for features X and Y as:
-
-  P(Y=y_i|X=x_i) / P(Y=y_i)
-
-This quantitatively captures the notion of probabilistic independence, such that
-when X and Y are independent, the lift will be 1. It also indicates the degree
-to which the presence of x_i increases or decreases the probablity of the
-presence of y_i. When X or Y is multivalent, the expressions `X=x_i` and `Y=y_i`
-are intepreted as the set membership checks, `x_i \in X` and `y_i \in Y`.
-
-When Y is a label and Xs are the set of categorical features, lift can be used
-to assess feature importance. However, in the presence of correlated features,
-because lift is computed independently for each feature, it will not be a
-reliable indicator of the expected impact on model quality from adding or
-removing that feature.
-
-This TransformStatsGenerator computes feature value lift for all pairs of X and
-Y, where Y is a single, user-configured feature and X is either a manually
-specified list of features, or all categorical features in the provided schema.
-"""
+"""Provides LiftStatsGenerator for quantifying feature-label correlations."""
 
 import collections
 import datetime
 import operator
 from typing import Any, Dict, Hashable, Iterator, Iterable, List, Optional, Sequence, Text, Tuple, TypeVar, Union
 
 import apache_beam as beam
@@ -815,15 +794,15 @@
   automatically inferred as the set of categorical features in the schema
   (excluding y_path).
   """
 
   def __init__(self, y_path: types.FeaturePath,
                schema: Optional[schema_pb2.Schema],
                x_paths: Optional[Iterable[types.FeaturePath]],
-               y_boundaries: Optional[Sequence[float]], min_x_count: int,
+               y_boundaries: Optional[Iterable[float]], min_x_count: int,
                top_k_per_y: Optional[int], bottom_k_per_y: Optional[int],
                example_weight_map: ExampleWeightMap,
                output_custom_stats: bool, name: Text) -> None:
     """Initializes a lift statistics generator.
 
     Args:
       y_path: The path to use as Y in the lift expression: lift = P(Y=y|X=x) /
@@ -963,27 +942,91 @@
         | 'ComputeWeightedLift' >> self._weighted_generator)
 
     return ((unweighted_protos, weighted_protos)
             | 'MergeUnweightedAndWeightedProtos' >> beam.Flatten())
 
 
 class LiftStatsGenerator(stats_generator.TransformStatsGenerator):
-  """A transform stats generator for computing lift between two features."""
+  r"""A transform stats generator for computing lift between two features.
+
+  We define the feature value lift(x_i, y_i) for features X and Y as:
+
+    P(Y=y_i|X=x_i) / P(Y=y_i)
+
+  This quantitatively captures the notion of probabilistic independence, such
+  that when X and Y are independent, the lift will be 1. It also indicates the
+  degree to which the presence of x_i increases or decreases the probablity of
+  the presence of y_i. When X or Y is multivalent, the expressions `X=x_i` and
+  `Y=y_i` are intepreted as the set membership checks, `x_i \in X` and
+  `y_i \in Y`.
+
+  When Y is a label and Xs are the set of categorical features, lift can be used
+  to assess feature importance. However, in the presence of correlated features,
+  because lift is computed independently for each feature, it will not be a
+  reliable indicator of the expected impact on model quality from adding or
+  removing that feature.
+
+  This generator computes lift for a set of feature pairs (y, x_1), ... (y, x_k)
+  for a collection of x_paths, and a single y_path. The y_path must be either
+  a categorical feature, or numeric feature (in which case binning boundaries
+  are also required). The x_paths can be manually provided or will be
+  automatically inferred as the set of categorical features in the schema
+  (excluding y_path).
+
+  This calculation can also be done using per-example weights. If no
+  ExampleWeightMap is provided, or there is no weight for y_path, only
+  unweighted lift will be computed. In the case where the ExampleWeightMap
+  contains a weight_path or a per-feature override for y_path (y_weight), a
+  weighted version of lift will be computed in which each example is treated as
+  if it occured y_weight times.
+  """
 
   def __init__(self,
                y_path: types.FeaturePath,
                schema: Optional[schema_pb2.Schema] = None,
                x_paths: Optional[Iterable[types.FeaturePath]] = None,
-               y_boundaries: Optional[Sequence[float]] = None,
+               y_boundaries: Optional[Iterable[float]] = None,
                min_x_count: int = 0,
                top_k_per_y: Optional[int] = None,
                bottom_k_per_y: Optional[int] = None,
                example_weight_map: ExampleWeightMap = ExampleWeightMap(),
                output_custom_stats: Optional[bool] = False,
                name: Text = 'LiftStatsGenerator') -> None:
+    """Initializes a LiftStatsGenerator.
+
+    Args:
+      y_path: The path to use as Y in the lift expression: lift = P(Y=y|X=x) /
+        P(Y=y).
+     schema: An optional schema for the dataset. If not provided, x_paths must
+       be specified. If x_paths are not specified, the schema is used to
+       identify all categorical columns for which Lift should be computed.
+      x_paths: An optional list of path to use as X in the lift expression: lift
+        = P(Y=y|X=x) / P(Y=y). If None (default), all categorical features,
+        exluding the feature passed as y_path, will be used.
+      y_boundaries: An optional list of boundaries to be used for binning
+        y_path. If provided with b boundaries, the binned values will be treated
+        as a categorical feature with b+1 different values. For example, the
+        y_boundaries value [0.1, 0.8] would lead to three buckets: [-inf, 0.1),
+          [0.1, 0.8) and [0.8, inf].
+      min_x_count: The minimum number of examples in which a specific x value
+        must appear, in order for its lift to be output.
+      top_k_per_y: Optionally, the number of top x values per y value, ordered
+        by descending lift, for which to output lift. If both top_k_per_y and
+        bottom_k_per_y are unset, all values will be output.
+      bottom_k_per_y: Optionally, the number of bottom x values per y value,
+        ordered by descending lift, for which to output lift. If both
+        top_k_per_y and bottom_k_per_y are unset, all values will be output.
+      example_weight_map: Optionally, an ExampleWeightMap that maps a
+        FeaturePath to its corresponding weight column. If provided and if
+        it's not an empty map (i.e. no feature has a corresponding weight column
+        ), unweighted lift stats will be populated, otherwise both unweighted
+        and weighted lift stats will be populated.
+      output_custom_stats: Whether to output custom stats for use with Facets.
+      name: An optional unique name associated with the statistics generator.
+    """
     super(LiftStatsGenerator, self).__init__(
         name,
         ptransform=_UnweightedAndWeightedLiftStatsGenerator(
             example_weight_map=example_weight_map,
             schema=schema,
             y_path=y_path,
             x_paths=x_paths,
```

## tensorflow_data_validation/utils/feature_partition_util.py

```diff
@@ -30,14 +30,18 @@
   def __init__(self, partitions: int):
     self.num_partitions = partitions
 
   def assign(self, feature_name: Union[bytes, str]) -> int:
     """Assigns a feature partition based on the name of a feature."""
     if isinstance(feature_name, bytes):
       feature_name = feature_name.decode('utf8')
+    # TODO(b/236190177): Remove when binding is fixed.
+    if '\x00' in feature_name:
+      feature_name = feature_name.replace('\x00', '?')
+
     partition = farmhash.fingerprint32(feature_name) % self.num_partitions
     return partition
 
   def assign_sequence(self, *parts: Union[bytes, str]) -> int:
     """Assigns a feature partition based on a sequence of bytes or strings."""
     partition = 0
     for part in parts:
```

## tensorflow_data_validation/utils/feature_partition_util_test.py

```diff
@@ -337,14 +337,41 @@
   name: "xyz"
   features {
     path {
       step: "f1"
     }
   }
 }""")]
+}, {
+    'testcase_name':
+        'does_not_crash_embedded_null_b236190177',
+    'num_partitions':
+        10,
+    'statistics': [
+        """
+        datasets: {
+            name: 'abc'
+            features: {
+              path: {
+                step: '\x00'
+              }
+            }
+        }
+        """
+    ],
+    'expected': [(6, """
+        datasets: {
+            name: 'abc'
+            features: {
+              path: {
+                step: '\x00'
+              }
+            }
+        }
+        """)]
 }]
 
 
 class KeyAndSplitByFeatureFnTest(parameterized.TestCase):
 
   @parameterized.named_parameters(_KEY_AND_SPLIT_TEST_CASES)
   def test_splits_statistics(
```

## tensorflow_data_validation/utils/statistics_io_impl.py

```diff
@@ -40,14 +40,18 @@
     """
     raise NotImplementedError
 
   def glob(self, output_path_prefix: str) -> Iterator[str]:
     """Return files matching the pattern produced by record_sink_impl."""
     raise NotImplementedError
 
+  def file_suffix(self) -> str:
+    """Returns a file suffix (e.g., .tfrecords)."""
+    raise NotImplementedError
+
 
 def get_io_provider(
     file_format: Optional[str] = None) -> StatisticsIOProvider:
   """Get a StatisticsIOProvider for writing and reading sharded stats.
 
   Args:
     file_format: Optional file format. Supports only tfrecords. If unset,
@@ -57,47 +61,42 @@
     A StatisticsIOProvider.
   """
 
   if file_format is None:
     file_format = 'tfrecords'
   if file_format not in ('tfrecords',):
     raise ValueError('Unrecognized file_format %s' % file_format)
-  return _ProviderImpl(file_format)
+  return _TFRecordProviderImpl()
 
 
-class _ProviderImpl(StatisticsIOProvider):
+class _TFRecordProviderImpl(StatisticsIOProvider):
   """TFRecord backed impl."""
 
-  def __init__(self, file_format: str):
-    self._file_format = file_format
-
   def record_sink_impl(self,
-                       output_path_prefix: str,
-                       file_format: Optional[str] = None) -> beam.PTransform:
-    if self._file_format == 'tfrecords':
-      return beam.io.WriteToTFRecord(
-          output_path_prefix,
-          coder=beam.coders.ProtoCoder(
-              statistics_pb2.DatasetFeatureStatisticsList))
-    else:
-      raise ValueError(
-          'Unrecognized file format %s.' % file_format)
+                       output_path_prefix: str) -> beam.PTransform:
+    return beam.io.WriteToTFRecord(
+        output_path_prefix,
+        coder=beam.coders.ProtoCoder(
+            statistics_pb2.DatasetFeatureStatisticsList))
 
   def glob(self, output_path_prefix) -> Iterator[str]:
     """Returns filenames matching the output pattern of record_sink_impl."""
     return tf.io.gfile.glob(output_path_prefix + '-*-of-*')
 
   def record_iterator_impl(
       self,
       paths: Iterable[str],
   ) -> Iterator[statistics_pb2.DatasetFeatureStatisticsList]:
     """Provides iterators over tfrecord backed statistics."""
-    if self._file_format == 'tfrecords':
-      iter_fn = tf.compat.v1.io.tf_record_iterator
-    else:
-      raise NotImplementedError('Unrecognized file_format %s' %
-                                self._file_format)
     for path in paths:
-      for record in iter_fn(path):
+      for record in tf.compat.v1.io.tf_record_iterator(path):
         stats_shard = statistics_pb2.DatasetFeatureStatisticsList()
         stats_shard.ParseFromString(record)
         yield stats_shard
+
+  def file_suffix(self) -> str:
+    """Returns a file suffix (e.g., .tfrecords)."""
+    return '.tfrecords'
+
+
+def should_write_sharded():
+  return False
```

## tensorflow_data_validation/utils/stats_util.py

```diff
@@ -652,14 +652,17 @@
   """
   if input_path_prefix is None == input_paths is None:
     raise ValueError('Must provide one of input_paths_prefix, input_paths.')
   if io_provider is None:
     io_provider = statistics_io_impl.get_io_provider()
   if input_path_prefix is not None:
     input_paths = io_provider.glob(input_path_prefix)
+  if not input_paths:
+    raise ValueError('No input paths found paths=%s, pattern=%s' %
+                     (input_paths, input_path_prefix))
   acc = statistics.DatasetListAccumulator()
   stats_iter = io_provider.record_iterator_impl(input_paths)
   for stats_list in stats_iter:
     for dataset in stats_list.datasets:
       acc.MergeDatasetFeatureStatistics(dataset.SerializeToString())
   stats = statistics_pb2.DatasetFeatureStatisticsList()
   stats.ParseFromString(acc.Get())
```

## tensorflow_data_validation/utils/variance_util.py

```diff
@@ -9,14 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Utilities for calculating numerically stable variance."""
 
+from typing import Optional
+
 import numpy as np
 
 
 class WeightedMeanVarAccumulator(object):
   """Tracks quantities for numerically stable mean and variance calculation."""
   __slots__ = ['count', 'mean', 'variance', 'weights_mean']
 
@@ -127,15 +129,15 @@
       other: A MeanVarAccumulator to merge with self.
     """
     self._combine(other.count, other.mean, other.variance)
 
   def _combine(self, b_count: int, b_mean: float,
                b_variance: float):
     """Combine unweighted mean and variance parameters, updating accumulator."""
-    # In the case of very inbalanced sizes we prefer ratio ~= 0
+    # In the case of very imbalanced sizes we prefer ratio ~= 0
     a_count, a_mean, a_variance = self.count, self.mean, self.variance
     if b_count > a_count:
       a_count, b_count = b_count, a_count
       a_mean, b_mean = b_mean, a_mean
       a_variance, b_variance = b_variance, a_variance
     new_count = a_count + b_count
     if new_count == 0:
@@ -144,7 +146,72 @@
     new_mean = a_mean + ratio * (b_mean - a_mean)
     new_variance = a_variance + ratio * (
         b_variance - a_variance + (b_mean - new_mean) *
         (b_mean - a_mean))
     self.count = new_count
     self.mean = new_mean
     self.variance = new_variance
+
+
+class MeanCovAccumulator(object):
+  """Tracks values for numerically stable mean and covariance calculation."""
+  __slots__ = ['count', 'mean', 'covariance']
+
+  def __init__(self):
+    self.count = 0
+    self.mean = None
+    self.covariance = None
+
+  def update(self, array: np.ndarray):
+    """Updates a MeanCovAccumulator with a batch of values.
+
+    Args:
+      array: An ndarray with numeric type.
+    """
+    count = len(array)
+    if count == 0:
+      return
+    elif count == 1:
+      dim = array[0].size
+      covariance = np.zeros((dim, dim), dtype=np.float64)
+    else:
+      covariance = np.cov(array, rowvar=False)
+    mean = np.mean(array, axis=0)
+    self._combine(count, mean, covariance)
+
+  def merge(self, other: 'MeanCovAccumulator'):
+    """Combines two MeanCovAccumulator, updating in place.
+
+    Args:
+      other: A MeanCovAccumulator to merge with self.
+    """
+    self._combine(other.count, other.mean, other.covariance)
+
+  def _combine(self, b_count: int, b_mean: Optional[np.ndarray],
+               b_covariance: Optional[np.ndarray]):
+    """Combine unweighted mean and covariance parameters, updating accumulator."""
+    a_count, a_mean, a_covariance = self.count, self.mean, self.covariance
+    new_count = a_count + b_count
+    if new_count == a_count:
+      return
+    elif new_count == b_count:
+      # Avoid division by zero, which would happen otherwise if b_count=1
+      new_mean = b_mean
+      new_covariance = b_covariance
+    else:
+      if a_mean is None:
+        a_mean = np.zeros((np.shape(b_mean)), dtype=np.float64)
+      if a_covariance is None:
+        a_covariance = np.zeros((np.shape(b_covariance)), dtype=np.float64)
+      ratio = b_count / new_count
+      new_mean = a_mean + ratio * (b_mean - a_mean)
+      new_covariance = (a_covariance * (a_count - 1) +
+                        b_covariance * (b_count - 1) +
+                        (np.outer(
+                            (a_mean - new_mean),
+                            (a_mean - new_mean) * a_count)) +
+                        (np.outer(
+                            (b_mean - new_mean),
+                            (b_mean - new_mean) * b_count))) / (new_count - 1)
+    self.count = new_count
+    self.mean = new_mean
+    self.covariance = new_covariance
```

## tensorflow_data_validation/utils/variance_util_test.py

```diff
@@ -85,14 +85,52 @@
         'distribution_variance': 1.0,
         'use_weights': False
     },
 ]
 
 _RELATIVE_ERROR_TOLERANCE = 1e-6
 
+_MEAN_COV_ACCUMULATOR_TEST_CASES = [
+    {
+        'testcase_name': 'unit_normal',
+        'array_size': 10,
+        'distribution_mean': 0.0,
+        'distribution_variance': 1.0,
+        'num_vectors': 1000
+    },
+    {
+        'testcase_name': 'large_pos_shift',
+        'array_size': 10,
+        'distribution_mean': 100000.0,
+        'distribution_variance': 1.0,
+        'num_vectors': 1000
+    },
+    {
+        'testcase_name': 'large_var',
+        'array_size': 10,
+        'distribution_mean': 0.0,
+        'distribution_variance': 10000.0,
+        'num_vectors': 1000
+    },
+    {
+        'testcase_name': 'large_array_large_mean_large_var',
+        'array_size': 100,
+        'distribution_mean': 1000.0,
+        'distribution_variance': 1000.0,
+        'num_vectors': 1000
+    },
+    {
+        'testcase_name': 'small_array',
+        'array_size': 3,
+        'distribution_mean': 0.0,
+        'distribution_variance': 1.0,
+        'num_vectors': 1000
+    },
+]
+
 
 class MeanVarAccumulatorTest(parameterized.TestCase):
 
   @parameterized.named_parameters(
       {
           'testcase_name': '1d_no_weights',
           'values': np.array([1, 2, 3, 4, 5]),
@@ -205,9 +243,172 @@
     accumulator1 = variance_util.MeanVarAccumulator()
     accumulator2 = variance_util.MeanVarAccumulator()
     accumulator1.merge(accumulator2)
     self.assertEqual(accumulator1.mean, 0)
     self.assertEqual(accumulator1.variance, 0)
 
 
+class MeanCovAccumulatorTest(parameterized.TestCase):
+
+  @parameterized.named_parameters(
+      {
+          'testcase_name': '1d x 3',
+          'vectors': np.array([[1],
+                               [-6],
+                               [15]]),
+      }, {
+          'testcase_name': '5d x 5',
+          'vectors': np.array([[1, 2.4e-9, -3, 43333, 5.1],
+                               [-1, 6.99, 8e12, 9, 250],
+                               [15, -391746.2, -7.3, 30, 14],
+                               [1000, 0.1, -1e6, 12, 49],
+                               [88, -3e10, 7e-9, 0.2, 983]]),
+      })
+  def test_initialize_from_array(self, vectors):
+    accumulator = variance_util.MeanCovAccumulator()
+    accumulator.update(vectors)
+    expected_mean = np.mean(vectors, axis=0)
+    expected_covariance = np.cov(vectors, rowvar=False).ravel()
+    actual_mean = accumulator.mean
+    actual_covariance = accumulator.covariance.ravel()
+
+    self.assertEqual(expected_mean.size, actual_mean.size)
+    self.assertEqual(expected_covariance.size, actual_covariance.size)
+    for expected, actual in zip(expected_mean, actual_mean):
+      self.assertAlmostEqual(expected, actual)
+    for expected, actual in zip(expected_covariance, actual_covariance):
+      self.assertAlmostEqual(expected, actual)
+
+  @parameterized.named_parameters(*_MEAN_COV_ACCUMULATOR_TEST_CASES)
+  def test_merges_random_array(self, array_size, distribution_mean,
+                               distribution_variance, num_vectors):
+    rng = np.random.default_rng(4444444)
+    vectors = []
+    for _ in range(num_vectors):
+      vector = rng.standard_normal(array_size) * np.sqrt(
+          distribution_variance) + distribution_mean
+      vectors.append(vector)
+    vectors = np.asarray(vectors)
+
+    expected_mean = np.mean(vectors, axis=0)
+    expected_covariance = np.cov(vectors, rowvar=False).ravel()
+
+    # Check a variety of splits of the data.
+    for split in range(0, vectors.size, 1 + int(vectors.size / 100)):
+      accumulator1 = variance_util.MeanCovAccumulator()
+      accumulator1.update(vectors[:split])
+      accumulator2 = variance_util.MeanCovAccumulator()
+      accumulator2.update(vectors[split:])
+      accumulator1.merge(accumulator2)
+      actual_mean = accumulator1.mean
+      actual_covariance = accumulator1.covariance.ravel()
+
+      self.assertEqual(expected_mean.size, actual_mean.size)
+      self.assertEqual(expected_covariance.size, actual_covariance.size)
+      for expected, actual in zip(expected_mean, actual_mean):
+        self.assertAlmostEqual(expected, actual)
+      for expected, actual in zip(expected_covariance, actual_covariance):
+        self.assertAlmostEqual(expected, actual)
+
+  @parameterized.named_parameters(*_MEAN_COV_ACCUMULATOR_TEST_CASES)
+  def test_update_random_array(self, array_size, distribution_mean,
+                               distribution_variance, num_vectors):
+
+    rng = np.random.default_rng(4444444)
+    vectors = []
+    for _ in range(num_vectors):
+      vector = rng.standard_normal(array_size) * np.sqrt(
+          distribution_variance) + distribution_mean
+      vectors.append(vector)
+    vectors = np.asarray(vectors)
+    accumulator = variance_util.MeanCovAccumulator()
+
+    # Iterate over chunks updating - array_size should be divisible by 10.
+    batch_size = 10
+    for idx in range(0, vectors.size, batch_size):
+      accumulator.update(vectors[idx:idx + batch_size])
+
+    expected_mean = np.mean(vectors, axis=0)
+    expected_covariance = np.cov(vectors, rowvar=False).ravel()
+    actual_mean = accumulator.mean
+    actual_covariance = accumulator.covariance.ravel()
+
+    self.assertEqual(expected_mean.size, actual_mean.size)
+    self.assertEqual(expected_covariance.size, actual_covariance.size)
+    for expected, actual in zip(expected_mean, actual_mean):
+      self.assertAlmostEqual(expected, actual)
+    for expected, actual in zip(expected_covariance, actual_covariance):
+      self.assertAlmostEqual(expected, actual)
+
+  # Checks handling for division by zero when computing covariance
+  def test_single_observations(self):
+    vectors1 = np.array([[1, 2, 3]])
+    vectors2 = np.array([[4, 5, 6]])
+    vectors3 = np.array([[7, 8, 9]])
+    accumulator1 = variance_util.MeanCovAccumulator()
+    accumulator2 = variance_util.MeanCovAccumulator()
+    accumulator3 = variance_util.MeanCovAccumulator()
+
+    accumulator1.update(vectors1)
+    self.assertListEqual([1, 2, 3], list(accumulator1.mean))
+    self.assertListEqual([0, 0, 0, 0, 0, 0, 0, 0, 0],
+                         list(accumulator1.covariance.ravel()))
+
+    accumulator1.update(vectors2)
+    self.assertListEqual([2.5, 3.5, 4.5], list(accumulator1.mean))
+    self.assertListEqual([4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],
+                         list(accumulator1.covariance.ravel()))
+
+    accumulator3.update(vectors3)
+    accumulator2.merge(accumulator3)
+    self.assertListEqual([7, 8, 9], list(accumulator2.mean))
+    self.assertListEqual([0, 0, 0, 0, 0, 0, 0, 0, 0],
+                         list(accumulator2.covariance.ravel()))
+
+    accumulator1.merge(accumulator2)
+    self.assertListEqual([4, 5, 6], list(accumulator1.mean))
+    self.assertListEqual([9, 9, 9, 9, 9, 9, 9, 9, 9],
+                         list(accumulator1.covariance.ravel()))
+
+  def test_combines_empty_non_empty(self):
+    vectors = np.array([[-1, 3, 6],
+                        [2, -5, 8],
+                        [4, 7, -9]])
+    accumulator1 = variance_util.MeanCovAccumulator()
+    accumulator2 = variance_util.MeanCovAccumulator()
+    accumulator2.update(vectors)
+    accumulator1.merge(accumulator2)
+    expected_mean = list(np.mean(vectors, axis=0))
+    expected_covariance = list(np.cov(vectors, rowvar=False).ravel())
+    actual_mean = list(accumulator1.mean)
+    actual_covariance = list(accumulator1.covariance.ravel())
+
+    self.assertListEqual(expected_mean, actual_mean)
+    self.assertListEqual(expected_covariance, actual_covariance)
+
+  def test_combines_non_empty_empty(self):
+    vectors = np.array([[-1, 3, 6],
+                        [2, -5, 8],
+                        [4, 7, -9]])
+    accumulator1 = variance_util.MeanCovAccumulator()
+    accumulator2 = variance_util.MeanCovAccumulator()
+    accumulator2.update(vectors)
+    accumulator2.merge(accumulator1)
+    expected_mean = list(np.mean(vectors, axis=0))
+    expected_covariance = list(np.cov(vectors, rowvar=False).ravel())
+    actual_mean = list(accumulator2.mean)
+    actual_covariance = list(accumulator2.covariance.ravel())
+
+    self.assertListEqual(expected_mean, actual_mean)
+    self.assertListEqual(expected_covariance, actual_covariance)
+
+  def test_combines_two_empty(self):
+    accumulator1 = variance_util.MeanCovAccumulator()
+    accumulator2 = variance_util.MeanCovAccumulator()
+    accumulator1.merge(accumulator2)
+
+    self.assertIsNone(accumulator1.mean)
+    self.assertIsNone(accumulator1.covariance)
+
+
 if __name__ == '__main__':
   absltest.main()
```

## Comparing `tensorflow_data_validation-1.8.0.dist-info/LICENSE` & `tensorflow_data_validation-1.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensorflow_data_validation-1.8.0.dist-info/METADATA` & `tensorflow_data_validation-1.9.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensorflow-data-validation
-Version: 1.8.0
+Version: 1.9.0
 Summary: A library for exploring and validating machine learning data.
 Home-page: https://www.tensorflow.org/tfx/data_validation/get_started
 Download-URL: https://github.com/tensorflow/data-validation/tags
 Author: Google LLC
 Author-email: tensorflow-extended-dev@googlegroups.com
 License: Apache 2.0
 Keywords: tensorflow data validation tfx
@@ -37,17 +37,17 @@
 Requires-Dist: joblib (<0.15,>=0.12)
 Requires-Dist: numpy (<2,>=1.16)
 Requires-Dist: pandas (<2,>=1.0)
 Requires-Dist: protobuf (<4,>=3.13)
 Requires-Dist: pyarrow (<6,>=1)
 Requires-Dist: pyfarmhash (<0.4,>=0.2)
 Requires-Dist: six (<2,>=1.12)
-Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5)
-Requires-Dist: tensorflow-metadata (<1.9,>=1.8.0)
-Requires-Dist: tfx-bsl (<1.9,>=1.8.0)
+Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5)
+Requires-Dist: tensorflow-metadata (<1.10,>=1.9.0)
+Requires-Dist: tfx-bsl (<1.10,>=1.9.0)
 Provides-Extra: all
 Requires-Dist: scikit-learn (<0.24,>=0.23) ; extra == 'all'
 Requires-Dist: scipy (<2,>=1.5) ; extra == 'all'
 Requires-Dist: ipython (<8,>=7) ; extra == 'all'
 Provides-Extra: mutual-information
 Requires-Dist: scikit-learn (<0.24,>=0.23) ; extra == 'mutual-information'
 Requires-Dist: scipy (<2,>=1.5) ; extra == 'mutual-information'
@@ -217,15 +217,16 @@
 
 The following table shows the  package versions that are
 compatible with each other. This is determined by our testing framework, but
 other *untested* combinations may also work.
 
 tensorflow-data-validation                                                            | apache-beam[gcp] | pyarrow | tensorflow        | tensorflow-metadata | tensorflow-transform | tfx-bsl
 ------------------------------------------------------------------------------------- | ---------------- | ------- | ----------------- | ------------------- | -------------------- | -------
-[GitHub master](https://github.com/tensorflow/data-validation/blob/master/RELEASE.md) | 2.38.0           | 5.0.0   | nightly (1.x/2.x) | 1.8.0               | n/a                  | 1.8.0
+[GitHub master](https://github.com/tensorflow/data-validation/blob/master/RELEASE.md) | 2.38.0           | 5.0.0   | nightly (1.x/2.x) | 1.9.0               | n/a                  | 1.9.0
+[1.9.0](https://github.com/tensorflow/data-validation/blob/v1.9.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.9        | 1.9.0               | n/a                  | 1.9.0
 [1.8.0](https://github.com/tensorflow/data-validation/blob/v1.8.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.8        | 1.8.0               | n/a                  | 1.8.0
 [1.7.0](https://github.com/tensorflow/data-validation/blob/v1.7.0/RELEASE.md)         | 2.36.0           | 5.0.0   | 1.15 / 2.8        | 1.7.0               | n/a                  | 1.7.0
 [1.6.0](https://github.com/tensorflow/data-validation/blob/v1.6.0/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15 / 2.7        | 1.6.0               | n/a                  | 1.6.0
 [1.5.0](https://github.com/tensorflow/data-validation/blob/v1.5.0/RELEASE.md)         | 2.34.0           | 5.0.0   | 1.15 / 2.7        | 1.5.0               | n/a                  | 1.5.0
 [1.4.0](https://github.com/tensorflow/data-validation/blob/v1.4.0/RELEASE.md)         | 2.32.0           | 4.0.1   | 1.15 / 2.6        | 1.4.0               | n/a                  | 1.4.0
 [1.3.0](https://github.com/tensorflow/data-validation/blob/v1.3.0/RELEASE.md)         | 2.32.0           | 2.0.0   | 1.15 / 2.6        | 1.2.0               | n/a                  | 1.3.0
 [1.2.0](https://github.com/tensorflow/data-validation/blob/v1.2.0/RELEASE.md)         | 2.31.0           | 2.0.0   | 1.15 / 2.5        | 1.2.0               | n/a                  | 1.2.0
```

## Comparing `tensorflow_data_validation-1.8.0.dist-info/RECORD` & `tensorflow_data_validation-1.9.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-tensorflow_data_validation/__init__.py,sha256=j61OELhLMYzCUA3gedUoKK1V0dWfMtYWYfiLD4LtBuw,4427
+tensorflow_data_validation/__init__.py,sha256=pZ0jkFtSxdYhVsqlRh463HOIb-ruNKDwRq7HDpbcQCc,4598
 tensorflow_data_validation/constants.py,sha256=CwoJ_wi1jmMiZ-5X2IpCyfIDh74IFDvaU2lT1G8BtY4,1457
 tensorflow_data_validation/types.py,sha256=1fPvdUEOo7mfipesHHY8w1EFLoQaT4LCejyn4hmJj9Y,4888
 tensorflow_data_validation/types_test.py,sha256=zOI1B5Sl10rT9Zuy3mISToOI3YOj-3w1z0CCNldJMOE,3231
-tensorflow_data_validation/version.py,sha256=TFVi6DTQp39epWjACPW8hei06_Q9yfXWT9vqJA31-uI,700
+tensorflow_data_validation/version.py,sha256=G3cKNurkMXYGihHhLjO15NRiFBIUmDTSAizntqoArb8,700
 tensorflow_data_validation/anomalies/__init__.py,sha256=XKlLWzkDdHasH2PaCA48sW4RApqUPKtsb2voaLKyQDE,590
 tensorflow_data_validation/anomalies/proto/__init__.py,sha256=UmsqlEFsGPb1ux8Kj1Yl0NLS7eQhePD2JRvgQSxzF10,590
 tensorflow_data_validation/anomalies/proto/validation_config_pb2.py,sha256=1PeBFIOghNskhlA7daRrJtNFi4NAf9E6gvcB04caHQQ,5623
 tensorflow_data_validation/anomalies/proto/validation_metadata_pb2.py,sha256=TzhFeY5i8WokSBfBNIugABNrV0TpD11j4qabM90ixc0,6710
 tensorflow_data_validation/api/__init__.py,sha256=XKlLWzkDdHasH2PaCA48sW4RApqUPKtsb2voaLKyQDE,590
-tensorflow_data_validation/api/stats_api.py,sha256=umkaXABZi1yjY1F3qMnpiTdxtz913bmqZwxIvSExN44,8665
+tensorflow_data_validation/api/stats_api.py,sha256=oCKU415dA2lajF-PVQw7-0rmyMQkN849cry1rjgwMj0,8827
 tensorflow_data_validation/api/stats_api_test.py,sha256=25mgjOu57_awrjHSM7uVORzCi7VLTD3hGEIaYSc6XW8,22724
-tensorflow_data_validation/api/validation_api.py,sha256=pcIPcikD407py0Dn5opW_frqrTcodvaCkJf5SCbqxBI,39017
-tensorflow_data_validation/api/validation_api_test.py,sha256=papZjrfT9TuvmwkTs8ajuxxKOQ3FQoRdOujR9jHQNe0,111489
+tensorflow_data_validation/api/validation_api.py,sha256=XRLu3vQAlE5T3QXSkmIJ6K0ujMZdGfixbvlsrxNYyc0,38909
+tensorflow_data_validation/api/validation_api_test.py,sha256=W_4dQz5s3HSrxgQMkLO3YMF_mB9cPi-8iAMGHsN0Udo,111439
 tensorflow_data_validation/api/validation_options.py,sha256=kKEBy4AvX5ddAj-iEcPWNF0hoolNqacwiiWVoVL7ecY,2488
 tensorflow_data_validation/api/validation_options_test.py,sha256=G-L3KXYAaL80GXIMQbbKod3I5e6cPxajOSK5Yy1OKxA,1893
 tensorflow_data_validation/arrow/__init__.py,sha256=xjQcxuGgPUfxt1tJqPAztbX33ez_2M4QobBSs-V2z5s,588
-tensorflow_data_validation/arrow/arrow_util.py,sha256=vaja_BNaSM3dyw1RFFWZ_UBOkID6C0hy-y17lvUr09I,17118
+tensorflow_data_validation/arrow/arrow_util.py,sha256=D3Oa3nikVMRNkcqvR-baCEbfJAbQ4Ze4KVPRYL96j9E,17117
 tensorflow_data_validation/arrow/arrow_util_test.py,sha256=jIdQGXFE4kEvl4b6Y6nnAOjmDZ9ZAcHSQVwtcSVlXkY,23028
 tensorflow_data_validation/arrow/decoded_examples_to_arrow.py,sha256=_UVQ8Vzx7Xh48Q80jb54wyYFUmKDLqDbsXl-_esucq8,3668
 tensorflow_data_validation/arrow/decoded_examples_to_arrow_test.py,sha256=jWbAWX1DYX1aretm8uoiqGm22ZeRUN63_HpYs8DmiYw,8005
 tensorflow_data_validation/coders/__init__.py,sha256=XKlLWzkDdHasH2PaCA48sW4RApqUPKtsb2voaLKyQDE,590
 tensorflow_data_validation/coders/csv_decoder.py,sha256=AvQDq0dcXs20ZlK-wF2_52B19K53TQSEZLcbFnJZFXM,3827
 tensorflow_data_validation/coders/csv_decoder_test.py,sha256=sfthwfRYCwVmyHh2Zvvy5oO3ufXGHwakyOhLe9RSTn4,16744
 tensorflow_data_validation/pywrap/__init__.py,sha256=UmsqlEFsGPb1ux8Kj1Yl0NLS7eQhePD2JRvgQSxzF10,590
-tensorflow_data_validation/pywrap/tensorflow_data_validation_extension.pyd,sha256=spl0beUfvBRKfy_RycK6n5S6v-is0SXeMG4L7q4q4o0,2199552
+tensorflow_data_validation/pywrap/tensorflow_data_validation_extension.pyd,sha256=HMoyFIDu_3L5c9Q5PBmLY6YjikilYzQHC1sEQTpd17Q,2421760
 tensorflow_data_validation/skew/__init__.py,sha256=iRhZqzc266x3n_YWHfRljxN8i7lzYzUp7ypA8UwcV8g,588
-tensorflow_data_validation/skew/feature_skew_detector.py,sha256=Mv7slpBRrIKE1UPsZY8y6knUuhYp9RmqryG2wJ0pLGE,17185
-tensorflow_data_validation/skew/feature_skew_detector_test.py,sha256=D5ZWdREPSWoSlqTEVSAE69GrADAVZECd8PqNSjC6SnY,20037
+tensorflow_data_validation/skew/feature_skew_detector.py,sha256=fgbtz6QVRjYlfxbyzTx63c3-VQvllndw_yGAGzKumu4,17351
+tensorflow_data_validation/skew/feature_skew_detector_test.py,sha256=rZZjZ5vpxkYrDiPtU2FToq8hZhCZiRNC5RhlYUPX7-s,23489
 tensorflow_data_validation/skew/protos/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tensorflow_data_validation/skew/protos/feature_skew_results_pb2.py,sha256=FGa-Ok5Pd5k8dXeOrov2SFFXhJlVOmMRkmLAPXuv6_k,9944
+tensorflow_data_validation/skew/protos/feature_skew_results_pb2.py,sha256=GR84QCO1kqs9SYtpRzCSzr0_1e_Q43GblhBN9wUZRAc,9861
 tensorflow_data_validation/statistics/__init__.py,sha256=XKlLWzkDdHasH2PaCA48sW4RApqUPKtsb2voaLKyQDE,590
-tensorflow_data_validation/statistics/stats_impl.py,sha256=HxslSBsf0aXSjw-jjsNQdorDWV9dNU_dpplN9w-Ji4A,37899
+tensorflow_data_validation/statistics/stats_impl.py,sha256=Tw-7DNxGijfBRHBKgE9waAYyZYVjKzs55KqhKmXMI4U,38460
 tensorflow_data_validation/statistics/stats_impl_test.py,sha256=55LrJrL8ZzM4VNYUW_llTw7YDtdYOgsa-RtbgYeEv0A,127501
-tensorflow_data_validation/statistics/stats_options.py,sha256=07jxK_zNn2W6SWnTj39-QPK6CQxN8wU1M44hkO7yW1Y,24773
-tensorflow_data_validation/statistics/stats_options_test.py,sha256=XB4yOfVDrS2uSK9m6kZCI3K0kzJReuP5K5Ta46zBSCQ,16262
+tensorflow_data_validation/statistics/stats_options.py,sha256=xrwQoVcWTCcB0WZmbWBiThKc52CjJvCbtQMYs_e8fLM,24773
+tensorflow_data_validation/statistics/stats_options_test.py,sha256=nVtZcIIKbRB47UgAQM3mD9O18vkMwDMcq7kc3A40WFI,16269
 tensorflow_data_validation/statistics/generators/__init__.py,sha256=XKlLWzkDdHasH2PaCA48sW4RApqUPKtsb2voaLKyQDE,590
 tensorflow_data_validation/statistics/generators/basic_stats_generator.py,sha256=zPOOhMMPwHjllb0018UkY7p_qDiq9R29ZJzcnBYWNMU,55459
 tensorflow_data_validation/statistics/generators/basic_stats_generator_test.py,sha256=U3e-JczS1kV_jIO1AEJrJDjDrno2QUH2u6vkqUXD9Uw,114071
 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator.py,sha256=Fus1uJ8fQtkpetQ6h_BmBGQdZ8OtH8J9nPutPcUAA-I,10048
 tensorflow_data_validation/statistics/generators/cross_feature_stats_generator_test.py,sha256=egkW8mLWmpIPPvZlHSitZHYi0GUUzFPOe4BfuWMqOsE,6100
 tensorflow_data_validation/statistics/generators/image_stats_generator.py,sha256=XUmEOkGYGJo3Wz1Nb4tfwjtJYHuvbDKME5192Zq0-RA,14195
 tensorflow_data_validation/statistics/generators/image_stats_generator_test.py,sha256=SHXxBX7GR1ttOcS28Vml8P_Bi-62J8B3FBJ4BvG1bAY,14315
 tensorflow_data_validation/statistics/generators/input_batch.py,sha256=R23iSNREfOHCmf_2po3fJWbwbfsm0JfMEHKjGqGxqcI,5199
 tensorflow_data_validation/statistics/generators/input_batch_test.py,sha256=EaSXNKRwolRc4XGP4LSBzySlSb9infwUX4GAjb3V4E4,6902
-tensorflow_data_validation/statistics/generators/lift_stats_generator.py,sha256=xP_Zfh1W1Ee9lkw_esDHs9APFkOs77vpds0-z-lLmYY,43483
+tensorflow_data_validation/statistics/generators/lift_stats_generator.py,sha256=mInJl4LoHrEE613FXCMMwcnrEG7ez1RcngL5k48AOI4,46166
 tensorflow_data_validation/statistics/generators/lift_stats_generator_test.py,sha256=mOlRPn6srm43Tp--HG4uwLt26G5CgNDJ6ORWQGPaPsk,76379
 tensorflow_data_validation/statistics/generators/mutual_information.py,sha256=r4DccsEqysEIiu2HAJ4xrW3FOaCVU06uNcQD1siiMO4,27733
 tensorflow_data_validation/statistics/generators/mutual_information_test.py,sha256=vzW065xqMMXpbDmPdbHtBjZI1mAyDGlg2jEFi0ncUjo,55105
 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator.py,sha256=dkqvnCGZikEouKw5xjiRFc0tKj2--PEA5HO8Nj3AEak,9572
 tensorflow_data_validation/statistics/generators/natural_language_domain_inferring_stats_generator_test.py,sha256=juoDC6J70tyTYxxobEfg5YpVSXwxHNc4Lj566HMP0Gs,8979
 tensorflow_data_validation/statistics/generators/natural_language_stats_generator.py,sha256=_kw-pOLenNd_TppF1clpNyDWDfQqxTR1kIUqwSE_yHw,27926
 tensorflow_data_validation/statistics/generators/natural_language_stats_generator_test.py,sha256=YeVilYxS46dJbXEpMVXpYNSVOSrA6IcrFkK45Wuh0go,18426
@@ -78,41 +78,42 @@
 tensorflow_data_validation/utils/batch_util_test.py,sha256=TVmtr7dNqFlTA8fYMO-zRADi-Q4HCmPMRQ4D75jjvbA,2549
 tensorflow_data_validation/utils/bin_util.py,sha256=BGqq3m2R2JFLuSVq1sbXgFCjrq3VxEYuF_gpsgCXQ-s,3923
 tensorflow_data_validation/utils/bin_util_test.py,sha256=iR_6OYlbKkBd0QChb8xMmFGrEmCBfSdWIcb5636UXew,2014
 tensorflow_data_validation/utils/display_util.py,sha256=Pky4eIMDstci_Qee6SYgEAL2WdSupHzoIqGgEIsS8eU,23064
 tensorflow_data_validation/utils/display_util_test.py,sha256=HtDxphHpdn9Goq0BvgaXiAfM8DFy_0TcypAMQYyj8LE,27977
 tensorflow_data_validation/utils/example_weight_map.py,sha256=kWFxCkzHRe0xfR8ArYHqRuQ-IPC85hZ-1Zp0JSu2TUg,2328
 tensorflow_data_validation/utils/example_weight_map_test.py,sha256=IOQvCNREVLH7Yj0-lElz1HYfpQYABqQff8EfDGOCiZI,2230
-tensorflow_data_validation/utils/feature_partition_util.py,sha256=qGzlEmzH7ZOsiYw2nOXysn3_mS5Rz8qU2SaDU7csadA,6874
-tensorflow_data_validation/utils/feature_partition_util_test.py,sha256=cGVEdnljgwuYkhpOfbAA6Z4IN7Wklt8k-CnMMoCywNE,10048
+tensorflow_data_validation/utils/feature_partition_util.py,sha256=9-ba6NkcjGysDnaMBmZe_qSBJH2bTPyd3d-IS_Aio-s,7020
+tensorflow_data_validation/utils/feature_partition_util_test.py,sha256=1WZs5S2m4Pn4mXQOps8NTrTkxmP9eJFU3_tz6e1NBqI,10594
 tensorflow_data_validation/utils/io_util.py,sha256=AlfqgJCu5PXVpidbwoGHT7qI435G6BfDdypz90Ij4l4,4211
 tensorflow_data_validation/utils/io_util_test.py,sha256=__sMvQQ3RCmZzvtKNpA9h_Hc7b2bfwHHHLj3TQ3ht2c,2212
 tensorflow_data_validation/utils/mutual_information_util.py,sha256=6J28gAp-y7G1UefDvVSe1aP_NYdTApl8G-_K0QVnpXU,26635
 tensorflow_data_validation/utils/mutual_information_util_test.py,sha256=V5CVVZhkNGy5qhtwioHkyDLjf2iMztmdLsbL6qTTwPw,17807
+tensorflow_data_validation/utils/preprocessing_util.py,sha256=ICs1-i5OPgr6rTvxuNh4t1BUa0N1bO0KqcvJqU8cJg0,806
 tensorflow_data_validation/utils/quantiles_util.py,sha256=S2fukrskj1T7sYEqf2kPBP1zWLsLFCbrdQfr0ElyODI,13141
 tensorflow_data_validation/utils/quantiles_util_test.py,sha256=X0yY1g7uhZxDCxGz6vWwwPFeUOS29Xm9A2G1c23tg2g,12699
 tensorflow_data_validation/utils/schema_util.py,sha256=yuSuPtWiQdTS6nL74fuzxHc6YlD47BULh4sXm-DRZns,12874
 tensorflow_data_validation/utils/schema_util_test.py,sha256=4txxgIqGicSm75Yn1vddZCjnWeZTYs3QucwVHsqjx8k,19682
 tensorflow_data_validation/utils/slicing_util.py,sha256=cpxxCXc43IWfBqgV0K2ya5iV0shBsVzghcxwkcYH3Zk,12322
 tensorflow_data_validation/utils/slicing_util_test.py,sha256=KGEB9gvJpO6cB9NLKBhnpPtPzDjg3CE7e835NLNg00M,11529
-tensorflow_data_validation/utils/statistics_io_impl.py,sha256=mStG9IZ8GbsAJQvAqBtDMRvygSuGKFFm227H6KtzdPc,3587
+tensorflow_data_validation/utils/statistics_io_impl.py,sha256=s2rKZsyPEIgyHKMIjvwHE-okBpzZCGKi20gDbZ5pNY8,3396
 tensorflow_data_validation/utils/statistics_io_impl_test.py,sha256=LJRCZgtvgYwZR88w8d117o-zy_p5m07sjxei7BuIQco,1610
 tensorflow_data_validation/utils/stats_gen_lib.py,sha256=Z050_MzICVbv7brgH7YQEqoso9aKGYPeaP3F0EggXr4,14740
 tensorflow_data_validation/utils/stats_gen_lib_test.py,sha256=T8C8s6F50FCp7KzH8Ymw9tCOnWlbzvpQvUkH2492QV8,24602
-tensorflow_data_validation/utils/stats_util.py,sha256=3Erq2sM7EVigDlb_J_OAiFbnU7ZAR02RP3JTz6pEzx4,23949
+tensorflow_data_validation/utils/stats_util.py,sha256=Pjcmi6Y9I4_T_NXAoXSgphC2FAh4UbqqT3-eYuq7S_I,24096
 tensorflow_data_validation/utils/stats_util_test.py,sha256=h5nSy9DDeIGV1fhyTG-NxQwW0XSuuBwqbigKybD_Ktw,19199
 tensorflow_data_validation/utils/test_util.py,sha256=nX_-Wy8GXyiAJRSQdFu53033Rbl_7QamN1CiJedZ8sY,19302
 tensorflow_data_validation/utils/test_util_test.py,sha256=w4lmEOHulQLirQ1_VIvWAMEk3exVoya2gRP67ziDKBM,9096
 tensorflow_data_validation/utils/top_k_uniques_stats_util.py,sha256=SIbSSYY52cAN3U1dA7GDbZ5rS3vzRhlSnD5gZodaPwo,12271
 tensorflow_data_validation/utils/top_k_uniques_stats_util_test.py,sha256=CmaOYpH-U_3LWDLIFeAaLGI8GTqYKzL4XBQ6AfdF-cY,11756
 tensorflow_data_validation/utils/validation_lib.py,sha256=mxucNWIwAFHFg-okMYj35_Nm6DiosBgw2kULQDV5ZIk,13362
 tensorflow_data_validation/utils/validation_lib_test.py,sha256=cnfabZJ70jOLd27nmxebifKj0NywfxF33CJzSD-6ngs,18748
-tensorflow_data_validation/utils/variance_util.py,sha256=5u37CgXVSFch3FDizJlfVSAgnva7x4IwpPX6a8r3Bec,5350
-tensorflow_data_validation/utils/variance_util_test.py,sha256=nw8DHk52trWK8B8S3jUWynxp8OT2pXXJMg3Lkuaqi94,7666
+tensorflow_data_validation/utils/variance_util.py,sha256=IoI66P1alp0-af_gmZYBcHDZ4_QEdNVyOYspAxjVqqI,7711
+tensorflow_data_validation/utils/variance_util_test.py,sha256=DEYnEc21RL0qz9BAgX3soy1H-EXxoR5ZR-L3gUmXTsU,15746
 tensorflow_data_validation/utils/vocab_util.py,sha256=3W7MtvJNeDI6vOJPRVntXYKKcFolFByaIgIU4elqUyY,2144
 tensorflow_data_validation/utils/vocab_util_test.py,sha256=d0Bs-K0LTzqdRoyJV7EF8OtclkOvZXkLxRoWSOvQsbc,1719
-tensorflow_data_validation-1.8.0.dist-info/LICENSE,sha256=5dSPQX_ultui1Ycz9zUeFgeTTVyFu4edm-c8WUtx7cA,11573
-tensorflow_data_validation-1.8.0.dist-info/METADATA,sha256=BtslJjHqDZihkxB9Q8OMVJfNfPgw1bxvi1x3ozpDtIU,17090
-tensorflow_data_validation-1.8.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-tensorflow_data_validation-1.8.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-tensorflow_data_validation-1.8.0.dist-info/top_level.txt,sha256=IBiFi7bLCghlkTw2UzUC7zDE1c-wz8jgl7WMldgLPvY,27
-tensorflow_data_validation-1.8.0.dist-info/RECORD,,
+tensorflow_data_validation-1.9.0.dist-info/LICENSE,sha256=5dSPQX_ultui1Ycz9zUeFgeTTVyFu4edm-c8WUtx7cA,11573
+tensorflow_data_validation-1.9.0.dist-info/METADATA,sha256=_c3C6xwv7fKJCBZ4bxEWQ1tu4vYfSjS3qZMs0mpXHi4,17288
+tensorflow_data_validation-1.9.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
+tensorflow_data_validation-1.9.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+tensorflow_data_validation-1.9.0.dist-info/top_level.txt,sha256=IBiFi7bLCghlkTw2UzUC7zDE1c-wz8jgl7WMldgLPvY,27
+tensorflow_data_validation-1.9.0.dist-info/RECORD,,
```

