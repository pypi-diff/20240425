# Comparing `tmp/nncf-2.8.1.tar.gz` & `tmp/nncf-2.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "nncf-2.8.1.tar", last modified: Fri Feb  9 09:45:52 2024, max compression
+gzip compressed data, was "nncf-2.9.0.tar", last modified: Wed Mar  6 11:39:27 2024, max compression
```

## Comparing `nncf-2.8.1.tar` & `nncf-2.9.0.tar`

### file list

```diff
@@ -1,736 +1,733 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.656464 nncf-2.8.1/
--rw-r--r--   0 runner    (1001) docker     (127)    11357 2024-02-09 09:45:37.000000 nncf-2.8.1/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)      122 2024-02-09 09:45:37.000000 nncf-2.8.1/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)    43289 2024-02-09 09:45:52.656464 nncf-2.8.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    40428 2024-02-09 09:45:37.000000 nncf-2.8.1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.576465 nncf-2.8.1/licensing/
--rw-r--r--   0 runner    (1001) docker     (127)    87030 2024-02-09 09:45:37.000000 nncf-2.8.1/licensing/third-party-programs.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.576465 nncf-2.8.1/nncf/
--rw-r--r--   0 runner    (1001) docker     (127)     3248 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.576465 nncf-2.8.1/nncf/api/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15338 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/api/compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     1005 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/api/statistics.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.576465 nncf-2.8.1/nncf/common/
--rw-r--r--   0 runner    (1001) docker     (127)      643 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (127)      891 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20689 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)     5815 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/accuracy_aware_training/runner_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/accuracy_aware_training/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)    27172 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/accuracy_aware_training/training_loop.py
--rw-r--r--   0 runner    (1001) docker     (127)      961 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/collector.py
--rw-r--r--   0 runner    (1001) docker     (127)    15199 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (127)    12876 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2763 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/deprecation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1022 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     2372 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     5883 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/graph/
--rw-r--r--   0 runner    (1001) docker     (127)     1026 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      846 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/definitions.py
--rw-r--r--   0 runner    (1001) docker     (127)    31874 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     6868 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/graph_matching.py
--rw-r--r--   0 runner    (1001) docker     (127)     8781 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/layer_attributes.py
--rw-r--r--   0 runner    (1001) docker     (127)     1362 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5872 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/operator_metatypes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/graph/patterns/
--rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/patterns/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6963 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/patterns/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    18209 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/patterns/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2741 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/transformations/command_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     7528 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)     4435 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/graph/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/hardware/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11375 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.580464 nncf-2.8.1/nncf/common/hardware/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     7845 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/configs/cpu.json
--rw-r--r--   0 runner    (1001) docker     (127)     6447 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/configs/gpu.json
--rw-r--r--   0 runner    (1001) docker     (127)     3113 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/configs/template.json
--rw-r--r--   0 runner    (1001) docker     (127)     8499 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/configs/vpu.json
--rw-r--r--   0 runner    (1001) docker     (127)     1899 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/hardware/opset.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.584464 nncf-2.8.1/nncf/common/initialization/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/initialization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4138 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/initialization/batchnorm_adaptation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/initialization/dataloader.py
--rw-r--r--   0 runner    (1001) docker     (127)    21984 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/insertion_point_graph.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.584464 nncf-2.8.1/nncf/common/logging/
--rw-r--r--   0 runner    (1001) docker     (127)      647 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2775 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/logging/logger.py
--rw-r--r--   0 runner    (1001) docker     (127)     3412 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/logging/progress_bar.py
--rw-r--r--   0 runner    (1001) docker     (127)     6333 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/logging/track_progress.py
--rw-r--r--   0 runner    (1001) docker     (127)      824 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/plotting.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.584464 nncf-2.8.1/nncf/common/pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5811 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/clusterization.py
--rw-r--r--   0 runner    (1001) docker     (127)     7915 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/mask_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)    10317 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/model_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    19556 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/node_selector.py
--rw-r--r--   0 runner    (1001) docker     (127)    15468 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     8174 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (127)    10239 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/shape_pruning_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     8102 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)      847 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)     6394 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/symbolic_mask.py
--rw-r--r--   0 runner    (1001) docker     (127)     3009 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)    16525 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     7919 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/pruning/weights_flops_calculator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.584464 nncf-2.8.1/nncf/common/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5039 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     5598 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/config_assignment.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.584464 nncf-2.8.1/nncf/common/quantization/initialization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/initialization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7614 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/initialization/range.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.588464 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    81485 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     7503 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/grouping.py
--rw-r--r--   0 runner    (1001) docker     (127)    86385 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)     4493 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1504 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_propagation/visualizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7199 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_removal.py
--rw-r--r--   0 runner    (1001) docker     (127)    23464 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizer_setup.py
--rw-r--r--   0 runner    (1001) docker     (127)     2514 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7672 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)    13696 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/quantization/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)    10292 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (127)     5847 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/scopes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.588464 nncf-2.8.1/nncf/common/sparsity/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4257 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/sparsity/collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/sparsity/controller.py
--rw-r--r--   0 runner    (1001) docker     (127)    13327 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/sparsity/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7312 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/sparsity/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)     4667 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/stateful_classes_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     5057 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/strip.py
--rw-r--r--   0 runner    (1001) docker     (127)     1477 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.588464 nncf-2.8.1/nncf/common/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6277 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (127)    23164 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/reduction.py
--rw-r--r--   0 runner    (1001) docker     (127)     5917 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/statistic_point.py
--rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/tensor_statistics/statistics.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.588464 nncf-2.8.1/nncf/common/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1213 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/api_marker.py
--rw-r--r--   0 runner    (1001) docker     (127)     5134 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     1044 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/debug.py
--rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/decorators.py
--rw-r--r--   0 runner    (1001) docker     (127)     5041 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/dot_file_rw.py
--rw-r--r--   0 runner    (1001) docker     (127)     2301 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/helpers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.588464 nncf-2.8.1/nncf/common/utils/logger/
--rw-r--r--   0 runner    (1001) docker     (127)     1247 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1996 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/os.py
--rw-r--r--   0 runner    (1001) docker     (127)     8787 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/patcher.py
--rw-r--r--   0 runner    (1001) docker     (127)     1854 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     3281 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/tensorboard.py
--rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/common/utils/timer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/config/
--rw-r--r--   0 runner    (1001) docker     (127)      704 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6950 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2080 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/definitions.py
--rw-r--r--   0 runner    (1001) docker     (127)    10034 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/extractors.py
--rw-r--r--   0 runner    (1001) docker     (127)    10150 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schema.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/config/schemata/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6986 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/accuracy_aware.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/config/schemata/algo/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2258 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/binarization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1258 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/const_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (127)     9086 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/filter_pruning.py
--rw-r--r--   0 runner    (1001) docker     (127)     2434 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/knowledge_distillation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2853 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/magnitude_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (127)    26337 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2248 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/algo/rb_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (127)     1820 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/basic.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/config/schemata/common/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1118 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/common/compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     1743 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/common/initialization.py
--rw-r--r--   0 runner    (1001) docker     (127)     5491 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/common/sparsity.py
--rw-r--r--   0 runner    (1001) docker     (127)     2589 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/common/targeting.py
--rw-r--r--   0 runner    (1001) docker     (127)     2803 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/defaults.py
--rw-r--r--   0 runner    (1001) docker     (127)    20019 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/schemata/experimental_schema.py
--rw-r--r--   0 runner    (1001) docker     (127)     3133 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/structures.py
--rw-r--r--   0 runner    (1001) docker     (127)     1046 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/telemetry_extractors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/config/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/data/
--rw-r--r--   0 runner    (1001) docker     (127)      630 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5785 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/data/dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/definitions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/experimental/common/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.592464 nncf-2.8.1/nncf/experimental/common/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5895 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/graph/netron.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/common/pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3992 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/pruning/block_hierarchy.py
--rw-r--r--   0 runner    (1001) docker     (127)     6441 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/pruning/nodes_grouping.py
--rw-r--r--   0 runner    (1001) docker     (127)    38922 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (127)    10445 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/pruning/propagation_data.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/common/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    34784 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1286 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/common/tensor_statistics/statistical_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      983 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/definitions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/tensor/functions/
--rw-r--r--   0 runner    (1001) docker     (127)     3339 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/dispatcher.py
--rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/linalg.py
--rw-r--r--   0 runner    (1001) docker     (127)    17261 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/numeric.py
--rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/numpy_linalg.py
--rw-r--r--   0 runner    (1001) docker     (127)     8126 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/numpy_numeric.py
--rw-r--r--   0 runner    (1001) docker     (127)      980 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/torch_linalg.py
--rw-r--r--   0 runner    (1001) docker     (127)     7969 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/functions/torch_numeric.py
--rw-r--r--   0 runner    (1001) docker     (127)     6370 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensor/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/tensorflow/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5805 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/tensorflow/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9324 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/argprovider.py
--rw-r--r--   0 runner    (1001) docker     (127)    17594 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/converter.py
--rw-r--r--   0 runner    (1001) docker     (127)     2308 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2178 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/node_attributes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.596464 nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3330 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (127)     2173 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)     5569 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/nncf_network.py
--rw-r--r--   0 runner    (1001) docker     (127)     9832 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/patch_tf.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/tensorflow/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16853 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/quantization/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4815 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (127)     5464 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1791 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/tensorflow/scope.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/torch/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/torch/nas/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/
--rw-r--r--   0 runner    (1001) docker     (127)      989 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9588 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    21751 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py
--rw-r--r--   0 runner    (1001) docker     (127)    24465 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)    56812 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py
--rw-r--r--   0 runner    (1001) docker     (127)     7993 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     4270 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py
--rw-r--r--   0 runner    (1001) docker     (127)      786 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py
--rw-r--r--   0 runner    (1001) docker     (127)     2842 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py
--rw-r--r--   0 runner    (1001) docker     (127)    14646 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4230 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.600464 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7980 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4109 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    30566 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/search.py
--rw-r--r--   0 runner    (1001) docker     (127)     6533 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/supernet.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2829 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py
--rw-r--r--   0 runner    (1001) docker     (127)     8170 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5574 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     8420 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    10279 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py
--rw-r--r--   0 runner    (1001) docker     (127)    13296 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4510 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py
--rw-r--r--   0 runner    (1001) docker     (127)    13371 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7311 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/pruning/operations.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/replace_custom_modules/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/replace_custom_modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5837 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/replace_custom_modules/timm_custom_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/search_building_blocks/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/search_building_blocks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    29982 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/search_building_blocks/search_blocks.py
--rw-r--r--   0 runner    (1001) docker     (127)    16263 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/search_building_blocks/search_graph.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/sparsity/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10952 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1434 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    14807 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1770 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    16594 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)    22403 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/onnx/
--rw-r--r--   0 runner    (1001) docker     (127)      633 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1759 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/onnx/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.604464 nncf-2.8.1/nncf/onnx/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4364 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/metatypes/groups.py
--rw-r--r--   0 runner    (1001) docker     (127)    21229 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/metatypes/onnx_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (127)    21215 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    17622 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/nncf_graph_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    10313 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/node_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9835 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/onnx_helper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/onnx/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/transformations/command_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4867 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/graph/transformations/commands.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/onnx/hardware/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      969 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (127)    19931 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/hardware/fused_patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/onnx/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1321 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     6396 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/quantization/ignored_patterns.py
--rw-r--r--   0 runner    (1001) docker     (127)     3474 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     3320 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/quantization/quantizer_parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/onnx/statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4012 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (127)     8317 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1391 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)      901 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/onnx/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/openvino/
--rw-r--r--   0 runner    (1001) docker     (127)      637 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4416 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/openvino/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2179 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/layer_attributes.py
--rw-r--r--   0 runner    (1001) docker     (127)     5102 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/layout.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.608464 nncf-2.8.1/nncf/openvino/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6918 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/metatypes/groups.py
--rw-r--r--   0 runner    (1001) docker     (127)    21417 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/metatypes/openvino_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (127)    33270 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2878 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9665 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/nncf_graph_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    19429 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/node_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3483 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/transformations/command_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     7360 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/graph/transformations/commands.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/hardware/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      975 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (127)    30710 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/hardware/fused_patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/pot/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7778 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/pot/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4028 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/quantization/accuracy_aware.py
--rw-r--r--   0 runner    (1001) docker     (127)    21974 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)      868 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/pot/telemetry_extractors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1571 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/backend_parameters.py
--rw-r--r--   0 runner    (1001) docker     (127)     1343 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     7167 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/ignored_patterns.py
--rw-r--r--   0 runner    (1001) docker     (127)    12972 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/quantize_ifmodel.py
--rw-r--r--   0 runner    (1001) docker     (127)    16654 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1711 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/rt_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/openvino/statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6179 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (127)    13811 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1385 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/openvino/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     5765 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      963 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18665 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/advanced_parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.612464 nncf-2.8.1/nncf/quantization/algorithms/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.616464 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20701 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     5913 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)    13716 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4801 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/openvino_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     2084 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/rank_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    12453 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/ranker.py
--rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/subset_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/algorithm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.616464 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    31860 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7633 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     5234 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/onnx_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     6013 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/bias_correction/openvino_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.616464 nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    21626 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     6319 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     6722 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/openvino_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.616464 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16024 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     6694 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     4355 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     4535 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     5045 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/torch_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.616464 nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17737 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/param_grid.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/quantization/algorithms/min_max/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    49670 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     9549 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)    10557 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/onnx_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)    12147 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/openvino_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)    15121 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/min_max/torch_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     8475 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/quantization/algorithms/post_training/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/post_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5126 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/post_training/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7710 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/post_training/pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17845 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     8531 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     8071 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/openvino_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    21610 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     5859 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     2625 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9570 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/mixed_precision.py
--rw-r--r--   0 runner    (1001) docker     (127)     7658 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/openvino_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)    11632 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/torch_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     9668 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/algorithms/weight_compression/weight_lowering.py
--rw-r--r--   0 runner    (1001) docker     (127)    15000 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/fake_quantize.py
--rw-r--r--   0 runner    (1001) docker     (127)     6565 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/passes.py
--rw-r--r--   0 runner    (1001) docker     (127)    20720 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     6740 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/range_estimator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1138 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/quantization/telemetry_extractors.py
--rw-r--r--   0 runner    (1001) docker     (127)     6374 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/scopes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/telemetry/
--rw-r--r--   0 runner    (1001) docker     (127)      792 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/telemetry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3721 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/telemetry/decorator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1183 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/telemetry/events.py
--rw-r--r--   0 runner    (1001) docker     (127)     2025 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/telemetry/extractors.py
--rw-r--r--   0 runner    (1001) docker     (127)     4589 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/telemetry/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.620464 nncf-2.8.1/nncf/tensorflow/
--rw-r--r--   0 runner    (1001) docker     (127)     2533 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5252 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)     2872 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/algorithm_selector.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/api/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2560 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/api/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2847 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/api/compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2825 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/batchnorm_adaptation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/callbacks/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/callbacks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/callbacks/checkpoint_callback.py
--rw-r--r--   0 runner    (1001) docker     (127)     3115 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/callbacks/statistics_callback.py
--rw-r--r--   0 runner    (1001) docker     (127)     3893 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)      801 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    39312 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7667 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/metatypes/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    21819 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/metatypes/keras_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/metatypes/matcher.py
--rw-r--r--   0 runner    (1001) docker     (127)    11914 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/metatypes/tf_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)    24284 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2920 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/pattern_operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     5380 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18303 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (127)     5765 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)     9051 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/graph/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.624464 nncf-2.8.1/nncf/tensorflow/hardware/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      965 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3406 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/hardware/fused_patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/helpers/
--rw-r--r--   0 runner    (1001) docker     (127)      683 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/helpers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2285 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/helpers/callback_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     5830 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/helpers/model_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/helpers/model_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     1081 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/helpers/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2519 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/initialization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/layers/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/layers/custom_objects.py
--rw-r--r--   0 runner    (1001) docker     (127)     2675 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/layers/data_layout.py
--rw-r--r--   0 runner    (1001) docker     (127)     3519 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/layers/operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    10770 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/layers/wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)      947 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16759 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/base_algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1721 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/callbacks.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      645 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26411 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     5677 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     2178 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4742 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/pruning/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    36858 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     3234 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     3416 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1830 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    11376 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (127)     4218 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7303 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)    20392 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2452 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/quantization/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.628464 nncf-2.8.1/nncf/tensorflow/sparsity/
--rw-r--r--   0 runner    (1001) docker     (127)      645 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2448 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/base_algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2099 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (127)     3562 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/collector.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.632464 nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12695 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2946 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.632464 nncf-2.8.1/nncf/tensorflow/sparsity/rb/
--rw-r--r--   0 runner    (1001) docker     (127)      660 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/rb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/rb/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1075 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/rb/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2813 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/rb/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     5305 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/rb/operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2186 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/sparsity/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1086 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.632464 nncf-2.8.1/nncf/tensorflow/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9181 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2428 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tensor_statistics/reduction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2811 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tensor_statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1527 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/tf_internals.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.632464 nncf-2.8.1/nncf/tensorflow/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1049 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/utils/hook_handle.py
--rw-r--r--   0 runner    (1001) docker     (127)      982 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/utils/node.py
--rw-r--r--   0 runner    (1001) docker     (127)      944 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/utils/scopes_handle.py
--rw-r--r--   0 runner    (1001) docker     (127)     2893 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/tensorflow/utils/state.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/
--rw-r--r--   0 runner    (1001) docker     (127)     3186 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7430 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)      940 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/accuracy_aware_training/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3396 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/algo_selector.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/automl/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/automl/agent/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/agent/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/automl/agent/ddpg/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/agent/ddpg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10034 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/agent/ddpg/ddpg.py
--rw-r--r--   0 runner    (1001) docker     (127)    10701 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/agent/ddpg/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/automl/environment/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/environment/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    33721 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/automl/environment/quantization_env.py
--rw-r--r--   0 runner    (1001) docker     (127)     1277 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/batchnorm_adaptation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.636464 nncf-2.8.1/nncf/torch/binarization/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9450 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     7177 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/binarize_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     4053 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/extensions.py
--rw-r--r--   0 runner    (1001) docker     (127)     4717 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     3864 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/binarization/reference.py
--rw-r--r--   0 runner    (1001) docker     (127)    22373 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/checkpoint_loading.py
--rw-r--r--   0 runner    (1001) docker     (127)     5674 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (127)     9890 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/compression_method_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     3359 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/debug.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/dynamic_graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19437 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/context.py
--rw-r--r--   0 runner    (1001) docker     (127)    32625 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     4376 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/graph_tracer.py
--rw-r--r--   0 runner    (1001) docker     (127)    15666 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/io_handling.py
--rw-r--r--   0 runner    (1001) docker     (127)     8907 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/layer_attributes_handlers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1830 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/op_input_processing.py
--rw-r--r--   0 runner    (1001) docker     (127)     1645 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/operation_address.py
--rw-r--r--   0 runner    (1001) docker     (127)    14892 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/patch_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (127)     3890 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/scope.py
--rw-r--r--   0 runner    (1001) docker     (127)     1573 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/scope_access.py
--rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)     8829 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/trace_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/trace_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)    12268 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/dynamic_graph/wrappers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     7745 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     5333 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/include/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/include/binarization/
--rw-r--r--   0 runner    (1001) docker     (127)      488 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/binarization/functions_cuda_impl.h
--rw-r--r--   0 runner    (1001) docker     (127)      264 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/common_cpu_funcs.h
--rw-r--r--   0 runner    (1001) docker     (127)     3966 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/common_cuda_defs.cuh
--rw-r--r--   0 runner    (1001) docker     (127)     4525 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/common_cuda_funcs.cuh
--rw-r--r--   0 runner    (1001) docker     (127)      312 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/common_defs.h
--rw-r--r--   0 runner    (1001) docker     (127)      556 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/dispatch.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/include/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      507 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/include/quantization/functions_cuda_impl.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.572464 nncf-2.8.1/nncf/torch/extensions/src/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.572464 nncf-2.8.1/nncf/torch/extensions/src/binarization/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/src/binarization/cpu/
--rw-r--r--   0 runner    (1001) docker     (127)     3758 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/src/binarization/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     1469 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    12988 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.572464 nncf-2.8.1/nncf/torch/extensions/src/common/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/src/common/cpu/
--rw-r--r--   0 runner    (1001) docker     (127)      870 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.572464 nncf-2.8.1/nncf/torch/extensions/src/quantization/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/src/quantization/cpu/
--rw-r--r--   0 runner    (1001) docker     (127)     3706 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.640464 nncf-2.8.1/nncf/torch/extensions/src/quantization/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    23431 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu
--rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/external_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     1399 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4790 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     6397 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/graph_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    35619 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/operator_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (127)     2465 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/pattern_operations.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1360 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/transformations/command_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)     7409 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (127)      718 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/graph/transformations/layout.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/hardware/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      962 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (127)    12717 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/hardware/fused_patterns.py
--rw-r--r--   0 runner    (1001) docker     (127)    12388 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/initialization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/knowledge_distillation/
--rw-r--r--   0 runner    (1001) docker     (127)      659 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/knowledge_distillation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4128 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/knowledge_distillation/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     3299 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7639 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     4331 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/layer_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    35754 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/model_analyzer.py
--rw-r--r--   0 runner    (1001) docker     (127)    18705 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/model_creation.py
--rw-r--r--   0 runner    (1001) docker     (127)    12124 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5209 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/module_operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/nested_objects_traversal.py
--rw-r--r--   0 runner    (1001) docker     (127)    14095 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/nncf_module_replacement.py
--rw-r--r--   0 runner    (1001) docker     (127)    53009 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/nncf_network.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12335 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/base_algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/export_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/pruning/filter_pruning/
--rw-r--r--   0 runner    (1001) docker     (127)      645 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    33537 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     2110 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.644464 nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14181 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py
--rw-r--r--   0 runner    (1001) docker     (127)     5367 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/legr.py
--rw-r--r--   0 runner    (1001) docker     (127)     3610 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/filter_pruning/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    31863 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     1230 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)     2159 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4845 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/pruning/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.648464 nncf-2.8.1/nncf/torch/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      746 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3977 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/adjust_padding.py
--rw-r--r--   0 runner    (1001) docker     (127)    85565 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1190 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/base_ctrl.py
--rw-r--r--   0 runner    (1001) docker     (127)     8379 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/debug_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     5644 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     4053 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/extensions.py
--rw-r--r--   0 runner    (1001) docker     (127)     1632 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/external_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6632 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/hessian_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/ignored_patterns.py
--rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/init_precision.py
--rw-r--r--   0 runner    (1001) docker     (127)    16719 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (127)    41478 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    17426 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     2363 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_constraints.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/quantization/precision_init/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5543 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/adjacent_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)    22389 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/autoq_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     7037 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/base_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     8757 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/bitwidth_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     3108 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/compression_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)      796 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/definitions.py
--rw-r--r--   0 runner    (1001) docker     (127)    10727 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/hawq_debug.py
--rw-r--r--   0 runner    (1001) docker     (127)    42874 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/hawq_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     3452 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/manual_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     2237 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/perturbations.py
--rw-r--r--   0 runner    (1001) docker     (127)     3305 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/precision_init/traces_order.py
--rw-r--r--   0 runner    (1001) docker     (127)    10863 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/quantize_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2790 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/quantize_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     4296 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/reference.py
--rw-r--r--   0 runner    (1001) docker     (127)     3740 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (127)     4169 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)     7118 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/strip.py
--rw-r--r--   0 runner    (1001) docker     (127)     1376 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/structs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1746 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/quantization/translator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2513 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/return_types.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/sparsity/
--rw-r--r--   0 runner    (1001) docker     (127)      645 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6123 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/base_algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     3642 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/collector.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/sparsity/const/
--rw-r--r--   0 runner    (1001) docker     (127)      668 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/const/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2520 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/const/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)      734 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/sparsity/magnitude/
--rw-r--r--   0 runner    (1001) docker     (127)      653 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/magnitude/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8734 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/magnitude/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)      948 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/magnitude/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/sparsity/rb/
--rw-r--r--   0 runner    (1001) docker     (127)      660 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/rb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/rb/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1060 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/rb/functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2430 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/rb/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     3829 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/sparsity/rb/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3205 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/strip.py
--rw-r--r--   0 runner    (1001) docker     (127)     7774 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/structures.py
--rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf/torch/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (127)      580 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5596 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/tensor_statistics/algo.py
--rw-r--r--   0 runner    (1001) docker     (127)    23516 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     3408 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/tensor_statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (127)    15640 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/torch/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      693 2024-02-09 09:45:37.000000 nncf-2.8.1/nncf/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-02-09 09:45:52.652464 nncf-2.8.1/nncf.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    43289 2024-02-09 09:45:52.000000 nncf-2.8.1/nncf.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    25542 2024-02-09 09:45:52.000000 nncf-2.8.1/nncf.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-02-09 09:45:52.000000 nncf-2.8.1/nncf.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      992 2024-02-09 09:45:52.000000 nncf-2.8.1/nncf.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        5 2024-02-09 09:45:52.000000 nncf-2.8.1/nncf.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-02-09 09:45:52.656464 nncf-2.8.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     7158 2024-02-09 09:45:37.000000 nncf-2.8.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.242727 nncf-2.9.0/
+-rw-r--r--   0 runner    (1001) docker     (127)    11357 2024-03-06 11:39:13.000000 nncf-2.9.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)      122 2024-03-06 11:39:13.000000 nncf-2.9.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)    41087 2024-03-06 11:39:27.242727 nncf-2.9.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    38226 2024-03-06 11:39:13.000000 nncf-2.9.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.154727 nncf-2.9.0/licensing/
+-rw-r--r--   0 runner    (1001) docker     (127)    87030 2024-03-06 11:39:14.000000 nncf-2.9.0/licensing/third-party-programs.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.154727 nncf-2.9.0/nncf/
+-rw-r--r--   0 runner    (1001) docker     (127)     4348 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.158727 nncf-2.9.0/nncf/api/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15338 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/api/compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1005 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/api/statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.158727 nncf-2.9.0/nncf/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      643 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (127)      891 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20689 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5859 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/accuracy_aware_training/runner_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/accuracy_aware_training/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27230 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/accuracy_aware_training/training_loop.py
+-rw-r--r--   0 runner    (1001) docker     (127)      961 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15259 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12900 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2763 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/deprecation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1022 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2372 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6250 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)     1026 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31904 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6849 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/graph_matching.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8781 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/layer_attributes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1362 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5890 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/operator_metatypes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/graph/patterns/
+-rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/patterns/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6963 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/patterns/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18211 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/patterns/patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2741 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7528 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4435 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/graph/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/hardware/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11403 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.162727 nncf-2.9.0/nncf/common/hardware/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     7845 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/configs/cpu.json
+-rw-r--r--   0 runner    (1001) docker     (127)     6447 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/configs/gpu.json
+-rw-r--r--   0 runner    (1001) docker     (127)     8499 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/configs/npu.json
+-rw-r--r--   0 runner    (1001) docker     (127)     3113 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/configs/template.json
+-rw-r--r--   0 runner    (1001) docker     (127)     1899 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hardware/opset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2354 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/hook_handle.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.166727 nncf-2.9.0/nncf/common/initialization/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/initialization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4138 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/initialization/batchnorm_adaptation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/initialization/dataloader.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21984 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/insertion_point_graph.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.166727 nncf-2.9.0/nncf/common/logging/
+-rw-r--r--   0 runner    (1001) docker     (127)      647 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2775 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/logging/logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3412 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/logging/progress_bar.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6333 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/logging/track_progress.py
+-rw-r--r--   0 runner    (1001) docker     (127)      824 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/plotting.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.166727 nncf-2.9.0/nncf/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5811 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/clusterization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7915 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/mask_propagation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10317 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/model_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19556 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/node_selector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15464 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8174 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10259 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/shape_pruning_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8102 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      847 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6410 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/symbolic_mask.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3009 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16549 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7919 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/pruning/weights_flops_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.166727 nncf-2.9.0/nncf/common/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5039 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5591 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/config_assignment.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.170727 nncf-2.9.0/nncf/common/quantization/initialization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/initialization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8153 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/initialization/range.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.170727 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    81562 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7503 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/grouping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    86502 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4493 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1504 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_propagation/visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8347 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_removal.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23500 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizer_setup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2514 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7672 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13754 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/quantization/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10300 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/scopes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.170727 nncf-2.9.0/nncf/common/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4289 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/sparsity/collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1410 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/sparsity/controller.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/sparsity/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7771 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/sparsity/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4667 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/stateful_classes_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5057 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1620 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/strip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.170727 nncf-2.9.0/nncf/common/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6297 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23169 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/reduction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5910 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/statistic_point.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3608 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/tensor_statistics/statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/common/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1213 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/api_marker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5163 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1044 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/debug.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5041 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/dot_file_rw.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2301 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/helpers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/common/utils/logger/
+-rw-r--r--   0 runner    (1001) docker     (127)     1247 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2017 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/os.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8787 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/patcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1874 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3281 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/tensorboard.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/common/utils/timer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/config/
+-rw-r--r--   0 runner    (1001) docker     (127)      704 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6968 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2080 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10106 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/extractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10150 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schema.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/config/schemata/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6986 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/accuracy_aware.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/config/schemata/algo/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2258 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/binarization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1258 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/const_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9086 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/filter_pruning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2434 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/knowledge_distillation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2853 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/magnitude_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26337 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2248 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/algo/rb_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1820 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/basic.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/config/schemata/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1118 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/common/compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1743 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/common/initialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5491 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/common/sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2589 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/common/targeting.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2803 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/defaults.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20019 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/schemata/experimental_schema.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3133 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/structures.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1046 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/telemetry_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/config/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.174727 nncf-2.9.0/nncf/data/
+-rw-r--r--   0 runner    (1001) docker     (127)      630 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5785 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/data/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2708 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/errors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5895 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/graph/netron.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3992 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/pruning/block_hierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6441 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/pruning/nodes_grouping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38922 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10445 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/pruning/propagation_data.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/common/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35847 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1286 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/common/tensor_statistics/statistical_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      983 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/definitions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/tensor/functions/
+-rw-r--r--   0 runner    (1001) docker     (127)     3787 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/dispatcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/linalg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/numeric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/numpy_linalg.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9418 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/numpy_numeric.py
+-rw-r--r--   0 runner    (1001) docker     (127)      980 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/torch_linalg.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/functions/torch_numeric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6370 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensor/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.178727 nncf-2.9.0/nncf/experimental/tensorflow/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5805 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/tensorflow/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9324 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/argprovider.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17612 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2308 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2178 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/node_attributes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3330 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2173 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5569 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/nncf_network.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9832 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/patch_tf.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/tensorflow/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16889 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/quantization/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4815 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5464 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1791 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/tensorflow/scope.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/torch/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/torch/nas/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/
+-rw-r--r--   0 runner    (1001) docker     (127)      989 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.182727 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9588 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21751 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24465 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)    56744 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7993 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4270 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py
+-rw-r--r--   0 runner    (1001) docker     (127)      786 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2842 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14646 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4248 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7973 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4109 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30502 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6533 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/supernet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2829 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8170 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5574 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8420 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10279 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13272 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4510 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13371 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7311 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/pruning/operations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/replace_custom_modules/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/replace_custom_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5837 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/replace_custom_modules/timm_custom_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/search_building_blocks/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/search_building_blocks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29982 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/search_building_blocks/search_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16263 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/search_building_blocks/search_graph.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10990 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1434 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14825 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1770 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16614 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22403 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.186727 nncf-2.9.0/nncf/onnx/
+-rw-r--r--   0 runner    (1001) docker     (127)      633 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1759 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4499 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/metatypes/groups.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21247 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/metatypes/onnx_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22607 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17642 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/nncf_graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10266 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/node_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9878 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/onnx_helper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2775 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4530 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/graph/transformations/commands.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/hardware/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      969 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19931 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/hardware/fused_patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1321 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6396 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8642 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3450 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/quantization/quantizer_parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/onnx/statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11056 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1391 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      971 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/onnx/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.190727 nncf-2.9.0/nncf/openvino/
+-rw-r--r--   0 runner    (1001) docker     (127)      637 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4474 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2179 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/layer_attributes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5115 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/layout.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6918 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/metatypes/groups.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21415 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/metatypes/openvino_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33709 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4211 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9665 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/nncf_graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18605 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/node_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3483 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7695 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/graph/transformations/commands.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/hardware/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30710 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/hardware/fused_patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1547 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/backend_parameters.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1343 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7167 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13180 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/quantize_ifmodel.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16233 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1711 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/rt_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.194727 nncf-2.9.0/nncf/openvino/statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7106 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11744 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1385 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/openvino/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18816 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/advanced_parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/algorithms/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21032 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5914 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13963 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4693 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4801 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2126 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/rank_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/ranker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/subset_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/algorithm.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31745 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7939 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5076 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5338 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/bias_correction/openvino_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21626 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6319 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6740 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/openvino_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.198727 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16068 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6914 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4188 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4585 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5095 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/torch_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.202727 nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17737 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/param_grid.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.202727 nncf-2.9.0/nncf/quantization/algorithms/min_max/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    50189 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9650 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11321 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12059 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15271 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/min_max/torch_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8475 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.202727 nncf-2.9.0/nncf/quantization/algorithms/post_training/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/post_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5126 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/post_training/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7566 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/post_training/pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.202727 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18976 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7566 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7559 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6909 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/torch_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.202727 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23037 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11553 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/awq.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1621 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/awq_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6810 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2644 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9553 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/mixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9206 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12934 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/torch_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10672 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/algorithms/weight_compression/weight_lowering.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15019 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/fake_quantize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6565 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/passes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22366 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6740 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/range_estimator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1138 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/quantization/telemetry_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6446 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/scopes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (127)      792 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/telemetry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3721 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/telemetry/decorator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1183 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/telemetry/events.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2025 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/telemetry/extractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4589 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/telemetry/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/tensorflow/
+-rw-r--r--   0 runner    (1001) docker     (127)     2561 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5252 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2872 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/algorithm_selector.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/tensorflow/api/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/api/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2847 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/api/compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2825 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/batchnorm_adaptation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/tensorflow/callbacks/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/callbacks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/callbacks/checkpoint_callback.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3115 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/callbacks/statistics_callback.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3893 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      801 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.206727 nncf-2.9.0/nncf/tensorflow/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39336 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7667 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/metatypes/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21837 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/metatypes/keras_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/metatypes/matcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11914 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/metatypes/tf_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24356 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2920 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/pattern_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5380 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18303 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5765 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9055 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/graph/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/hardware/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      965 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3406 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/hardware/fused_patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/helpers/
+-rw-r--r--   0 runner    (1001) docker     (127)      683 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/helpers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2285 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/helpers/callback_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5850 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/helpers/model_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/helpers/model_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/helpers/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2519 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/initialization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/layers/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/layers/custom_objects.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2675 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/layers/data_layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3455 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/layers/operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10818 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/layers/wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      947 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16759 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/base_algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1721 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/callbacks.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.210727 nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26454 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5677 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2178 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4770 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/pruning/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36890 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3234 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3416 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1830 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11398 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4218 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7327 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20476 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2452 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/quantization/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (127)      645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2448 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/base_algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2099 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3562 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/collector.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12835 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2946 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/sparsity/rb/
+-rw-r--r--   0 runner    (1001) docker     (127)      660 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/rb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8642 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/rb/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1075 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/rb/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2813 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/rb/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5305 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/rb/operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2186 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/sparsity/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1169 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9181 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2428 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tensor_statistics/reduction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2811 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tensor_statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1527 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/tf_internals.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.214727 nncf-2.9.0/nncf/tensorflow/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      982 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/utils/node.py
+-rw-r--r--   0 runner    (1001) docker     (127)      944 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/utils/scopes_handle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2893 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/tensorflow/utils/state.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.218727 nncf-2.9.0/nncf/torch/
+-rw-r--r--   0 runner    (1001) docker     (127)     3186 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.218727 nncf-2.9.0/nncf/torch/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7398 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)      940 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/accuracy_aware_training/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3396 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/algo_selector.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/automl/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/automl/agent/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/agent/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/automl/agent/ddpg/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/agent/ddpg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10034 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/agent/ddpg/ddpg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10721 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/agent/ddpg/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/automl/environment/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/environment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33714 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/automl/environment/quantization_env.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1277 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/batchnorm_adaptation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/binarization/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9621 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7157 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/binarize_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4075 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4717 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3893 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/binarization/reference.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22384 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/checkpoint_loading.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5694 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9950 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/compression_method_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3359 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/debug.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/dynamic_graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19621 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/context.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32625 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4376 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/graph_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15694 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/io_handling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8907 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/layer_attributes_handlers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1830 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/op_input_processing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/operation_address.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14920 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/patch_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3915 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/scope.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1591 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/scope_access.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8857 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/trace_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/trace_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11969 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/dynamic_graph/wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7745 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.222727 nncf-2.9.0/nncf/torch/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     5355 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/include/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/include/binarization/
+-rw-r--r--   0 runner    (1001) docker     (127)      488 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/binarization/functions_cuda_impl.h
+-rw-r--r--   0 runner    (1001) docker     (127)      264 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/common_cpu_funcs.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3966 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/common_cuda_defs.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)     4525 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/common_cuda_funcs.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)      312 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/common_defs.h
+-rw-r--r--   0 runner    (1001) docker     (127)      556 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/dispatch.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/include/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      507 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/include/quantization/functions_cuda_impl.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.150727 nncf-2.9.0/nncf/torch/extensions/src/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.150727 nncf-2.9.0/nncf/torch/extensions/src/binarization/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/src/binarization/cpu/
+-rw-r--r--   0 runner    (1001) docker     (127)     3758 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/src/binarization/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     1469 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    12988 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.150727 nncf-2.9.0/nncf/torch/extensions/src/common/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/src/common/cpu/
+-rw-r--r--   0 runner    (1001) docker     (127)      870 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.150727 nncf-2.9.0/nncf/torch/extensions/src/quantization/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/src/quantization/cpu/
+-rw-r--r--   0 runner    (1001) docker     (127)     3706 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/extensions/src/quantization/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    23431 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/external_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1399 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4808 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6397 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35619 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/operator_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2465 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/pattern_operations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1937 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7780 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (127)      718 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/graph/transformations/layout.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.226727 nncf-2.9.0/nncf/torch/hardware/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      962 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12717 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/hardware/fused_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12388 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/initialization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.230727 nncf-2.9.0/nncf/torch/knowledge_distillation/
+-rw-r--r--   0 runner    (1001) docker     (127)      659 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/knowledge_distillation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4121 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/knowledge_distillation/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3299 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7639 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4390 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/layer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35848 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/model_analyzer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18528 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/model_creation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12328 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5209 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/module_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/nested_objects_traversal.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14119 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/nncf_module_replacement.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54493 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/nncf_network.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.230727 nncf-2.9.0/nncf/torch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12335 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/base_algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/export_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.230727 nncf-2.9.0/nncf/torch/pruning/filter_pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33635 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2110 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.230727 nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14181 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5367 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3628 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/filter_pruning/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31881 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1230 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2159 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/pruning/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.234727 nncf-2.9.0/nncf/torch/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      746 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3995 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/adjust_padding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    85618 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1190 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/base_ctrl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8442 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/debug_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5644 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4075 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1632 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/external_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6632 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/hessian_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/init_precision.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16927 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42022 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17419 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2363 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_constraints.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.234727 nncf-2.9.0/nncf/torch/quantization/precision_init/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5543 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22389 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/autoq_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7030 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/base_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8757 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/bitwidth_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3108 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/compression_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)      796 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10727 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/hawq_debug.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42909 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/hawq_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3452 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/manual_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2237 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/perturbations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3305 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/precision_init/traces_order.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10858 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/quantize_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3704 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4319 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/reference.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3740 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4169 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7129 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/strip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1376 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/structs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1746 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/quantization/translator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2526 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/return_types.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.234727 nncf-2.9.0/nncf/torch/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (127)      645 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6244 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/base_algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3642 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/collector.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.234727 nncf-2.9.0/nncf/torch/sparsity/const/
+-rw-r--r--   0 runner    (1001) docker     (127)      668 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/const/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2520 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/const/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)      734 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.238727 nncf-2.9.0/nncf/torch/sparsity/magnitude/
+-rw-r--r--   0 runner    (1001) docker     (127)      653 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/magnitude/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8734 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/magnitude/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)      948 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/magnitude/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.238727 nncf-2.9.0/nncf/torch/sparsity/rb/
+-rw-r--r--   0 runner    (1001) docker     (127)      660 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/rb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7388 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/rb/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1060 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/rb/functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2430 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/rb/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3840 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/sparsity/rb/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.238727 nncf-2.9.0/nncf/torch/statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3290 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/strip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7774 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/structures.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.238727 nncf-2.9.0/nncf/torch/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5772 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/tensor_statistics/algo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23519 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3408 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/tensor_statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15658 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/torch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      693 2024-03-06 11:39:14.000000 nncf-2.9.0/nncf/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-06 11:39:27.238727 nncf-2.9.0/nncf.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    41087 2024-03-06 11:39:27.000000 nncf-2.9.0/nncf.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    25546 2024-03-06 11:39:27.000000 nncf-2.9.0/nncf.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-06 11:39:27.000000 nncf-2.9.0/nncf.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      992 2024-03-06 11:39:27.000000 nncf-2.9.0/nncf.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        5 2024-03-06 11:39:27.000000 nncf-2.9.0/nncf.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-06 11:39:27.242727 nncf-2.9.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     7158 2024-03-06 11:39:14.000000 nncf-2.9.0/setup.py
```

### Comparing `nncf-2.8.1/LICENSE` & `nncf-2.9.0/LICENSE`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/PKG-INFO` & `nncf-2.9.0/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nncf
-Version: 2.8.1
+Version: 2.9.0
 Summary: Neural Networks Compression Framework
 Home-page: https://github.com/openvinotoolkit/nncf
 Author: Intel
 Author-email: alexander.kozlov@intel.com
 License: Apache-2.0
 Keywords: compression,quantization,sparsity,mixed-precision-training,quantization-aware-training,hawq,classification,pruning,object-detection,semantic-segmentation,nas,nlp,bert,transformers,mmdetection
 Classifier: Programming Language :: Python :: 3
@@ -53,22 +53,22 @@
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "torch"
 Provides-Extra: pytorch
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "pytorch"
 Provides-Extra: onnx
 Requires-Dist: onnx~=1.13.1; extra == "onnx"
 Requires-Dist: onnxruntime~=1.14.1; python_version < "3.11" and extra == "onnx"
 Provides-Extra: openvino
-Requires-Dist: openvino==2023.3; extra == "openvino"
+Requires-Dist: openvino==2024.0; extra == "openvino"
 Provides-Extra: all
 Requires-Dist: tensorflow~=2.12.0; extra == "all"
 Requires-Dist: tensorflow-metadata<=1.13.0; extra == "all"
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "all"
 Requires-Dist: onnx~=1.13.1; extra == "all"
 Requires-Dist: onnxruntime~=1.14.1; python_version < "3.11" and extra == "all"
-Requires-Dist: openvino==2023.3; extra == "all"
+Requires-Dist: openvino==2024.0; extra == "all"
 
 <div align="center">
 
 # Neural Network Compression Framework (NNCF)
 
 [Key Features](#key-features) 
 [Installation](#installation-guide) 
@@ -338,49 +338,53 @@
 
 For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
 
 ### Jupyter* Notebook Tutorials and Demos
 
 A collection of ready-to-run Jupyter* notebooks tutorials and demos are available to explain and display NNCF compression algorithms for optimizing models for inference with the OpenVINO Toolkit.
 
-| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                                   |                                  Compression Algorithm                                  |  Backend   |               Domain                |
-|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
-| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                                           |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
-| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                             |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
-| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                                      |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
-| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                              |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
-| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb)             |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
-| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                               |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
-| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                       |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
-| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                             |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
+| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                       |                                  Compression Algorithm                                  |  Backend   |               Domain                |
+|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
+| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                               |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
+| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                 |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
+| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                          |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                  |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
+| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb) |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
+| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                   |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
+| [PyTorch Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)                                                                                                                                                                                                                                       |                                Training-Time Compression                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                           |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
+| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                 |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
 
 Below is a list of notebooks demonstrating OpenVINO conversion and inference together with NNCF compression for models from various domains.
 
-| Demo Model                                                                                                                                                                                                                                                                                                                                                |                                                                                                                 Compression Algorithm                                                                                                                  |  Backend  |                              Domain                               |
-|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------:|:-----------------------------------------------------------------:|
-| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)            |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  | Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation |
-| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                               |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                         Object Detection                          |
-| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                  |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                         |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                           |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Image-to-Image                           |
-| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                      |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                |                                                                                                   Post-Training Quantization,<br>Weight Compression                                                                                                    | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Multi-Modal Retrieval                       |
-| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                        |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb) |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                              |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                      NLP, Grammar Correction                      |
-| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                         |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                  |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                           NLP, Chat Bot                           |
+| Demo Model                                                                                                                                                                                                                                                                                                                                                                                  |               Compression Algorithm               |  Backend  |                                Domain                                |
+|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------:|:---------:|:--------------------------------------------------------------------:|
+| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)                                              |            Post-Training Quantization             | OpenVINO  |  Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation   |
+| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                           Object Detection                           |
+| [EfficientSAM](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/274-efficient-sam)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                                                    |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                                                           |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                                                             |            Post-Training Quantization             | OpenVINO  |                            Image-to-Image                            |
+| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [Segmind-VegaRT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/248-stable-diffusion-xl/248-segmind-vegart.ipynb)                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                                                        |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                                                  |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                                                  | Post-Training Quantization,<br>Weight Compression | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                        Multi-Modal Retrieval                         |
+| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                                                          |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb)                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                       NLP, Grammar Correction                        |
+| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                                                           |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                                                    |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Stable-Zephyr-3b](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/273-stable-zephyr-3b-chatbot)                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
+| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
 
 ### Post-Training Quantization Examples
 
 Compact scripts demonstrating quantization and corresponding inference speed boost:
 
 | Example Name                                                                                                                             |              Compression Algorithm               |  Backend   |         Domain         |
 |:-----------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------:|:----------:|:----------------------:|
@@ -416,21 +420,14 @@
 
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
 - [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
 
   NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
 
-### Git patches for third-party repository
-
-See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
-
-- [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
-**NOTE**: this patch is deprecated and will be removed from NNCF repository in future releases.
-
 ## Installation Guide
 
 For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 
 ```bash
@@ -447,16 +444,14 @@
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 
 ```bash
 conda install -c conda-forge nncf
 ```
 
-You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#demos-tutorials-and-samples).
-
 ### System requirements
 
 - Ubuntu\* 18.04 or later (64-bit)
 - Python\* 3.7 or later
 - Supported frameworks:
   - PyTorch\* >=2.0, <2.2
   - TensorFlow\* >=2.8.4, <=2.12.1
```

### Comparing `nncf-2.8.1/README.md` & `nncf-2.9.0/README.md`

 * *Files 5% similar despite different names*

```diff
@@ -270,49 +270,53 @@
 
 For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
 
 ### Jupyter* Notebook Tutorials and Demos
 
 A collection of ready-to-run Jupyter* notebooks tutorials and demos are available to explain and display NNCF compression algorithms for optimizing models for inference with the OpenVINO Toolkit.
 
-| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                                   |                                  Compression Algorithm                                  |  Backend   |               Domain                |
-|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
-| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                                           |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
-| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                             |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
-| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                                      |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
-| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                              |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
-| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb)             |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
-| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                               |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
-| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                       |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
-| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                             |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
+| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                       |                                  Compression Algorithm                                  |  Backend   |               Domain                |
+|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
+| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                               |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
+| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                 |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
+| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                          |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                  |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
+| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb) |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
+| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                   |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
+| [PyTorch Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)                                                                                                                                                                                                                                       |                                Training-Time Compression                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                           |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
+| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                 |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
 
 Below is a list of notebooks demonstrating OpenVINO conversion and inference together with NNCF compression for models from various domains.
 
-| Demo Model                                                                                                                                                                                                                                                                                                                                                |                                                                                                                 Compression Algorithm                                                                                                                  |  Backend  |                              Domain                               |
-|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------:|:-----------------------------------------------------------------:|
-| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)            |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  | Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation |
-| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                               |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                         Object Detection                          |
-| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                  |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                         |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                           |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Image-to-Image                           |
-| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                      |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                |                                                                                                   Post-Training Quantization,<br>Weight Compression                                                                                                    | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Multi-Modal Retrieval                       |
-| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                        |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb) |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                              |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                      NLP, Grammar Correction                      |
-| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                         |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                  |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                           NLP, Chat Bot                           |
+| Demo Model                                                                                                                                                                                                                                                                                                                                                                                  |               Compression Algorithm               |  Backend  |                                Domain                                |
+|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------:|:---------:|:--------------------------------------------------------------------:|
+| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)                                              |            Post-Training Quantization             | OpenVINO  |  Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation   |
+| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                           Object Detection                           |
+| [EfficientSAM](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/274-efficient-sam)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                                                    |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                                                           |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                                                             |            Post-Training Quantization             | OpenVINO  |                            Image-to-Image                            |
+| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [Segmind-VegaRT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/248-stable-diffusion-xl/248-segmind-vegart.ipynb)                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                                                        |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                                                  |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                                                  | Post-Training Quantization,<br>Weight Compression | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                        Multi-Modal Retrieval                         |
+| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                                                          |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb)                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                       NLP, Grammar Correction                        |
+| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                                                           |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                                                    |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Stable-Zephyr-3b](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/273-stable-zephyr-3b-chatbot)                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
+| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
 
 ### Post-Training Quantization Examples
 
 Compact scripts demonstrating quantization and corresponding inference speed boost:
 
 | Example Name                                                                                                                             |              Compression Algorithm               |  Backend   |         Domain         |
 |:-----------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------:|:----------:|:----------------------:|
@@ -348,21 +352,14 @@
 
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
 - [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
 
   NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
 
-### Git patches for third-party repository
-
-See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
-
-- [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
-**NOTE**: this patch is deprecated and will be removed from NNCF repository in future releases.
-
 ## Installation Guide
 
 For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 
 ```bash
@@ -379,16 +376,14 @@
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 
 ```bash
 conda install -c conda-forge nncf
 ```
 
-You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#demos-tutorials-and-samples).
-
 ### System requirements
 
 - Ubuntu\* 18.04 or later (64-bit)
 - Python\* 3.7 or later
 - Supported frameworks:
   - PyTorch\* >=2.0, <2.2
   - TensorFlow\* >=2.8.4, <=2.12.1
```

### Comparing `nncf-2.8.1/licensing/third-party-programs.txt` & `nncf-2.9.0/licensing/third-party-programs.txt`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/__init__.py` & `nncf-2.9.0/nncf/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,24 +14,41 @@
 
 from nncf.common.logging import nncf_logger as nncf_logger
 from nncf.common.logging.logger import disable_logging as disable_logging
 from nncf.common.logging.logger import set_log_level as set_log_level
 from nncf.common.strip import strip as strip
 from nncf.config import NNCFConfig as NNCFConfig
 from nncf.data import Dataset as Dataset
+from nncf.errors import BufferFullError as BufferFullError
+from nncf.errors import InstallationError as InstallationError
+from nncf.errors import InternalError as InternalError
+from nncf.errors import InvalidCollectorTypeError as InvalidCollectorTypeError
+from nncf.errors import InvalidPathError as InvalidPathError
+from nncf.errors import InvalidQuantizerGroupError as InvalidQuantizerGroupError
+from nncf.errors import ModuleNotFoundError as ModuleNotFoundError
+from nncf.errors import ParameterNotSupportedError as ParameterNotSupportedError
+from nncf.errors import UnknownDatasetError as UnknownDatasetError
+from nncf.errors import UnsupportedBackendError as UnsupportedBackendError
+from nncf.errors import UnsupportedDatasetError as UnsupportedDatasetError
+from nncf.errors import UnsupportedModelError as UnsupportedModelError
+from nncf.errors import UnsupportedVersionError as UnsupportedVersionError
+from nncf.errors import ValidationError as ValidationError
 from nncf.parameters import CompressWeightsMode as CompressWeightsMode
 from nncf.parameters import DropType as DropType
 from nncf.parameters import ModelType as ModelType
 from nncf.parameters import QuantizationMode as QuantizationMode
 from nncf.parameters import SensitivityMetric as SensitivityMetric
 from nncf.parameters import TargetDevice as TargetDevice
 from nncf.quantization import QuantizationPreset as QuantizationPreset
 from nncf.quantization import compress_weights as compress_weights
 from nncf.quantization import quantize as quantize
 from nncf.quantization import quantize_with_accuracy_control as quantize_with_accuracy_control
+from nncf.quantization.advanced_parameters import (
+    AdvancedAccuracyRestorerParameters as AdvancedAccuracyRestorerParameters,
+)
 from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters as AdvancedQuantizationParameters
 from nncf.scopes import IgnoredScope as IgnoredScope
 from nncf.version import __version__ as __version__
 
 _SUPPORTED_FRAMEWORKS = ["torch", "tensorflow", "onnx", "openvino"]
```

### Comparing `nncf-2.8.1/nncf/api/__init__.py` & `nncf-2.9.0/nncf/api/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/api/compression.py` & `nncf-2.9.0/nncf/api/compression.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/api/statistics.py` & `nncf-2.9.0/nncf/api/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/__init__.py` & `nncf-2.9.0/nncf/common/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/accuracy_aware_training/__init__.py` & `nncf-2.9.0/nncf/common/accuracy_aware_training/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/accuracy_aware_training/runner.py` & `nncf-2.9.0/nncf/common/accuracy_aware_training/runner.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/accuracy_aware_training/runner_factory.py` & `nncf-2.9.0/nncf/common/accuracy_aware_training/runner_factory.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
 from typing import Dict
 
+import nncf
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.accuracy_aware_training.runner import BaseAccuracyAwareTrainingRunner
 from nncf.common.accuracy_aware_training.runner import BaseAdaptiveCompressionLevelTrainingRunner
 from nncf.common.accuracy_aware_training.runner import TrainingRunner
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 
@@ -75,15 +76,15 @@
             return TFAccuracyAwareTrainingRunner(
                 self.accuracy_aware_training_params,
                 self.uncompressed_model_accuracy,
                 self.verbose,
                 self.dump_checkpoints,
                 self.lr_updates_needed,
             )
-        raise RuntimeError("Got an unsupported value of nncf_backend")
+        raise nncf.UnsupportedBackendError("Got an unsupported value of nncf_backend")
 
 
 class AdaptiveCompressionLevelTrainingRunnerCreator(TrainingRunnerCreator):
     """
     Class creates an Adaptive Compression Level Training Runner depending on an used backend.
     """
 
@@ -135,8 +136,8 @@
                 self.uncompressed_model_accuracy,
                 self.verbose,
                 self.dump_checkpoints,
                 self.lr_updates_needed,
                 self.minimal_compression_rate,
                 self.maximal_compression_rate,
             )
-        raise RuntimeError("Got an unsupported value of nncf_backend")
+        raise nncf.UnsupportedBackendError("Got an unsupported value of nncf_backend")
```

### Comparing `nncf-2.8.1/nncf/common/accuracy_aware_training/statistics.py` & `nncf-2.9.0/nncf/common/accuracy_aware_training/statistics.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/accuracy_aware_training/training_loop.py` & `nncf-2.9.0/nncf/common/accuracy_aware_training/training_loop.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -16,14 +16,15 @@
 from abc import abstractmethod
 from functools import partial
 from typing import Callable, Optional, TypeVar, Union
 
 import numpy as np
 from scipy.interpolate import interp1d
 
+import nncf
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.accuracy_aware_training.runner import BaseAccuracyAwareTrainingRunner
 from nncf.common.accuracy_aware_training.runner_factory import AdaptiveCompressionLevelTrainingRunnerCreator
 from nncf.common.accuracy_aware_training.runner_factory import EarlyExitTrainingRunnerCreator
 from nncf.common.accuracy_aware_training.statistics import TrainingLoopStatistics
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
 from nncf.common.logging import nncf_logger
@@ -274,15 +275,15 @@
         minimal_compression_rate: float = 0.0,
         maximal_compression_rate: float = 0.95,
         dump_checkpoints: bool = True,
     ):
         super().__init__(compression_controller)
         self.adaptive_controller = self._get_adaptive_compression_ctrl(compression_controller)
         if self.adaptive_controller is None:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "No compression algorithm supported by the accuracy-aware training "
                 "runner was specified in the config"
             )
 
         maximal_compression_rate = min(maximal_compression_rate, self.adaptive_controller.maximal_compression_rate)
 
         accuracy_aware_training_params = extract_accuracy_aware_training_params(nncf_config)
@@ -301,15 +302,15 @@
 
     def _get_adaptive_compression_ctrl(self, compression_controller):
         def _adaptive_compression_controllers():
             def remove_registry_prefix(algo_name):
                 for prefix in ("pt_", "tf_"):
                     if algo_name.startswith(prefix):
                         return algo_name[len(prefix) :]
-                raise RuntimeError(
+                raise nncf.ValidationError(
                     "Compression algorithm names in the adaptive controllers "
                     'registry should be prefixed with "pt_" or "tf_" depending on the '
                     "backend framework"
                 )
 
             return {
                 remove_registry_prefix(algo_name): controller_cls
@@ -319,19 +320,21 @@
         adaptive_compression_controllers = _adaptive_compression_controllers()
 
         if isinstance(compression_controller, CompositeCompressionAlgorithmController):
             for controller in compression_controller.child_ctrls:
                 for ctrl_type in adaptive_compression_controllers.values():
                     if isinstance(controller, ctrl_type):
                         return controller
-        elif isinstance(compression_controller, CompressionAlgorithmController):
-            if compression_controller.name in adaptive_compression_controllers:
-                return compression_controller
+        elif (
+            isinstance(compression_controller, CompressionAlgorithmController)
+            and compression_controller.name in adaptive_compression_controllers
+        ):
+            return compression_controller
 
-        raise RuntimeError(
+        raise nncf.InternalError(
             "No compression algorithm that supports adaptive compression accuracy-aware training was specified"
         )
 
     def run(
         self,
         model: TModel,
         train_epoch_fn: Callable,
@@ -566,8 +569,8 @@
         return EarlyExitCompressionTrainingLoop(
             nncf_config, compression_ctrl, uncompressed_model_accuracy, **additional_runner_args
         )
     if accuracy_aware_training_mode == AccuracyAwareTrainingMode.ADAPTIVE_COMPRESSION_LEVEL:
         return AdaptiveCompressionTrainingLoop(
             nncf_config, compression_ctrl, uncompressed_model_accuracy, **additional_runner_args
         )
-    raise RuntimeError("Incorrect accuracy aware mode in the config file")
+    raise nncf.InternalError("Incorrect accuracy aware mode in the config file")
```

### Comparing `nncf-2.8.1/nncf/common/collector.py` & `nncf-2.9.0/nncf/common/collector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/composite_compression.py` & `nncf-2.9.0/nncf/common/composite_compression.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Dict, List, Optional, Tuple
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmBuilder
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.api.compression import TModel
@@ -72,15 +73,15 @@
         Traverses through all children and calculates the total compression
         loss value.
 
         :return: The compression loss value.
         """
 
         if len(self._child_losses) == 0:
-            raise RuntimeError("Cannot calculate the loss value because the number of child loss is 0.")
+            raise nncf.InternalError("Cannot calculate the loss value because the number of child loss is 0.")
 
         result_loss = 0
         for loss in self._child_losses:
             result_loss += loss()
         return result_loss
 
 
@@ -196,15 +197,17 @@
     def add(self, child_ctrl: CompressionAlgorithmController) -> None:
         """
         Add `CompressionAlgorithmController` instance to the list of children.
 
         :param child_ctrl: A `CompressionAlgorithmController` instance.
         """
         if child_ctrl.model is not self.model:
-            raise RuntimeError("Cannot create a composite controller from controllers belonging to different models!")
+            raise nncf.InternalError(
+                "Cannot create a composite controller from controllers belonging to different models!"
+            )
 
         self._child_ctrls.append(child_ctrl)
         self._loss.add(child_ctrl.loss)
         self._scheduler.add(child_ctrl.scheduler)
 
     def compression_stage(self) -> CompressionStage:
         """
@@ -341,15 +344,15 @@
         self._scheduler = CompositeCompressionScheduler()
         for ctrl in self.child_ctrls:
             ctrl.disable_scheduler()
             self._scheduler.add(ctrl.scheduler)
 
     def get_compression_state(self) -> Dict[str, Any]:
         if self._builder_state is None:
-            raise RuntimeError("Internal error: builder state is not set for the controller")
+            raise nncf.InternalError("Internal error: builder state is not set for the controller")
 
         return {self.BUILDER_STATE: self._builder_state, self.CONTROLLER_STATE: self.get_state()}
 
     def set_builder_state_with_name(self, name: str, builder_state: Dict):
         """
         Sets state of the builder and the corresponding algorithm name. Should be called by the builder to set its
         state and registered algorithm key.
```

### Comparing `nncf-2.8.1/nncf/common/compression.py` & `nncf-2.9.0/nncf/common/compression.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, TypeVar
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmBuilder
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.logging import nncf_logger
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.backend import BackendType
@@ -62,15 +63,15 @@
         super().__init__(target_model)
         self._name = None
         self._builder_state = None
 
     @property
     def name(self):
         if self._name is None:
-            raise RuntimeError("Internal error: name of the controller is not set!")
+            raise nncf.InternalError("Internal error: name of the controller is not set!")
         return self._name
 
     @property
     def compression_rate(self) -> float:
         return None
 
     @compression_rate.setter
@@ -173,15 +174,15 @@
         Returns compression state - builder and controller state.
         This state should be used to resume compression via `compression_state` argument of `create_compressed_model`
         method.
 
         :return: The compression state.
         """
         if self._builder_state is None:
-            raise RuntimeError("Internal error: builder state is not set for the controller")
+            raise nncf.InternalError("Internal error: builder state is not set for the controller")
 
         return {self.BUILDER_STATE: self._builder_state, self.CONTROLLER_STATE: self.get_state()}
 
     @property
     def maximal_compression_rate(self) -> float:
         return 1.0
```

### Comparing `nncf-2.8.1/nncf/common/deprecation.py` & `nncf-2.9.0/nncf/common/deprecation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/engine.py` & `nncf-2.9.0/nncf/common/engine.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/exporter.py` & `nncf-2.9.0/nncf/common/exporter.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/factory.py` & `nncf-2.9.0/nncf/common/factory.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import TypeVar
 
+import nncf
 from nncf.common.engine import Engine
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.command_creation import CommandCreator
 from nncf.common.tensor_statistics import aggregator
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
@@ -39,42 +40,43 @@
             return GraphConverter.create_nncf_graph(model)
         if model_backend == BackendType.OPENVINO:
             from nncf.openvino.graph.nncf_graph_builder import GraphConverter
 
             return GraphConverter.create_nncf_graph(model)
         if model_backend == BackendType.TORCH:
             return model.nncf.get_graph()
-        raise RuntimeError(
+        raise nncf.UnsupportedBackendError(
             "Cannot create backend-specific graph because {} is not supported!".format(model_backend.value)
         )
 
 
 class ModelTransformerFactory:
     @staticmethod
-    def create(model: TModel) -> ModelTransformer:
+    def create(model: TModel, inplace: bool = False) -> ModelTransformer:
         """
         Factory method to create backend-specific ModelTransformer instance based on the input model.
 
         :param model: backend-specific model instance
+        :param inplace: apply transformations inplace
         :return: backend-specific ModelTransformer instance
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
             from nncf.onnx.graph.model_transformer import ONNXModelTransformer
 
             return ONNXModelTransformer(model)
         if model_backend == BackendType.OPENVINO:
             from nncf.openvino.graph.model_transformer import OVModelTransformer
 
-            return OVModelTransformer(model)
+            return OVModelTransformer(model, inplace=inplace)
         if model_backend == BackendType.TORCH:
             from nncf.torch.model_transformer import PTModelTransformer
 
             return PTModelTransformer(model)
-        raise RuntimeError(
+        raise nncf.UnsupportedBackendError(
             "Cannot create backend-specific model transformer because {} is not supported!".format(model_backend.value)
         )
 
 
 class EngineFactory:
     @staticmethod
     def create(model: TModel) -> Engine:
@@ -93,15 +95,15 @@
             from nncf.openvino.engine import OVNativeEngine
 
             return OVNativeEngine(model)
         if model_backend == BackendType.TORCH:
             from nncf.torch.engine import PTEngine
 
             return PTEngine(model)
-        raise RuntimeError(
+        raise nncf.UnsupportedBackendError(
             "Cannot create backend-specific engine because {} is not supported!".format(model_backend.value)
         )
 
 
 class CommandCreatorFactory:
     @staticmethod
     def create(model: TModel) -> CommandCreator:
@@ -112,15 +114,21 @@
         :return: backend-specific CommandCreator instance
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.OPENVINO:
             from nncf.openvino.graph.transformations.command_creation import OVCommandCreator
 
             return OVCommandCreator()
-        raise RuntimeError(
+
+        if model_backend == BackendType.ONNX:
+            from nncf.onnx.graph.transformations.command_creation import ONNXCommandCreator
+
+            return ONNXCommandCreator()
+
+        raise nncf.UnsupportedBackendError(
             "Cannot create backend-specific command creator because {} is not supported!".format(model_backend.value)
         )
 
 
 class StatisticsAggregatorFactory:
     @staticmethod
     def create(model: TModel, dataset: Dataset) -> aggregator.StatisticsAggregator:
@@ -139,12 +147,12 @@
             from nncf.openvino.statistics.aggregator import OVStatisticsAggregator
 
             return OVStatisticsAggregator(dataset)
         if model_backend == BackendType.TORCH:
             from nncf.torch.statistics.aggregator import PTStatisticsAggregator
 
             return PTStatisticsAggregator(dataset)
-        raise RuntimeError(
+        raise nncf.UnsupportedBackendError(
             "Cannot create backend-specific statistics aggregator because {} is not supported!".format(
                 model_backend.value
             )
         )
```

### Comparing `nncf-2.8.1/nncf/common/graph/__init__.py` & `nncf-2.9.0/nncf/common/graph/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/definitions.py` & `nncf-2.9.0/nncf/common/graph/definitions.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/graph.py` & `nncf-2.9.0/nncf/common/graph/graph.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 from collections import defaultdict
 from copy import deepcopy
 from typing import Any, Callable, Dict, Generator, KeysView, List, Optional, Tuple, Type, ValuesView
 
 import networkx as nx
 import networkx.algorithms.isomorphism as iso
 
+import nncf
 from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import Dtype
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.patterns import GraphPattern
@@ -635,17 +636,17 @@
             node.pop("label")
 
         return out_graph
 
     def get_node_by_name(self, name: NNCFNodeName) -> NNCFNode:
         node_ids = self._node_name_to_node_id_map.get(name, None)
         if node_ids is None:
-            raise RuntimeError("Could not find a node {} in NNCFGraph!".format(name))
+            raise nncf.InternalError("Could not find a node {} in NNCFGraph!".format(name))
         if len(node_ids) > 1:
-            raise RuntimeError(f"More than one node in NNCFGraph matches name {name}")
+            raise nncf.InternalError(f"More than one node in NNCFGraph matches name {name}")
 
         node_key = f"{node_ids[0]} {name}"
         return self._nodes[node_key]
 
     def __eq__(self, other: "NNCFGraph"):
         nm = iso.categorical_node_match(
             [NNCFNode.ID_NODE_ATTR, NNCFNode.KEY_NODE_ATTR, NNCFNode.LAYER_ATTRIBUTES], [None, None, None]
@@ -688,15 +689,15 @@
                 parallel_input_port_ids=data[NNCFGraph.PARALLEL_INPUT_PORT_IDS_ATTR],
             )
             if from_node_key in match:
                 output_nncf_edges.append(nncf_edge)
             elif to_node_key in match:
                 input_nncf_edges.append(nncf_edge)
             else:
-                raise RuntimeError("Invalid graph expression supplied!")
+                raise nncf.InternalError("Invalid graph expression supplied!")
 
         return NNCFGraphPatternIO(input_nncf_edges, output_nncf_edges)
 
     def get_nx_edge(self, node_u: NNCFNode, node_v: NNCFNode):
         nx_node_u = self._nx_graph.nodes[self._node_id_to_key_dict[node_u.node_id]]
         nx_node_v = self._nx_graph.nodes[self._node_id_to_key_dict[node_v.node_id]]
         return self._nx_graph.edges[nx_node_u["key"], nx_node_v["key"]]
```

### Comparing `nncf-2.8.1/nncf/common/graph/graph_matching.py` & `nncf-2.9.0/nncf/common/graph/graph_matching.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -25,18 +25,17 @@
         if attr == GraphPattern.METATYPE_ATTR:
             # GraphPattern.ANY_PATTERN_NODE_TYPE and GraphPattern.NON_PATTERN_NODE_TYPE
             # are matched to any node type.
             if GraphPattern.ANY_PATTERN_NODE_TYPE in node_2[attr] or GraphPattern.NON_PATTERN_NODE_TYPE in node_2[attr]:
                 continue
             # Torch and TF pattern mapping based on 'type' section,
             # While ONNX mapping based on metatypes -
-            # to support all of them, we need to check the existane of the attributes
-            if GraphPattern.NODE_TYPE_ATTR in node_1:
-                if node_1[GraphPattern.NODE_TYPE_ATTR] in node_2[attr]:
-                    continue
+            # to support all of them, we need to check the existence of the attributes
+            if GraphPattern.NODE_TYPE_ATTR in node_1 and node_1[GraphPattern.NODE_TYPE_ATTR] in node_2[attr]:
+                continue
         if node_1[attr] not in node_2[attr]:
             return False
     return True
 
 
 def _sort_patterns_by_len(pattern: nx.DiGraph) -> int:
     """
```

### Comparing `nncf-2.8.1/nncf/common/graph/layer_attributes.py` & `nncf-2.9.0/nncf/common/graph/layer_attributes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/model_transformer.py` & `nncf-2.9.0/nncf/common/graph/model_transformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/operator_metatypes.py` & `nncf-2.9.0/nncf/common/graph/operator_metatypes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List, Optional, Type
 
+import nncf
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.utils.registry import Registry
 
 
 class OperatorMetatype:
     """
     Base class for grouping framework operators based on their semantic meaning.
@@ -98,15 +99,15 @@
             cls_name = name_
             if cls_name is None:
                 cls_name = obj.__name__
             super_register(obj, cls_name)
             op_names = obj.get_all_aliases()
             for name in op_names:
                 if name in self._op_name_to_op_meta_dict and not obj.subtype_check(self._op_name_to_op_meta_dict[name]):
-                    raise RuntimeError(
+                    raise nncf.InternalError(
                         "Inconsistent operator metatype registry - single patched "
                         "op name maps to multiple metatypes!"
                     )
 
                 self._op_name_to_op_meta_dict[name] = obj
             return obj
```

### Comparing `nncf-2.8.1/nncf/common/graph/patterns/__init__.py` & `nncf-2.9.0/nncf/common/graph/patterns/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/patterns/manager.py` & `nncf-2.9.0/nncf/common/graph/patterns/manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/patterns/patterns.py` & `nncf-2.9.0/nncf/common/graph/patterns/patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 from dataclasses import dataclass
 from enum import Enum
 from typing import Dict, Hashable, List, Optional, Tuple
 
 import networkx as nx
 import networkx.algorithms.isomorphism as ism
 
+import nncf
 from nncf.common.utils.dot_file_rw import write_dot_graph
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
 
 
 class Patterns:
     """
@@ -228,17 +229,16 @@
             remapped_edges = []
             for edge in edges:
                 new_edge = (edge[0], node_mapping[edge[1]])
                 remapped_edges.append(new_edge)
             self._graph.add_edges_from(remapped_edges)
 
     def add_node(self, **attrs) -> int:
-        if GraphPattern.METATYPE_ATTR in attrs:
-            if not isinstance(attrs[GraphPattern.METATYPE_ATTR], list):
-                attrs[GraphPattern.METATYPE_ATTR] = [attrs[GraphPattern.METATYPE_ATTR]]
+        if GraphPattern.METATYPE_ATTR in attrs and not isinstance(attrs[GraphPattern.METATYPE_ATTR], list):
+            attrs[GraphPattern.METATYPE_ATTR] = [attrs[GraphPattern.METATYPE_ATTR]]
         self._graph.add_node(self._node_counter, **attrs)
         self._node_counter += 1
         return self._node_counter - 1
 
     def add_edge(self, u_name, v_name) -> None:
         self._graph.add_edge(u_name, v_name)
 
@@ -254,15 +254,15 @@
 
 def merge_two_types_of_operations(first_op: Dict, second_op: Dict, label: str) -> Dict:
     if GraphPattern.METATYPE_ATTR in first_op and GraphPattern.METATYPE_ATTR in second_op:
         res = {GraphPattern.METATYPE_ATTR: first_op[GraphPattern.METATYPE_ATTR]}
         res[GraphPattern.METATYPE_ATTR].extend(second_op[GraphPattern.METATYPE_ATTR])
         res[GraphPattern.LABEL_ATTR] = label
         return res
-    raise RuntimeError("Incorrect dicts of operations")
+    raise nncf.InternalError("Incorrect dicts of operations")
 
 
 @dataclass
 class PatternDesc:
     """
     Contains needed fields for the description of the pattern.
 
@@ -354,15 +354,15 @@
     LINEAR_ACTIVATIONS_UNSQUEEZE_BN_SQUEEZE = PatternDesc("linear_activations_unsqueeze_bn_squeeze")
     SCALE_SHIFT_ACTIVATIONS = PatternDesc("scale_shift_activations")
     MVN_SCALE_SHIFT_ACTIVATIONS = PatternDesc("mvn_scale_shift_activations")
 
     # DEVICE PATTERNS
     HSWISH_ACTIVATION_CLAMP_MULTIPLY = PatternDesc(
         "hswish_activation_clamp_multiply",
-        devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU, TargetDevice.VPU],
+        devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU, TargetDevice.NPU],
     )
     LINEAR_SCALE_SHIFT = PatternDesc(
         "linear_scale_shift", devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU]
     )
     LINEAR_BIASED_SCALE_SHIFT = PatternDesc(
         "linear_biased_scale_shift", devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU]
     )
@@ -390,12 +390,12 @@
     """
     Describes the patterns, which nodes should be ignored during FakeQuantize placement.
     """
 
     MULTIHEAD_ATTENTION_OUTPUT = PatternDesc(
         "multihead_attention_output",
         model_types=[ModelType.TRANSFORMER],
-        devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU, TargetDevice.VPU],
+        devices=[TargetDevice.ANY, TargetDevice.CPU, TargetDevice.GPU, TargetDevice.NPU],
     )
     SE_BLOCK = PatternDesc("se_block")
     FC_BN_HSWISH_ACTIVATION = PatternDesc("fc_bn_hswish_activation")
     EQUAL_LOGICALNOT = PatternDesc("equal_logicalnot")
```

### Comparing `nncf-2.8.1/nncf/common/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/common/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/transformations/command_creation.py` & `nncf-2.9.0/nncf/common/graph/transformations/command_creation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/transformations/commands.py` & `nncf-2.9.0/nncf/common/graph/transformations/commands.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/transformations/layout.py` & `nncf-2.9.0/nncf/common/graph/transformations/layout.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/graph/utils.py` & `nncf-2.9.0/nncf/common/graph/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/hardware/__init__.py` & `nncf-2.9.0/nncf/common/hardware/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/hardware/config.py` & `nncf-2.9.0/nncf/common/hardware/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,35 +13,36 @@
 from collections import OrderedDict
 from enum import Enum
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Set, Type
 
 import jstyleson as json
 
+import nncf
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization import quantizers as quant
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.utils.helpers import product_dict
 from nncf.common.utils.os import safe_open
 from nncf.definitions import HW_CONFIG_RELATIVE_DIR
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
 
 
 class HWConfigType(Enum):
     CPU = "CPU"
     GPU = "GPU"
-    VPU = "VPU"
+    NPU = "NPU"
 
 
 HW_CONFIG_TYPE_TARGET_DEVICE_MAP = {
     "ANY": HWConfigType.CPU.value,
     "CPU": HWConfigType.CPU.value,
-    "VPU": HWConfigType.VPU.value,
+    "NPU": HWConfigType.NPU.value,
     "GPU": HWConfigType.GPU.value,
     "CPU_SPR": HWConfigType.CPU.value,
 }
 
 
 HWConfigOpName = str
 
@@ -62,15 +63,15 @@
 class HWConfig(list, ABC):
     QUANTIZATION_ALGORITHM_NAME = "quantization"
     ATTRIBUTES_NAME = "attributes"
     SCALE_ATTRIBUTE_NAME = "scales"
     UNIFIED_TYPE_NAME = "unified"
     ADJUST_PADDING_ATTRIBUTE_NAME = "adjust_padding"
 
-    TYPE_TO_CONF_NAME_DICT = {HWConfigType.CPU: "cpu.json", HWConfigType.VPU: "vpu.json", HWConfigType.GPU: "gpu.json"}
+    TYPE_TO_CONF_NAME_DICT = {HWConfigType.CPU: "cpu.json", HWConfigType.NPU: "npu.json", HWConfigType.GPU: "gpu.json"}
 
     def __init__(self):
         super().__init__()
         self.registered_algorithm_configs = {}
         self.target_device = None
 
     @abstractmethod
@@ -136,23 +137,23 @@
 
     @staticmethod
     def get_quantization_mode_from_config_value(str_val: str):
         if str_val == "symmetric":
             return QuantizationMode.SYMMETRIC
         if str_val == "asymmetric":
             return QuantizationMode.ASYMMETRIC
-        raise RuntimeError("Invalid quantization type specified in HW config")
+        raise nncf.ValidationError("Invalid quantization type specified in HW config")
 
     @staticmethod
     def get_is_per_channel_from_config_value(str_val: str):
         if str_val == "perchannel":
             return True
         if str_val == "pertensor":
             return False
-        raise RuntimeError("Invalid quantization granularity specified in HW config")
+        raise nncf.ValidationError("Invalid quantization granularity specified in HW config")
 
     @staticmethod
     def get_qconf_from_hw_config_subdict(quantization_subdict: Dict):
         bits = quantization_subdict["bits"]
         mode = HWConfig.get_quantization_mode_from_config_value(quantization_subdict["mode"])
         is_per_channel = HWConfig.get_is_per_channel_from_config_value(quantization_subdict["granularity"])
         signedness_to_force = None
```

### Comparing `nncf-2.8.1/nncf/common/hardware/configs/cpu.json` & `nncf-2.9.0/nncf/common/hardware/configs/cpu.json`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/common/hardware/configs/gpu.json` & `nncf-2.9.0/nncf/common/hardware/configs/gpu.json`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/common/hardware/configs/template.json` & `nncf-2.9.0/nncf/common/hardware/configs/template.json`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/common/hardware/configs/vpu.json` & `nncf-2.9.0/nncf/common/hardware/configs/npu.json`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 {
-    "target_device": "VPU",
+    "target_device": "NPU",
     "config": {
         "quantization": {
             "q8_tn": {
                 "bits": 8,
                 "mode": ["asymmetric", "symmetric"],
                 "granularity": "pertensor"
             },
```

### Comparing `nncf-2.8.1/nncf/common/hardware/opset.py` & `nncf-2.9.0/nncf/common/hardware/opset.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/initialization/__init__.py` & `nncf-2.9.0/nncf/common/pruning/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,10 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Functions and classes utilized during the user dataset-driven initialization of the compression algorithms.
-"""
```

### Comparing `nncf-2.8.1/nncf/common/initialization/batchnorm_adaptation.py` & `nncf-2.9.0/nncf/common/initialization/batchnorm_adaptation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/initialization/dataloader.py` & `nncf-2.9.0/nncf/common/initialization/dataloader.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/insertion_point_graph.py` & `nncf-2.9.0/nncf/common/insertion_point_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/logging/__init__.py` & `nncf-2.9.0/nncf/common/logging/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/logging/logger.py` & `nncf-2.9.0/nncf/common/logging/logger.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/logging/progress_bar.py` & `nncf-2.9.0/nncf/common/logging/progress_bar.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/logging/track_progress.py` & `nncf-2.9.0/nncf/common/logging/track_progress.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/plotting.py` & `nncf-2.9.0/nncf/common/plotting.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/__init__.py` & `nncf-2.9.0/nncf/common/quantization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/clusterization.py` & `nncf-2.9.0/nncf/common/pruning/clusterization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/mask_propagation.py` & `nncf-2.9.0/nncf/common/pruning/mask_propagation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/model_analysis.py` & `nncf-2.9.0/nncf/common/pruning/model_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/node_selector.py` & `nncf-2.9.0/nncf/common/pruning/node_selector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/operations.py` & `nncf-2.9.0/nncf/common/pruning/operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -311,15 +311,15 @@
         output_shapes = [edge.tensor_shape[chunk_axis] for edge in output_edges]
 
         # if identity split detected
         if len(output_shapes) == 1:
             # propagate as is
             return input_mask
 
-        if not input_mask.shape[0] == sum(output_shapes):
+        if input_mask.shape[0] != sum(output_shapes):
             return None
 
         split_masks = tensor_processor.split(input_mask, output_shapes)
         result_masks = cls.match_multiple_output_masks(split_masks, output_edges, chunk_axis)
 
         return result_masks
```

### Comparing `nncf-2.8.1/nncf/common/pruning/schedulers.py` & `nncf-2.9.0/nncf/common/pruning/schedulers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/shape_pruning_processor.py` & `nncf-2.9.0/nncf/common/pruning/shape_pruning_processor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Callable, Dict, List, Tuple
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.structs import PrunedLayerInfoBase
@@ -164,15 +165,15 @@
         for input_mask in get_input_masks(next_node, graph):
             if not input_mask:
                 continue
             for mask_producer in input_mask.mask_producers:
                 if mask_producer.id in cluster_nodes_idxs:
                     return mask_producer.sparse_multiplier
 
-        raise RuntimeError(f"Next node for cluster {cluster.elements} doesn't have closing mask")
+        raise nncf.ValidationError(f"Next node for cluster {cluster.elements} doesn't have closing mask")
 
     def get_next_nodes(
         self, graph: NNCFGraph, pruning_groups: Clusterization[PrunedLayerInfoBase]
     ) -> Dict[int, Dict[str, Any]]:
         """
         Finds nodes of `prunable_types` types that receive the output of a pruned cluster as input
         and collects all info specified in NextNode.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/statistics.py` & `nncf-2.9.0/nncf/common/pruning/statistics.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/structs.py` & `nncf-2.9.0/nncf/common/pruning/structs.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/symbolic_mask.py` & `nncf-2.9.0/nncf/common/pruning/symbolic_mask.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List, Union
 
+import nncf
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
 from nncf.common.tensor import NNCFTensor
 
 
 class SymbolicMaskProducer:
     """
     Container of information about a NNCFNode which is produsing a symbolic mask.
@@ -105,15 +106,15 @@
         producers = SymbolicMaskProducer.merge_producers(tensors)
         return SymbolicMask(ret_shape, producers)
 
     @classmethod
     def ones(cls, shape: Union[int, List[int]], device) -> SymbolicMask:
         if isinstance(shape, list):
             if len(shape) != 1:
-                raise RuntimeError(f"Unexpected shape = {shape} for 1D symbolic mask")
+                raise nncf.ValidationError(f"Unexpected shape = {shape} for 1D symbolic mask")
             shape = shape[0]
 
         return SymbolicMask(shape)
 
     @classmethod
     def assert_allclose(cls, tensors: List[SymbolicMask]) -> None:
         for input_mask in tensors[1:]:
@@ -135,15 +136,15 @@
         In case input_masks have different shape don't propagate any masks.
 
         :param input_masks: Given input masks.
         :return: Elementwise pruning operation output mask.
         """
         producers = SymbolicMaskProducer.merge_producers(input_masks)
         for input_mask in input_masks[1:]:
-            if not input_masks[0].shape == input_mask.shape:
+            if input_masks[0].shape != input_mask.shape:
                 return AmbiguousSymbolicMask(producers)
 
         return SymbolicMask(input_masks[0].shape[0], producers)
 
     @classmethod
     def split(cls, tensor: SymbolicMask, output_shapes: List[int]) -> List[SymbolicMask]:
         if any(shape <= 0 for shape in output_shapes) or tensor.shape[0] != sum(output_shapes):
```

### Comparing `nncf-2.8.1/nncf/common/pruning/tensor_processor.py` & `nncf-2.9.0/nncf/common/pruning/tensor_processor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/utils.py` & `nncf-2.9.0/nncf/common/pruning/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,14 +12,15 @@
 import math
 from enum import Enum
 from functools import partial
 from typing import Dict, List, Optional, Tuple, Union
 
 import numpy as np
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.tensor import NNCFTensor
 from nncf.common.utils.registry import Registry
@@ -381,30 +382,30 @@
     :return: Count of input channels of the given node.
     """
     layer_attrs: Union[ConvolutionLayerAttributes, LinearLayerAttributes] = node.layer_attributes
     if isinstance(layer_attrs, ConvolutionLayerAttributes):
         return layer_attrs.in_channels
     if isinstance(layer_attrs, LinearLayerAttributes):
         return layer_attrs.in_features
-    raise RuntimeError(f"Can't get count of input channels from node {node}")
+    raise nncf.InternalError(f"Can't get count of input channels from node {node}")
 
 
 def get_output_channels(node: NNCFNode) -> int:
     """
     Returns count of output channels of an prunable node.
 
     :param node: Given prunable node.
     :return: Count of output channels of the given node.
     """
     layer_attrs: Union[ConvolutionLayerAttributes, LinearLayerAttributes] = node.layer_attributes
     if isinstance(layer_attrs, ConvolutionLayerAttributes):
         return layer_attrs.out_channels
     if isinstance(layer_attrs, LinearLayerAttributes):
         return layer_attrs.out_features
-    raise RuntimeError(f"Can't get count of output channels from node {node}")
+    raise nncf.InternalError(f"Can't get count of output channels from node {node}")
 
 
 def identity_mask_propagation(node: NNCFNode, graph: NNCFGraph) -> None:
     """
     Propagates input mask through NNCFNode.
 
     :param node: Graph node to perform identity mask propagation on.
```

### Comparing `nncf-2.8.1/nncf/common/pruning/weights_flops_calculator.py` & `nncf-2.9.0/nncf/common/pruning/weights_flops_calculator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/__init__.py` & `nncf-2.9.0/nncf/common/quantization/initialization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/collectors.py` & `nncf-2.9.0/nncf/common/quantization/collectors.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/config_assignment.py` & `nncf-2.9.0/nncf/common/quantization/config_assignment.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -32,15 +32,15 @@
     :param scope_overrides: A dictionary of scope strings vs. dict of overrides for the corresponding
       scope.
     :return: The base configuration with overrides applied on top of it.
     """
     qconfig = deepcopy(base_config)
     if scope_overrides is None:
         scope_overrides = {}
-    for overridden_scope in scope_overrides.keys():
+    for overridden_scope in scope_overrides:
         if matches_any(scope_str, overridden_scope):
             config_overrides = scope_overrides[overridden_scope]
             if config_overrides.get("bits") is not None:
                 qconfig.num_bits = config_overrides["bits"]
             if config_overrides.get("mode") is not None:
                 qconfig.mode = config_overrides["mode"]
             if config_overrides.get("per_channel") is not None:
```

### Comparing `nncf-2.8.1/nncf/common/quantization/initialization/__init__.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/initialization/range.py` & `nncf-2.9.0/nncf/common/quantization/initialization/range.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Dict, List, Optional
 
 from nncf.common.initialization.dataloader import NNCFDataLoader
-from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
+from nncf.common.quantization.structs import QuantizationScheme
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.config.schemata.defaults import NUM_INIT_SAMPLES
 
 
 class RangeInitConfig:
     """
     The `RangeInitConfig` class representing the quantization range initialization
@@ -144,41 +144,63 @@
 
 
 class RangeInitCollectorParams:
     """
     Defines low-level parameters that are used to instantiate statistic collectors.
     """
 
-    def __init__(self, is_weights: bool, mode: QuantizationMode, per_channel: bool):
+    def __init__(self, is_weights: bool, scheme: QuantizationScheme, per_channel: bool):
         """
         Initializes Range Initialization Collector Parameters.
 
         :param is_weights: Boolean that defines tensor type. True for Weights, False for Activations.
-        :param mode: Quantization mode: symmetric or asymmetric.
+        :param scheme: Quantization scheme: symmetric or asymmetric.
         :param per_channel: Quantization granularity.
         """
         self._is_weights = is_weights
-        self._mode = mode
-        self._per_channel = per_channel
+        self._scheme = scheme
+        self._is_per_channel = per_channel
+
+    @property
+    def is_weights(self) -> bool:
+        """
+        Returns boolean that defines tensor type.
+        True for Weights, False for Activations.
+        """
+        return self._is_weights
+
+    @property
+    def scheme(self) -> QuantizationScheme:
+        """
+        Returns quantization scheme: symmetric or asymmetric.
+        """
+        return self._scheme
+
+    @property
+    def is_per_channel(self) -> bool:
+        """
+        Returns quantization granularity.
+        """
+        return self._is_per_channel
 
     def use_per_sample_stats(self, per_sample_stats) -> bool:
         """
         For activations, if per_sample_stats is True, statistics will be collected per-sample.
         For weights statistics are always collected per-batch.
 
         :param per_sample_stats: Defined by certain collector design.
         :return: A boolean that defines whether to collect statistics per-sample or per-batch.
         """
         return per_sample_stats and (not self._is_weights)
 
     @property
     def use_abs_max(self) -> bool:
         """Applies abs(max) for symmetric quantization."""
-        return self._mode == QuantizationMode.SYMMETRIC
+        return self._scheme == QuantizationScheme.SYMMETRIC
 
     @property
     def use_means_of_mins(self) -> bool:
-        return not self._is_weights and not self._per_channel and self._mode == "asymmetric"
+        return not self._is_weights and not self._is_per_channel and self._scheme == "asymmetric"
 
     @property
     def use_means_of_maxs(self) -> bool:
-        return not self._is_weights and not self._per_channel
+        return not self._is_weights and not self._is_per_channel
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/__init__.py` & `nncf-2.9.0/nncf/common/sparsity/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/graph.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,14 +13,15 @@
 from copy import copy
 from copy import deepcopy
 from dataclasses import dataclass
 from typing import Any, Callable, Deque, Dict, List, Optional, Set, Tuple, Type, Union
 
 import networkx as nx
 
+import nncf
 from nncf import nncf_logger
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import NoopMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatype
@@ -217,15 +218,15 @@
     ) -> QuantizerPropagationStateGraphNodeType:
         if ipg_node_type == InsertionPointGraphNodeType.PRE_HOOK:
             return QuantizerPropagationStateGraphNodeType.PRE_HOOK
         if ipg_node_type == InsertionPointGraphNodeType.POST_HOOK:
             return QuantizerPropagationStateGraphNodeType.POST_HOOK
         if ipg_node_type == InsertionPointGraphNodeType.OPERATOR:
             return QuantizerPropagationStateGraphNodeType.OPERATOR
-        raise RuntimeError("Invalid insertion point graph node type.")
+        raise nncf.ValidationError("Invalid insertion point graph node type.")
 
     @staticmethod
     def get_barrier_node_key(node_key: str) -> str:
         return f"{QuantizerPropagationStateGraph.BARRIER_NODE_KEY_POSTFIX} {node_key}"
 
     def mark_act_quantizer_as_dependent_on_weights(self, pq: PropagatingQuantizer, operator_node_key: str):
         """
@@ -249,15 +250,15 @@
             op_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
             is QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS
         )
         if (
             pq in self._pqs_after_weight_dependent_output_quantized_nodes
             and self._pqs_after_weight_dependent_output_quantized_nodes[pq] != operator_node_key
         ):
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"Propagating quantizer {pq.id} is already marked as depending on node "
                 f"{operator_node_key} weight quantization!"
             )
         self._pqs_after_weight_dependent_output_quantized_nodes[pq] = operator_node_key
 
     @staticmethod
     def is_insertion_point(qpsg_node_type: QuantizerPropagationStateGraphNodeType) -> bool:
@@ -325,15 +326,15 @@
             for affected_edge_tuple in prop_quantizer.affected_edges:
                 edge = self.edges[affected_edge_tuple]
                 affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
                 for pq in surviving_quantizers:
                     affecting_quantizers.append(pq)
             self.remove_propagating_quantizer(prop_quantizer)
         else:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "Surviving_quantizers not found !"
                 " Nodes quantized with quantizer #{} will be lost".format(prop_quantizer.id)
             )
 
     @staticmethod
     def _get_major_unified_scale_type(type_list: List[Optional[UnifiedScaleType]]) -> Optional[UnifiedScaleType]:
         """
@@ -366,15 +367,15 @@
             paths = self.get_paths_to_immediately_dominating_insertion_points(branching_node_key)
             for path in paths:
                 assert len(path) == 1
                 edge_from_pre_hook_ip_to_op = path[0]
                 pre_hook_ip = edge_from_pre_hook_ip_to_op[0]
                 target_ip_node_keys.append(pre_hook_ip)
         else:
-            raise RuntimeError("Unsupported branching QPSG node type: {}".format(branching_node_type))
+            raise nncf.InternalError("Unsupported branching QPSG node type: {}".format(branching_node_type))
 
         if not target_ip_node_keys:
             return []
 
         for idx, pq in enumerate(quantizers_to_merge):
             branch_qconf_list = branch_qconf_lists[idx]
             if branch_qconf_list is not None:
@@ -416,15 +417,15 @@
                 target_ip_node = self.nodes[target_ip_node_key]
                 assert target_ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] is None
                 target_ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = merge_pq
                 target_ip_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(merge_pq)
                 if merge_gid is not None:
                     self._unified_scale_group_manager.add_to_group(merge_gid, merge_pq)
             else:
-                raise RuntimeError("Unsupported target type for merge PQ insertion: {}".format(target_type))
+                raise nncf.InternalError("Unsupported target type for merge PQ insertion: {}".format(target_type))
 
             merge_pqs.append(merge_pq)
 
         unified_scale_gids_to_merge = set()
         for idx, pq in enumerate(quantizers_to_merge):
             branch_qconf_list = branch_qconf_lists[idx]
             if branch_qconf_list is None and pq.unified_scale_type is not None:
@@ -559,15 +560,15 @@
         unified_scale_group_id_override: Optional[int] = None,
     ) -> PropagatingQuantizer:
         ip_node = self.nodes[ip_node_key]
         ip_type = ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
         if ip_type != QuantizerPropagationStateGraphNodeType.PRE_HOOK:
             # The insertion point key should immediately precede a quantizable op,
             # otherwise it is hard to determine affected node here (although possible)
-            raise RuntimeError("Can only add propagating quantizers into pre-hook spots!")
+            raise nncf.InternalError("Can only add propagating quantizers into pre-hook spots!")
 
         prop_quantizer = PropagatingQuantizer(
             self._get_next_prop_quantizer_id(), qconf_list, ip_node_key, unified_scale_type
         )
 
         if unified_scale_type is not None:
             if unified_scale_group_id_override is None:
@@ -599,56 +600,56 @@
             + list(prop_quantizer.affected_ip_nodes)
         )
         if prop_quantizer.last_accepting_location_node_key is not None:
             node_keys_to_verify.append(prop_quantizer.last_accepting_location_node_key)
 
         for node_key in node_keys_to_verify:
             if node_key not in self.nodes:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "Unknown node referenced by propagating quantizer to be registered: {}".format(node_key)
                 )
         edge_keys_to_verify = list(prop_quantizer.affected_edges) + list(prop_quantizer.propagation_path)
         for edge_key in edge_keys_to_verify:
             if edge_key not in self.edges:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "Unknown edge referenced by propagating quantizer to be registered: {}".format(edge_key)
                 )
 
     @staticmethod
     def _verify_qconfig_matching(
         prop_quantizer: PropagatingQuantizer, existing_prop_quantizers: List[PropagatingQuantizer]
     ):
         for existing_pq in existing_prop_quantizers:
             if existing_pq.potential_quant_configs != prop_quantizer.potential_quant_configs:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "Configurations of the quantizer to be registered are conflicting with "
                     "existing quantizer {}".format(existing_pq.id)
                 )
 
     def register_propagating_quantizer(self, prop_quantizer: PropagatingQuantizer):
         """Will only succeed if the new quantizer information is consistent with the rest of the graph state."""
         all_pqs = self.collect_all_propagating_quantizers()
         for existing_pq_id in all_pqs:
             if prop_quantizer.id == existing_pq_id:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "The propagating quantizer to be registered has an ID that is already assigned to "
                     "an existing propagating quantizer!"
                 )
         target_node = self.nodes[prop_quantizer.current_location_node_key]
         pq_in_target_node = target_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
         if pq_in_target_node is not None:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "The propagating quantizer to be registered is occupying the same position "
                 "as an existing propagating quantizer {}!".format(pq_in_target_node.id)
             )
         target_node_affecting_quantizers = target_node[
             QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR
         ]
         if target_node_affecting_quantizers:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "Cannot register a propagating quantizer into a node that is already "
                 "affected by existing propagating quantizers (ids: {})!".format(
                     [pq.id for pq in target_node_affecting_quantizers]
                 )
             )
 
         self._verify_nodes_and_edges_for_pq(prop_quantizer)
@@ -757,15 +758,15 @@
         def recursive_helper(curr_node_key: str, target_node_list: List[str]):
             successors = self.successors(curr_node_key)
             for successor_key in successors:
                 successor = self.nodes[successor_key]
                 successor_node_type = successor[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
                 if successor_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                     trait = successor[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
-                    if not trait == QuantizationTrait.QUANTIZATION_AGNOSTIC:
+                    if trait != QuantizationTrait.QUANTIZATION_AGNOSTIC:
                         target_node_list.append(successor_key)
                         return
                 recursive_helper(successor_key, target_node_list)
 
         recursive_helper(node_key, ret_node_key_list)
         return ret_node_key_list
 
@@ -957,15 +958,15 @@
                             unified_scale_group_vs_pq_node_id_dict[gid] = [quant_node_key]
 
             elif node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 out_graph.add_node(node_key)
             elif node_type == QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER:
                 out_graph.add_node(node_key, color="green", label=node["label"])
             else:
-                raise RuntimeError("Invalid QuantizerPropagationStateGraph node!")
+                raise nncf.InternalError("Invalid QuantizerPropagationStateGraph node!")
         for u, v in self.edges:
             edge = self.edges[u, v]
             attrs = {}
             affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             if affecting_quantizers:
                 label = ", ".join([str(pq.id) for pq in affecting_quantizers])
                 attrs = {"color": "blue", "label": label}
@@ -1072,15 +1073,15 @@
             if self.is_insertion_point(curr_node_type):
                 pq = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
                 if pq is not None:
                     curr_input_quantizer_ids_list.append(pq.id)
                     return
             elif curr_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 trait = curr_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
-                if not trait == QuantizationTrait.QUANTIZATION_AGNOSTIC:
+                if trait != QuantizationTrait.QUANTIZATION_AGNOSTIC:
                     return
             elif curr_node_type == QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER:
                 return
 
             for successor_key in self.successors(curr_node_key):
                 recursive_helper(successor_key, curr_input_quantizer_ids_list)
 
@@ -1281,15 +1282,15 @@
                 )
                 qp_id = pq.id
                 pqid_vs_qpid[pq.id] = qp_id
                 setup.quantization_points[qp_id] = quant_point
                 grouped_ids.add(qp_id)
 
             gid = setup.register_shared_inputs_group(list(grouped_ids))
-            for weighted_node_name in weight_quantizable_node_names_vs_configs.keys():
+            for weighted_node_name in weight_quantizable_node_names_vs_configs:
                 for affected_node_key in group.affected_op_node_keys:
                     underlying_node_names = [
                         n.node_name for n in self.op_node_keys_to_underlying_nodes_mapping[affected_node_key]
                     ]
                     if weighted_node_name in underlying_node_names:
                         qm_node_vs_same_op_gid[weighted_node_name] = gid
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/grouping.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/grouping.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/solver.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/solver.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -15,14 +15,15 @@
 from collections import deque
 from copy import deepcopy
 from enum import Enum
 from typing import Deque, Dict, List, Optional, Set, Tuple
 
 import networkx as nx
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.hardware.config import HWConfig
 from nncf.common.insertion_point_graph import InsertionPointGraph
@@ -139,15 +140,17 @@
         :param quantization_point_id: The ID of the quantization point.
         :param constrained_config_list: The list of configs (of which every config is already present in the
           currently available in the quantization point's set of available config) that will replace the list
           of the quantizer configs for the quantization point defined by `quantization_point_id`.
         """
         prior_list = self.quantizer_setup.quantization_points[quantization_point_id].possible_qconfigs
         if not all(qc in prior_list for qc in constrained_config_list):
-            raise RuntimeError("Constrained config list is incompatible with the result of the quantizer propagation!")
+            raise nncf.InternalError(
+                "Constrained config list is incompatible with the result of the quantizer propagation!"
+            )
         # TODO (vshampor): only allow to constrain 'input-group'-wise?
         self.quantizer_setup.quantization_points[quantization_point_id].possible_qconfigs = constrained_config_list
 
         if quantization_point_id in self._quantization_point_id_vs_prop_quantizer:
             pq = self._quantization_point_id_vs_prop_quantizer[quantization_point_id]
             pq.potential_quant_configs = constrained_config_list
 
@@ -184,15 +187,15 @@
                     compatible_initial_qconfs = list(
                         filter(
                             is_final_qconfig_compatible_to_initial,
                             self.quantizer_setup.quantization_points[qp_id].possible_qconfigs,
                         )
                     )
                     if not compatible_initial_qconfs:
-                        raise RuntimeError(
+                        raise nncf.InternalError(
                             "The final quantizer setup has configurations that were not present in the "
                             "initial proposal!"
                         )
                     if final_qconfig.signedness_to_force is None:
                         initial_qconfs_signedness_values = {qc.signedness_to_force for qc in compatible_initial_qconfs}
                         if None not in initial_qconfs_signedness_values and len(initial_qconfs_signedness_values) == 1:
                             # The initial configs were either all forced-signed or all forced-unsigned - should set
@@ -425,17 +428,19 @@
 
         self._additional_unified_scale_op_scopes = additional_unified_scale_op_scopes
 
         # Will handle the "wildcard" quantization situation for the time being
         if default_qconfig_list is not None:
             for op_meta, qconf_list in self._operator_allowed_qconfigs_map.items():
                 trait = self._operator_quantization_trait_map.get(op_meta, QuantizationTrait.NON_QUANTIZABLE)
-                if trait == QuantizationTrait.INPUTS_QUANTIZABLE:
-                    if HWConfig.is_qconf_list_corresponding_to_unspecified_op(qconf_list):
-                        self._operator_allowed_qconfigs_map[op_meta] = default_qconfig_list
+                if (
+                    trait == QuantizationTrait.INPUTS_QUANTIZABLE
+                    and HWConfig.is_qconf_list_corresponding_to_unspecified_op(qconf_list)
+                ):
+                    self._operator_allowed_qconfigs_map[op_meta] = default_qconfig_list
         self._active_propagating_quantizers_queue = deque()
         self._finished_propagating_quantizers: List[PropagatingQuantizer] = []
         self._quantizers_waiting_for_branch_merge = QuantizersWaitingForMergeManager()
 
         self._potential_quantizers = {}
         self._num_potential_quantized_activations = 0
         self._quantizable_layer_nodes = quantizable_layer_nodes
@@ -583,15 +588,15 @@
                 final_weight_quantizable_node_names_vs_qconfig_dict[qp.insertion_point.target_node_name] = [
                     qp.qconfig
                 ]  # sic!
 
         if Counter(final_weight_quantizable_node_names_vs_qconfig_dict.keys()) != Counter(
             self._weight_quantizable_node_names_vs_qconfigs.keys()
         ):
-            raise RuntimeError("Final weight quantizer setup is inconsistent with initial solver assumptions!")
+            raise nncf.InternalError("Final weight quantizer setup is inconsistent with initial solver assumptions!")
 
         multi_setup_with_one_config_per_point = quant_prop_graph.create_quantizer_setup(
             final_weight_quantizable_node_names_vs_qconfig_dict
         )
         final_setup = multi_setup_with_one_config_per_point.select_first_qconfig_for_each_point()
         return final_setup
 
@@ -724,15 +729,15 @@
         # only concat unified scale groups appear here
         unified_scale_grouped_paths = (
             quant_prop_graph.get_paths_to_immediately_dominating_insertion_points_grouped_by_unified_scales(
                 curr_node_key, self._unified_scales_operation_set, self._scales_unification_map
             )
         )
 
-        unified_scale_path_groups_vs_pqs = {k: [] for k in unified_scale_grouped_paths.keys() if k is not None}
+        unified_scale_path_groups_vs_pqs = {k: [] for k in unified_scale_grouped_paths if k is not None}
         existing_pq_assigned = False
         for gid, path_group in unified_scale_grouped_paths.items():
             for _ in path_group:
                 if existing_pq_assigned:
                     pq = quant_prop_graph.clone_propagating_quantizer(curr_prop_quantizer)
                     did_clone = True
                 else:
@@ -1010,23 +1015,23 @@
                 matching_indices = list(
                     filter(
                         lambda x: target_insertion_points[x].target_node_name == group_member_node_name,
                         range(len(target_insertion_points)),
                     )
                 )
                 if len(matching_indices) == 0:
-                    raise RuntimeError(
+                    raise nncf.ValidationError(
                         "No match for linked quantizer entry {} among activation quantizers!".format(
                             group_member_node_name
                         )
                     )
 
                 for target_idx in matching_indices:
                     if target_idx in insertion_point_indices_vs_group_id:
-                        raise RuntimeError(
+                        raise nncf.InternalError(
                             "Linked activation quantizer groups {} and {} "
                             "overlap!".format(group_idx, insertion_point_indices_vs_group_id[target_idx])
                         )
                 for target_idx in matching_indices:
                     insertion_point_indices_vs_group_id[target_idx] = group_idx
 
         for i in range(len(target_insertion_points)):
@@ -1125,15 +1130,15 @@
             # 1. keep ForwardTraceOnly ops in graph after all, to be able to track shape changes
             # 2. transpose input tensors to the quantization modules on the fly to accommodate scale,
             #    or vice versa, transpose scale to accommodate shape; need to handle exporting as well
             per_tensor_qconf_list = list(filter(lambda x: x.per_channel is False, qconf_list))
             op_meta_name = metatype.__class__.__name__
             if len(per_tensor_qconf_list) != len(qconf_list):
                 if not per_tensor_qconf_list:
-                    raise RuntimeError(
+                    raise nncf.InternalError(
                         "Unified scales currently do not support per-channel configuration - dropping"
                         "per-channel configuration options for {} resulted in no valid quantization "
                         "configs!".format(op_meta_name)
                     )
                 nncf_logger.warning(
                     f"Unified scales currently do not support per-channel configuration - dropping"
                     f"per-channel configuration options for {op_meta_name}"
@@ -1424,15 +1429,15 @@
             return False
 
         if self._propagation_strategy == PropagationStrategy.MERGE_WITH_POTENTIAL_REQUANTIZATION:
             compatible_fn = compatible_with_requant
         elif self._propagation_strategy == PropagationStrategy.MERGE_WITH_SINGLE_FQ_RESULT:
             compatible_fn = compatible_wo_requant
         else:
-            raise RuntimeError(f"Unknown propagation strategy: {self._propagation_strategy}")
+            raise nncf.ValidationError(f"Unknown propagation strategy: {self._propagation_strategy}")
 
         for qconf in qconfigs_union:
             if all(compatible_fn(qconf, qconf_list) for qconf_list in potential_qconfigs_for_each_branch):
                 merged_qconfig_list.append(qconf)
 
         nncf_logger.debug(f"Merged list before sorting: {';'.join([str(qc) for qc in merged_qconfig_list])}")
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/structs.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/structs.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_propagation/visualizer.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_propagation/visualizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_removal.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_removal.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import List, Tuple, TypeVar
+from typing import Callable, List, Tuple, TypeVar
 
 from nncf.common.factory import CommandCreatorFactory
 from nncf.common.factory import ModelTransformerFactory
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.layout import TransformationLayout
@@ -58,14 +58,25 @@
             if relative.metatype in quantizer_metatypes:
                 if is_parents:
                     if relative in seen_children:
                         continue
                     if relative not in to_cut:
                         to_cut.append(relative)
                     to_see_children.append(relative)
+                    # We should see parents for the `relative` node here only if they are
+                    # all quantizers. This covers the quantize-dequantize case, where we
+                    # should see parents for the dequantize node.
+                    if all(x.metatype in quantizer_metatypes for x in graph.get_previous_nodes(relative)):
+                        to_see_parents.append(relative)
+                elif node.metatype in quantizer_metatypes:
+                    # `node` is a quantizer (quantize-dequantize case) here, and `relative`
+                    # is the dequantizer. So, we should cut `relative` and look at its children.
+                    if relative not in to_cut:
+                        to_cut.append(relative)
+                    to_see_children.append(relative)
                 else:
                     seen_children.append(relative)
             elif relative.metatype not in const_metatypes:
                 if relative not in seen_parents:
                     to_see_parents.append(relative)
                 if relative not in seen_children and relative.metatype in quantize_agnostic_metatypes:
                     to_see_children.append(relative)
@@ -90,14 +101,16 @@
 def revert_operations_to_floating_point_precision(
     operations: List[NNCFNode],
     quantizers: List[NNCFNode],
     quantized_model: TModel,
     quantized_model_graph: NNCFGraph,
     restore_mode: RestoreMode,
     op_with_weights_metatypes: List[OperatorMetatype],
+    is_node_with_weight_fn: Callable[[NNCFNode], bool],
+    get_weight_tensor_port_ids_fn: Callable[[NNCFNode], List[int]],
 ) -> TModel:
     """
     Reverts provided operations to floating-point precision by removing
     quantizers. Restores original bias for operations with bias.
     Restores original weights for operations with weights.
 
     :param operations: List of operations to revert in floating-point precision.
@@ -105,14 +118,17 @@
         operations to floating-point precision.
     :param quantized_model: Quantized model in which provided operations
         should be reverted to floating-point precision.
     :param quantized_model_graph: The graph which was built for `quantized_model`.
     :param restore_mode: Restore mode.
     :param op_with_weights_metatypes: List of operation metatypes that can be reverted to representation
         with int8 weights.
+    :param is_node_with_weight_fn: Checks if the node has a weight or not. Returns `True` if `node` corresponds
+        to the operation with weights, `False` otherwise.
+    :param get_weight_tensor_port_ids_fn: Returns node's input port indices with weights tensors.
     :return: The model where `operations` were reverted to floating-point precision.
     """
     transformation_layout = TransformationLayout()
 
     command_creator = CommandCreatorFactory.create(quantized_model)
 
     should_remove_fq = {}
@@ -140,16 +156,16 @@
             transformation_layout.register(
                 command_creator.create_command_to_update_bias(node, original_bias, quantized_model_graph)
             )
 
         if not should_revert_weights_to_fp32.get(node.node_name, True):
             continue
 
-        if node.layer_attributes and node.layer_attributes.constant_attributes is not None:
-            weight_port_ids = node.layer_attributes.get_const_port_ids()
+        if is_node_with_weight_fn(node):
+            weight_port_ids = get_weight_tensor_port_ids_fn(node)
             for port_id in weight_port_ids:
                 original_weight = node.attributes.get(f"original_weight.{port_id}", None)
                 if original_weight is not None:
                     transformation_layout.register(
                         command_creator.create_command_to_update_weight(node, original_weight, port_id)
                     )
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizer_setup.py` & `nncf-2.9.0/nncf/common/quantization/quantizer_setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from abc import ABC
 from collections import Counter
 from copy import deepcopy
 from enum import Enum
 from typing import Any, Dict, List, Optional, Set
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import UnifiedScaleType
 from nncf.common.quantization.structs import WeightQuantizerId
@@ -248,25 +249,25 @@
             new_id = 0
         self.quantization_points[new_id] = qp
 
     def register_unified_scale_group(self, qp_group: List[QuantizationPointId]) -> int:
         for qp_id in qp_group:
             gid = self.get_unified_scale_group_id(qp_id) is not None
             if gid:
-                raise RuntimeError("QP id {} is already in unified scale group {}".format(qp_id, gid))
+                raise nncf.InternalError("QP id {} is already in unified scale group {}".format(qp_id, gid))
         gid = self._next_unified_scale_gid
         self.unified_scale_groups[self._next_unified_scale_gid] = set(qp_group)
         self._next_unified_scale_gid += 1
         return gid
 
     def register_shared_inputs_group(self, qp_group: List[QuantizationPointId]) -> int:
         for qp_id in qp_group:
             gid = self.get_shared_inputs_group_id(qp_id) is not None
             if gid:
-                raise RuntimeError("QP id {} is already in shared input group {}".format(qp_id, gid))
+                raise nncf.InternalError("QP id {} is already in shared input group {}".format(qp_id, gid))
         gid = self._next_shared_inputs_gid
         self.shared_input_operation_set_groups[self._next_shared_inputs_gid] = set(qp_group)
         self._next_shared_inputs_gid += 1
         return gid
 
     def __discard_independent(self, id_: QuantizationPointId):
         if id_ in self.quantization_points:
@@ -304,21 +305,21 @@
             if qp_id in shared_inputs_group:
                 return gid
         return None
 
     def register_existing_qp_id_in_unified_scale_group(self, qp_id: QuantizationPointId, unified_scale_gid: int):
         gid = self.get_unified_scale_group_id(qp_id)
         if gid is not None:
-            raise RuntimeError("QP id {} is already in unified scale group {}".format(qp_id, gid))
+            raise nncf.InternalError("QP id {} is already in unified scale group {}".format(qp_id, gid))
         self.unified_scale_groups[unified_scale_gid].add(qp_id)
 
     def register_existing_qp_id_in_shared_input_group(self, qp_id: QuantizationPointId, shared_inputs_gid: int):
         gid = self.get_shared_inputs_group_id(qp_id)
         if gid is not None:
-            raise RuntimeError("QP id {} is already in shared inputs group {}".format(qp_id, gid))
+            raise nncf.InternalError("QP id {} is already in shared inputs group {}".format(qp_id, gid))
         self.shared_input_operation_set_groups[shared_inputs_gid].add(qp_id)
 
     def remove_unified_scale_from_point(self, qp_id: QuantizationPointId):
         gid = self.get_unified_scale_group_id(qp_id)
         if gid is None:
             nncf_logger.debug(
                 f"Attempted to remove QP id {qp_id} from associated unified scale group, but the QP"
```

### Comparing `nncf-2.8.1/nncf/common/quantization/quantizers.py` & `nncf-2.9.0/nncf/common/quantization/quantizers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/statistics.py` & `nncf-2.9.0/nncf/common/quantization/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/quantization/structs.py` & `nncf-2.9.0/nncf/common/quantization/structs.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from copy import deepcopy
 from enum import Enum
 from typing import Any, Dict, List, Optional
 
+import nncf
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.utils.api_marker import api
 from nncf.config.schemata.defaults import QUANTIZATION_BITS
 from nncf.config.schemata.defaults import QUANTIZATION_PER_CHANNEL
 from nncf.parameters import TargetDevice
 
@@ -189,15 +190,17 @@
         to set up constraints.
         E.g. QuantizationConstraint(bits=8, per_channel=True) will set up
         a constraint that corresponds to all 8-bit per-channel quantizers, either
         symmetric or asymmetric, either signed or unsigned.
         """
         for attr_name in kwargs:
             if not hasattr(QuantizationConstraints.REF_QCONF_OBJ, attr_name):
-                raise RuntimeError("Invalid constraint - QuantizerConfig has no attribute '{}'".format(attr_name))
+                raise nncf.ValidationError(
+                    "Invalid constraint - QuantizerConfig has no attribute '{}'".format(attr_name)
+                )
         self.qconf_attr_vs_constraint_dict = kwargs
 
     def apply_constraints_to(self, qconfig: QuantizerConfig) -> QuantizerConfig:
         for attr_name, constraint in self.qconf_attr_vs_constraint_dict.items():
             if constraint is not None:
                 setattr(qconfig, attr_name, constraint)
         return qconfig
```

### Comparing `nncf-2.8.1/nncf/common/schedulers.py` & `nncf-2.9.0/nncf/common/schedulers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -157,15 +157,15 @@
         scheduler.epoch_step()
         for i, (x, y) in enumerate(dataset):
              scheduler.step()
              ...
     ```
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         """
         Initializes the internal state of the compression scheduler specified by:
             - `current_step` is the index of the global training step, counted
             from 0 to the end of training. The initial value is -1
             - `current_epoch` is the training epoch index (numbering from zero).
             The initial value is -1.
```

### Comparing `nncf-2.8.1/nncf/common/scopes.py` & `nncf-2.9.0/nncf/common/scopes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import re
 from typing import List, Optional, Set, Union
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.structs import QuantizerId
 from nncf.scopes import IgnoredScope
 from nncf.scopes import convert_ignored_scope_to_list
@@ -133,9 +134,9 @@
         err_message += (
             "Refer to the original_graph.dot to discover the operations "
             "in the model currently visible to NNCF and specify the ignored/target "
             "scopes in terms of the names there."
         )
 
         if validate_scopes:
-            raise RuntimeError(err_message)
+            raise nncf.ValidationError(err_message)
         nncf_logger.info(err_message)
```

### Comparing `nncf-2.8.1/nncf/common/sparsity/__init__.py` & `nncf-2.9.0/nncf/common/tensor_statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/sparsity/collector.py` & `nncf-2.9.0/nncf/common/sparsity/collector.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -68,15 +68,17 @@
         return self._sparsity_level
 
     @property
     def is_sparse(self) -> bool:
         return self._is_sparse
 
 
-def _calculate_sparsity_level_for_model(weight_descriptions: List[WeightDescription]) -> float:
+def _calculate_sparsity_level_for_model(
+    weight_descriptions: List[WeightDescription],
+) -> float:
     """
     Calculates the sparsity level for the whole model.
 
     :param weight_descriptions: Descriptions for weights of the model.
     :return: Sparsity level for the whole model.
     """
     total_params = sum(w.num_params for w in weight_descriptions)
@@ -117,11 +119,13 @@
             if not w.is_sparse:
                 continue
 
             weight_percentage = 100 * (w.num_params / total_params)
             sparse_layers_summary.append(SparsifiedLayerSummary(w.name, w.shape, w.sparsity_level, weight_percentage))
 
         sparse_model_stats = SparsifiedModelStatistics(
-            sparsity_level_for_model, sparsity_level_for_sparse_layers, sparse_layers_summary
+            sparsity_level_for_model,
+            sparsity_level_for_sparse_layers,
+            sparse_layers_summary,
         )
 
         return sparse_model_stats
```

### Comparing `nncf-2.8.1/nncf/common/sparsity/controller.py` & `nncf-2.9.0/nncf/common/sparsity/controller.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,30 +1,39 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from abc import abstractmethod
+
 from nncf.api.compression import CompressionAlgorithmController
 
 
 class SparsityController(CompressionAlgorithmController):
     """
     This is the class from which all sparsity controllers inherit.
     """
 
-    def set_sparsity_level(self, sparsity_level: float):
+    def set_sparsity_level(self, sparsity_level: float) -> None:
         """
         Sets the sparsity level that should be applied to the model's weights.
 
         :param sparsity_level: Sparsity level that should be applied to the model's weights.
         """
 
-    def freeze(self):
+    def freeze(self) -> None:
         """
         Freezes all sparsity masks. Sparsity masks will not be trained after calling this method.
         """
+
+    @property
+    @abstractmethod
+    def current_sparsity_level(self) -> float:
+        """
+        Returns the current sparsity level of the underlying model.
+        """
```

### Comparing `nncf-2.8.1/nncf/common/sparsity/schedulers.py` & `nncf-2.9.0/nncf/common/sparsity/schedulers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -15,24 +15,25 @@
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.schedulers import ExponentialDecaySchedule
 from nncf.common.schedulers import MultiStepSchedule
 from nncf.common.schedulers import PolynomialDecaySchedule
 from nncf.common.sparsity.controller import SparsityController
 from nncf.common.utils.registry import Registry
 from nncf.config.schemata.defaults import SPARSITY_FREEZE_EPOCH
+from nncf.config.schemata.defaults import SPARSITY_INIT
 from nncf.config.schemata.defaults import SPARSITY_MULTISTEP_SPARSITY_LEVELS
 from nncf.config.schemata.defaults import SPARSITY_MULTISTEP_STEPS
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_CONCAVE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_PATIENCE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_POWER
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP
 from nncf.config.schemata.defaults import SPARSITY_TARGET
 from nncf.config.schemata.defaults import SPARSITY_TARGET_EPOCH
 
-SPARSITY_SCHEDULERS = Registry("sparsity_schedulers")
+SPARSITY_SCHEDULERS: Registry = Registry("sparsity_schedulers")
 
 
 class SparsityScheduler(BaseCompressionScheduler):
     """
     This is the class from which all sparsity schedulers inherit.
 
     A sparsity scheduler is an object which specifies the sparsity
@@ -46,25 +47,25 @@
     :param target_level: Sparsity level at which the schedule ends.
     :param target_epoch: Zero-based index of the epoch from which the
         sparsity level of the model will be equal to the `target_level`.
     :param freeze_epoch: Zero-based index of the epoch from which the sparsity
         mask will be frozen and will not be trained.
     """
 
-    def __init__(self, controller: SparsityController, params: dict):
+    def __init__(self, controller: SparsityController, params: Dict[str, Any]) -> None:
         """
         Initializes the internal state of the sparsity scheduler.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__()
         self._controller = controller
-        self.initial_level = params.get("sparsity_init")
-        self.target_level = params.get("sparsity_target", SPARSITY_TARGET)
+        self.initial_level: float = params.get("sparsity_init", SPARSITY_INIT)
+        self.target_level: float = params.get("sparsity_target", SPARSITY_TARGET)
         self.target_epoch = params.get("sparsity_target_epoch", SPARSITY_TARGET_EPOCH)
         self.freeze_epoch = params.get("sparsity_freeze_epoch", SPARSITY_FREEZE_EPOCH)
 
     def _calculate_sparsity_level(self) -> float:
         """
         Calculates a sparsity level that should be applied to the weights
         for the `current_epoch` or for step in the `current_epoch`.
@@ -109,15 +110,15 @@
     If `update_per_optimizer_step` was only provided then scheduler
     will use first epoch to calculate `steps_per_epoch`
     parameter. In this case, `current_epoch` and `current_step` will
     not be updated on this epoch. The scheduler will start calculation
     after `steps_per_epoch` will be calculated.
     """
 
-    def __init__(self, controller: SparsityController, params: dict):
+    def __init__(self, controller: SparsityController, params: Dict[str, Any]):
         """
         Initializes a sparsity scheduler with a polynomial decay schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
@@ -177,20 +178,23 @@
         of the scheduler object will not be changed.
         """
         self._should_skip = False
         if self._update_per_optimizer_step:
             if self._steps_per_epoch is None and self._steps_in_current_epoch > 0:
                 self._steps_per_epoch = self._steps_in_current_epoch
 
-            if self._steps_per_epoch is not None and self._steps_in_current_epoch > 0:
-                if self._steps_per_epoch != self._steps_in_current_epoch:
-                    raise Exception(
-                        "Actual steps per epoch and steps per epoch from the scheduler "
-                        "parameters are different. Scheduling may be incorrect."
-                    )
+            if (
+                self._steps_per_epoch is not None
+                and self._steps_in_current_epoch > 0
+                and self._steps_per_epoch != self._steps_in_current_epoch
+            ):
+                raise Exception(
+                    "Actual steps per epoch and steps per epoch from the scheduler "
+                    "parameters are different. Scheduling may be incorrect."
+                )
 
             if self._steps_per_epoch is None:
                 self._should_skip = True
                 nncf_logger.warning(
                     "Scheduler set to update sparsity level per optimizer step, "
                     "but steps_per_epoch was not set in config. Will only start updating "
                     "sparsity level after measuring the actual steps per epoch as signaled "
@@ -206,55 +210,56 @@
     This scheduler applies exponential decay to the density level
     to calculate the sparsity level for the `current_epoch`.
     The density level for the `current_epoch` is calculated as
 
         current_density = 1.0 - current_level
     """
 
-    def __init__(self, controller: SparsityController, params: dict):
+    def __init__(self, controller: SparsityController, params: Dict[str, Any]):
         """
         Initializes a sparsity scheduler with an exponential decay schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
+
         initial_density = 1.0 - self.initial_level
         target_density = 1.0 - self.target_level
         self.schedule = ExponentialDecaySchedule(initial_density, target_density, self.target_epoch)
 
     def epoch_step(self, next_epoch: Optional[int] = None) -> None:
         super().epoch_step(next_epoch)
         self._update_sparsity_level()
 
     def _calculate_sparsity_level(self) -> float:
-        current_density = self.schedule(self.current_epoch)
-        current_level = 1.0 - current_density
+        current_density: float = self.schedule(self.current_epoch)
+        current_level: float = 1.0 - current_density
         return min(current_level, self.target_level)
 
 
 @SPARSITY_SCHEDULERS.register("adaptive")
 class AdaptiveSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with an adaptive schedule.
     """
 
-    def __init__(self, controller: SparsityController, params: dict):
+    def __init__(self, controller: SparsityController, params: Dict[str, Any]):
         """
         Initializes a sparsity scheduler with an adaptive schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
         self.decay_step = params.get("step", 0.05)
         self.eps = params.get("eps", 0.03)
         self.patience = params.get("patience", SPARSITY_SCHEDULER_PATIENCE)
         self.num_bad_epochs = 0
-        self._current_level = self.initial_level
+        self._current_level: float = self.initial_level
 
     @property
     def current_sparsity_level(self) -> float:
         """
         Returns sparsity level for the `current_epoch` or for step
         in the `current_epoch`.
 
@@ -263,15 +268,15 @@
         return self._current_level
 
     def epoch_step(self, next_epoch: Optional[int] = None) -> None:
         super().epoch_step(next_epoch)
         self._update_sparsity_level()
 
     def _calculate_sparsity_level(self) -> float:
-        if self._controller.loss.current_sparsity >= self._current_level - self.eps:
+        if self._controller.current_sparsity_level >= self._current_level - self.eps:
             self.num_bad_epochs += 1
 
         current_level = self._current_level
         if self.num_bad_epochs >= self.patience:
             self.num_bad_epochs = 0
             current_level = current_level + self.decay_step
 
@@ -293,15 +298,15 @@
 
 @SPARSITY_SCHEDULERS.register("multistep")
 class MultiStepSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with a piecewise constant schedule.
     """
 
-    def __init__(self, controller: SparsityController, params: dict):
+    def __init__(self, controller: SparsityController, params: Dict[str, Any]):
         """
         Initializes a sparsity scheduler with a piecewise constant schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
```

### Comparing `nncf-2.8.1/nncf/common/sparsity/statistics.py` & `nncf-2.9.0/nncf/common/sparsity/statistics.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -23,15 +23,21 @@
 
     :param name: Layer's name.
     :param weight_shape: Weight's shape.
     :param sparsity_level: Sparsity level of the sparsified layer.
     :param weight_percentage: Proportion of the layer's weights in the whole model.
     """
 
-    def __init__(self, name: str, weight_shape: List[int], sparsity_level: float, weight_percentage: float):
+    def __init__(
+        self,
+        name: str,
+        weight_shape: List[int],
+        sparsity_level: float,
+        weight_percentage: float,
+    ):
         self.name = name
         self.weight_shape = weight_shape
         self.sparsity_level = sparsity_level
         self.weight_percentage = weight_percentage
 
 
 @api()
@@ -56,20 +62,28 @@
         self.sparsified_layers_summary = sparsified_layers_summary
 
     def to_str(self) -> str:
         model_string = create_table(
             header=["Statistic's name", "Value"],
             rows=[
                 ["Sparsity level of the whole model", self.sparsity_level],
-                ["Sparsity level of all sparsified layers", self.sparsity_level_for_layers],
+                [
+                    "Sparsity level of all sparsified layers",
+                    self.sparsity_level_for_layers,
+                ],
             ],
         )
 
         layers_string = create_table(
-            header=["Layer's name", "Weight's shape", "Sparsity level", "Weight's percentage"],
+            header=[
+                "Layer's name",
+                "Weight's shape",
+                "Sparsity level",
+                "Weight's percentage",
+            ],
             rows=[
                 [s.name, s.weight_shape, s.sparsity_level, s.weight_percentage] for s in self.sparsified_layers_summary
             ],
         )
 
         pretty_string = (
             f"Statistics of the sparsified model:\n{model_string}\n\n"
@@ -102,21 +116,25 @@
     ):
         self.model_statistics = model_statistics
         self.thresholds = thresholds
         self.target_sparsity_level = target_sparsity_level
 
     def to_str(self) -> str:
         thresholds_string = create_table(
-            ["Layer's name", "Sparsity threshold"], [[s.name, s.threshold] for s in self.thresholds]
+            ["Layer's name", "Sparsity threshold"],
+            [[s.name, s.threshold] for s in self.thresholds],
         )
 
         algorithm_string = create_table(
             header=["Statistic's name", "Value"],
             rows=[
-                ["A target level of the sparsity for the algorithm for the current epoch", self.target_sparsity_level],
+                [
+                    "A target level of the sparsity for the algorithm for the current epoch",
+                    self.target_sparsity_level,
+                ],
             ],
         )
 
         pretty_string = (
             f"{self.model_statistics.to_str()}\n\n"
             f"Statistics of the magnitude sparsity algorithm:\n{algorithm_string}\n{thresholds_string}"
         )
@@ -146,26 +164,35 @@
 
     :param model_statistics: Statistics of the sparsified model.
     :param target_sparsity_level: A target level of the sparsity for the algorithm for the current epoch.
     :param mean_sparse_prob: The probability that one weight will be zeroed.
     """
 
     def __init__(
-        self, model_statistics: SparsifiedModelStatistics, target_sparsity_level: float, mean_sparse_prob: float
+        self,
+        model_statistics: SparsifiedModelStatistics,
+        target_sparsity_level: float,
+        mean_sparse_prob: float,
     ):
         self.model_statistics = model_statistics
         self.target_sparsity_level = target_sparsity_level
         self.mean_sparse_prob = mean_sparse_prob
 
     def to_str(self) -> str:
         algorithm_string = create_table(
             header=["Statistic's name", "Value"],
             rows=[
-                ["A target level of the sparsity for the algorithm for the current epoch", self.target_sparsity_level],
-                ["The probability that one weight will be zeroed", self.mean_sparse_prob],
+                [
+                    "A target level of the sparsity for the algorithm for the current epoch",
+                    self.target_sparsity_level,
+                ],
+                [
+                    "The probability that one weight will be zeroed",
+                    self.mean_sparse_prob,
+                ],
             ],
         )
 
         pretty_string = (
             f"{self.model_statistics.to_str()}\n\n Statistics of the RB-sparsity algorithm:\n{algorithm_string}"
         )
         return pretty_string
@@ -192,15 +219,18 @@
         self.importance_regularization_factor = importance_regularization_factor
 
     def to_str(self) -> str:
         algorithm_string = create_table(
             header=["Statistic's name", "Value"],
             rows=[
                 ["Mask Importance Threshold", self.importance_threshold],
-                ["Importance Regularization Factor", self.importance_regularization_factor],
+                [
+                    "Importance Regularization Factor",
+                    self.importance_regularization_factor,
+                ],
             ],
         )
 
         pretty_string = (
             f"{self.model_statistics.to_str()}\n\n"
             f"Statistics of the movement-sparsity algorithm:\n{algorithm_string}"
         )
```

### Comparing `nncf-2.8.1/nncf/common/stateful_classes_registry.py` & `nncf-2.9.0/nncf/common/stateful_classes_registry.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/statistics.py` & `nncf-2.9.0/nncf/common/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/strip.py` & `nncf-2.9.0/nncf/common/strip.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from typing import TypeVar
 
+import nncf
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 
 TModel = TypeVar("TModel")
 
 
@@ -32,8 +33,8 @@
     """
     model_backend = get_backend(model)
     if model_backend == BackendType.TORCH:
         from nncf.torch import strip as strip_pt
 
         return strip_pt(model, do_copy)
 
-    raise RuntimeError(f"Method `strip` does not support for {model_backend.value} backend.")
+    raise nncf.UnsupportedBackendError(f"Method `strip` does not support for {model_backend.value} backend.")
```

### Comparing `nncf-2.8.1/nncf/common/tensor.py` & `nncf-2.9.0/nncf/common/tensor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from abc import abstractmethod
 from typing import List, Optional, TypeVar
 
+import nncf
+
 TensorType = TypeVar("TensorType")
 DeviceType = TypeVar("DeviceType")
 TensorElementsType = TypeVar("TensorElementsType")
 
 
 class NNCFTensor:
     """
@@ -31,17 +32,16 @@
     @property
     def tensor(self) -> TensorType:
         return self._tensor
 
     @property
     def shape(self) -> List[int]:
         if self._tensor is None:
-            raise RuntimeError("Attempt to get shape of empty NNCFTensor")
+            raise nncf.InternalError("Attempt to get shape of empty NNCFTensor")
         return self._tensor.shape
 
     @property
-    @abstractmethod
     def device(self) -> DeviceType:
-        pass
+        raise NotImplementedError
 
     def is_empty(self) -> bool:
-        return False
+        raise NotImplementedError
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/__init__.py` & `nncf-2.9.0/nncf/common/utils/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/aggregator.py` & `nncf-2.9.0/nncf/common/tensor_statistics/aggregator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 from itertools import islice
 from typing import Any, Dict, TypeVar
 
+import nncf
 from nncf.common import factory
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.logging.track_progress import track
 from nncf.common.tensor import NNCFTensor
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.data.dataset import Dataset
@@ -66,15 +67,15 @@
             description="Statistics collection",
         ):
             outputs = engine.infer(input_data)
             processed_outputs = self._process_outputs(outputs)
             self._register_statistics(processed_outputs, merged_statistics)
             empty_statistics = False
         if empty_statistics:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "Calibration dataset must not be empty. Please provide calibration dataset with at least one sample."
             )
 
     def register_statistic_points(self, statistic_points: StatisticPointsContainer) -> None:
         """
         Register statistic points for statistics collection and recalculates the maximum number samples
         for collecting statistics, based on the maximum value from the all algorithms.
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/collectors.py` & `nncf-2.9.0/nncf/common/tensor_statistics/collectors.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,15 +17,15 @@
 import numpy as np
 
 from nncf.common.tensor import NNCFTensor
 from nncf.common.tensor import TensorElementsType
 from nncf.common.tensor import TensorType
 from nncf.common.tensor_statistics.reduction import get_per_channel_history
 
-ReductionAxes = Tuple[int]
+ReductionAxes = Tuple[int, ...]
 
 
 class TensorStatisticCollectorBase(ABC):
     """Collector estimate statistics at the quantization point based on the provided reduction shape."""
 
     def __init__(self, reduction_shape: Optional[ReductionAxes] = None, num_samples: Optional[int] = None):
         """
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/reduction.py` & `nncf-2.9.0/nncf/common/tensor_statistics/reduction.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/statistic_point.py` & `nncf-2.9.0/nncf/common/tensor_statistics/statistic_point.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -54,15 +54,15 @@
         """
         target_node_name = statistic_point.target_point.target_node_name
         if target_node_name not in self.data:
             self.data[target_node_name] = [statistic_point]
         else:
             for _statistic_point in self.data[target_node_name]:
                 if _statistic_point.target_point == statistic_point.target_point:
-                    for algorithm in statistic_point.algorithm_to_tensor_collectors.keys():
+                    for algorithm in statistic_point.algorithm_to_tensor_collectors:
                         if algorithm in _statistic_point.algorithm_to_tensor_collectors:
                             _statistic_point.algorithm_to_tensor_collectors[algorithm].extend(
                                 statistic_point.algorithm_to_tensor_collectors[algorithm]
                             )
                         else:
                             _statistic_point.algorithm_to_tensor_collectors[
                                 algorithm
```

### Comparing `nncf-2.8.1/nncf/common/tensor_statistics/statistics.py` & `nncf-2.9.0/nncf/common/tensor_statistics/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -83,15 +83,15 @@
 
     def __init__(self, percentile_vs_values_dict):
         self.percentile_vs_values_dict = percentile_vs_values_dict
 
     def __eq__(self, other: "PercentileTensorStatistic", rtol=1e-9) -> bool:
         if Counter(self.percentile_vs_values_dict.keys()) != Counter(other.percentile_vs_values_dict.keys()):
             return False
-        for pct in self.percentile_vs_values_dict.keys():
+        for pct in self.percentile_vs_values_dict:
             if not self.tensor_eq(self.percentile_vs_values_dict[pct], other.percentile_vs_values_dict[pct]):
                 return False
         return True
 
 
 class RawTensorStatistic(TensorStatistic):
     VALUES_STATS = "values"
```

### Comparing `nncf-2.8.1/nncf/common/utils/__init__.py` & `nncf-2.9.0/nncf/config/schemata/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/api_marker.py` & `nncf-2.9.0/nncf/common/utils/api_marker.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/backend.py` & `nncf-2.9.0/nncf/common/utils/backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,24 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import importlib
 from copy import deepcopy
 from enum import Enum
 from typing import List, TypeVar
 
+import nncf
+
 TModel = TypeVar("TModel")
 
 
 class BackendType(Enum):
     TORCH = "Torch"
     TENSORFLOW = "Tensorflow"
     ONNX = "ONNX"
@@ -124,15 +126,15 @@
 
     if BackendType.ONNX in available_backends and is_onnx_model(model):
         return BackendType.ONNX
 
     if BackendType.OPENVINO in available_backends and is_openvino_model(model):
         return BackendType.OPENVINO
 
-    raise RuntimeError(
+    raise nncf.UnsupportedBackendError(
         "Could not infer the backend framework from the model type because "
         "the framework is not available or the model type is unsupported. "
         "The available frameworks found: {}.".format(", ".join([b.value for b in available_backends]))
     )
 
 
 def copy_model(model: TModel) -> TModel:
```

### Comparing `nncf-2.8.1/nncf/common/utils/debug.py` & `nncf-2.9.0/nncf/common/utils/debug.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/decorators.py` & `nncf-2.9.0/nncf/common/utils/decorators.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/dot_file_rw.py` & `nncf-2.9.0/nncf/common/utils/dot_file_rw.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/helpers.py` & `nncf-2.9.0/nncf/common/utils/helpers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/logger/__init__.py` & `nncf-2.9.0/nncf/common/utils/logger/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/os.py` & `nncf-2.9.0/nncf/common/utils/os.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,18 +10,20 @@
 # limitations under the License.
 import sys
 from contextlib import contextmanager
 from pathlib import Path
 
 import psutil
 
+import nncf
+
 
 def fail_if_symlink(file: Path):
     if file.is_symlink():
-        raise RuntimeError("File {} is a symbolic link, aborting.".format(str(file)))
+        raise nncf.ValidationError("File {} is a symbolic link, aborting.".format(str(file)))
 
 
 @contextmanager
 def safe_open(file: Path, *args, **kwargs):
     """
     Safe function to open file and return a stream.
```

### Comparing `nncf-2.8.1/nncf/common/utils/patcher.py` & `nncf-2.9.0/nncf/common/utils/patcher.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/registry.py` & `nncf-2.9.0/nncf/common/utils/registry.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 class Registry:
     REGISTERED_NAME_ATTR = "_registered_name"
 
-    def __init__(self, name, add_name_as_attr=False):
+    def __init__(self, name: str, add_name_as_attr: bool = False):
         self._name = name
         self._registry_dict = {}
         self._add_name_as_attr = add_name_as_attr
 
     @property
     def registry_dict(self):
         return self._registry_dict
@@ -26,15 +26,15 @@
         return self._registry_dict.values()
 
     def _register(self, obj, name):
         if name in self._registry_dict:
             raise KeyError("{} is already registered in {}".format(name, self._name))
         self._registry_dict[name] = obj
 
-    def register(self, name=None):
+    def register(self, name: str = None):
         def wrap(obj):
             cls_name = name
             if cls_name is None:
                 cls_name = obj.__name__
             if self._add_name_as_attr:
                 setattr(obj, self.REGISTERED_NAME_ATTR, name)
             self._register(obj, cls_name)
```

### Comparing `nncf-2.8.1/nncf/common/utils/tensorboard.py` & `nncf-2.9.0/nncf/common/utils/tensorboard.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/common/utils/timer.py` & `nncf-2.9.0/nncf/common/utils/timer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/__init__.py` & `nncf-2.9.0/nncf/config/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/config.py` & `nncf-2.9.0/nncf/config/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,14 +12,15 @@
 from copy import deepcopy
 from pathlib import Path
 from typing import Dict, List, Optional, Type
 
 import jsonschema
 import jstyleson as json
 
+import nncf
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.os import safe_open
 from nncf.config.definitions import SCHEMA_VISUALIZATION_URL
 from nncf.config.schema import NNCF_CONFIG_SCHEMA
 from nncf.config.schema import REF_VS_ALGO_SCHEMA
 from nncf.config.schema import validate_single_compression_algo_schema
@@ -67,15 +68,15 @@
         Attach the supplied list of extra configuration structures to this configuration object.
 
         :param struct_list: List of extra configuration structures.
         """
         for struct in struct_list:
             struct_id = struct.get_id()
             if struct_id in self.__nncf_extra_structs:
-                raise RuntimeError(f"{struct_id} is already registered as extra struct in NNCFConfig!")
+                raise nncf.InternalError(f"{struct_id} is already registered as extra struct in NNCFConfig!")
             self.__nncf_extra_structs[struct_id] = struct
 
     def get_extra_struct(self, struct_cls: Type[NNCFExtraConfigStruct]) -> NNCFExtraConfigStruct:
         return self.__nncf_extra_structs[struct_cls.get_id()]
 
     def has_extra_struct(self, struct_cls: Type[NNCFExtraConfigStruct]) -> NNCFExtraConfigStruct:
         return struct_cls.get_id() in self.__nncf_extra_structs
```

### Comparing `nncf-2.8.1/nncf/config/definitions.py` & `nncf-2.9.0/nncf/config/definitions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/extractors.py` & `nncf-2.9.0/nncf/config/extractors.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Dict, List, Optional
 
+import nncf
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.initialization.range import PerLayerRangeInitConfig
 from nncf.common.quantization.initialization.range import RangeInitConfig
 from nncf.config.config import NNCFConfig
 from nncf.config.schemata.defaults import NUM_BN_ADAPTATION_SAMPLES
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import QuantizationRangeInitArgs
@@ -48,33 +49,35 @@
         assert isinstance(compression_section, dict)
         algo_list = [compression_section]
 
     from nncf.common.compression import NO_COMPRESSION_ALGORITHM_NAME
 
     if algo_name_to_match == NO_COMPRESSION_ALGORITHM_NAME:
         if len(algo_list) > 0:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 f"No algorithm configuration should be specified "
                 f"when you try to extract {algo_name_to_match} from the NNCF config!"
             )
         return {}
 
     matches = []
     for compression_algo_dict in algo_list:
         algo_name = compression_algo_dict["algorithm"]
         if algo_name == algo_name_to_match:
             matches.append(compression_algo_dict)
 
     if len(matches) > 1:
-        raise RuntimeError(
+        raise nncf.ValidationError(
             f"Multiple algorithm configurations specified for the same "
             f"algo {algo_name_to_match} in the NNCF config!"
         )
     if not matches:
-        raise RuntimeError(f"Did not find an algorithm configuration for algo {algo_name_to_match} in the NNCF config!")
+        raise nncf.InternalError(
+            f"Did not find an algorithm configuration for algo {algo_name_to_match} in the NNCF config!"
+        )
     return next(iter(matches))
 
 
 def extract_range_init_params(config: NNCFConfig, algorithm_name: str = "quantization") -> Optional[Dict[str, object]]:
     """
     Extracts parameters of the quantization range initialization algorithm from the
     compression algorithm NNCFconfig.
@@ -199,21 +202,21 @@
         if params["mode"] == AccuracyAwareTrainingMode.EARLY_EXIT:
             return
         if params["mode"] == AccuracyAwareTrainingMode.ADAPTIVE_COMPRESSION_LEVEL:
             algorithms = extract_algorithm_names(config)
             if NNCFAlgorithmNames.FILTER_PRUNING in algorithms and any(
                 algo in NNCFAlgorithmNames.SPARSITY for algo in algorithms
             ):
-                raise RuntimeError(
+                raise nncf.ValidationError(
                     "adaptive_compression_level mode supports filter_pruning or sparsity algorithms"
                     "separately. Please, choose only one algorithm with adaptive compression level. "
                     "Take a note that you still can use it combined with quantization."
                 )
             if len(algorithms) == 1 and algorithms[0] == NNCFAlgorithmNames.QUANTIZATION:
-                raise RuntimeError("adaptive_compression_level mode doesn't support quantization")
+                raise nncf.ValidationError("adaptive_compression_level mode doesn't support quantization")
 
     accuracy_aware_training_config = config.get("accuracy_aware_training", None)
 
     mode = accuracy_aware_training_config.get("mode")
     params = {"mode": mode}
 
     if accuracy_aware_training_config.get("params") is not None:
```

### Comparing `nncf-2.8.1/nncf/config/schema.py` & `nncf-2.9.0/nncf/config/schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -74,15 +74,15 @@
             description="Keyword to be used when passing the tensor to the model's 'forward' method - "
             "leave unspecified to pass the corresponding argument as a positional arg.",
         ),
     },
     "additionalProperties": False,
 }
 
-TARGET_DEVICE_SCHEMA = {"type": "string", "enum": ["ANY", "CPU", "GPU", "VPU", "TRIAL", "CPU_SPR"]}
+TARGET_DEVICE_SCHEMA = {"type": "string", "enum": ["ANY", "CPU", "GPU", "NPU", "TRIAL", "CPU_SPR"]}
 
 
 NNCF_CONFIG_SCHEMA = {
     "$schema": "http://json-schema.org/draft-07/schema",
     "title": "NNCF configuration file schema",
     "description": "The NNCF configuration file follows the JSON format and is the primary way to configure "
     "the result of NNCF application to a given user model. This configuration file is "
```

### Comparing `nncf-2.8.1/nncf/config/schemata/__init__.py` & `nncf-2.9.0/nncf/config/schemata/algo/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/accuracy_aware.py` & `nncf-2.9.0/nncf/config/schemata/accuracy_aware.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/__init__.py` & `nncf-2.9.0/nncf/config/schemata/common/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/binarization.py` & `nncf-2.9.0/nncf/config/schemata/algo/binarization.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/const_sparsity.py` & `nncf-2.9.0/nncf/config/schemata/algo/const_sparsity.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/filter_pruning.py` & `nncf-2.9.0/nncf/config/schemata/algo/filter_pruning.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/knowledge_distillation.py` & `nncf-2.9.0/nncf/config/schemata/algo/knowledge_distillation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/magnitude_sparsity.py` & `nncf-2.9.0/nncf/config/schemata/algo/magnitude_sparsity.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/quantization.py` & `nncf-2.9.0/nncf/config/schemata/algo/quantization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/algo/rb_sparsity.py` & `nncf-2.9.0/nncf/config/schemata/algo/rb_sparsity.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/basic.py` & `nncf-2.9.0/nncf/config/schemata/basic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/common/__init__.py` & `nncf-2.9.0/nncf/experimental/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/common/compression.py` & `nncf-2.9.0/nncf/config/schemata/common/compression.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/common/initialization.py` & `nncf-2.9.0/nncf/config/schemata/common/initialization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/common/sparsity.py` & `nncf-2.9.0/nncf/config/schemata/common/sparsity.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/common/targeting.py` & `nncf-2.9.0/nncf/config/schemata/common/targeting.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/defaults.py` & `nncf-2.9.0/nncf/config/schemata/defaults.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/schemata/experimental_schema.py` & `nncf-2.9.0/nncf/config/schemata/experimental_schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/structures.py` & `nncf-2.9.0/nncf/config/structures.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/telemetry_extractors.py` & `nncf-2.9.0/nncf/config/telemetry_extractors.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/config/utils.py` & `nncf-2.9.0/nncf/config/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/data/__init__.py` & `nncf-2.9.0/nncf/data/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/data/dataset.py` & `nncf-2.9.0/nncf/data/dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/definitions.py` & `nncf-2.9.0/nncf/definitions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/__init__.py` & `nncf-2.9.0/nncf/experimental/common/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/__init__.py` & `nncf-2.9.0/nncf/experimental/common/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/graph/__init__.py` & `nncf-2.9.0/nncf/experimental/common/pruning/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/graph/netron.py` & `nncf-2.9.0/nncf/experimental/common/graph/netron.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/pruning/__init__.py` & `nncf-2.9.0/nncf/experimental/common/tensor_statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/pruning/block_hierarchy.py` & `nncf-2.9.0/nncf/experimental/common/pruning/block_hierarchy.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/pruning/nodes_grouping.py` & `nncf-2.9.0/nncf/experimental/common/pruning/nodes_grouping.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/pruning/operations.py` & `nncf-2.9.0/nncf/experimental/common/pruning/operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/pruning/propagation_data.py` & `nncf-2.9.0/nncf/experimental/common/pruning/propagation_data.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/tensor_statistics/__init__.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/common/tensor_statistics/collectors.py` & `nncf-2.9.0/nncf/experimental/common/tensor_statistics/collectors.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from abc import ABC
 from abc import abstractmethod
 from collections import defaultdict
 from collections import deque
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, TypeVar, Union
 
+import nncf
 from nncf.common.tensor import TensorType
 from nncf.common.tensor_statistics.collectors import NNCFCollectorTensorProcessor
 from nncf.common.tensor_statistics.collectors import NNCFTensor
 from nncf.common.tensor_statistics.collectors import ReductionAxes
 from nncf.common.tensor_statistics.statistics import MeanTensorStatistic
 from nncf.common.tensor_statistics.statistics import MedianMADTensorStatistic
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
@@ -50,14 +51,18 @@
 
     @property
     def inplace(self):
         return self._inplace
 
     @property
     def output_port_id(self) -> int:
+        """
+        Port id of the last node of the reducer subgraph if statistic is inplace.
+        Port id of the reducer output return node if statistic is not inplace.
+        """
         return 0
 
     @property
     def name(self):
         return self.__class__.__name__ + str(self.__hash__())
 
     @staticmethod
@@ -70,33 +75,25 @@
         """
         Specifies the reduction rule in terms of NNCFCollectorTensorProcessor.
 
         :param x: Tensor to register.
         """
 
     @abstractmethod
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        """
-        Returns target output names from target model that is
-            modified for statistic collection.
-
-        :param target_node_name: Target node name for reducer.
-        :param port_id: Target port id for target node name for reducer.
-        :return: Target output names for reducer.
-        """
-
-    @abstractmethod
     def get_inplace_fn(self) -> Optional[InplaceInsertionFNType]:
         """
         Returns correspondent inplace operation builder if inplace operations are available in backend.
 
         :return: Inplace operation builder if possible else None.
         """
 
     def __call__(self, x: List[NNCFTensor]):
+        if any(t.is_empty() for t in x):
+            return None
+
         if self.inplace:
             return x
 
         return self._reduce_out_of_place(x)
 
     def __eq__(self, __o: object) -> bool:
         return (
@@ -255,19 +252,19 @@
 
         :param container_key: Container key to pass aggregated statistic to.
         :param reducer: TensorReducer instance for the statistic collection branch.
         :param aggregator: TensorAggregator instance for the statistic collection branch.
         :reducer_output_port_id: Reducer target output port id.
         """
         if container_key in self._stat_container_kwargs_map:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"Two different statistic branches for one container key {container_key} are encountered"
             )
         if any(aggr is aggregator for aggr in self._aggregators.values()):
-            raise RuntimeError(f"One aggregator instance {aggregator} for different branches is encountered")
+            raise nncf.InternalError(f"One aggregator instance {aggregator} for different branches is encountered")
 
         self._reducers.add(reducer)
         key = (hash(reducer), reducer_output_port_id, hash(aggregator))
 
         if key not in self._aggregators:
             self._aggregators[key] = aggregator
         self._stat_container_kwargs_map[container_key] = key
@@ -295,17 +292,17 @@
         if not self._enabled:
             return
 
         reduced_inputs = {}
         for reducer in self._reducers:
             reducer_hash = hash(reducer)
             input_ = inputs[reducer_hash]
-            if any(tensor.is_empty() for tensor in input_):
-                continue
-            reduced_inputs[reducer_hash] = reducer(input_)
+            reduced_input = reducer(input_)
+            if reduced_input is not None:
+                reduced_inputs[reducer_hash] = reduced_input
 
         for (
             (reducer_hash, reducer_port_id, _),
             aggregator,
         ) in self._aggregators.items():
             if reducer_hash in reduced_inputs:
                 aggregator.register_reduced_input(reduced_inputs[reducer_hash][reducer_port_id])
@@ -426,15 +423,15 @@
             if PercentileTensorStatistic.TENSOR_STATISTIC_OUTPUT_KEY in kwargs:
                 percentile_vs_values_dict = kwargs[PercentileTensorStatistic.TENSOR_STATISTIC_OUTPUT_KEY]
             else:
                 percentile_vs_values_dict = {}
                 for (_, percentile), value in kwargs.items():
                     percentile_vs_values_dict[percentile] = value
             return statistic_container_cls(percentile_vs_values_dict=percentile_vs_values_dict)
-        raise RuntimeError(
+        raise nncf.InternalError(
             f"Statistic collector class {statistic_container_cls} is not supported by the TensorCollector class."
         )
 
 
 class MergedTensorCollector(TensorCollector):
     """
     Tensor collector that merge several tensor collectors in one.
@@ -478,14 +475,22 @@
     def get_inplace_fn(self) -> Optional[InplaceInsertionFNType]:
         return None
 
     def _reduce_out_of_place(self, x: List[TensorType]) -> List[TensorType]:
         return x
 
 
+class RawReducer(NoopReducer):
+    def __init__(self):
+        super().__init__()
+
+    def __call__(self, x: List[NNCFTensor]):
+        return self._reduce_out_of_place(x)
+
+
 class MinReducer(TensorReducerBase):
     def _reduce_out_of_place(self, x: List[NNCFTensor]) -> List[NNCFTensor]:
         x = x[0]
         reduction_axes = self._get_reduction_axes(x)
         return [self._tensor_processor.reduce_min(x, reduction_axes, keepdims=self._keepdims)]
 
 
@@ -617,17 +622,16 @@
             aggregated = self._aggregation_fn(stacked_tensors, axis=0, keepdims=self._keepdims)
             aggregated = self._tensor_processor.squeeze(aggregated, 0)
             self._container = [aggregated]
         else:
             self._container.append(reduced)
 
     def _aggregate_impl(self) -> NNCFTensor:
-        if 0 in self._aggregation_axes:
-            if self._keepdims:
-                return self._container[0].tensor
+        if 0 in self._aggregation_axes and self._keepdims:
+            return self._container[0].tensor
         return self._tensor_processor.stack(self._container).tensor
 
     @abstractmethod
     def _aggregation_fn(self, stacked_value: NNCFTensor, axis: AggregationAxes, keepdims: bool) -> NNCFTensor:
         pass
 
 
@@ -649,17 +653,38 @@
     all samples in a container and aggregate them in one step.
     """
 
     def _register_reduced_input_impl(self, x: TensorType) -> None:
         self._container.append(x)
 
     def _aggregate_impl(self) -> NNCFTensor:
-        stacked_val = self._tensor_processor.stack(self._container)
-        aggregated = self._aggregation_fn(stacked_val, axis=self._aggregation_axes, keepdims=self._keepdims)
-        return self._tensor_processor.squeeze(aggregated, 0).tensor
+        # Case when all registered tensors have identical shape
+        if all(self._container[0].shape == x.shape for x in self._container):
+            stacked_value = self._tensor_processor.stack(self._container)
+            aggregated = self._aggregation_fn(stacked_value, axis=self._aggregation_axes, keepdims=self._keepdims)
+            return self._tensor_processor.squeeze(aggregated, 0).tensor
+        online_axes = tuple(x - 1 for x in self._aggregation_axes if x > 0)
+
+        # Case when some registered tensors have different shapes and
+        # 0 is present in the aggregation axes
+        if 0 in self._aggregation_axes:
+            stacked_value, shape_after_aggregation = _moveaxes_flatten_cat(
+                self._container, online_axes, self._tensor_processor
+            )
+            aggregated = self._aggregation_fn(stacked_value, axis=0, keepdims=False)
+            if self._keepdims:
+                aggregated = self._tensor_processor.reshape(aggregated, shape_after_aggregation)
+            return aggregated.tensor
+
+        # Case when some registered tensors have different shapes and
+        # 0 is not present in the aggregation axes
+        ret_val = []
+        for tensor in self._container:
+            ret_val.append(self._aggregation_fn(tensor, axis=online_axes, keepdims=self._keepdims))
+        return self._tensor_processor.stack(ret_val, axis=0).tensor
 
     @abstractmethod
     def _aggregation_fn(self, stacked_value: NNCFTensor, axis: AggregationAxes, keepdims: bool) -> NNCFTensor:
         pass
 
 
 class MeanAggregator(OfflineAggregatorBase):
@@ -682,53 +707,50 @@
         quantile: float = 0.01,
     ):
         super().__init__(tensor_processor, aggregation_axes=aggregation_axes, num_samples=num_samples)
         self._window_size = window_size
         self._container = deque(maxlen=window_size)
         self._quantile = quantile
 
-    def _aggregate_impl(self) -> NNCFTensor:
-        stacked_samples = self._tensor_processor.stack(self._container)
+    def _aggregation_fn(self, stacked_value: NNCFTensor, axis: int, keepdims: bool) -> NNCFTensor:
         low_values, high_values = self._tensor_processor.quantile(
-            stacked_samples,
-            quantile=(self._quantile, 1 - self._quantile),
-            axis=self._aggregation_axes,
+            stacked_value, quantile=(self._quantile, 1 - self._quantile), axis=axis
         )
         tp = self._tensor_processor
-        outliers_mask = tp.logical_or(tp.less(stacked_samples, low_values), tp.less(high_values, stacked_samples))
-        aggregated = self._aggregation_fn(
-            stacked_samples=stacked_samples,
+        outliers_mask = tp.logical_or(tp.less(stacked_value, low_values), tp.less(high_values, stacked_value))
+        aggregated = self._masked_aggregation_fn(
+            stacked_samples=stacked_value,
             mask=outliers_mask,
-            axis=self._aggregation_axes,
-            keepdims=self._keepdims,
+            axis=axis,
+            keepdims=keepdims,
         )
-        return self._tensor_processor.squeeze(aggregated, 0).tensor
+        return aggregated
 
     @abstractmethod
-    def _aggregation_fn(
+    def _masked_aggregation_fn(
         self, stacked_samples: NNCFTensor, mask: NNCFTensor, axis: AggregationAxes, keepdims: bool
     ) -> NNCFTensor:
         pass
 
     def __eq__(self, __o: object) -> bool:
         return super().__eq__(__o) and self._quantile == __o._quantile
 
     def __hash__(self) -> int:
         return hash((self.__class__.__name__, self._quantile))
 
 
 class MeanNoOutliersAggregator(NoOutliersAggregatorBase):
-    def _aggregation_fn(
+    def _masked_aggregation_fn(
         self, stacked_samples: NNCFTensor, mask: NNCFTensor, axis: AggregationAxes, keepdims: bool
     ) -> NNCFTensor:
         return self._tensor_processor.masked_mean(stacked_samples, axis=axis, mask=mask, keepdims=keepdims)
 
 
 class MedianNoOutliersAggregator(NoOutliersAggregatorBase):
-    def _aggregation_fn(
+    def _masked_aggregation_fn(
         self, stacked_samples: NNCFTensor, mask: NNCFTensor, axis: AggregationAxes, keepdims: bool
     ) -> NNCFTensor:
         return self._tensor_processor.masked_median(stacked_samples, axis=axis, mask=mask, keepdims=keepdims)
 
 
 class MedianAbsoluteDeviationAggregator(AggregatorBase):
     def __init__(
```

### Comparing `nncf-2.8.1/nncf/experimental/common/tensor_statistics/statistical_functions.py` & `nncf-2.9.0/nncf/experimental/common/tensor_statistics/statistical_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/__init__.py` & `nncf-2.9.0/nncf/experimental/tensor/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/definitions.py` & `nncf-2.9.0/nncf/experimental/tensor/definitions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/__init__.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -10,38 +10,44 @@
 # limitations under the License.
 
 from nncf.experimental.tensor.functions import linalg as linalg
 from nncf.experimental.tensor.functions.numeric import abs as abs
 from nncf.experimental.tensor.functions.numeric import all as all
 from nncf.experimental.tensor.functions.numeric import allclose as allclose
 from nncf.experimental.tensor.functions.numeric import any as any
+from nncf.experimental.tensor.functions.numeric import argsort as argsort
 from nncf.experimental.tensor.functions.numeric import as_tensor_like as as_tensor_like
 from nncf.experimental.tensor.functions.numeric import astype as astype
 from nncf.experimental.tensor.functions.numeric import clip as clip
 from nncf.experimental.tensor.functions.numeric import count_nonzero as count_nonzero
 from nncf.experimental.tensor.functions.numeric import device as device
 from nncf.experimental.tensor.functions.numeric import dtype as dtype
 from nncf.experimental.tensor.functions.numeric import finfo as finfo
 from nncf.experimental.tensor.functions.numeric import flatten as flatten
 from nncf.experimental.tensor.functions.numeric import isclose as isclose
 from nncf.experimental.tensor.functions.numeric import isempty as isempty
 from nncf.experimental.tensor.functions.numeric import item as item
+from nncf.experimental.tensor.functions.numeric import matmul as matmul
 from nncf.experimental.tensor.functions.numeric import max as max
 from nncf.experimental.tensor.functions.numeric import maximum as maximum
 from nncf.experimental.tensor.functions.numeric import mean as mean
 from nncf.experimental.tensor.functions.numeric import min as min
 from nncf.experimental.tensor.functions.numeric import minimum as minimum
 from nncf.experimental.tensor.functions.numeric import moveaxis as moveaxis
 from nncf.experimental.tensor.functions.numeric import multiply as multiply
 from nncf.experimental.tensor.functions.numeric import ones_like as ones_like
+from nncf.experimental.tensor.functions.numeric import power as power
+from nncf.experimental.tensor.functions.numeric import quantile as quantile
 from nncf.experimental.tensor.functions.numeric import reshape as reshape
 from nncf.experimental.tensor.functions.numeric import round as round
 from nncf.experimental.tensor.functions.numeric import squeeze as squeeze
 from nncf.experimental.tensor.functions.numeric import stack as stack
 from nncf.experimental.tensor.functions.numeric import sum as sum
+from nncf.experimental.tensor.functions.numeric import transpose as transpose
+from nncf.experimental.tensor.functions.numeric import unsqueeze as unsqueeze
 from nncf.experimental.tensor.functions.numeric import unstack as unstack
 from nncf.experimental.tensor.functions.numeric import var as var
 from nncf.experimental.tensor.functions.numeric import where as where
 from nncf.experimental.tensor.functions.numeric import zeros_like as zeros_like
 
 
 def _initialize_backends():
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/dispatcher.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/dispatcher.py`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/linalg.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/linalg.py`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/numeric.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/numeric.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -379,14 +379,53 @@
     :return: An array of the same type as a, containing the rounded values.
     """
     return Tensor(round(a.data, decimals))
 
 
 @functools.singledispatch
 @tensor_guard
+def power(a: Tensor, exponent: Union[Tensor, float]) -> Tensor:
+    """
+    Takes the power of each element in input with exponent and returns a tensor with the result.
+    Exponent can be either a single float number or a broadcastable Tensor. In case exponent is
+    a brodcastable tensor, the exponent is being broadcasted and the return tensor contains
+    the power of each element in input with exponent elementwise.
+
+    :param a: Input data.
+    :param exponent: Exponent value.
+    :return: The result of the power of each element in input with given exponent.
+    """
+    return Tensor(power(a.data, unwrap_tensor_data(exponent)))
+
+
+@functools.singledispatch
+@tensor_guard
+def quantile(
+    a: Tensor,
+    q: Union[float, List[float]],
+    axis: Optional[Union[int, Tuple[int]]] = None,
+    keepdims: Optional[bool] = None,
+) -> Tensor:
+    """
+    Compute the quantile(s) of the data along the specified axis.
+
+    :param a: Given tensor.
+    :params q: Quantile or sequence of quantiles to compute, which must be between
+        0 and 1 inclusive.
+    :param axis: Axis or axes along which the quantiles are computed.
+    :param keepdims: If True, the axes which are reduced are left in the result
+        as dimensions with size one.
+    :return: An tensor with quantiles, the first axis of the result corresponds
+        to the quantiles, other axes of the result correspond to the quantiles values.
+    """
+    return Tensor(quantile(a.data, q, axis, keepdims))
+
+
+@functools.singledispatch
+@tensor_guard
 def _binary_op_nowarn(a: Tensor, b: Union[Tensor, float], operator_fn: Callable) -> Tensor:
     """
     Applies a binary operation with disable warnings.
 
     :param a: The first tensor.
     :param b: The second tensor.
     :param operator_fn: The binary operation function.
@@ -518,7 +557,62 @@
     """
     Return number of elements in the tensor.
 
     :param a: The input tensor
     :return: The size of the input tensor.
     """
     return size(a.data)
+
+
+@functools.singledispatch
+@tensor_guard
+def matmul(x1: Tensor, x2: Union[Tensor, float]) -> Tensor:
+    """
+    Matrix multiplication.
+
+    :param x1: The first input tensor.
+    :param x2: The second input tensor or number.
+    :return: The product of x1 and x2, matmul.
+    """
+    return Tensor(matmul(x1.data, unwrap_tensor_data(x2)))
+
+
+@functools.singledispatch
+@tensor_guard
+def unsqueeze(a: Tensor, axis: Optional[Union[int, Tuple[int, ...]]] = None) -> Tensor:
+    """
+    Add axes of length one to a.
+
+    :param a: The input tensor.
+    :param axis: Selects a subset of the entries of length one in the shape.
+    :return: The input array, but with expanded shape with len 1 defined in axis.
+    """
+    return Tensor(unsqueeze(a.data, axis=axis))
+
+
+@functools.singledispatch
+@tensor_guard
+def transpose(a: Tensor, axes: Optional[Tuple[int, ...]] = None) -> Tensor:
+    """
+    Returns an array with axes transposed.
+
+    :param a: The input tensor.
+    :param axes: list of permutations or None.
+    :return: array with permuted axes.
+    """
+    return Tensor(transpose(a.data, axes=axes))
+
+
+@functools.singledispatch
+@tensor_guard
+def argsort(a: Tensor, axis: int = -1, descending: bool = False, stable: bool = False) -> Tensor:
+    """
+    Returns the indices that would sort an array.
+
+    :param a: The input tensor.
+    :param axis: Axis along which to sort. The default is -1 (the last axis). If None, the flattened array is used.
+    :param descending: Controls the sorting order (ascending or descending).
+    :param stable: If True then the sorting routine becomes stable, preserving the order of equivalent elements.
+        If False, the relative order of values which compare equal is not guaranteed. True is slower.
+    :return: Array of indices that sort a along the specified axis.
+    """
+    return Tensor(argsort(a.data, axis=axis))
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/numpy_linalg.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/numpy_linalg.py`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/numpy_numeric.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/numpy_numeric.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -175,14 +175,29 @@
 
 
 @register_numpy_types(numeric.round)
 def _(a: Union[np.ndarray, np.generic], decimals: int = 0) -> np.ndarray:
     return np.round(a, decimals=decimals)
 
 
+@register_numpy_types(numeric.power)
+def _(a: Union[np.ndarray, np.generic], exponent: Union[np.ndarray, float]) -> Union[np.ndarray, np.generic]:
+    return np.power(a, exponent)
+
+
+@register_numpy_types(numeric.quantile)
+def _(
+    a: Union[np.ndarray, np.generic],
+    q: Union[float, List[float]],
+    axis: Optional[Union[int, Tuple[int]]] = None,
+    keepdims: Optional[bool] = None,
+) -> Union[np.ndarray, np.generic]:
+    return np.array(np.quantile(a, q=q, axis=axis, keepdims=keepdims))
+
+
 @register_numpy_types(numeric._binary_op_nowarn)
 def _(
     a: Union[np.ndarray, np.generic], b: Union[np.ndarray, np.generic, float], operator_fn: Callable
 ) -> Union[np.ndarray, np.generic]:
     # Run operator with disabled warning
     with np.errstate(invalid="ignore", divide="ignore"):
         return operator_fn(a, b)
@@ -243,7 +258,31 @@
 ) -> np.ndarray:
     return np.array(np.var(a, axis=axis, keepdims=keepdims, ddof=ddof))
 
 
 @register_numpy_types(numeric.size)
 def _(a: Union[np.ndarray, np.generic]) -> int:
     return a.size
+
+
+@register_numpy_types(numeric.matmul)
+def _(x1: Union[np.ndarray, np.generic], x2: Union[np.ndarray, np.generic, float]) -> np.ndarray:
+    return np.matmul(x1, x2)
+
+
+@register_numpy_types(numeric.unsqueeze)
+def _(
+    a: Union[np.ndarray, np.generic], axis: Optional[Union[int, Tuple[int, ...]]] = None
+) -> Union[np.ndarray, np.generic]:
+    return np.expand_dims(a, axis=axis)
+
+
+@register_numpy_types(numeric.transpose)
+def _(a: Union[np.ndarray, np.generic], axes: Optional[Tuple[int, ...]] = None) -> Union[np.ndarray, np.generic]:
+    return np.transpose(a, axes=axes)
+
+
+@register_numpy_types(numeric.argsort)
+def _(
+    a: Union[np.ndarray, np.generic], axis: Optional[int] = None, descending=False, stable=False
+) -> Union[np.ndarray, np.generic]:
+    return np.argsort(a, axis=axis)
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/torch_linalg.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/torch_linalg.py`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/experimental/tensor/functions/torch_numeric.py` & `nncf-2.9.0/nncf/experimental/tensor/functions/torch_numeric.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Callable, List, Optional, Tuple, Union
 
+import numpy as np
 import torch
 
 from nncf.experimental.tensor import TensorDataType
 from nncf.experimental.tensor import TensorDeviceType
 from nncf.experimental.tensor.definitions import TypeInfo
 from nncf.experimental.tensor.functions import numeric as numeric
 
@@ -40,15 +41,15 @@
     return DEVICE_MAP[a.device.type]
 
 
 @numeric.squeeze.register(torch.Tensor)
 def _(a: torch.Tensor, axis: Optional[Union[int, Tuple[int, ...]]] = None) -> torch.Tensor:
     if axis is None:
         return a.squeeze()
-    if isinstance(axis, Tuple) and any(1 != a.shape[i] for i in axis):
+    if isinstance(axis, Tuple) and any(a.shape[i] != 1 for i in axis):
         # Make Numpy behavior, torch.squeeze skips axes that are not equal to one..
         raise ValueError("Cannot select an axis to squeeze out which has size not equal to one")
     return a.squeeze(axis)
 
 
 @numeric.flatten.register(torch.Tensor)
 def _(a: torch.Tensor) -> torch.Tensor:
@@ -187,14 +188,39 @@
 
 
 @numeric.round.register(torch.Tensor)
 def _(a: torch.Tensor, decimals=0) -> torch.Tensor:
     return torch.round(a, decimals=decimals)
 
 
+@numeric.power.register(torch.Tensor)
+def _(a: torch.Tensor, exponent: Union[torch.Tensor, float]) -> torch.Tensor:
+    return torch.pow(a, exponent=exponent)
+
+
+@numeric.quantile.register(torch.Tensor)
+def _(
+    a: torch.Tensor,
+    q: Union[float, List[float]],
+    axis: Optional[Union[int, Tuple[int]]] = None,
+    keepdims: Optional[bool] = None,
+) -> torch.Tensor:
+    device = a.device
+    # See https://github.com/pytorch/pytorch/issues/61582
+    # https://github.com/pytorch/pytorch/issues/64947
+    if a.numel() <= 16_000_000 and isinstance(axis, int) and a.dtype in [torch.float32, torch.float64]:
+        return torch.quantile(
+            a,
+            torch.tensor(q, dtype=a.dtype, device=a.device),
+            axis,
+            keepdims,
+        ).type(torch.float64)
+    return torch.tensor(np.quantile(a.detach().cpu().numpy(), q=q, axis=axis, keepdims=keepdims)).to(device)
+
+
 @numeric._binary_op_nowarn.register(torch.Tensor)
 def _(a: torch.Tensor, b: Union[torch.Tensor, float], operator_fn: Callable) -> torch.Tensor:
     return operator_fn(a, b)
 
 
 @numeric._binary_reverse_op_nowarn.register(torch.Tensor)
 def _(a: torch.Tensor, b: Union[torch.Tensor, float], operator_fn: Callable) -> torch.Tensor:
@@ -238,7 +264,27 @@
 ) -> torch.Tensor:
     return torch.var(a, dim=axis, keepdim=keepdims, correction=ddof)
 
 
 @numeric.size.register(torch.Tensor)
 def _(a: torch.Tensor) -> int:
     return torch.numel(a)
+
+
+@numeric.matmul.register(torch.Tensor)
+def _(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
+    return torch.matmul(x1, x2)
+
+
+@numeric.unsqueeze.register(torch.Tensor)
+def _(a: torch.Tensor, axis: Optional[Union[int, Tuple[int, ...]]] = None) -> torch.Tensor:
+    return torch.unsqueeze(a, dim=axis)
+
+
+@numeric.transpose.register(torch.Tensor)
+def _(a: torch.Tensor, axes: Optional[Tuple[int, ...]] = None) -> torch.Tensor:
+    return a.t()
+
+
+@numeric.argsort.register(torch.Tensor)
+def _(a: torch.Tensor, axis: Optional[int] = None, descending=False, stable=False) -> torch.Tensor:
+    return torch.argsort(a, dim=axis, descending=descending, stable=stable)
```

### Comparing `nncf-2.8.1/nncf/experimental/tensor/tensor.py` & `nncf-2.9.0/nncf/experimental/tensor/tensor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/__init__.py` & `nncf-2.9.0/nncf/experimental/tensorflow/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/context.py` & `nncf-2.9.0/nncf/experimental/tensorflow/context.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/__init__.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/argprovider.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/argprovider.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/converter.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 
 import tensorflow as tf
 from tensorflow.lite.python.util import get_grappler_config as _get_grappler_config
 from tensorflow.lite.python.util import run_graph_optimizations as _run_graph_optimizations
 from tensorflow.python.framework import convert_to_constants as _convert_to_constants
 from tensorflow.python.keras.saving import saving_utils as _saving_utils
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph.layer_attributes import Dtype
 from nncf.experimental.tensorflow.graph.node_attributes import TFNodeAttributes
 from nncf.experimental.tensorflow.graph.node_attributes import TFWeightedNodeAttributes
 from nncf.tensorflow.graph.converter import TFModelConverter
 from nncf.tensorflow.graph.metatypes.common import ALL_LAYER_METATYPES_WITH_WEIGHTS
 from nncf.tensorflow.graph.metatypes.matcher import get_op_metatype
@@ -373,15 +374,15 @@
         :return: An instance of the `Dtype`.
         """
         if dtype.is_floating:
             tensor_dtype = Dtype.FLOAT
         elif dtype.is_integer:
             tensor_dtype = Dtype.INTEGER
         else:
-            raise RuntimeError(f"Unexpected dtype of tensor: {dtype}")
+            raise nncf.InternalError(f"Unexpected dtype of tensor: {dtype}")
 
         return tensor_dtype
 
     @staticmethod
     def _get_op_name_to_const_op_names_map(
         graph, marked_ops: Dict[str, tf.Operation]
     ) -> Dict[str, List[Tuple[str, bool]]]:
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/model_transformer.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/model_transformer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/node_attributes.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/node_attributes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/experimental/tensorflow/quantization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/commands.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/commands.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/graph/transformations/layout.py` & `nncf-2.9.0/nncf/experimental/tensorflow/graph/transformations/layout.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/nncf_network.py` & `nncf-2.9.0/nncf/experimental/tensorflow/nncf_network.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/patch_tf.py` & `nncf-2.9.0/nncf/experimental/tensorflow/patch_tf.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/quantization/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/quantization/algorithm.py` & `nncf-2.9.0/nncf/experimental/tensorflow/quantization/algorithm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Dict, List, Optional
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.graph.utils import get_first_nodes_of_type
 from nncf.common.logging import nncf_logger
@@ -186,15 +187,15 @@
                 qp.channel_axes,
             )
 
             self._op_names.append(quantizer.name)
 
             for qp_id in unified_scales_group:
                 if was_processed[qp_id]:
-                    raise RuntimeError("Unexpected behavior")
+                    raise nncf.InternalError("Unexpected behavior")
                 was_processed[qp_id] = True
 
                 curr_qp = quantization_points[qp_id]
                 # Checks
                 assert curr_qp.quantizer_spec.get_state() == qp.quantizer_spec.get_state()
                 assert curr_qp.input_shape == qp.input_shape
                 assert curr_qp.channel_axes == qp.channel_axes
@@ -267,37 +268,37 @@
             target_node = nncf_graph.get_node_by_name(qp.insertion_point.target_node_name)
 
             if qp.is_weight_quantization_point():
                 # Check correctness
                 if target_node.node_name in node_name_to_qconfig_map:
                     assigned_qconfig = node_name_to_qconfig_map[target_node.node_name]
                     if qp.qconfig != assigned_qconfig:
-                        raise RuntimeError(
+                        raise nncf.InternalError(
                             "Inconsistent quantizer configurations selected by solver for one "
                             f"and the same quantizable op! Tried to assign {qp.qconfig} to "
                             f"{target_node.node_name} as specified by QP {qp_id}, but the op "
                             f"already has quantizer config {assigned_qconfig} assigned to it!"
                         )
                     continue  # The operation has already been quantized
                 node_name_to_qconfig_map[target_node.node_name] = qp.qconfig
 
                 # Parameters
                 half_range = self._get_half_range(qp.qconfig, target_node, first_conv_nodes)
                 narrow_range = not half_range
                 target_type = TargetType.OPERATOR_PRE_HOOK
                 if not issubclass(target_node.metatype, TFOpWithWeightsMetatype):
-                    raise RuntimeError(f"Unexpected type of metatype: {type(target_node.metatype)}")
+                    raise nncf.InternalError(f"Unexpected type of metatype: {type(target_node.metatype)}")
                 port_ids = [weight_def.port_id for weight_def in target_node.metatype.weight_definitions]
 
             else:
                 assert qp.is_activation_quantization_point()
 
                 # Check correctness
                 if not isinstance(qp.insertion_point, ActivationQuantizationInsertionPoint):
-                    raise RuntimeError(f"Unexpected type of insertion point: {type(qp.insertion_point)}")
+                    raise nncf.InternalError(f"Unexpected type of insertion point: {type(qp.insertion_point)}")
 
                 # Parameters
                 half_range = False
                 narrow_range = False
                 if qp.insertion_point.input_port_id is not None:
                     port_ids = [qp.insertion_point.input_port_id]  # Input port ids
                     target_type = TargetType.OPERATOR_PRE_HOOK
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/quantization/init_range.py` & `nncf-2.9.0/nncf/experimental/tensorflow/quantization/init_range.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/quantization/quantizers.py` & `nncf-2.9.0/nncf/experimental/tensorflow/quantization/quantizers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/tensorflow/scope.py` & `nncf-2.9.0/nncf/experimental/tensorflow/scope.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/nas/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 from collections import OrderedDict
 from copy import deepcopy
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from torch import nn
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
@@ -277,19 +278,19 @@
         """
         super().__init__(max_width=max_width, node_name=node_name)
         if fixed_width_list is None:
             fixed_width_list = []
         if fixed_width_list:
             fixed_width_list.sort(reverse=True)
             if fixed_width_list[0] > max_width:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Width list for {node_name} contains invalid values: {fixed_width_list}, {max_width}"
                 )
             if fixed_width_list[0] != max_width:
-                raise RuntimeError(f"Max width for {node_name} is not aligned with pre-trained model")
+                raise nncf.ValidationError(f"Max width for {node_name} is not aligned with pre-trained model")
             self._width_list = fixed_width_list
         else:
             self._width_list = self._generate_width_list(self._max_width, params)
 
     @property
     def width_list(self) -> List[int]:
         """
@@ -355,15 +356,15 @@
             p.width_multipliers.sort(reverse=True)
             if p.width_multipliers[0] < 1:
                 width_list.append(max_width)
             for multiplier in p.width_multipliers:
                 if p.max_num_widths == len(width_list):
                     break
                 if 0 >= multiplier > 1:
-                    raise RuntimeError(f"Wrong value for multiplier: {multiplier}")
+                    raise nncf.InternalError(f"Wrong value for multiplier: {multiplier}")
                 w = int(max_width * multiplier)
                 w = w - (w % ALIGNMENT_CONSTANT_FOR_MULTIPLIERS)
                 w = max(w, p.min_width)
                 if w in width_list:
                     continue
                 width_list.append(w)
         return width_list
@@ -574,15 +575,15 @@
     @property
     def width_num_params_indicator(self):
         return self._width_num_params_indicator
 
     @width_num_params_indicator.setter
     def width_num_params_indicator(self, width_num_params_indicator):
         if width_num_params_indicator == 0 or width_num_params_indicator < -1:
-            raise RuntimeError(f"Invalid width indicator: {width_num_params_indicator}")
+            raise nncf.InternalError(f"Invalid width indicator: {width_num_params_indicator}")
         self._width_num_params_indicator = width_num_params_indicator
 
     @property
     def propagation_graph(self) -> PTNNCFGraph:
         """
         :return: nncf graph that is used for propagating pruning and reordering masks
         """
@@ -705,43 +706,42 @@
                 if input_width:
                     dynamic_input_width_op.set_active_width(input_width)
                     was_set = True
 
             if not was_set and node_name not in names_of_processed_nodes:
                 nncf_logger.debug(f"input width was not set in scope={node.node_name}")
 
-            if self._add_dynamic_inputs:
-                if node_name in self._add_dynamic_inputs and not was_set:
-                    nncf_logger.debug(f"setting input width by user's request for scope={node_name}")
-                    nodes_to_check = [node]
-                    while any(elem is None for elem in input_masks):
-                        previous_nodes = []
-                        for node in nodes_to_check:
-                            previous_nodes.append(self._propagation_graph.get_previous_nodes(node))
-                        nodes_to_check.clear()
-                        previous_nodes = [item for nodes in previous_nodes for item in nodes]
-                        if not previous_nodes:
-                            break
-                        for previous in previous_nodes:
-                            if "output_mask" in previous.attributes:
-                                if previous.attributes["output_mask"] is not None:
-                                    input_masks.append(previous.attributes["output_mask"])
-                                    input_masks = [i for i in input_masks if i]
-                                else:
-                                    nodes_to_check.append(previous)
+            if self._add_dynamic_inputs and node_name in self._add_dynamic_inputs and not was_set:
+                nncf_logger.debug(f"setting input width by user's request for scope={node_name}")
+                nodes_to_check = [node]
+                while any(elem is None for elem in input_masks):
+                    previous_nodes = []
+                    for node in nodes_to_check:
+                        previous_nodes.append(self._propagation_graph.get_previous_nodes(node))
+                    nodes_to_check.clear()
+                    previous_nodes = [item for nodes in previous_nodes for item in nodes]
+                    if not previous_nodes:
+                        break
+                    for previous in previous_nodes:
+                        if "output_mask" in previous.attributes:
+                            if previous.attributes["output_mask"] is not None:
+                                input_masks.append(previous.attributes["output_mask"])
+                                input_masks = [i for i in input_masks if i]
                             else:
                                 nodes_to_check.append(previous)
-                    if input_masks:
-                        input_mask = input_masks[0]
-                        input_width = self.mask_to_width(input_mask)
-                        if input_width:
-                            dynamic_input_width_op.set_active_width(input_width)
-                            was_set = True
-                    if was_set:
-                        nncf_logger.debug(f"Success setting up user's request for dynamic input at scope={node_name}")
+                        else:
+                            nodes_to_check.append(previous)
+                if input_masks:
+                    input_mask = input_masks[0]
+                    input_width = self.mask_to_width(input_mask)
+                    if input_width:
+                        dynamic_input_width_op.set_active_width(input_width)
+                        was_set = True
+                if was_set:
+                    nncf_logger.debug(f"Success setting up user's request for dynamic input at scope={node_name}")
 
     def get_active_in_out_width_values(
         self,
     ) -> Tuple[Dict[NNCFNodeName, int], Dict[NNCFNodeName, int]]:
         """
         Collects the active number of input and output channels (width) for each elastic layer in the graph.
 
@@ -905,15 +905,15 @@
         return op.width_list[:width_list_len]
 
     def _collect_ops_data_by_selection_rule(self, selection_rule: Callable) -> Dict[PruningGroupID, Any]:
         elastic_width_config = {}
         for cluster in self._pruned_module_groups_info.get_all_clusters():
             all_max_out_channels = {el.elastic_op.max_width for el in cluster.elements}
             if len(all_max_out_channels) != 1:
-                raise RuntimeError("Invalid grouping of layers with different number of output channels")
+                raise nncf.InternalError("Invalid grouping of layers with different number of output channels")
 
             first_elastic_width_info = next(iter(cluster.elements))
             op = first_elastic_width_info.elastic_op
             selected_width = selection_rule(op)
             elastic_width_config[cluster.id] = selected_width
             nncf_logger.debug(f"Select width={cluster.id} for group #{selected_width}")
         return elastic_width_config
@@ -1036,15 +1036,15 @@
             list_of_node_ids = []
             for node_name in grouped_node_names:
                 node = graph.get_node_by_name(node_name)
                 metatype = node.metatype
                 list_of_node_ids.append(node.node_id)
                 layer_attrs = node.layer_attributes
                 if metatype not in metatype_vs_elastic_op_creator:
-                    raise RuntimeError(f"Elastic width is not supported for {metatype}")
+                    raise nncf.InternalError(f"Elastic width is not supported for {metatype}")
                 elastic_op_creator = metatype_vs_elastic_op_creator[metatype]
 
                 elastic_width_operation = elastic_op_creator(
                     layer_attrs,
                     node_name,
                     self._params,
                     self._overwrite_groups_widths[i] if self._overwriting_pruning_groups else [],
@@ -1126,15 +1126,15 @@
         self._params = params
         self._grouped_node_names_to_prune = state[self._state_names.GROUPED_NODE_NAMES_TO_PRUNE]
 
         if params_from_state.get(self._state_names.OVERWRITE_GROUP_WIDTHS, None) is not None:
             self._overwrite_groups_widths = params_from_state[self._state_names.OVERWRITE_GROUP_WIDTHS]
             self._overwriting_pruning_groups = True
             if len(self._grouped_node_names_to_prune) != len(self._overwrite_groups_widths):
-                raise RuntimeError("Mismatch between number of groups for pruning and their corresponding widths")
+                raise nncf.InternalError("Mismatch between number of groups for pruning and their corresponding widths")
         if params_from_state.get(self._state_names.ADD_DYNAMIC_INPUTS, None) is not None:
             self._add_dynamic_inputs = params_from_state[self._state_names.ADD_DYNAMIC_INPUTS]
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional
 
 import networkx as nx
 
+import nncf
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.pruning.utils import get_input_masks
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_width import ElasticWidthHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
@@ -86,10 +87,10 @@
     @staticmethod
     def _get_original_node_from_compressed_node_name(
         node_name: NNCFNodeName, width_handler: ElasticWidthHandler
     ) -> Optional[NNCFNode]:
         try:
             propagation_graph: PTNNCFGraph = width_handler.propagation_graph
             result = propagation_graph.get_node_by_name(node_name)
-        except RuntimeError:
+        except nncf.InternalError:
             result = None
         return result
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -90,15 +90,15 @@
 
     def retrieve_from_cache(self, subnet_config_repr: Tuple[float, ...]) -> Tuple[bool, float]:
         """
         Checks if sub-network info is in cache and returns the corresponding value.
         :param subnet_config_repr: tuple representing the values for the associated design variables.
         :return: (True if the information is in cache, and corresponding value stored in cache, 0 otherwise)
         """
-        if subnet_config_repr in self.cache.keys():
+        if subnet_config_repr in self.cache:
             return True, self.cache[subnet_config_repr]
         return False, 0
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns state of the evaluatar
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/search.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -22,14 +22,15 @@
 from pymoo.operators.crossover.sbx import SBX
 from pymoo.operators.mutation.pm import PM
 from pymoo.operators.repair.rounding import RoundingRepair
 from pymoo.operators.sampling.rnd import IntegerRandomSampling
 from pymoo.optimize import minimize
 from torch.utils.data.dataloader import DataLoader
 
+import nncf
 from nncf import NNCFConfig
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.logging import nncf_logger
 from nncf.common.plotting import noninteractive_plotting
 from nncf.common.utils.decorators import skip_if_dependency_unavailable
 from nncf.common.utils.os import safe_open
 from nncf.common.utils.registry import Registry
@@ -329,15 +330,15 @@
         )
         self._num_vars = 0
         self._vars_lower = 0
         self._vars_upper = []
 
         self._num_vars, self._vars_upper = self._elasticity_ctrl.multi_elasticity_handler.get_design_vars_info()
         if self._num_vars == 0 or self._vars_lower is None:
-            raise RuntimeError("Search space is empty")
+            raise nncf.InternalError("Search space is empty")
 
         self._result = None
         bn_adapt_params = search_config.get("batchnorm_adaptation", {})
         bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(nncf_config, bn_adapt_params)
         self.bn_adaptation = BatchnormAdaptationAlgorithm(**bn_adapt_algo_kwargs) if bn_adapt_algo_kwargs else None
 
         self._problem = None
@@ -349,15 +350,15 @@
         """
         Gets a list of the evaluators used by the search algorithm.
 
         :return: List of available evaluators.
         """
         if self._evaluator_handlers:
             return self._evaluator_handlers
-        raise RuntimeError("Evaluator handlers haven't been defined")
+        raise nncf.ValidationError("Evaluator handlers haven't been defined")
 
     @property
     def acc_delta(self) -> float:
         """
         :return: Value allowed to deviate from when selecting a sub-network in the pareto front.
         """
         return self.search_params.acc_delta
@@ -735,21 +736,18 @@
         Saves information of current best sub-network discovered by the search algorithm.
 
         :param config: Best sub-network configuration
         :return:
         """
         acc_within_tolerance = self._accuracy_evaluator_handler.current_value
         pair_objective = self._efficiency_evaluator_handler.current_value
-        if acc_within_tolerance < (self._lower_bound_acc * -1.0):
-            if pair_objective < self._search.best_pair_objective:
-                self._search.best_pair_objective = pair_objective
-                self._search.best_config = config
-                self._search.best_vals = [
-                    evaluator_handler.current_value for evaluator_handler in self._evaluator_handlers
-                ]
-                checkpoint_path = Path(self._search.checkpoint_save_dir, "subnetwork_best.pth")
-                checkpoint = {
-                    "best_acc1": acc_within_tolerance * -1.0,
-                    "best_efficiency": pair_objective,
-                    "subnet_config": config,
-                }
-                torch.save(checkpoint, checkpoint_path)
+        if acc_within_tolerance < (self._lower_bound_acc * -1.0) and pair_objective < self._search.best_pair_objective:
+            self._search.best_pair_objective = pair_objective
+            self._search.best_config = config
+            self._search.best_vals = [evaluator_handler.current_value for evaluator_handler in self._evaluator_handlers]
+            checkpoint_path = Path(self._search.checkpoint_save_dir, "subnetwork_best.pth")
+            checkpoint = {
+                "best_acc1": acc_within_tolerance * -1.0,
+                "best_efficiency": pair_objective,
+                "subnet_config": config,
+            }
+            torch.save(checkpoint, checkpoint_path)
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/search/supernet.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/search/supernet.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/pruning/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -171,19 +171,18 @@
 
         :param next_epoch: The epoch index for which the compression scheduler
             will update the state of the compression method.
         """
         super().epoch_step(next_epoch)
         self._lr_scheduler.epoch_step(next_epoch)
         stage_desc, stage_desc_idx = self.get_current_stage_desc()
-        if stage_desc is not None:
-            if stage_desc_idx != self.current_stage_idx:
-                self._lr_scheduler.stage_step(stage_desc)
-                self._training_ctrl.set_stage(stage_desc)
-                self.current_stage_idx = stage_desc_idx
+        if stage_desc is not None and stage_desc_idx != self.current_stage_idx:
+            self._lr_scheduler.stage_step(stage_desc)
+            self._training_ctrl.set_stage(stage_desc)
+            self.current_stage_idx = stage_desc_idx
 
     def is_final_stage(self) -> bool:
         """
         :return: True, if final stage has been reached, False - otherwise
         """
         return self.current_stage_idx == len(self.list_stage_descriptors) - 1
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py` & `nncf-2.9.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/pruning/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/replace_custom_modules/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/pruning/operations.py` & `nncf-2.9.0/nncf/experimental/torch/pruning/operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/replace_custom_modules/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/search_building_blocks/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/replace_custom_modules/timm_custom_modules.py` & `nncf-2.9.0/nncf/experimental/torch/replace_custom_modules/timm_custom_modules.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/search_building_blocks/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/search_building_blocks/search_blocks.py` & `nncf-2.9.0/nncf/experimental/torch/search_building_blocks/search_blocks.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/search_building_blocks/search_graph.py` & `nncf-2.9.0/nncf/experimental/torch/search_building_blocks/search_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/__init__.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/__init__.py` & `nncf-2.9.0/nncf/onnx/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/algo.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 import inspect
 from copy import deepcopy
 from typing import List
 
 import torch
 import torch.distributed as dist
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionStage
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.logging import nncf_logger
 from nncf.common.scopes import matches_any
@@ -65,15 +66,15 @@
         matched_scopes = []
         for configs_per_scopes in self._sparse_configs_by_scopes:
             target_scopes = configs_per_scopes.target_scopes
             if matches_any(node_name, target_scopes):
                 sparse_cfg = configs_per_scopes.sparse_config
                 matched_scopes.append(target_scopes)
         if len(matched_scopes) >= 2:
-            raise RuntimeError(f'"{node_name}" is matched by multiple items in `sparse_structure_by_scopes`.')
+            raise nncf.InternalError(f'"{node_name}" is matched by multiple items in `sparse_structure_by_scopes`.')
 
         return MovementSparsifier(
             target_module_node,
             sparse_cfg=sparse_cfg,
             frozen=False,
             compression_lr_multiplier=compression_lr_multiplier,
             layerwise_loss_lambda=0.5,
@@ -105,15 +106,15 @@
                     TransformationPriority.SPARSIFICATION_PRIORITY,
                 )
             )
             sparsified_module = target_model.nncf.get_containing_module(node_name)
             self._sparsified_module_info.append(SparseModuleInfo(node_name, sparsified_module, sparsifying_operation))
 
         if not insertion_commands:
-            raise RuntimeError("No sparsifiable layer found for movement sparsity algorithm.")
+            raise nncf.InternalError("No sparsifiable layer found for movement sparsity algorithm.")
         return insertion_commands
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return MovementSparsityController(model, self._sparsified_module_info, self.config)
 
 
 MODEL_FAMILIES = ["bert", "wav2vec2", "swin", "mobilebert", "distilbert", "clip", "vit"]
@@ -145,15 +146,15 @@
         self._scheduler_params = MovementSchedulerParams.from_dict(params)
         self._scheduler = MovementPolynomialThresholdScheduler(self, self._scheduler_params)
         self._loss = ImportanceLoss(sparsify_operations)
         self._config = config
 
         if self._scheduler.enable_structured_masking:
             if not is_supported_model_family(self.model):
-                raise RuntimeError(
+                raise nncf.UnsupportedModelError(
                     "You set `enable_structured_masking=True`, but no supported model is detected. "
                     f"Supported model families: {MODEL_FAMILIES}."
                 )
             self._structured_mask_handler = StructuredMaskHandler(self.model, self.sparsified_module_info)
 
     @property
     def compression_rate(self) -> float:
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/functions.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/layers.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/layers.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,14 +12,15 @@
 from copy import deepcopy
 from enum import Enum
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 from torch import nn
 
+import nncf
 from nncf.common.graph import NNCFNode
 from nncf.experimental.torch.sparsity.movement.functions import binary_mask_by_threshold
 from nncf.torch.layer_utils import COMPRESSION_MODULES
 from nncf.torch.layer_utils import CompressionParameter
 from nncf.torch.sparsity.functions import apply_binary_mask as apply_binary_mask_impl
 from nncf.torch.sparsity.layers import BinaryMask
 from nncf.torch.utils import is_tracing_state
@@ -300,15 +301,15 @@
         if sparse_structure == SparseStructure.PER_DIM:
             score_shape = []
             for axis, (dim, factor) in enumerate(zip(weight_shape, sparse_factors)):
                 assert dim % factor == 0, f"{factor} is not a factor of axis {axis} with dim size {dim}."
                 score_shape.append(dim // factor)
             return tuple(score_shape)
 
-        raise RuntimeError("Unknown sparse structure.")
+        raise nncf.InternalError("Unknown sparse structure.")
 
     @staticmethod
     def _get_sparse_factors(weight_shape: List[int], sparse_config: SparseConfig) -> Tuple[int, int]:
         sparse_factors = sparse_config.sparse_factors
         if sparse_config.mode == SparseStructure.BLOCK:
             r, c = sparse_factors
             assert weight_shape[0] % r == 0, f"r: {r} is not a factor of dim axis 0."
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/loss.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/scheduler.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/scheduler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -311,20 +311,23 @@
         of the scheduler object will not be changed.
         """
         self._should_skip = False
 
         if self._steps_per_epoch is None and self._steps_in_current_epoch > 0:
             self._steps_per_epoch = self._steps_in_current_epoch
 
-        if self._steps_per_epoch is not None and self._steps_in_current_epoch > 0:
-            if self._steps_per_epoch != self._steps_in_current_epoch:
-                raise Exception(
-                    "Actual steps per epoch and steps per epoch from the scheduler "
-                    "parameters are different. Scheduling may be incorrect."
-                )
+        if (
+            self._steps_per_epoch is not None
+            and self._steps_in_current_epoch > 0
+            and self._steps_per_epoch != self._steps_in_current_epoch
+        ):
+            raise Exception(
+                "Actual steps per epoch and steps per epoch from the scheduler "
+                "parameters are different. Scheduling may be incorrect."
+            )
 
         if self._steps_per_epoch is None:
             self._should_skip = True
             nncf_logger.info(
                 "Movement sparsity scheduler updates importance threshold and regularization"
                 "factor per optimizer step, but steps_per_epoch was not set in config. Will "
                 "measure the actual steps per epoch as signaled by a .epoch_step() call."
```

### Comparing `nncf-2.8.1/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py` & `nncf-2.9.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/__init__.py` & `nncf-2.9.0/nncf/onnx/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/engine.py` & `nncf-2.9.0/nncf/onnx/engine.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/__init__.py` & `nncf-2.9.0/nncf/onnx/graph/metatypes/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/metatypes/__init__.py` & `nncf-2.9.0/nncf/onnx/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/metatypes/groups.py` & `nncf-2.9.0/nncf/onnx/graph/metatypes/groups.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -117,7 +117,13 @@
 
 
 # Contains the operation metatypes for which bias can be applied.
 OPERATIONS_WITH_BIAS = [
     onnx_metatypes.ONNXConvolutionMetatype,
     onnx_metatypes.ONNXDepthwiseConvolutionMetatype,
 ]
+
+
+QUANTIZE_DEQUANTIZE_OPERATIONS = [
+    onnx_metatypes.ONNXQuantizeLinearMetatype,
+    onnx_metatypes.ONNXDequantizeLinearMetatype,
+]
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/metatypes/onnx_metatypes.py` & `nncf-2.9.0/nncf/onnx/graph/metatypes/onnx_metatypes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Dict, List, Optional, Type
 
 import onnx
 
+import nncf
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 from nncf.onnx.graph.onnx_helper import get_parent
 from nncf.onnx.graph.onnx_helper import get_parents_node_mapping
 from nncf.onnx.graph.onnx_helper import get_tensor
 from nncf.onnx.graph.onnx_helper import has_tensor
@@ -43,15 +44,15 @@
     @classmethod
     def determine_subtype(cls, model: onnx.ModelProto, node: onnx.NodeProto) -> Optional[Type[OperatorMetatype]]:
         matches = []
         for subtype in cls.get_subtypes():
             if subtype.matches(model, node):
                 matches.append(subtype)
         if len(matches) > 1:
-            raise RuntimeError("Multiple subtypes match operator call - cannot determine single subtype.")
+            raise nncf.InternalError("Multiple subtypes match operator call - cannot determine single subtype.")
         if not matches:
             return None
         return matches[0]
 
 
 class ONNXOpWithWeightsMetatype(ONNXOpMetatype):
     """
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/model_transformer.py` & `nncf-2.9.0/nncf/onnx/graph/model_transformer.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,26 +11,27 @@
 from collections import Counter
 from copy import deepcopy
 from typing import Dict, List, Set, Tuple, Union
 
 import numpy as np
 import onnx
 
+import nncf
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.onnx.graph.node_utils import get_input_edge
 from nncf.onnx.graph.onnx_helper import get_children
 from nncf.onnx.graph.onnx_helper import get_children_node_mapping
 from nncf.onnx.graph.onnx_helper import get_edge_dtype
 from nncf.onnx.graph.onnx_helper import get_edge_info_mapping
 from nncf.onnx.graph.onnx_helper import get_name_to_node_map
 from nncf.onnx.graph.onnx_helper import get_node_index
 from nncf.onnx.graph.onnx_helper import get_tensor
-from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
+from nncf.onnx.graph.transformations.commands import ONNXInitializerUpdateCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
 from nncf.onnx.graph.transformations.commands import ONNXOutputInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXQDQNodeRemovingCommand
 from nncf.onnx.graph.transformations.commands import ONNXQuantizerInsertionCommand
 
 
 class ONNXModelTransformer(ModelTransformer):
@@ -83,41 +84,41 @@
         returns a new instance of the original model.
 
         :param transformation_layout: Transformation commands.
         :return: The new instance of a model with applied transformations.
         """
         quantizer_insert_transformations = []
         output_insert_transformations = []
-        bias_correction_transformations = []
+        initializer_update_transformations = []
         qdq_node_removing_transformations = []
         model_extraction_transformation = None
         transformations = transformation_layout.transformations
         # No transformation applied
         if not transformations:
             return deepcopy(self._model)
         for transformation in transformations:
             if isinstance(transformation, ONNXQuantizerInsertionCommand):
                 quantizer_insert_transformations.append(transformation)
             elif isinstance(transformation, ONNXOutputInsertionCommand):
                 output_insert_transformations.append(transformation)
-            elif isinstance(transformation, ONNXBiasCorrectionCommand):
-                bias_correction_transformations.append(transformation)
             elif isinstance(transformation, ONNXModelExtractionCommand):
                 model_extraction_transformation = transformation
             elif isinstance(transformation, ONNXQDQNodeRemovingCommand):
                 qdq_node_removing_transformations.append(transformation)
+            elif isinstance(transformation, ONNXInitializerUpdateCommand):
+                initializer_update_transformations.append(transformation)
         # Inplace transformations, using deepcopy of model
-        if quantizer_insert_transformations or bias_correction_transformations or qdq_node_removing_transformations:
+        if quantizer_insert_transformations or initializer_update_transformations or qdq_node_removing_transformations:
             model = deepcopy(self._model)
             if quantizer_insert_transformations:
                 model = self._apply_quantizer_insertion_transformations(model, quantizer_insert_transformations)
-            if bias_correction_transformations:
-                model = self._apply_bias_correction_transformations(model, bias_correction_transformations)
             if qdq_node_removing_transformations:
                 model = self._apply_qdq_node_removing_transformations(model, qdq_node_removing_transformations)
+            if initializer_update_transformations:
+                model = self._apply_initializer_update_transformations(model, initializer_update_transformations)
         # Transformations that create new model
         if output_insert_transformations:
             model = self._apply_output_insertion_transformations(output_insert_transformations)
         if model_extraction_transformation:
             model = self._apply_model_extraction_transformation(model_extraction_transformation)
         return model
 
@@ -268,15 +269,15 @@
         onnx_scale = [scale.tolist()] if not per_channel else scale
         onnx_zero_point = [zero_point.tolist()] if not per_channel else zero_point
         if tensor_type == np.uint8:
             onnx_tensor_type = onnx.TensorProto.UINT8
         elif tensor_type == np.int8:
             onnx_tensor_type = onnx.TensorProto.INT8
         else:
-            raise RuntimeError(f"Incorrect tensor type - {tensor_type}.")
+            raise nncf.ValidationError(f"Incorrect tensor type - {tensor_type}.")
         assert quantizer.input[1] == dequantizer.input[1] and quantizer.input[2] == dequantizer.input[2]
         scale_tensor_name = quantizer.input[1]
         zero_point_tensor_name = quantizer.input[2]
         onnx_scale_tensor = onnx.helper.make_tensor(scale_tensor_name, onnx.TensorProto.FLOAT, dims, onnx_scale)
         onnx_zero_point_tensor = onnx.helper.make_tensor(
             zero_point_tensor_name, onnx_tensor_type, dims, onnx_zero_point
         )
@@ -322,15 +323,15 @@
             transformation, quantizer, dequantizer
         )
 
         # If several nodes on one edge
         input_nodes = []
         input_nodes.extend(children_node_mapping[target_edge_name])
         if not input_nodes:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"Can not add the quantizer to the {target_edge_name} edge. This edge does not have end node."
             )
 
         if transformation.target_point.type == TargetType.PRE_LAYER_OPERATION:
             # If we need to change only target nodes input
             target_node = node_mapping[transformation.target_point.target_node_name]
             for i, inp in enumerate(target_node.input):
@@ -351,53 +352,51 @@
         model.graph.initializer.extend([onnx_scale_tensor, onnx_zero_point_tensor])
         model.graph.value_info.extend([onnx_scale_value_info, onnx_zero_point_info])
         insert_index = get_node_index(model, input_nodes[0].name)
         model.graph.node.insert(insert_index, quantizer)
         model.graph.node.insert(insert_index + 1, dequantizer)
         return model
 
-    def _apply_bias_correction_transformations(
-        self, model: onnx.ModelProto, transformations: List[ONNXBiasCorrectionCommand]
+    def _apply_initializer_update_transformations(
+        self, model: onnx.ModelProto, transformations: List[ONNXInitializerUpdateCommand]
     ) -> onnx.ModelProto:
         """
         Creates a copy of original model and applies bias correction transformations on the model.
 
         :param model: Model to apply transformations.
         :param transformations: Bias correction transformations.
         :return: Copy of original model with updated biases.
         """
-        node_mapping = get_name_to_node_map(model)
+        name_to_node_map = get_name_to_node_map(model)
         for transformation in transformations:
-            bias_tensor_position = transformation.target_point.port_id
-            node_name = transformation.target_point.target_node_name
-            onnx_node = node_mapping[node_name]
-            bias_initializer_name = onnx_node.input[bias_tensor_position]
-            bias_initializer = get_tensor(model, bias_initializer_name)
+            node = name_to_node_map[transformation.target_point.target_node_name]
+            initializer_name = node.input[transformation.target_point.port_id]
+            initializer = get_tensor(model, initializer_name)
 
-            new_bias_tensor = onnx.numpy_helper.from_array(transformation.bias_value, bias_initializer_name)
-            bias_initializer.CopyFrom(new_bias_tensor)
+            new_tensor = onnx.numpy_helper.from_array(transformation.new_value, initializer_name)
+            initializer.CopyFrom(new_tensor)
         return model
 
     def _apply_model_extraction_transformation(self, transformation: ONNXModelExtractionCommand) -> onnx.ModelProto:
         """
         Returns a new model that is a sub-model from the original between provided inputs and outputs.
 
         :param transformation: Model extraction transformation.
         :return: Extracted sub-model.
         """
         input_tensor_names = []
         node_mapping = get_name_to_node_map(self._model)
-        for input_node_name in transformation.inputs:
-            input_onnx_node = node_mapping[input_node_name]
-            input_tensor_names.append(input_onnx_node.input[0])
+        for input_name, input_port_id in transformation.input_ids:
+            input_onnx_node = node_mapping[input_name]
+            input_tensor_names.append(input_onnx_node.input[input_port_id])
 
         output_tensor_names = []
-        for output_node_name in transformation.outputs:
-            output_onnx_node = node_mapping[output_node_name]
-            output_tensor_names.append(output_onnx_node.output[0])
+        for output_name, output_port_id in transformation.output_ids:
+            output_onnx_node = node_mapping[output_name]
+            output_tensor_names.append(output_onnx_node.output[output_port_id])
 
         if not output_tensor_names:
             output_tensor_names = [n.name for n in self._model.graph.output]
 
         return self.onnx_model_extractor.extract_model(input_tensor_names, output_tensor_names)
 
     def _apply_qdq_node_removing_transformations(
@@ -406,28 +405,53 @@
         """
         Returns a copy of original model with removed nodes.
 
         :param model: Model to apply transformations.
         :param transformations: Nodes removing transformations.
         :return: Model with removed nodes.
         """
-        for transformation in transformations:
-            node_mapping = get_name_to_node_map(model)
-            children_node_mapping = get_children_node_mapping(model)
-            node = node_mapping[transformation.target_point.target_node_name]
-
-            node_children = get_children(node, children_node_mapping)
-            for node_child in node_children:
-                for input_id, input_obj in enumerate(node_child.input):
-                    if input_obj == node.output[0]:
-                        node_child.input[input_id] = node.input[0]
-
-            initializers = {i.name: i for i in model.graph.initializer}
-            value_infos = {i.name: i for i in model.graph.value_info}
-            for initializer_name in node.input:
-                if initializer_name in initializers:
-                    model.graph.initializer.remove(initializers[initializer_name])
-                if initializer_name in value_infos:
-                    model.graph.value_info.remove(value_infos[initializer_name])
+        name_to_node_map = get_name_to_node_map(model)
+        children_node_mapping = get_children_node_mapping(model)
+        # We combine quantize and dequantize nodes into pairs here because it
+        # does not make sense to remove only the quantize node or the dequantize
+        # node. They should be removed together.
+        was_processed = {t.target_point.target_node_name: False for t in transformations}
+        quantize_dequantize_pairs = []
+        for node_name in was_processed:
+            if was_processed[node_name]:
+                continue
+            quantize_node_proto = name_to_node_map[node_name]
+            if quantize_node_proto.op_type != "QuantizeLinear":
+                continue
+            # `quantize_node_proto` has only one child, which is the dequantize node.
+            dequantize_node_proto = next(iter(get_children(quantize_node_proto, children_node_mapping)))
+            assert dequantize_node_proto.op_type == "DequantizeLinear"
+
+            quantize_dequantize_pairs.append((quantize_node_proto, dequantize_node_proto))
+            was_processed[quantize_node_proto.name] = True
+            was_processed[dequantize_node_proto.name] = True
+
+        if not all(was_processed.values()):
+            raise RuntimeError("Invalid transformation commands.")
+
+        initializers = {i.name: i for i in model.graph.initializer}
+        value_infos = {i.name: i for i in model.graph.value_info}
+
+        for quantize_node_proto, dequantize_node_proto in quantize_dequantize_pairs:
+            # Unlink Q-DQ subgraph from graph
+            children = get_children(dequantize_node_proto, children_node_mapping)
+            for child in children:
+                for port_id, input_name in enumerate(child.input):
+                    if input_name == dequantize_node_proto.output[0]:
+                        child.input[port_id] = quantize_node_proto.input[0]
+            # QuantizeLinear and DequantizeLinear nodes have common initializers in ports 1 and 2.
+            for i in [1, 2]:
+                model.graph.initializer.remove(initializers[quantize_node_proto.input[i]])
+                model.graph.value_info.remove(value_infos[quantize_node_proto.input[i]])
+
+            for node_proto in [quantize_node_proto, dequantize_node_proto]:
+                model.graph.value_info.remove(value_infos[node_proto.output[0]])
+
+            model.graph.node.remove(quantize_node_proto)
+            model.graph.node.remove(dequantize_node_proto)
 
-            model.graph.node.remove(node)
         return model
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/model_utils.py` & `nncf-2.9.0/nncf/onnx/graph/model_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/nncf_graph_builder.py` & `nncf-2.9.0/nncf/onnx/graph/nncf_graph_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from collections import Counter
 from typing import Any, Dict, List, Optional, Set
 
 import onnx
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import Dtype
 from nncf.common.graph.operator_metatypes import InputNoopMetatype
@@ -221,15 +222,15 @@
         for i, node in enumerate(model.graph.node):
             if node.name == "":
                 node.name = node.op_type + "_nncf_" + str(i)
 
         name_counter = Counter([node.name for node in model.graph.node])
 
         if max(name_counter.values()) > 1:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 f"Nodes {[(name, cnt) for name, cnt in name_counter.items() if cnt > 1]} "
                 "(name, counts) occurred more than once. "
                 "NNCF expects every node to have a unique name."
             )
 
         return model
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/node_utils.py` & `nncf-2.9.0/nncf/onnx/graph/node_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -98,15 +98,15 @@
     :param node: NNCFNode.
     :param nncf_graph: NNCGraph.
     :return: True if any weight port id of node is quantized,
     False - if all weights are not quantized or the node can not have weight.
     """
     is_quantized_weight = False
     if node.layer_attributes.has_weight():
-        for port_id in node.layer_attributes.weight_attrs.keys():
+        for port_id in node.layer_attributes.weight_attrs:
             is_quantized_weight = is_quantized_weight or is_port_quantized(node, nncf_graph, port_id)
     return is_quantized_weight
 
 
 def is_port_quantized(node: NNCFNode, nncf_graph: NNCFGraph, port_id: int) -> bool:
     """
     Returns True if a port_id is quantized - have ONNXDequantizeLinearMetatype as a parent node.
@@ -155,24 +155,23 @@
     Returns weight tensor axis, along which quantizer parameters are calculated.
 
     :param node: NNCFNode, which has a weight on input port_id.
     :param port_id: Input port id on which there is a weight of a node.
     :return: Axis, along which quantizer parameters are calculated.
     """
     weight_channel_axis = node.metatype.weight_channel_axis
-    if node.layer_attributes.has_node_attrs():
-        if node.metatype == om.ONNXGemmMetatype:
-            weight_shape = node.layer_attributes.weight_attrs[port_id]["shape"]
-            if (
-                port_id == 0
-                and node.layer_attributes.node_attrs["transA"] == 1
-                or port_id == 1
-                and node.layer_attributes.node_attrs["transB"] == 1
-            ):
-                weight_channel_axis = transpose_axis(weight_shape, weight_channel_axis)
+    if node.layer_attributes.has_node_attrs() and node.metatype == om.ONNXGemmMetatype:
+        weight_shape = node.layer_attributes.weight_attrs[port_id]["shape"]
+        if (
+            port_id == 0
+            and node.layer_attributes.node_attrs["transA"] == 1
+            or port_id == 1
+            and node.layer_attributes.node_attrs["transB"] == 1
+        ):
+            weight_channel_axis = transpose_axis(weight_shape, weight_channel_axis)
     return weight_channel_axis
 
 
 def _get_activation_quantization_axis() -> int:
     """
     Returns activation tensor axis, along which quantizer parameters are calculated.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/onnx_helper.py` & `nncf-2.9.0/nncf/onnx/graph/onnx_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,16 @@
 from collections import defaultdict
 from typing import Dict, Iterator, List, Optional, Union
 
 import numpy as np
 import onnx
 from onnx import numpy_helper
 
+import nncf
+
 
 def get_name_to_node_map(model: onnx.ModelProto) -> Dict[str, onnx.NodeProto]:
     """
     Returns mapping from node name to the node.
 
     :param model: Model from mapping is built.
     :return: Mapping.
@@ -91,29 +93,29 @@
     :param input_name: Name of the ONNX model Input.
     :param to_node: Node, which has input edge with 'input_name' name.
     :return: input port number for 'to_node', which is connected to 'input_name'.
     """
     for input_port_id, port in enumerate(to_node.input):
         if port == input_name:
             return input_port_id
-    raise RuntimeError(f"The node {to_node} does not have input edge with the name {input_name}")
+    raise nncf.ValidationError(f"The node {to_node} does not have input edge with the name {input_name}")
 
 
 def get_output_port_id_for_node_before_output(output_name: str, from_node: onnx.NodeProto) -> int:
     """
     Returns output_port_id for 'from_node' connected with the model output with the name 'output_name'.
 
     :param output_name: Name of the ONNX model Output.
     :param from_node: Node, which has output edge with 'output_name' name.
     :return: output port number for 'from_node', which is connected to 'output_name'.
     """
     for output_port_id, port in enumerate(from_node.output):
         if port == output_name:
             return output_port_id
-    raise RuntimeError(f"The node {from_node} does not have output edge with the name {output_name}")
+    raise nncf.ValidationError(f"The node {from_node} does not have output edge with the name {output_name}")
 
 
 def get_port_ids_between_nodes(from_node: onnx.NodeProto, to_node: onnx.NodeProto) -> Dict[str, int]:
     """
     Returns input_port_id and output_port_id between 'from_node' and 'to_node'.
 
     :param from_node: Node, whose output is connected to 'to_node' node.
@@ -124,15 +126,15 @@
     for port_id, port in enumerate(to_node.input):
         if port in from_node.output:
             output["input_port_id"] = port_id
     for port_id, port in enumerate(from_node.output):
         if port in to_node.input:
             output["output_port_id"] = port_id
     if output["output_port_id"] is None or output["input_port_id"] is None:
-        raise RuntimeError(f"The nodes {from_node.name} and {to_node.name} do not have edges between.")
+        raise nncf.InternalError(f"The nodes {from_node.name} and {to_node.name} do not have edges between.")
     return output
 
 
 def get_node_index(model: onnx.ModelProto, node_name: str) -> Optional[int]:
     """
     Returns the node index in the model.
 
@@ -183,15 +185,15 @@
     :param model: ONNX model.
     :param tensor_name: Name of the tensor.
     :return: The Initializer.
     """
     for tensor in _get_all_tensors(model):
         if tensor.name == tensor_name:
             return tensor
-    raise RuntimeError("There is no tensor with the name {}".format(tensor_name))
+    raise nncf.ValidationError("There is no tensor with the name {}".format(tensor_name))
 
 
 def get_tensor_value(model: onnx.ModelProto, tensor_name: str) -> np.ndarray:
     """
     Returns tensor value of a tensor with the name 'tensor_name'.
 
     :param model: ONNX model.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/onnx/hardware/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/graph/transformations/commands.py` & `nncf-2.9.0/nncf/onnx/graph/transformations/commands.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -68,58 +68,48 @@
         self.quantizer_parameters = quantizer_parameters
 
 
 class ONNXOutputInsertionCommand(ONNXInsertionCommand):
     pass
 
 
-class ONNXBiasCorrectionCommand(TransformationCommand):
+class ONNXInitializerUpdateCommand(TransformationCommand):
     """
-    Corrects bias value in the model based on the input value.
+    Update initializer in the value.
     """
 
-    def __init__(self, target_point: ONNXTargetPoint, bias_value: np.ndarray):
+    def __init__(self, target_point: ONNXTargetPoint, new_value: np.ndarray):
         """
-        :param target_point: The TargetPoint instance for the correction that contains layer's information.
-        :param bias_value: The bias shift value (numpy format) that will be added to the original bias value.
+        :param target_point: Target point.
+        :param new_value: New value for initializer.
         """
         super().__init__(TransformationType.CHANGE, target_point)
-        self.bias_value = bias_value
+        self.new_value = new_value
 
 
 class ONNXModelExtractionCommand(Command):
     """
     Extracts sub-graph based on the sub-model input and output names.
     """
 
-    def __init__(self, inputs: List[str], outputs: List[str]):
+    def __init__(self, input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]):
         """
-        :param inputs: List of the input names that denote the sub-graph beginning.
-        :param outputs: List of the output names that denote the sub-graph ending.
+        :param input_ids: List of the input IDs: pairs of node names and correspondent input port ids.
+            Each pair denotes the sub-graph beginning.
+        :param output_ids: List of the output IDs: pairs of node names and correspondent output port ids.
+            Each pair denotes the sub-graph ending.
         """
         super().__init__(TransformationType.EXTRACT)
-        self.inputs = inputs
-        self.outputs = outputs
+        self.input_ids = input_ids
+        self.output_ids = output_ids
 
 
 class ONNXQDQNodeRemovingCommand(TransformationCommand):
     """
     Removes Quantizer or Dequantizer nodes from the model.
     """
 
     def __init__(self, target_point: ONNXTargetPoint):
         """
         :param target_point: The TargetPoint instance for the layer that contains information for removing.
         """
         super().__init__(TransformationType.REMOVE, target_point)
-
-
-class ONNXNullBiasInsertionCommand(TransformationCommand):
-    """
-    Inserts null bias for the corresponding node.
-    """
-
-    def __init__(self, target_point: ONNXTargetPoint):
-        """
-        :param target_point: The TargetPoint instance for the insertion that contains layer's information.
-        """
-        super().__init__(TransformationType.INSERT, target_point)
```

### Comparing `nncf-2.8.1/nncf/onnx/hardware/__init__.py` & `nncf-2.9.0/nncf/onnx/quantization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/hardware/config.py` & `nncf-2.9.0/nncf/onnx/hardware/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/hardware/fused_patterns.py` & `nncf-2.9.0/nncf/onnx/hardware/fused_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/quantization/__init__.py` & `nncf-2.9.0/nncf/onnx/statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/quantization/default_quantization.py` & `nncf-2.9.0/nncf/onnx/quantization/default_quantization.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/quantization/ignored_patterns.py` & `nncf-2.9.0/nncf/onnx/quantization/ignored_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/quantization/quantizer_parameters.py` & `nncf-2.9.0/nncf/onnx/quantization/quantizer_parameters.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -61,14 +61,17 @@
         raise ValueError(
             "ONNX Quantize/Dequantize pairs only support input_high == output_high and input_low == output_low."
         )
 
     level_low, level_high = get_level_low_level_high(tensor_type)
     narrow_range = levels == 2**num_bits - 1
     scale, zero_point = calculate_scale_zero_point(input_low, input_high, level_low, level_high, narrow_range)
+    # ONNX demands parameters to be a scalar or 1-D Tensor.
+    scale = np.squeeze(scale)
+    zero_point = np.squeeze(zero_point)
     return ONNXQuantizerLayerParameters(scale.data, zero_point.data, tensor_type, axis)
 
 
 def get_level_low_level_high(tensor_type: np.dtype) -> Tuple[int, int]:
     """
     Returns the minimum and maximum level for the quantizer.
     In ONNX opset Q/DequantizeLinear-13 uses only two levels: [-128, 127] and [0, 255].
```

### Comparing `nncf-2.8.1/nncf/onnx/statistics/__init__.py` & `nncf-2.9.0/nncf/openvino/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/onnx/statistics/aggregator.py` & `nncf-2.9.0/nncf/onnx/statistics/aggregator.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -16,14 +16,15 @@
 
 from nncf.common.factory import TModel
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.tensor_statistics.aggregator import StatisticsAggregator
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
+from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.onnx.graph.node_utils import get_input_edge
 from nncf.onnx.graph.node_utils import get_input_edges_mapping
 from nncf.onnx.graph.onnx_helper import get_name_to_node_map
 from nncf.onnx.graph.transformations.commands import ONNXOutputInsertionCommand
 from nncf.onnx.tensor import ONNXNNCFTensor
 
 
@@ -33,31 +34,39 @@
         self.node_mapping = get_name_to_node_map(model)
         self._registered_weights = set()
         super().collect_statistics(model, graph)
 
     def _register_statistics(
         self, outputs: Dict[str, ONNXNNCFTensor], statistic_points: StatisticPointsContainer
     ) -> None:
-        for _statistic_points in statistic_points.values():
-            for statistic_point in _statistic_points:
-                target_point = statistic_point.target_point
-                port_id = target_point.port_id
-                if target_point.target_node_name in self.input_edges_mapping:  # Input case
-                    edge_name = get_input_edge(
-                        target_point.target_node_name,
-                        self.input_edges_mapping,
-                        self.node_mapping,
-                    )
-                elif target_point.type == TargetType.POST_LAYER_OPERATION:
-                    node = self.node_mapping[target_point.target_node_name]
-                    edge_name = node.output[port_id]
-                elif target_point.type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
-                    node = self.node_mapping[target_point.target_node_name]
-                    edge_name = node.input[port_id]
-                statistic_point.register_tensor(outputs[edge_name])
+        for _, statistic_point, tensor_collector in statistic_points.get_tensor_collectors():
+            target_point = statistic_point.target_point
+            port_id = target_point.port_id
+
+            if target_point.target_node_name in self.input_edges_mapping:  # Input case
+                edge_name = get_input_edge(
+                    target_point.target_node_name,
+                    self.input_edges_mapping,
+                    self.node_mapping,
+                )
+            elif target_point.type == TargetType.POST_LAYER_OPERATION:
+                node = self.node_mapping[target_point.target_node_name]
+                edge_name = node.output[port_id]
+            elif target_point.type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
+                node = self.node_mapping[target_point.target_node_name]
+                edge_name = node.input[port_id]
+            else:
+                RuntimeError(f"Unsupported target point type for statistic aggregator: {target_point.type}")
+
+            input_info = []
+            for reducer in tensor_collector.reducers:
+                input_info.append((hash(reducer), [edge_name]))
+
+            target_inputs = TensorCollector.get_tensor_collector_inputs(outputs, input_info)
+            tensor_collector.register_inputs(target_inputs)
 
     def _get_transformation_layout_extra_outputs(
         self, statistic_points: StatisticPointsContainer
     ) -> TransformationLayout:
         transformation_layout = TransformationLayout()
         transformation_commands = []
         for _statistic_points in statistic_points.values():
```

### Comparing `nncf-2.8.1/nncf/onnx/statistics/statistics.py` & `nncf-2.9.0/nncf/openvino/statistics/statistics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,23 +12,23 @@
 import numpy as np
 
 from nncf.common.tensor_statistics.statistics import MeanTensorStatistic
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.tensor_statistics.statistics import RawTensorStatistic
 
 
-class ONNXMinMaxTensorStatistic(MinMaxTensorStatistic):
+class OVMinMaxTensorStatistic(MinMaxTensorStatistic):
     @staticmethod
     def tensor_eq(tensor1: np.ndarray, tensor2: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.allclose(tensor1, tensor2, rtol=rtol))
 
 
-class ONNXMeanTensorStatistic(MeanTensorStatistic):
+class OVMeanTensorStatistic(MeanTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
 
 
-class ONNXRawTensorStatistic(RawTensorStatistic):
+class OVRawTensorStatistic(RawTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
```

### Comparing `nncf-2.8.1/nncf/onnx/tensor.py` & `nncf-2.9.0/nncf/openvino/tensor.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,27 +1,31 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 
 from nncf.common.tensor import NNCFTensor
+from nncf.parameters import TargetDevice
 
 
-class ONNXNNCFTensor(NNCFTensor):
+class OVNNCFTensor(NNCFTensor):
     """
-    A realisation of ONNX tensors wrapper for common NNCF algorithms.
+    A realisation of OpenVINO tensor wrapper for common NNCF algorithms.
     """
 
     def __init__(self, tensor: np.ndarray):
         super().__init__(tensor)
 
     @property
     def device(self):
-        return "CPU"
+        return TargetDevice.CPU.value
+
+    def is_empty(self) -> bool:
+        return self.tensor.size == 0
```

### Comparing `nncf-2.8.1/nncf/openvino/__init__.py` & `nncf-2.9.0/nncf/openvino/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/engine.py` & `nncf-2.9.0/nncf/openvino/engine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from typing import Dict, List, Tuple, Union
 
 import numpy as np
 import openvino.runtime as ov
 
+import nncf
 from nncf.common.engine import Engine
 from nncf.openvino.graph.model_utils import model_has_state
 from nncf.parameters import TargetDevice
 
 
 class OVCompiledModelEngine(Engine):
     """
@@ -44,19 +45,21 @@
         If there is a mismatch, the method throws a more specific and readable error than
         original error raised by the compiled model.
 
         :param input_data: Provided inputs to infer the model.
         """
         actual_num_inputs = 1 if isinstance(input_data, np.ndarray) else len(input_data)
         if actual_num_inputs != self.number_of_inputs:
-            raise RuntimeError(f"Model expects {self.number_of_inputs} inputs, but {actual_num_inputs} are provided.")
+            raise nncf.ValidationError(
+                f"Model expects {self.number_of_inputs} inputs, but {actual_num_inputs} are provided."
+            )
         if isinstance(input_data, dict):
             for name in input_data:
                 if isinstance(name, str) and name not in self.input_tensor_names:
-                    raise RuntimeError(f"Missing a required input: {name} to run the model.")
+                    raise nncf.ValidationError(f"Missing a required input: {name} to run the model.")
 
     def infer(
         self, input_data: Union[np.ndarray, List[np.ndarray], Tuple[np.ndarray], Dict[str, np.ndarray]]
     ) -> Dict[str, np.ndarray]:
         """
         Runs model on the provided input via OpenVINO Runtime.
         Returns the dictionary of model outputs by node names.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/__init__.py` & `nncf-2.9.0/nncf/openvino/graph/metatypes/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/layer_attributes.py` & `nncf-2.9.0/nncf/openvino/graph/layer_attributes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/layout.py` & `nncf-2.9.0/nncf/openvino/graph/layout.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -34,19 +34,19 @@
     C_IN = "channels_in"
     C_OUT = "channels_out"
     SPATIAL = "spatial"
     GROUPS = "groups"
 
 
 _CONV_BASE_CONST_LAYOUT = {
-    OVConvolutionMetatype: [OVLayoutElem.C_OUT, OVLayoutElem.C_IN],
-    OVConvolutionBackpropDataMetatype: [OVLayoutElem.C_IN, OVLayoutElem.C_OUT],
-    OVDepthwiseConvolutionMetatype: [OVLayoutElem.GROUPS, OVLayoutElem.C_OUT, OVLayoutElem.C_IN],
-    OVGroupConvolutionMetatype: [OVLayoutElem.GROUPS, OVLayoutElem.C_OUT, OVLayoutElem.C_IN],
-    OVGroupConvolutionBackpropDataMetatype: [OVLayoutElem.GROUPS, OVLayoutElem.C_IN, OVLayoutElem.C_OUT],
+    OVConvolutionMetatype: (OVLayoutElem.C_OUT, OVLayoutElem.C_IN),
+    OVConvolutionBackpropDataMetatype: (OVLayoutElem.C_IN, OVLayoutElem.C_OUT),
+    OVDepthwiseConvolutionMetatype: (OVLayoutElem.GROUPS, OVLayoutElem.C_OUT, OVLayoutElem.C_IN),
+    OVGroupConvolutionMetatype: (OVLayoutElem.GROUPS, OVLayoutElem.C_OUT, OVLayoutElem.C_IN),
+    OVGroupConvolutionBackpropDataMetatype: (OVLayoutElem.GROUPS, OVLayoutElem.C_IN, OVLayoutElem.C_OUT),
 }
 
 
 def get_conv_weights_layout_from_node(node: NNCFNode) -> List[OVLayoutElem]:
     """
     Calculates weights layout for a target convolution node.
 
@@ -81,17 +81,17 @@
     """
     Calculates weights layout for a target convolution node.
 
     :param ov_metatype: Target convolution node OpenVINO metatype.
     :param weights_shape: Shape of the target convolution node weight.
     :return: Target convolution node weights layout.
     """
-    weights_layout = _CONV_BASE_CONST_LAYOUT[ov_metatype]
-    kernel_size = weights_shape[len(weights_layout) :]
-    weights_layout += [OVLayoutElem.SPATIAL] * len(kernel_size)
+    base_layout = _CONV_BASE_CONST_LAYOUT[ov_metatype]
+    kernel_size = weights_shape[len(base_layout) :]
+    weights_layout = list(base_layout) + [OVLayoutElem.SPATIAL] * len(kernel_size)
     return tuple(weights_layout)
 
 
 def get_linear_weights_layout(weights_shape: Tuple[int, ...], transpose: bool, port_id: int) -> List[OVLayoutElem]:
     """
     Calculates weights layout for a target linear node.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/metatypes/__init__.py` & `nncf-2.9.0/nncf/openvino/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/metatypes/groups.py` & `nncf-2.9.0/nncf/openvino/graph/metatypes/groups.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/metatypes/openvino_metatypes.py` & `nncf-2.9.0/nncf/openvino/graph/metatypes/openvino_metatypes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from collections import deque
 from typing import List, Optional, Type
 
 import openvino.runtime as ov
 
+import nncf
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.graph.operator_metatypes import UnknownMetatype
 from nncf.common.hardware.opset import HWConfigOpName
 
@@ -43,15 +44,15 @@
     @classmethod
     def determine_subtype(cls, node: ov.Node) -> Optional[Type[OperatorMetatype]]:
         matches = []
         for subtype in cls.get_subtypes():
             if subtype.matches(node):
                 matches.append(subtype)
         if len(matches) > 1:
-            raise RuntimeError("Multiple subtypes match operator call - can not determine single subtype.")
+            raise nncf.InternalError("Multiple subtypes match operator call - can not determine single subtype.")
         if not matches:
             return None
         return matches[0]
 
 
 @OV_OPERATOR_METATYPES.register()
 class OVConvolutionMetatype(OVOpMetatype):
@@ -783,13 +784,12 @@
     Determine NNCF meta type for OpenVINO node.
 
     :param node: OpenVINO node.
     :return: NNCF meta type which corresponds to OpenVINO node.
     """
     node_type = node.get_type_name()
     metatype = OV_OPERATOR_METATYPES.get_operator_metatype_by_op_name(node_type)
-    if metatype is not UnknownMetatype:
-        if metatype.get_subtypes():
-            subtype = metatype.determine_subtype(node)
-            if subtype is not None:
-                metatype = subtype
+    if metatype is not UnknownMetatype and metatype.get_subtypes():
+        subtype = metatype.determine_subtype(node)
+        if subtype is not None:
+            metatype = subtype
     return metatype
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/model_transformer.py` & `nncf-2.9.0/nncf/openvino/graph/model_transformer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,18 +14,21 @@
 from typing import Callable, Dict, List, Tuple
 
 import numpy as np
 import openvino.runtime as ov
 from openvino._pyopenvino import DescriptorTensor
 from openvino.runtime import opset13 as opset
 
+import nncf
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.model_transformer import TModel
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.openvino.graph.model_utils import copy_rt_info
+from nncf.openvino.graph.node_utils import get_parameter_node_name
 from nncf.openvino.graph.node_utils import get_result_node_name
 from nncf.openvino.graph.transformations.commands import OVBiasCorrectionCommand
 from nncf.openvino.graph.transformations.commands import OVBiasInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVConvertInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVExtractIfBodyCommand
 from nncf.openvino.graph.transformations.commands import OVFQNodeRemovingCommand
 from nncf.openvino.graph.transformations.commands import OVInplaceFnInsertionCommand
@@ -40,16 +43,17 @@
 
 
 class OVModelTransformer(ModelTransformer):
     """
     Applies transformations to an OpenVINO model.
     """
 
-    def __init__(self, model: TModel):
+    def __init__(self, model: TModel, inplace: bool = False):
         super().__init__(model)
+        self._inplace = inplace
         self._command_transformation_ordered_pairs = [
             (OVFQNodeRemovingCommand, self._apply_fq_nodes_removing_transformation),
             (OVQuantizerInsertionCommand, self._apply_quantizer_insertion_transformations),
             (OVConvertInsertionCommand, self._apply_convert_insertion_transformations),
             (OVBiasCorrectionCommand, self._apply_bias_correction_transformations),
             (OVWeightUpdateCommand, self._apply_weight_update_transformations),
             (OVModelExtractionCommand, self._apply_model_extraction_transformation),
@@ -120,15 +124,18 @@
         """
 
         transformations = transformation_layout.transformations
         aggregated_transformations = defaultdict(list)
         for transformation in transformations:
             aggregated_transformations[transformation.__class__].append(transformation)
 
-        model = self._model.clone()
+        if self._inplace:
+            model = self._model
+        else:
+            model = self._model.clone()
         # Inplace transformations; Using deepcopy of model
         for transformation_cls, transformation_fn in self._command_transformation_ordered_pairs:
             transformations = aggregated_transformations[transformation_cls]
             if transformations:
                 model = transformation_fn(model, transformations)
 
         return model
@@ -196,17 +203,19 @@
             output_name = output.get_node().get_friendly_name()
             # TODO: (KodiaqQ) check out the models with the Split
             result_name = get_result_node_name(output_name, port_id)
             result = opset.result(output, name=result_name)
             OVModelTransformer._update_tensor_name([result.get_output_tensor(0)], result_name)
             extra_model_outputs.append(result)
 
-        return ov.Model(
+        model_with_outputs = ov.Model(
             results=results + extra_model_outputs, sinks=assign_ops, parameters=params, name=model.friendly_name
         )
+        copy_rt_info(model, model_with_outputs, path=["nncf"])
+        return model_with_outputs
 
     @staticmethod
     def _apply_fq_nodes_removing_transformation(
         model: ov.Model, transformations: List[OVFQNodeRemovingCommand]
     ) -> ov.Model:
         """
         Removes the layers from the model.
@@ -403,15 +412,15 @@
                 fake_quantize_params=fq_params,
                 fake_quantize_name=fq_name,
                 convert_to_fp16=convert_to_fp16,
             )
             for inp_node in target_inputs:
                 inp_node.replace_source_output(fq.output(0))
         else:
-            raise RuntimeError(f"Incorrect target point type {transform_type}")
+            raise nncf.InternalError(f"Incorrect target point type {transform_type}")
 
     @staticmethod
     def _insert_fake_convert_op(
         transformation: OVConvertInsertionCommand, name_to_node_mapping: Dict[str, ov.Node]
     ) -> None:
         """
         Inserts FakeConvert Operation to a model which name_to_node_mapping is passed.
@@ -457,15 +466,15 @@
                 fake_convert_params=fc_params,
                 fake_convert_name=fc_name,
                 convert_to_fp16=convert_to_fp16,
             )
             for inp_node in target_inputs:
                 inp_node.replace_source_output(fc.output(0))
         else:
-            raise RuntimeError(f"Incorrect target point type {transform_type}")
+            raise nncf.InternalError(f"Incorrect target point type {transform_type}")
 
     @staticmethod
     def _apply_bias_correction_transformations(model, transformations: List[OVBiasCorrectionCommand]) -> ov.Model:
         """
         Applies bias correction transformations on the model.
 
         :param model: Model to apply transformations.
@@ -502,15 +511,15 @@
                 const_node = curr_node
                 break
             if len(curr_node.inputs()) == 0:
                 break
             queue.append((curr_node.input(0), curr_node.input_value(0).get_node()))
 
         if const_node is None:
-            raise RuntimeError("Constant node was expected but could not find it.")
+            raise nncf.InternalError("Constant node was expected but could not find it.")
 
         const_shape = const_node.data.shape
         const_dtype = const_node.data.dtype
         const_value = np.reshape(const_value, const_shape).astype(const_dtype)
 
         # TODO(andrey-churkin): Replace on opset13.constant() in 2023.3 release
         new_const_node = ov.op.Constant(const_value, shared_memory=True)
@@ -542,48 +551,50 @@
 
         :param model: Model to apply transformations.
         :param transformation: Model extraction transformation.
         :return: Extracted sub-model.
         """
         transformation = transformations[-1]
         name_to_node_mapping = OVModelTransformer._get_name_to_node_mapping(model)
-        activation_node_names = OVModelTransformer._get_activation_node_names(model)
+
         params, results = [], []
-        for input_name in transformation.inputs:
+        for input_name, input_port_id in transformation.input_ids:
             input_node = name_to_node_mapping[input_name]
             if input_name in [tensor.node.get_friendly_name() for tensor in model.inputs]:
                 params.append(input_node)
                 continue
-            for input_port in input_node.inputs():
-                if input_port.get_source_output().get_node().name not in activation_node_names:
-                    continue
-                input_node_output = input_port.get_source_output()
-                parameter_name = f"Parameter_{input_name}"
-                new_param = opset.parameter(
-                    shape=input_node_output.partial_shape,
-                    dtype=input_node_output.get_element_type(),
-                    name=parameter_name,
-                )
-                input_port.replace_source_output(new_param.output(0))
-                new_param_tensors = [o.get_tensor() for o in new_param.outputs()]
-                OVModelTransformer._update_tensor_name(new_param_tensors, parameter_name)
-                params.append(new_param)
 
-        for output_name in transformation.outputs:
+            input_port = input_node.input(input_port_id)
+            input_node_output = input_port.get_source_output()
+            parameter_name = get_parameter_node_name(input_name, input_port_id)
+            new_param = opset.parameter(
+                shape=input_node_output.partial_shape,
+                dtype=input_node_output.get_element_type(),
+                name=parameter_name,
+            )
+            input_port.replace_source_output(new_param.output(0))
+            new_param_tensors = [o.get_tensor() for o in new_param.outputs()]
+            OVModelTransformer._update_tensor_name(new_param_tensors, parameter_name)
+            params.append(new_param)
+
+        for output_name, output_port_id in transformation.output_ids:
             output_node = name_to_node_mapping[output_name]
-            for node_out in output_node.outputs():
-                result_name = get_result_node_name(output_name, 0)
-                new_result = opset.result(node_out, name=result_name)
-                OVModelTransformer._update_tensor_name([new_result.get_output_tensor(0)], result_name)
-                results.append(new_result)
+
+            output_port = output_node.output(output_port_id)
+            result_name = get_result_node_name(output_name, output_port_id)
+            new_result = opset.result(output_port, name=result_name)
+            OVModelTransformer._update_tensor_name([new_result.get_output_tensor(0)], result_name)
+            results.append(new_result)
 
         if not results:
             results = model.get_results()
 
-        return ov.Model(results, params)
+        extracted_model = ov.Model(results, params)
+        copy_rt_info(model, extracted_model, path=["nncf"])
+        return extracted_model
 
     @staticmethod
     def _apply_insert_operation(model: ov.Model, transformations: OVInplaceFnInsertionCommand) -> ov.Model:
         """
         Applies inplace fn insertion transformation to the model.
 
         :param transformations: lisf of the OVInplaceFnInsertionCommand.
@@ -609,21 +620,23 @@
         transform_type = transformation.target_point.type
 
         node_name = transformation.target_point.target_node_name
         target_node = name_to_node_mapping[node_name]
         port_id = transformation.target_point.port_id
         fn_output_port_id = transformation.fn_output_port_id
         if transform_type == TargetType.POST_LAYER_OPERATION:
-            new_node = transformation.inplace_op_fn(target_node, port_id)
+            new_node = transformation.inplace_op_fn(target_node, port_id, transformation.last_inplace_node_name)
             return (new_node.output(fn_output_port_id), fn_output_port_id)
         if transform_type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
             output = target_node.input_value(port_id)
-            new_node = transformation.inplace_op_fn(output.get_node(), output.get_index())
+            new_node = transformation.inplace_op_fn(
+                output.get_node(), output.get_index(), transformation.last_inplace_node_name
+            )
             return (new_node.output(fn_output_port_id), fn_output_port_id)
-        raise RuntimeError(f"Transform type {transform_type} is not supported")
+        raise nncf.InternalError(f"Transform type {transform_type} is not supported")
 
     @staticmethod
     def _apply_bias_insertion_transformations(
         model: ov.Model, transformations: List[OVBiasInsertionCommand]
     ) -> ov.Model:
         """
         Inserts bias operation after corresponding layer.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/nncf_graph_builder.py` & `nncf-2.9.0/nncf/openvino/graph/nncf_graph_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/node_utils.py` & `nncf-2.9.0/nncf/openvino/graph/node_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type
 
 import numpy as np
 import openvino.runtime as ov
 import openvino.runtime.opset13 as opset
 
+import nncf
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.graph.layer_attributes import WeightedLayerAttributes
 from nncf.common.tensor_statistics.collectors import ReductionAxes
@@ -151,143 +152,140 @@
 
 def get_result_node_name(output_name: str, port_id: int) -> str:
     """
     Returns name of Result based on node name and its port.
 
     :param output_name: Node name.
     :param port_id: Node port.
-    :return: Name of result.
+    :return: Name of Result.
     """
 
     return f"Result_{output_name}.{port_id}"
 
 
+def get_parameter_node_name(parameter_name: str, port_id: int) -> str:
+    """
+    Returns name of Parameter based on node name and its port.
+
+    :param parameter_name: Node name.
+    :param port_id: Node port.
+    :return: Name of Parameter.
+    """
+
+    return f"Parameter_{parameter_name}.{port_id}"
+
+
 def get_ov_model_reduce_node_name(output_name: str, reduce_node_name: str, port_id: int) -> str:
     """
     Returns name of reduce node based on output name, node type and port id.
 
     :param output_name: Target node name.
     :param node_type: Reduce node name.
     :param port_id: Target port id of the target node.
     :return: Reduce node name.
     """
     return f"{output_name}_{reduce_node_name}.{port_id}"
 
 
 def get_inplace_reduce_op(
-    op: Type[ov.Node], reduce_node_name: str, reduction_axes: Optional[ReductionAxes], use_abs: bool
+    op: Type[ov.Node], reduction_axes: Optional[ReductionAxes], use_abs: bool
 ) -> InplaceInsertionFnType:
     """
     Returns inplace insertion function that adds reduce node to a passed node.
 
     :param op: OpenVINO reduction operation type to insert.
-    :param reduce_node_name: Reduce node name.
     :param reduction_axes: Target reduction axes for the reduction node.
         Reduce along all axes in case reduction_axes are None.
     :param use_abs: Wheather reduce absolute values of input tensors or not.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
 
-    def get_reduce_op(node: ov.Node, output_port_id: int) -> ov.Node:
-        output_name = node.get_friendly_name()
+    def get_reduce_op(node: ov.Node, output_port_id: int, output_node_name: str) -> ov.Node:
         reduction_axes_ = reduction_axes
-        name_output_port_id = output_port_id
         if reduction_axes_ is None:
             partial_shape = get_partial_shape_safe(node, output_port_id)
             reduction_axes_ = np.arange(partial_shape.rank.get_length()).astype(np.int64)
 
         if use_abs:
-            op_input = opset.abs(
-                node.output(output_port_id),
-                name=get_ov_model_reduce_node_name(output_name, "abs" + reduce_node_name, name_output_port_id),
-            )
+            op_input = opset.abs(node.output(output_port_id), name="abs_" + output_node_name)
             output_port_id = 0
         else:
             op_input = node
 
         return op(
             op_input.output(output_port_id),
             reduction_axes=np.array(reduction_axes_, dtype=np.int64),
             keep_dims=True,
-            name=get_ov_model_reduce_node_name(output_name, reduce_node_name, name_output_port_id),
+            name=output_node_name,
         )
 
     return get_reduce_op
 
 
-def get_inplace_min_op(node_name: str, reduction_axes: Optional[ReductionAxes]) -> InplaceInsertionFnType:
+def get_inplace_min_op(reduction_axes: Optional[ReductionAxes]) -> InplaceInsertionFnType:
     """
     Returns inplace min function that adds reduce min node to a passed node.
 
-    :param node_name: Min reduce node name.
     :param reduction_axes: Target reduction axes for the reduction node.
         Reduce along all axes in case reduction_axes are None.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
-    return get_inplace_reduce_op(opset.reduce_min, node_name, reduction_axes, False)
+    return get_inplace_reduce_op(opset.reduce_min, reduction_axes, False)
 
 
-def get_inplace_max_op(
-    node_name: str, reduction_axes: Optional[ReductionAxes], use_abs_max: bool
-) -> InplaceInsertionFnType:
+def get_inplace_max_op(reduction_axes: Optional[ReductionAxes], use_abs_max: bool) -> InplaceInsertionFnType:
     """
     Returns inplace max function that adds reduce max node to a passed node.
 
-    :param node_name: Max reduce node name.
     :param reduction_axes: Target reduction axes for the reduction node.
         Reduce along all axes in case reduction_axes are None.
     :param use_abs: Wheather reduce absolute values of input tensors or not.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
-    return get_inplace_reduce_op(opset.reduce_max, node_name, reduction_axes, use_abs_max)
+    return get_inplace_reduce_op(opset.reduce_max, reduction_axes, use_abs_max)
 
 
-def get_inplace_mean_op(node_name: str, reduction_axes: Optional[ReductionAxes]) -> InplaceInsertionFnType:
+def get_inplace_mean_op(reduction_axes: Optional[ReductionAxes]) -> InplaceInsertionFnType:
     """
     Returns inplace mean function that adds reduce mean node to a passed node.
 
-    :param node_name: Mean reduce node name.
     :param reduction_axes: Target reduction axes for the reduction node.
         Reduce along all axes in case reduction_axes are None.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
-    return get_inplace_reduce_op(opset.reduce_mean, node_name, reduction_axes, False)
+    return get_inplace_reduce_op(opset.reduce_mean, reduction_axes, False)
 
 
-def get_inplace_batch_mean_op(node_name: str) -> InplaceInsertionFnType:
+def get_inplace_batch_mean_op() -> InplaceInsertionFnType:
     """
     Returns inplace batch mean function that adds reduce batch mean node to a passed node.
 
-    :param node_name: Last node of batch mean subgraph name.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
-    return get_inplace_reduce_op(opset.reduce_mean, node_name, np.array(0), False)
+    return get_inplace_reduce_op(opset.reduce_mean, np.array(0), False)
 
 
-def get_inplace_mean_per_ch(op_type: str, axis: int) -> InplaceInsertionFnType:
+def get_inplace_mean_per_ch(axis: int) -> InplaceInsertionFnType:
     """
     Returns inplace mean per channel function that adds reduce mean per channel node
     to a passed node.
 
-    :param node_name: Last node of mean per channel subgraph name.
     :param axis: Channel axis.
     :returns: Inplace insertion function to use in ModelTransformer.
     """
 
-    def get_reduce_op(node: ov.Node, output_port_id: int) -> ov.Node:
-        output_name = node.get_friendly_name()
+    def get_reduce_op(node: ov.Node, output_port_id: int, output_node_name: str) -> ov.Node:
         input_shape = get_partial_shape_safe(node, output_port_id)
         input_shape = [dim.get_length() if dim.is_static else -1 for dim in input_shape]
-        name_output_port_id = output_port_id
         if len(input_shape) < 3:
             return opset.reduce_mean(
                 node.output(output_port_id),
                 reduction_axes=0,
                 keep_dims=False,
-                name=get_ov_model_reduce_node_name(output_name, op_type, name_output_port_id),
+                name=output_node_name,
             )
 
         ch_dim = 1
         if axis != ch_dim:
             transpose_dims = list(range(len(input_shape)))
             transpose_dims[axis], transpose_dims[ch_dim] = transpose_dims[ch_dim], transpose_dims[axis]
             transposed_shape = [input_shape[dim] for dim in transpose_dims]
@@ -295,41 +293,35 @@
             reshape_input_node = opset.transpose(node.output(output_port_id), transpose_dims)
             output_port_id = 0
         else:
             reshape_input_node = node
             transposed_shape = input_shape
 
         keeped_dims = transposed_shape[:2]
+        keeped_dims = [0 if dim < 0 else dim for dim in keeped_dims]
         squized_dims = -1 if -1 in transposed_shape[2:] else np.prod(transposed_shape[2:])
-        if (-1 in keeped_dims and squized_dims == -1) or keeped_dims.count(-1) > 1:
-            raise RuntimeError(
-                f"Could not insert mean_per_ch operation inplace"
-                f" for the node {node} because of"
-                f" input_shape: {input_shape} -> transposed_shape: {transposed_shape}"
-            )
-
         reshape_op = opset.reshape(
             reshape_input_node.output(output_port_id),
             output_shape=np.array((keeped_dims[0], keeped_dims[1], squized_dims)),
-            special_zero=False,
+            special_zero=True,
         )
         return opset.reduce_mean(
             reshape_op,
             reduction_axes=np.array((0, 2)),
             keep_dims=False,
-            name=get_ov_model_reduce_node_name(output_name, op_type, name_output_port_id),
+            name=output_node_name,
         )
 
     return get_reduce_op
 
 
 def get_partial_shape_safe(node, port_id) -> Tuple[int, ...]:
     partial_shape = node.get_output_partial_shape(port_id)
     if partial_shape.rank.is_dynamic or not partial_shape.all_non_negative:
-        raise RuntimeError(
+        raise nncf.ValidationError(
             f"Could not collect statistics for the node {node} because its output shape rank is dynamic or negative"
         )
     return partial_shape
 
 
 def get_reducer_output_node_names(
     node_type, target_node_name: str, port_id: int, fn_output_port_id: int, inplace: bool
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/openvino/hardware/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/transformations/command_creation.py` & `nncf-2.9.0/nncf/openvino/graph/transformations/command_creation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/graph/transformations/commands.py` & `nncf-2.9.0/nncf/openvino/graph/transformations/commands.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import List
+from typing import List, Tuple
 
 import numpy as np
 import openvino.runtime as ov
 
 from nncf.common.graph.transformations.commands import Command
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
@@ -50,18 +50,25 @@
 class OVOutputInsertionCommand(OVInsertionCommand):
     def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
 
 class OVInplaceFnInsertionCommand(OVInsertionCommand):
-    def __init__(self, target_point: OVTargetPoint, inplace_op_fn: InplaceInsertionFnType, fn_output_port_id: int):
+    def __init__(
+        self,
+        target_point: OVTargetPoint,
+        inplace_op_fn: InplaceInsertionFnType,
+        fn_output_port_id: int,
+        last_inplace_node_name: str,
+    ):
         super().__init__(target_point)
         self.inplace_op_fn = inplace_op_fn
         self.fn_output_port_id = fn_output_port_id
+        self.last_inplace_node_name = last_inplace_node_name
 
     def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
 
 class OVFQNodeRemovingCommand(TransformationCommand):
@@ -117,22 +124,24 @@
 
 
 class OVModelExtractionCommand(Command):
     """
     Extracts sub-graph based on the sub-model input and output names.
     """
 
-    def __init__(self, inputs: List[str], outputs: List[str]):
+    def __init__(self, input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]):
         """
-        :param inputs: List of the input names that denote the sub-graph beginning.
-        :param outputs: List of the output names that denote the sub-graph ending.
+        :param input_ids: List of the input IDs: pairs of node names and correspondent input port ids.
+            Each pair denotes the sub-graph beginning.
+        :param output_ids: List of the output IDs: pairs of node names and correspondent output port ids.
+            Each pair denotes the sub-graph ending.
         """
         super().__init__(TransformationType.EXTRACT)
-        self.inputs = inputs
-        self.outputs = outputs
+        self.input_ids = input_ids
+        self.output_ids = output_ids
 
 
 class OVBiasInsertionCommand(TransformationCommand):
     """
     Inserts bias for the corresponding node.
     """
```

### Comparing `nncf-2.8.1/nncf/openvino/hardware/__init__.py` & `nncf-2.9.0/nncf/openvino/quantization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/hardware/config.py` & `nncf-2.9.0/nncf/openvino/hardware/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/hardware/fused_patterns.py` & `nncf-2.9.0/nncf/openvino/hardware/fused_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/pot/__init__.py` & `nncf-2.9.0/nncf/openvino/statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/pot/quantization/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/pot/quantization/quantize_model.py` & `nncf-2.9.0/nncf/quantization/advanced_parameters.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,529 +1,453 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-import logging
-import tempfile
-from pathlib import Path
-from typing import Any, Callable, Dict, Iterable, Optional, Union
-
-import openvino.runtime as ov
-from openvino._offline_transformations import compress_quantize_weights_transformation
-from openvino.tools import pot
-
-from nncf.common.deprecation import warning_deprecated
-from nncf.common.logging import nncf_logger
-from nncf.common.quantization.structs import QuantizationPreset
-from nncf.data import Dataset
-from nncf.openvino.pot.engine import OVEngine
-from nncf.openvino.pot.quantization.accuracy_aware import NMSEBasedAccuracyAware
-from nncf.openvino.pot.telemetry_extractors import POTImplementation
-from nncf.openvino.quantization.backend_parameters import BackendParameters
-from nncf.openvino.quantization.backend_parameters import is_weight_compression_needed
-from nncf.parameters import DropType
-from nncf.parameters import ModelType
-from nncf.parameters import QuantizationMode
-from nncf.parameters import TargetDevice
-from nncf.quantization.advanced_parameters import AdvancedAccuracyRestorerParameters
-from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
-from nncf.quantization.advanced_parameters import OverflowFix
-from nncf.quantization.advanced_parameters import QuantizationParameters
+"""
+Structures and functions for passing advanced parameters to NNCF post-training quantization APIs.
+"""
+import sys
+from dataclasses import dataclass
+from dataclasses import field
+from dataclasses import fields
+from dataclasses import is_dataclass
+from enum import Enum
+from typing import Any, Dict, Optional, Union
+
+import nncf
+from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
+from nncf.common.utils.api_marker import api
+from nncf.quantization.range_estimator import AggregatorType
 from nncf.quantization.range_estimator import RangeEstimatorParameters
-from nncf.quantization.range_estimator import StatisticsCollectorParameters
 from nncf.quantization.range_estimator import StatisticsType
-from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeApi
-from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeWithAccuracyControlApi
-from nncf.scopes import IgnoredScope
-from nncf.telemetry import tracked_function
-from nncf.telemetry.events import NNCF_OV_CATEGORY
-
-
-def _convert_openvino_model_to_compressed_model(
-    model: ov.Model, target_device: str
-) -> pot.graph.nx_model.CompressedModel:
-    """
-    Serializes the provided OpenVINO model and loads the model in the POT representation.
-
-    :param model: The OpenVINO model.
-    :param target_device: The target device.
-    :return: The POT representation of the provided model.
-    """
-    with tempfile.TemporaryDirectory(dir=tempfile.gettempdir()) as tmp_dir:
-        xml_path = str(Path(tmp_dir) / "model.xml")
-        bin_path = str(Path(tmp_dir) / "model.bin")
-        ov.serialize(model, xml_path, bin_path)
-        model_config = {
-            "model_name": "model",
-            "model": xml_path,
-            "weights": bin_path,
-        }
-        pot_model = pot.load_model(model_config, target_device)
 
-    return pot_model
 
+@api()
+class OverflowFix(Enum):
+    """
+    This option controls whether to apply the overflow issue fix for the 8-bit
+    quantization.
+
+    8-bit instructions of older Intel CPU generations (based on SSE, AVX-2, and AVX-512
+    instruction sets) suffer from the so-called saturation (overflow) issue: in some
+    configurations, the output does not fit into an intermediate buffer and has to be
+    clamped. This can lead to an accuracy drop on the aforementioned architectures.
+    The fix set to use only half a quantization range to avoid overflow for specific
+    operations.
+
+    If you are going to infer the quantized model on the architectures with AVX-2, and
+    AVX-512 instruction sets, we recommend using FIRST_LAYER option as lower aggressive
+    fix of the overflow issue. If you still face significant accuracy drop, try using
+    ENABLE, but this may get worse the accuracy.
 
-def _convert_compressed_model_to_openvino_model(model: pot.graph.nx_model.CompressedModel) -> ov.Model:
+    :param ENABLE: All weights of all types of Convolutions and MatMul operations
+        are be quantized using a half of the 8-bit quantization range.
+    :param FIRST_LAYER: Weights of the first Convolutions of each model inputs
+        are quantized using a half of the 8-bit quantization range.
+    :param DISABLE: All weights are quantized using the full 8-bit quantization range.
     """
-    Saves the provided POT compressed model and loads it as `openvino.runtime.Model` object.
 
-    :param model: The POT compressed model.
-    :return: The `openvino.runtime.Model`  object which represents the provided model.
+    ENABLE = "enable"
+    FIRST_LAYER = "first_layer_only"
+    DISABLE = "disable"
+
+
+@api()
+class FP8Type(Enum):
     """
-    with tempfile.TemporaryDirectory(dir=tempfile.gettempdir()) as tmp_dir:
-        paths = pot.save_model(model, save_path=tmp_dir, model_name="model")
-        xml_path = paths[0]["model"]
-        bin_path = paths[0]["weights"]
-        ie = ov.Core()
-        ie.set_property({"ENABLE_MMAP": "NO"})
-        ov_model = ie.read_model(xml_path, bin_path)
-    return ov_model
+    Defines FP8 special types (https://arxiv.org/pdf/2209.05433.pdf).
 
+    :param E4M3: Mode with 4-bit exponent and 3-bit mantissa.
+    :param E5M2: Mode with 5-bit exponent and 2-bit mantissa.
 
-def _create_ignored_scope_config(ignored_scope: Optional[IgnoredScope]) -> Dict[str, Any]:
     """
-    Maps the content of `IgnoredScope` class to the `ignored` section of POT config.
 
-    :param ignored_scope: The ignored scope
-    :return: A POT ignored scope configuration as dict
+    E4M3 = "f8e4m3"
+    E5M2 = "f8e5m2"
+
+
+@api()
+@dataclass
+class QuantizationParameters:
     """
-    if ignored_scope is None:
-        return {}
+    Contains quantization parameters for weights or activations.
 
-    ignored = {}
-    if ignored_scope.names:
-        ignored["scope"] = ignored_scope.names
-    if ignored_scope.patterns:
-        raise RuntimeError(
-            "Quantization algorithm from the OpenVINO backend "
-            "does not support regular expressions in the ignored "
-            "scopes yet"
-        )
-    if ignored_scope.types:
-        ignored["operations"] = [{"type": type} for type in ignored_scope.types]
-    return ignored
-
-
-def _create_statistics_collector_config(statistics_collector_params: StatisticsCollectorParameters) -> Dict[str, Any]:
-    """
-    Creates a statistic collector configuration.
-
-    :param statistics_collector_params: Statistic collector parameters
-    :return: A POT statistic collector configuration as dict.
-    """
-    config = {}
-    if statistics_collector_params.statistics_type is not None:
-        config["type"] = statistics_collector_params.statistics_type.value
-    if statistics_collector_params.aggregator_type is not None:
-        config["aggregator"] = statistics_collector_params.aggregator_type.value
-    if statistics_collector_params.clipping_value is not None:
-        config["clipping_value"] = statistics_collector_params.clipping_value
-    if statistics_collector_params.statistics_type in [StatisticsType.QUANTILE, StatisticsType.ABS_QUANTILE]:
-        config["outlier_prob"] = statistics_collector_params.quantile_outlier_prob
+    :param num_bits: The number of bits to use for quantization.
+    :type num_bits: Optional[int]
+    :param mode: The quantization mode to use, such as 'symmetric', 'asymmetric', etc.
+    :type mode: nncf.common.quantization.structs.QuantizationMode
+    :param signedness_to_force: Whether to force the weights or activations to be
+        signed (True), unsigned (False)
+    :type signedness_to_force: Optional[bool]
+    :param per_channel: True if per-channel quantization is used, and False if
+        per-tensor quantization is used.
+    :type per_channel: Optional[bool]
+    :param narrow_range: Whether to use a narrow quantization range.
 
-    return config
+        If False, then the input will be quantized into quantization range
 
+        * [0; 2^num_bits - 1] for unsigned quantization and
+        * [-2^(num_bits - 1); 2^(num_bits - 1) - 1] for signed quantization
 
-def _create_range_estimator_config(range_estimator_params: RangeEstimatorParameters) -> Dict[str, Any]:
+        If True, then the ranges would be:
+
+        * [0; 2^num_bits - 2] for unsigned quantization and
+        * [-2^(num_bits - 1) + 1; 2^(num_bits - 1) - 1] for signed quantization
+    :type narrow_range: Optional[bool]
     """
-    Creates a range estimator configuration.
 
-    :param range_estimator_params: Range estimator parameters.
-    :return: A POT range estimator configuration as dict.
+    num_bits: Optional[int] = None
+    mode: Optional[QuantizationMode] = None
+    signedness_to_force: Optional[bool] = None
+    per_channel: Optional[bool] = None
+    narrow_range: Optional[bool] = None
+
+
+@api()
+@dataclass
+class FP8QuantizationParameters:
     """
-    config = {}
-    min_config = _create_statistics_collector_config(range_estimator_params.min)
-    if min_config:
-        config["min"] = min_config
+    Contains convert parameters for weights or activations.
 
-    max_config = _create_statistics_collector_config(range_estimator_params.max)
-    if max_config:
-        config["max"] = max_config
+    :param destination_type: Currently contains E4M3 or E5M2 for FP8 precision.
+    :type destination_type: FP8Type
+    """
 
-    return config
+    destination_type: Optional[FP8Type] = None
 
 
-def _create_quantization_group_config(
-    quantization_params: QuantizationParameters,
-    range_estimator_params: RangeEstimatorParameters,
-    backend_params: Dict[str, Any],
-) -> Dict[str, Any]:
+@api()
+@dataclass
+class AdvancedBiasCorrectionParameters:
     """
-    Creates a configuration for a quantization group such as activations or weights.
+    Contains advanced parameters for fine-tuning bias correction algorithm.
 
-    :param quantization_params: Quantization parameters.
-    :param backend_params: Backend specific parameters.
-    :return: A POT quantization group configuration as dict.
-    """
-    config = {}
-    if quantization_params is not None:
-        if quantization_params.num_bits is not None:
-            config["bits"] = quantization_params.num_bits
-
-        if quantization_params.mode is not None:
-            config["mode"] = str(quantization_params.mode)
-        if quantization_params.per_channel is not None:
-            config["perchannel"] = quantization_params.per_channel
-
-        not_supported_params = {
-            "narrow_range": quantization_params.narrow_range,
-            "signedness_to_force": quantization_params.signedness_to_force,
-        }
-        for name, value in not_supported_params.items():
-            if value is not None:
-                raise RuntimeError(
-                    "Quantization algorithm from the OpenVINO backend does not support "
-                    f"{name} directly, please, use backend specific parameters level_low "
-                    "and level_high to specify the quantization levels for activations "
-                    "and weights quantization groups to specify the quantization levels."
-                    'Example:\n {"activations" : {"level_low": 0, "level_high": 255}}\n'
-                    '{"weights" : {"level_low": -127, "level_high": 127}}'
-                )
-    if BackendParameters.LEVEL_LOW in backend_params:
-        config["level_low"] = backend_params[BackendParameters.LEVEL_LOW]
-    if BackendParameters.LEVEL_HIGH in backend_params:
-        config["level_high"] = backend_params[BackendParameters.LEVEL_HIGH]
-    config.update(_create_range_estimator_config(range_estimator_params))
+    :param apply_for_all_nodes: Whether to apply the correction to all nodes in the
+        model, or only to nodes that have a bias.
+    :type apply_for_all_nodes: bool
+    :param threshold: The threshold value determines the maximum bias correction value.
+        The bias correction are skipped If the value is higher than threshold.
+    :type threshold: Optional[float]
+    """
 
-    return config
+    apply_for_all_nodes: bool = False
+    threshold: Optional[float] = None
 
 
-def _create_quantization_config(
-    preset: Union[QuantizationPreset, None],
-    target_device: TargetDevice,
-    subset_size: int,
-    fast_bias_correction: bool,
-    model_type: Union[ModelType, None],
-    ignored_scope: Union[IgnoredScope, None],
-    advanced_parameters: Union[AdvancedQuantizationParameters, None],
-) -> Dict[str, Any]:
+@api()
+@dataclass
+class AdvancedSmoothQuantParameters:
     """
-    Creates a quantization configuration.
+    Contains advanced alpha parameters for SmoothQuant algorithm.
+    It regulates the calculation of the smooth scale for different node types.
+    A negative value switches off the algorithm for current node type. In case of inaccurate results,
+    this parameter may be adjusted in the range from 0 to 1 or set -1 to disable SmoothQuant algorithm.
 
-    :param preset: A preset controls the quantization mode (symmetric and asymmetric).
-        It can take the following values:
-        - `performance`: Symmetric quantization of weights and activations.
-        - `mixed`: Symmetric quantization of weights and asymmetric quantization of activations.
-        - `None`: `mixed` preset is used for `transformer` model type otherwise `performace`.
-    :param target_device: A target device the specificity of which will be
-        taken into account while compressing in order to obtain the best
-        performance for this type of device.
-    :param subset_size: Size of a subset to calculate activations
-        statistics used for quantization.
-    :param fast_bias_correction: Setting this option to `False` enables
-        a different bias correction method which is more accurate, in general,
-        and takes more time but requires less memory.
-    :param model_type: Model type is needed to specify additional patterns
-        in the model. Supported only `transformer` now.
-    :param ignored_scope: An ignored scope that defined the list of model
-        control flow graph nodes to be ignored during quantization.
-    :param advanced_parameters: Advanced quantization parameters for
-        fine-tuning the quantization algorithm.
-    :return: A POT quantization configuration as dict.
-    """
-    if preset is None:
-        preset = QuantizationPreset.MIXED if model_type == ModelType.TRANSFORMER else QuantizationPreset.PERFORMANCE
-
-    config = {
-        "target_device": target_device.value,
-        "preset": preset.value,
-        "stat_subset_size": subset_size,
-        "use_fast_bias": fast_bias_correction,
-    }
-
-    if model_type is not None:
-        config["model_type"] = model_type.value
-    if ignored_scope is not None:
-        config["ignored"] = _create_ignored_scope_config(ignored_scope)
-
-    if advanced_parameters is None:
-        return config
-
-    backend_activations_parameters = advanced_parameters.backend_params.get(BackendParameters.ACTIVATIONS, {})
-    activations_config = _create_quantization_group_config(
-        advanced_parameters.activations_quantization_params,
-        advanced_parameters.activations_range_estimator_params,
-        backend_activations_parameters,
-    )
-    if activations_config:
-        config["activations"] = activations_config
+    :param convolution: Whether to apply smoothing for Convolution layers.
+    :type convolution: float
+    :param matmul: Whether to apply smoothing for MatMul layers.
+    :type matmul: float
+    """
 
-    backend_weights_parameters = advanced_parameters.backend_params.get(BackendParameters.WEIGHTS, {})
-    weights_config = _create_quantization_group_config(
-        advanced_parameters.weights_quantization_params,
-        advanced_parameters.weights_range_estimator_params,
-        backend_weights_parameters,
-    )
-    if weights_config:
-        config["weights"] = weights_config
+    convolution: float = -1
+    matmul: float = 0.95
 
-    if advanced_parameters.overflow_fix == OverflowFix.ENABLE:
-        config["saturation_fix"] = "all"
-    elif advanced_parameters.overflow_fix == OverflowFix.FIRST_LAYER:
-        config["saturation_fix"] = "first_layer"
-    elif advanced_parameters.overflow_fix == OverflowFix.DISABLE:
-        config["saturation_fix"] = "no"
-
-    config["inplace_statistics"] = advanced_parameters.inplace_statistics
-
-    if advanced_parameters.quantize_outputs:
-        raise RuntimeError("Quantization algorithm from the OpenVINO backend does not support output quantization yet")
-
-    bias_correction_params = advanced_parameters.bias_correction_params
-    if bias_correction_params.apply_for_all_nodes is not None:
-        config["apply_for_all_nodes"] = bias_correction_params.apply_for_all_nodes
-    if bias_correction_params.threshold is not None:
-        config["threshold"] = bias_correction_params.threshold
 
-    return config
+class RestoreMode(Enum):
+    """
+    Specifies how to revert operations to their original precision.
 
+    :param ACTIVATIONS_AND_WEIGHTS: Operations will be reverted to floating-point precision.
+    :param ONLY_ACTIVATIONS: Operations with weights will be reverted to representation with int8 weights,
+        while all other operations will revert to floating-point precision.
+    """
 
-def _create_engine_config(
-    device: str,
-    default_stat_requests_number: int,
-    default_eval_requests_number: int,
-    advanced_parameters: AdvancedQuantizationParameters,
-) -> Dict[str, Any]:
+    ACTIVATIONS_AND_WEIGHTS = "activations_and_weights"
+    ONLY_ACTIVATIONS = "only_activations"
+
+
+@api()
+@dataclass
+class AdvancedQuantizationParameters:
     """
-    Creates a POT engine configuration.
+    Contains advanced parameters for fine-tuning quantization algorithm.
 
-    :param device: A target device.
-    :param stat_requests_number: The default number of infer requests that are used
-        to collect statistics.
-    :param default_eval_requests_number: The default number of infer requests that
-        are used for model evaluation.
-    :param advanced_parameters: Advanced quantization parameters.
-    :return: A POT engine configuration as dict.
-    """
-    engine_config = {
-        "device": device,
-        "stat_requests_number": default_stat_requests_number,
-        "eval_requests_number": default_eval_requests_number,
-    }
-
-    advanced_backend_params = [
-        BackendParameters.STAT_REQUESTS_NUMBER,
-        BackendParameters.EVAL_REQUESTS_NUMBER,
-    ]
-
-    for param_name in advanced_backend_params:
-        param_value = advanced_parameters.backend_params.get(param_name)
-        if param_value is not None:
-            engine_config[param_name] = param_value
-
-    return engine_config
-
-
-@tracked_function(
-    NNCF_OV_CATEGORY, [CompressionStartedWithQuantizeApi(), POTImplementation(), "target_device", "preset"]
-)
-def quantize_impl(
-    model: ov.Model,
-    calibration_dataset: Dataset,
-    mode: Optional[QuantizationMode] = None,
-    preset: Optional[QuantizationPreset] = None,
-    target_device: TargetDevice = TargetDevice.ANY,
-    subset_size: int = 300,
-    fast_bias_correction: bool = True,
-    model_type: Optional[ModelType] = None,
-    ignored_scope: Optional[IgnoredScope] = None,
-    advanced_parameters: Optional[AdvancedQuantizationParameters] = None,
-) -> ov.Model:
-    """
-    Implementation of the `quantize()` method for the OpenVINO backend.
-    """
-    pot.utils.logger.init_logger(level=logging.getLevelName(nncf_logger.getEffectiveLevel()))
-
-    if mode is not None:
-        raise ValueError(f"mode={mode} is not supported")
-
-    if advanced_parameters is None:
-        advanced_parameters = AdvancedQuantizationParameters()
-
-    if advanced_parameters.smooth_quant_alpha is not None:
-        warning_deprecated(
-            "`AdvancedQuantizationParameters(smooth_quant_alpha=..)` is deprecated."
-            "Please, use `AdvancedQuantizationParameters(smooth_quant_alphas)` option "
-            "with AdvancedSmoothQuantParameters(convolution=.., matmul=..) as value instead."
-        )
+    :param overflow_fix: This option controls whether to apply the overflow issue fix
+        for the 8-bit quantization, defaults to OverflowFix.FIRST_LAYER.
+    :type overflow_fix: nncf.quantization.advanced_parameters.OverflowFix
+    :param quantize_outputs: Whether to insert additional quantizers right before each
+        of the model outputs.
+    :type quantize_outputs: bool
+    :param inplace_statistics: Defines whether to calculate quantizers statistics by
+        backend graph operations or by default Python implementation, defaults to True.
+    :type inplace_statistics: bool
+    :param disable_channel_alignment: Whether to disable the channel alignment.
+    :type disable_channel_alignment: bool
+    :param disable_bias_correction: Whether to disable the bias correction.
+    :type disable_bias_correction: bool
+    :param activations_quantization_params: Quantization parameters for activations.
+    :type activations_quantization_params: nncf.quantization.advanced_parameters.QuantizationParameters
+    :param weights_quantization_params: Quantization parameters for weights.
+    :type weights_quantization_params: nncf.quantization.advanced_parameters.QuantizationParameters
+    :param activations_range_estimator_params: Range estimator parameters for activations.
+    :type activations_range_estimator_params: nncf.quantization.range_estimator.RangeEstimatorParameters
+    :param weights_range_estimator_params: Range estimator parameters for weights.
+    :type weights_range_estimator_params: nncf.quantization.range_estimator.RangeEstimatorParameters
+    :param bias_correction_params: Advanced bias correction parameters.
+    :type bias_correction_params: nncf.quantization.advanced_parameters.AdvancedBiasCorrectionParameters
+    :param smooth_quant_alphas: SmoothQuant-related parameters mapping.
+        It regulates the calculation of the smooth scale. The default value stored in AdvancedSmoothQuantParameters.
+        A negative value for each field switches off type smoothing. In case of inaccurate results,
+        fields may be adjusted in the range from 0 to 1 or set -1 to disable smoothing for type.
+    :type smooth_quant_alpha: AdvancedSmoothQuantParameters
+    :param smooth_quant_alpha: Deprecated SmoothQuant-related parameter.
+    :type smooth_quant_alpha: float
+    :param backend_params: Backend-specific parameters.
+    :type backend_params: Dict[str, Any]
+    """
 
-    sq_params = advanced_parameters.smooth_quant_alphas
+    # General parameters
+    overflow_fix: OverflowFix = OverflowFix.FIRST_LAYER
+    quantize_outputs: bool = False
+    inplace_statistics: bool = True
+    disable_channel_alignment: bool = True
+    disable_bias_correction: bool = False
 
-    if model_type == ModelType.TRANSFORMER and (sq_params.convolution > 0 or sq_params.matmul > 0):
-        nncf_logger.warning(
-            "IMPORTANT. The AdvancedSmoothQuantParameters parameter value > 0 IS NOT SUPPORTED for the POT backend!"
-            "Please, use `AdvancedSmoothQuantParameters(convolution = -1, matmul = -1)`."
-        )
+    # Advanced Quantization parameters
+    activations_quantization_params: Union[QuantizationParameters, FP8QuantizationParameters] = None
+    weights_quantization_params: Union[QuantizationParameters, FP8QuantizationParameters] = None
 
-    algorithm_parameters = _create_quantization_config(
-        preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters
-    )
+    # Range estimator parameters
+    activations_range_estimator_params: RangeEstimatorParameters = field(default_factory=RangeEstimatorParameters)
+    weights_range_estimator_params: RangeEstimatorParameters = field(default_factory=RangeEstimatorParameters)
 
-    if advanced_parameters.disable_bias_correction:
-        algorithms = [
-            {"name": "ActivationChannelAlignment", "params": algorithm_parameters},
-            {"name": "MinMaxQuantization", "params": algorithm_parameters},
-        ]
-    else:
-        algorithms = [{"name": "DefaultQuantization", "params": algorithm_parameters}]
+    # Advanced BiasCorrection algorithm parameters
+    bias_correction_params: AdvancedBiasCorrectionParameters = field(default_factory=AdvancedBiasCorrectionParameters)
 
-    pot_model = _convert_openvino_model_to_compressed_model(model, target_device)
+    # Advanced SmoothQuant algorithm parameters
+    smooth_quant_alphas: AdvancedSmoothQuantParameters = field(default_factory=AdvancedSmoothQuantParameters)
+    # Deprecated parameter
+    smooth_quant_alpha: float = None
 
-    engine_config = _create_engine_config(
-        device="CPU",
-        default_stat_requests_number=2,
-        default_eval_requests_number=2,
-        advanced_parameters=advanced_parameters,
-    )
+    # Backend specific parameters
+    backend_params: Dict[str, Any] = field(default_factory=dict)
 
-    engine = OVEngine(engine_config, calibration_dataset, calibration_dataset)
-    pipeline = pot.create_pipeline(algorithms, engine)
-    compressed_model = pipeline.run(pot_model)
-    quantized_model = _convert_compressed_model_to_openvino_model(compressed_model)
 
-    if is_weight_compression_needed(advanced_parameters):
-        compress_quantize_weights_transformation(quantized_model)
+@api()
+@dataclass
+class AdvancedAccuracyRestorerParameters:
+    """
+    Contains advanced parameters for fine-tuning the accuracy restorer algorithm.
 
-    return quantized_model
+    :param max_num_iterations: The maximum number of iterations of the algorithm.
+        In other words, the maximum number of layers that may be reverted back to
+        floating-point precision. By default, it is limited by the overall number of
+        quantized layers.
+    :type max_num_iterations: int
+    :param tune_hyperparams: Whether to tune of quantization parameters as a
+        preliminary step before reverting layers back to the floating-point precision.
+        It can bring an additional boost in performance and accuracy, at the cost of
+        increased overall quantization time. The default value is `False`.
+    :type tune_hyperparams: int
+    :param ranking_subset_size: Size of a subset that is used to rank layers by their
+        contribution to the accuracy drop.
+    :type ranking_subset_size: Optional[int]
+    :param num_ranking_workers: The number of parallel workers that are used to rank
+        quantization operations.
+    :type num_ranking_workers: Optional[int]
+    :param intermediate_model_dir: Path to the folder where the model, which was fully
+        quantized with initial parameters, should be saved.
+    :type intermediate_model_dir: Optional[str]
+    :param restore_mode: Specifies how to revert operations to their original precision.
+    :type restore_mode: RestoreMode
+    """
 
+    max_num_iterations: int = sys.maxsize
+    tune_hyperparams: bool = False
+    ranking_subset_size: Optional[int] = None
+    num_ranking_workers: Optional[int] = None
+    intermediate_model_dir: Optional[str] = None
+    restore_mode: RestoreMode = RestoreMode.ACTIVATIONS_AND_WEIGHTS
 
-def _create_accuracy_restorer_config(
-    max_drop: float, drop_type: DropType, advanced_parameters: Optional[AdvancedAccuracyRestorerParameters]
-) -> Dict[str, Any]:
+
+def changes_asdict(params: Any) -> Dict[str, Any]:
     """
-    Creates a accuracy restorer configuration.
+    Returns non None fields as dict
 
-    :param max_drop: The maximum accuracy drop that should be achieved after
-        the quantization.
-    :param drop_type: The accuracy drop type, which determines how the maximum accuracy
-        drop between the original model and the compressed model is calculated.
-    :param advanced_parameters: Advanced parameters for fine-tuning the accuracy
-        restorer algorithm.
-    :return: A POT accuracy restorer configuration as dict.
-    """
-    config = {
-        "maximal_drop": max_drop,
-        "drop_type": drop_type.value,
-        "metric_subset_ratio": 0.5,
-    }
-
-    if advanced_parameters is None:
-        return config
-
-    config["max_num_iterations"] = advanced_parameters.max_num_iterations
-    config["tune_hyperparams"] = advanced_parameters.tune_hyperparams
-    if advanced_parameters.ranking_subset_size is not None:
-        config["ranking_subset_size"] = advanced_parameters.ranking_subset_size
+    :param params: A dataclass instance
+    :return: A dict with non None fields
+    """
+    changes = {}
+    for f in fields(params):
+        value = getattr(params, f.name)
+        if value is not None:
+            changes[f.name] = value
+    return changes
 
-    return config
 
+def convert_to_dict_recursively(params: Any) -> Dict[str, Any]:
+    """
+    Converts dataclass to dict recursively
 
-@tracked_function(
-    NNCF_OV_CATEGORY,
-    [
-        CompressionStartedWithQuantizeWithAccuracyControlApi(),
-        POTImplementation(),
-        "target_device",
-        "preset",
-        "max_drop",
-        "drop_type",
-    ],
-)
-def quantize_with_accuracy_control_impl(
-    model: ov.Model,
-    calibration_dataset: Dataset,
-    validation_dataset: Dataset,
-    validation_fn: Callable[[ov.CompiledModel, Iterable[Any]], float],
-    max_drop: float = 0.01,
-    drop_type: DropType = DropType.ABSOLUTE,
-    preset: Optional[QuantizationPreset] = None,
-    target_device: TargetDevice = TargetDevice.ANY,
-    subset_size: int = 300,
-    fast_bias_correction: bool = True,
-    model_type: Optional[ModelType] = None,
-    ignored_scope: Optional[IgnoredScope] = None,
-    advanced_quantization_parameters: Optional[AdvancedQuantizationParameters] = None,
-    advanced_accuracy_restorer_parameters: Optional[AdvancedAccuracyRestorerParameters] = None,
-) -> ov.Model:
-    """
-    Implementation of the `quantize_with_accuracy_control()` method for the OpenVINO backend.
-    """
-    pot.utils.logger.init_logger(level=logging.getLevelName(nncf_logger.getEffectiveLevel()))
-
-    if advanced_quantization_parameters is None:
-        advanced_quantization_parameters = AdvancedQuantizationParameters()
-
-    if advanced_quantization_parameters.smooth_quant_alpha is not None:
-        warning_deprecated(
-            "`AdvancedQuantizationParameters(smooth_quant_alpha=..)` is deprecated."
-            "Please, use `AdvancedQuantizationParameters(smooth_quant_alphas)` option "
-            "with AdvancedSmoothQuantParameters(convolution=.., matmul=..) as value instead."
+    :param params: A dataclass instance
+    :return: A dataclass as dict
+    """
+    if params is None:
+        return {}
+
+    result = {}
+    for f in fields(params):
+        value = getattr(params, f.name)
+        if is_dataclass(value):
+            result[f.name] = convert_to_dict_recursively(value)
+        elif isinstance(value, Enum):
+            result[f.name] = value.value
+        else:
+            result[f.name] = value
+
+    return result
+
+
+def convert_quantization_parameters_to_dict(params: QuantizationParameters) -> Dict[str, Any]:
+    """
+    Converts quantization parameters to the dict in the legacy format
+
+    :param params: Quantization parameters
+    :return: Quantization parameters as dict in the legacy format
+    """
+    result = {}
+    if params is not None:
+        if params.num_bits is not None:
+            result["bits"] = params.num_bits
+        if params.mode is not None:
+            result["mode"] = params.mode
+        if params.signedness_to_force is not None:
+            result["signed"] = params.signedness_to_force
+        if params.per_channel is not None:
+            result["per_channel"] = params.per_channel
+        if params.narrow_range is not None:
+            raise nncf.ParameterNotSupportedError("narrow_range parameter is not supported in the legacy format")
+    return result
+
+
+def convert_range_estimator_parameters_to_dict(params: RangeEstimatorParameters) -> Dict[str, Any]:
+    """
+    Converts range estimator parameters to the dict in the legacy format
+
+    :param params: Range estimator parameters
+    :return: range estimator parameters as dict in the legacy format
+    """
+    if params.min.clipping_value is not None or params.max.clipping_value is not None:
+        raise nncf.ParameterNotSupportedError("clipping_value parameter is not supported in the legacy format")
+
+    result = {}
+    if (
+        params.min.statistics_type == StatisticsType.MIN
+        and params.min.aggregator_type == AggregatorType.MIN
+        and params.max.statistics_type == StatisticsType.MAX
+        and params.max.aggregator_type == AggregatorType.MAX
+    ):
+        result["type"] = "mixed_min_max"
+    elif (
+        params.min.statistics_type == StatisticsType.MIN
+        and params.min.aggregator_type == AggregatorType.MEAN
+        and params.max.statistics_type == StatisticsType.MAX
+        and params.max.aggregator_type == AggregatorType.MEAN
+    ):
+        result["type"] = "mean_min_max"
+    elif (
+        params.min.statistics_type == StatisticsType.QUANTILE
+        and params.min.aggregator_type == AggregatorType.MEAN
+        and params.max.statistics_type == StatisticsType.QUANTILE
+        and params.max.aggregator_type == AggregatorType.MEAN
+    ):
+        result["type"] = "mean_percentile"
+        result["params"] = {
+            "min_percentile": 1 - params.min.quantile_outlier_prob,
+            "max_percentile": 1 - params.max.quantile_outlier_prob,
+        }
+    elif (
+        params.min.statistics_type is None
+        and params.min.aggregator_type is None
+        and params.max.statistics_type is None
+        and params.max.aggregator_type is None
+    ):
+        return {}
+    else:
+        raise nncf.ParameterNotSupportedError(
+            f"The following range estimator parameters are not supported: {str(params)}"
         )
 
-    sq_params = advanced_quantization_parameters.smooth_quant_alphas
+    return result
 
-    if model_type == ModelType.TRANSFORMER and (sq_params.convolution > 0 or sq_params.matmul > 0):
-        nncf_logger.warning(
-            "IMPORTANT. The AdvancedSmoothQuantParameters parameter value > 0 IS NOT SUPPORTED for the POT backend!"
-            "Please, use `AdvancedSmoothQuantParameters(convolution = -1, matmul = -1)`."
-        )
 
-    if advanced_quantization_parameters.disable_bias_correction:
-        raise ValueError(
-            "Quantization algorithm with accuracy controll from the OpenVINO backend "
-            "does not support disabling bias correction algorithm yet"
-        )
+def apply_advanced_parameters_to_config(
+    config: Dict[str, Any], params: AdvancedQuantizationParameters
+) -> Dict[str, Any]:
+    """
+    Apply advanced parameters to the config in the legacy format
 
-    pot_model = _convert_openvino_model_to_compressed_model(model, target_device)
+    :param config: NNCF config in legacy format
+    :param params: Advanced quantization parameters
+    :return: advanced quantization parameters as dict in the legacy format
+    """
+    config["overflow_fix"] = params.overflow_fix.value
+    config["quantize_outputs"] = params.quantize_outputs
 
-    engine_config = _create_engine_config(
-        device="CPU",
-        default_stat_requests_number=1,
-        default_eval_requests_number=1,
-        advanced_parameters=advanced_quantization_parameters,
-    )
+    if params.disable_bias_correction:
+        initializer = config.get("initializer", {})
+        initializer["batchnorm_adaptation"] = {"num_bn_adaptation_samples": 0}
+        config["initializer"] = initializer
 
-    # Check whether it is possible to calculate the metric for one data item.
+    activations_config = convert_quantization_parameters_to_dict(params.activations_quantization_params)
+    if activations_config:
+        config["activations"] = activations_config
 
-    use_original_metric = True
-    try:
-        ie = ov.Core()
-        compiled_model = ie.compile_model(model, device_name="CPU")
-        _ = validation_fn(compiled_model, validation_dataset.get_data(indices=[0]))
-    except Exception:
-        use_original_metric = False
-    compression_algorithms = pot.algorithms.algorithm_selector.COMPRESSION_ALGORITHMS
-    if "NMSEBasedAccuracyAware" not in compression_algorithms.registry_dict:
-        compression_algorithms.register("NMSEBasedAccuracyAware")(NMSEBasedAccuracyAware)
-
-    algotrithm_parameters = _create_accuracy_restorer_config(max_drop, drop_type, advanced_accuracy_restorer_parameters)
-
-    algotrithm_parameters.update(
-        _create_quantization_config(
-            preset,
-            target_device,
-            subset_size,
-            fast_bias_correction,
-            model_type,
-            ignored_scope,
-            advanced_quantization_parameters,
-        )
-    )
+    weights_config = convert_quantization_parameters_to_dict(params.weights_quantization_params)
+    if weights_config:
+        config["weights"] = weights_config
 
-    algorithms = [{"name": "NMSEBasedAccuracyAware", "params": algotrithm_parameters}]
+    activations_init_range_config = convert_range_estimator_parameters_to_dict(
+        params.activations_range_estimator_params
+    )
+    weights_init_range_config = convert_range_estimator_parameters_to_dict(params.weights_range_estimator_params)
 
-    engine = OVEngine(engine_config, calibration_dataset, validation_dataset, validation_fn, use_original_metric)
-    pipeline = pot.create_pipeline(algorithms, engine)
-    compressed_model = pipeline.run(pot_model)
-    quantized_model = _convert_compressed_model_to_openvino_model(compressed_model)
+    if activations_init_range_config or weights_init_range_config:
+        initializer = config.get("initializer", {})
+        init_range = initializer.get("range", {})
+        global_num_init_samples = init_range.get("num_init_samples", None)
+        global_range_type = init_range.get("type", None)
+
+        activations_init_range_config["target_quantizer_group"] = "activations"
+        activations_init_range_config["target_scopes"] = "{re}.*"
+        if global_num_init_samples is not None:
+            activations_init_range_config["num_init_samples"] = global_num_init_samples
+        if "type" not in activations_init_range_config and global_range_type is not None:
+            activations_init_range_config["type"] = global_range_type
+
+        weights_init_range_config["target_quantizer_group"] = "weights"
+        weights_init_range_config["target_scopes"] = "{re}.*"
+        if global_num_init_samples is not None:
+            weights_init_range_config["num_init_samples"] = global_num_init_samples
+        if "type" not in weights_init_range_config and global_range_type is not None:
+            weights_init_range_config["type"] = global_range_type
+
+        initializer["range"] = [activations_init_range_config, weights_init_range_config]
+        config["initializer"] = initializer
+
+    if params.bias_correction_params.apply_for_all_nodes:
+        raise nncf.ParameterNotSupportedError(
+            "apply_for_all_nodes parameter of the BiasCorrection algorithm is not supported in the legacy format"
+        )
 
-    if is_weight_compression_needed(advanced_quantization_parameters):
-        compress_quantize_weights_transformation(quantized_model)
+    if params.bias_correction_params.threshold is not None:
+        raise nncf.ParameterNotSupportedError(
+            "threshold parameter of the BiasCorrection algorithm is not supported in the legacy format"
+        )
 
-    return quantized_model
+    return config
```

### Comparing `nncf-2.8.1/nncf/openvino/pot/telemetry_extractors.py` & `nncf-2.9.0/nncf/quantization/telemetry_extractors.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,10 +11,15 @@
 
 from typing import Any
 
 from nncf.telemetry.extractors import CollectedEvent
 from nncf.telemetry.extractors import TelemetryExtractor
 
 
-class POTImplementation(TelemetryExtractor):
+class CompressionStartedWithQuantizeApi(TelemetryExtractor):
     def extract(self, _: Any) -> CollectedEvent:
-        return CollectedEvent(name="pot implementation")
+        return CollectedEvent(name="compression_started", data="quantize_api")
+
+
+class CompressionStartedWithQuantizeWithAccuracyControlApi(TelemetryExtractor):
+    def extract(self, _: Any) -> CollectedEvent:
+        return CollectedEvent(name="compression_started", data="quantize_with_accuracy_control_api")
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/backend_parameters.py` & `nncf-2.9.0/nncf/openvino/quantization/backend_parameters.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,15 +18,14 @@
     COMPRESS_WEIGHTS = "compress_weights"
     STAT_REQUESTS_NUMBER = "stat_requests_number"
     EVAL_REQUESTS_NUMBER = "eval_requests_number"
     ACTIVATIONS = "activations"
     WEIGHTS = "weights"
     LEVEL_LOW = "level_low"
     LEVEL_HIGH = "level_high"
-    USE_POT = "use_pot"
 
 
 def is_weight_compression_needed(advanced_parameters: Optional[AdvancedQuantizationParameters]) -> bool:
     """
     Determines whether weight compression is needed based on the provided
     advanced quantization parameters.
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/default_quantization.py` & `nncf-2.9.0/nncf/openvino/quantization/default_quantization.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/ignored_patterns.py` & `nncf-2.9.0/nncf/openvino/quantization/ignored_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/quantize_ifmodel.py` & `nncf-2.9.0/nncf/openvino/quantization/quantize_ifmodel.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -24,14 +24,15 @@
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.track_progress import track
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.openvino.graph.metatypes.openvino_metatypes import OVIfMetatype
+from nncf.openvino.graph.model_utils import remove_friendly_name_duplicates
 from nncf.openvino.graph.node_utils import get_number_if_op
 from nncf.openvino.graph.transformations.commands import OVExtractIfBodyCommand
 from nncf.openvino.graph.transformations.commands import OVOutputInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVTargetPoint
 from nncf.openvino.graph.transformations.commands import OVUpdateIfBodyCommand
 from nncf.quantization.algorithms.algorithm import Algorithm
 
@@ -169,16 +170,20 @@
             factory.EngineFactory.create(parent_model_with_additional_outputs),
             parent_dataset,
             if_cond_input_name,
             then_model_input_names,
             else_model_input_names,
             subset_size,
         )
+
         then_model = _extract_if_body(model_transformer_fp32, if_node, True)
+        then_model = remove_friendly_name_duplicates(then_model)
         else_model = _extract_if_body(model_transformer_fp32, if_node, False)
+        else_model = remove_friendly_name_duplicates(else_model)
+
         then_quantized_model, current_model_num = apply_algorithm_if_bodies(
             algorithm,
             then_model,
             NNCFGraphFactory.create(then_model),
             then_dataset,
             subset_size,
             current_model_num + 1,
```

### Comparing `nncf-2.8.1/nncf/openvino/quantization/quantize_model.py` & `nncf-2.9.0/nncf/openvino/quantization/quantize_model.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,82 +1,58 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import importlib
 from copy import deepcopy
 from typing import Any, Callable, Iterable, List, Optional, Tuple, TypeVar, Union
 
 import openvino.runtime as ov
 from openvino._offline_transformations import compress_quantize_weights_transformation
 
+from nncf.common.factory import NNCFGraphFactory
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.data import Dataset
+from nncf.openvino.graph.model_utils import remove_friendly_name_duplicates
 from nncf.openvino.graph.nncf_graph_builder import GraphConverter
 from nncf.openvino.graph.node_utils import get_number_if_op
 from nncf.openvino.quantization.backend_parameters import BackendParameters
 from nncf.openvino.quantization.backend_parameters import is_weight_compression_needed
 from nncf.openvino.quantization.quantize_ifmodel import apply_algorithm_if_bodies
 from nncf.openvino.rt_info import dump_parameters
+from nncf.parameters import CompressWeightsMode
 from nncf.parameters import DropType
 from nncf.parameters import ModelType
 from nncf.parameters import QuantizationMode
+from nncf.parameters import SensitivityMetric
 from nncf.parameters import TargetDevice
 from nncf.quantization.advanced_parameters import AdvancedAccuracyRestorerParameters
 from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
 from nncf.quantization.advanced_parameters import convert_to_dict_recursively
 from nncf.quantization.algorithms.accuracy_control.algorithm import QuantizationAccuracyRestorer
 from nncf.quantization.algorithms.accuracy_control.algorithm import calculate_accuracy_drop
 from nncf.quantization.algorithms.accuracy_control.evaluator import Evaluator
 from nncf.quantization.algorithms.post_training.algorithm import PostTrainingQuantization
+from nncf.quantization.algorithms.weight_compression.algorithm import WeightCompression
 from nncf.quantization.quantize_model import quantize_with_tune_hyperparams
 from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeApi
 from nncf.scopes import IgnoredScope
 from nncf.telemetry.decorator import tracked_function
 from nncf.telemetry.events import NNCF_OV_CATEGORY
 
-USE_POT_AS_DEFAULT = False
-
 TTensor = TypeVar("TTensor")
 
 
-def should_use_pot(advanced_parameters: Optional[AdvancedQuantizationParameters]) -> bool:
-    """
-    Returns True if POT should be used for quantization, False otherwise.
-
-    :param advanced_parameters: Advanced quantization parameters.
-    :return: True if POT should be used, False otherwise.
-    :raises ImportError if POT is not found in the Python environment.
-    """
-    use_pot = USE_POT_AS_DEFAULT
-    if advanced_parameters is not None and advanced_parameters.backend_params is not None:
-        use_pot = advanced_parameters.backend_params.get(BackendParameters.USE_POT, USE_POT_AS_DEFAULT)
-
-    if not use_pot:
-        return False
-
-    try:
-        importlib.import_module("openvino.tools.pot")
-    except ImportError:
-        nncf_logger.error(
-            "OpenVINO POT was not found in your Python environment.\n"
-            "Please install the openvino-dev package, e.g. via pypi: pip install openvino-dev.\n"
-        )
-
-    return True
-
-
 @tracked_function(NNCF_OV_CATEGORY, [CompressionStartedWithQuantizeApi(), "target_device", "preset"])
 def native_quantize_if_op_impl(
     model: ov.Model,
     calibration_dataset: Dataset,
     mode: Optional[QuantizationMode] = None,
     preset: Optional[QuantizationPreset] = None,
     target_device: TargetDevice = TargetDevice.ANY,
@@ -245,14 +221,15 @@
         initial_metric_results.metric_value, quantized_metric_results.metric_value, max_drop, drop_type
     )
 
     nncf_logger.info(f"Accuracy drop: {accuracy_drop} ({drop_type})")
 
     # TODO(andrey-churkin): Collect statistics only once
     if advanced_accuracy_restorer_parameters.tune_hyperparams and not should_terminate:
+        model = remove_friendly_name_duplicates(model)
         tuned_quantized_model = quantize_with_tune_hyperparams(
             model,
             calibration_dataset,
             validation_dataset,
             validation_fn,
             initial_metric_results,
             quantized_metric_results,
@@ -333,22 +310,19 @@
     model_type: Optional[ModelType] = None,
     ignored_scope: Optional[IgnoredScope] = None,
     advanced_parameters: Optional[AdvancedQuantizationParameters] = None,
 ) -> ov.Model:
     """
     Implementation of the `quantize()` method for the OpenVINO backend.
     """
-    if should_use_pot(advanced_parameters):
-        from nncf.openvino.pot.quantization.quantize_model import quantize_impl as pot_quantize_impl
+    model = remove_friendly_name_duplicates(model)
 
-        quantize_fn = pot_quantize_impl
-    else:
-        quantize_fn = native_quantize_impl
-        if get_number_if_op(model) > 0:
-            quantize_fn = native_quantize_if_op_impl
+    quantize_fn = native_quantize_impl
+    if get_number_if_op(model) > 0:
+        quantize_fn = native_quantize_if_op_impl
 
     return quantize_fn(
         model=model,
         calibration_dataset=calibration_dataset,
         mode=mode,
         preset=preset,
         target_device=target_device,
@@ -392,22 +366,16 @@
     ignored_scope: Optional[IgnoredScope] = None,
     advanced_quantization_parameters: Optional[AdvancedQuantizationParameters] = None,
     advanced_accuracy_restorer_parameters: Optional[AdvancedAccuracyRestorerParameters] = None,
 ) -> ov.Model:
     """
     Implementation of the `quantize_with_accuracy_control()` method for the OpenVINO backend.
     """
-    if should_use_pot(advanced_quantization_parameters):
-        from nncf.openvino.pot.quantization.quantize_model import (
-            quantize_with_accuracy_control_impl as pot_quantize_with_accuracy_control_impl,
-        )
 
-        quantize_with_accuracy_control_fn = pot_quantize_with_accuracy_control_impl
-    else:
-        quantize_with_accuracy_control_fn = native_quantize_with_accuracy_control_impl
+    quantize_with_accuracy_control_fn = native_quantize_with_accuracy_control_impl
 
     val_func = wrap_validation_fn(validation_fn)
 
     return quantize_with_accuracy_control_fn(
         model,
         calibration_dataset,
         validation_dataset,
@@ -419,7 +387,31 @@
         subset_size,
         fast_bias_correction,
         model_type,
         ignored_scope,
         advanced_quantization_parameters,
         advanced_accuracy_restorer_parameters,
     )
+
+
+def compress_weights_impl(
+    model: ov.Model,
+    dataset: Dataset,
+    mode: CompressWeightsMode,
+    ratio: float,
+    group_size: int,
+    ignored_scope: IgnoredScope,
+    all_layers: bool,
+    sensitivity_metric: SensitivityMetric,
+    awq: bool,
+    subset_size: int,
+) -> ov.Model:
+    """
+    Implementation of the `compress_weights()` method for the OpenVINO backend.
+    """
+
+    model = remove_friendly_name_duplicates(model)
+    compression_algorithm = WeightCompression(
+        mode, ratio, group_size, ignored_scope, all_layers, sensitivity_metric, awq, subset_size
+    )
+    graph = NNCFGraphFactory.create(model)
+    return compression_algorithm.apply(model, graph, dataset=dataset)
```

### Comparing `nncf-2.8.1/nncf/openvino/rt_info.py` & `nncf-2.9.0/nncf/openvino/rt_info.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/statistics/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/bias_correction/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/openvino/statistics/aggregator.py` & `nncf-2.9.0/nncf/openvino/statistics/aggregator.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,61 +19,60 @@
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.tensor_statistics.aggregator import StatisticsAggregator
 from nncf.common.tensor_statistics.statistic_point import StatisticPoint
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.experimental.common.tensor_statistics.collectors import MergedTensorCollector
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
+from nncf.openvino.graph.node_utils import get_ov_model_reduce_node_name
+from nncf.openvino.graph.node_utils import get_reducer_output_node_names
 from nncf.openvino.graph.transformations.commands import OVInplaceFnInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVOutputInsertionCommand
 from nncf.openvino.tensor import OVNNCFTensor
 
 
 class OVStatisticsAggregator(StatisticsAggregator):
     def collect_statistics(self, model: ov.Model, graph: NNCFGraph) -> None:
         self._name_to_node_mapping = {op.get_friendly_name(): op for op in model.get_ops()}
         super().collect_statistics(model, graph)
 
     def _register_statistics(
         self, outputs: Dict[str, OVNNCFTensor], statistic_points: StatisticPointsContainer
     ) -> None:
         for _, statistic_point, tensor_collector in statistic_points.get_tensor_collectors():
-            target_point = statistic_point.target_point
-            node_name = target_point.target_node_name
-            port_id = target_point.port_id
-            if target_point.type == TargetType.POST_LAYER_OPERATION:
-                stat_node_name = node_name
-            elif target_point.type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
-                node = self._name_to_node_mapping[node_name]
-                output = node.input_value(port_id)
-                stat_node_name = output.get_node().get_friendly_name()
-                port_id = output.get_index()
-            else:
-                RuntimeError(f"Unsupported target point type for statistic aggregator: {target_point.type}")
-
-            input_info = tensor_collector.get_output_info(stat_node_name, port_id)
+            stat_node_name, port_id, _ = self._translate_to_post_layer_operation(statistic_point)
+            input_info = []
+            for reducer in tensor_collector.reducers:
+                input_info.append(
+                    [
+                        hash(reducer),
+                        get_reducer_output_node_names(
+                            reducer.name, stat_node_name, port_id, reducer.output_port_id, reducer.inplace
+                        ),
+                    ]
+                )
             target_inputs = TensorCollector.get_tensor_collector_inputs(outputs, input_info)
             tensor_collector.register_inputs(target_inputs)
 
     def _get_transformation_layout_extra_outputs(
         self, statistic_points: StatisticPointsContainer
     ) -> TransformationLayout:
         transformation_layout = TransformationLayout()
-        transformation_commands = []
         for _, statistic_point, tensor_collector in statistic_points.get_tensor_collectors():
-            for op_fn, fn_out_port_id in tensor_collector.get_inplace_fn_info():
-                transformation_commands.append(
-                    OVInplaceFnInsertionCommand(statistic_point.target_point, op_fn, fn_out_port_id)
-                )
-            if tensor_collector.any_stat_out_of_place():
-                transformation_commands.append(OVOutputInsertionCommand(statistic_point.target_point))
-
-        for transformation_command in transformation_commands:
-            transformation_layout.register(transformation_command)
-
+            for reducer in tensor_collector.reducers:
+                if reducer.inplace:
+                    target_node_name, target_node_port_id, _ = self._translate_to_post_layer_operation(statistic_point)
+                    output_name = get_ov_model_reduce_node_name(target_node_name, reducer.name, target_node_port_id)
+                    transformation_layout.register(
+                        OVInplaceFnInsertionCommand(
+                            statistic_point.target_point, reducer.get_inplace_fn(), reducer.output_port_id, output_name
+                        )
+                    )
+                else:
+                    transformation_layout.register(OVOutputInsertionCommand(statistic_point.target_point))
         return transformation_layout
 
     @staticmethod
     # TODO(dlyakhov) Move this to common part
     def _get_merged_statistic_points(
         statistic_points: StatisticPointsContainer, model: ov.Model, graph: NNCFGraph
     ) -> StatisticPointsContainer:
@@ -104,10 +103,27 @@
             target_point = merged_collectors_info["target_point"][0]
             collectors = merged_collectors_info["collectors"]
             merged_collector = MergedTensorCollector(collectors)
             stat_point = StatisticPoint(target_point, merged_collector, "Merged")
             merged_statistic_points.add_statistic_point(stat_point)
         return merged_statistic_points
 
+    def _translate_to_post_layer_operation(self, statistic_point: StatisticPoint):
+        target_point = statistic_point.target_point
+        node_name = target_point.target_node_name
+        port_id = target_point.port_id
+        if target_point.type == TargetType.POST_LAYER_OPERATION:
+            stat_node_name = node_name
+            target_point_type = target_point.type
+        elif target_point.type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
+            node = self._name_to_node_mapping[node_name]
+            output = node.input_value(port_id)
+            stat_node_name = output.get_node().get_friendly_name()
+            port_id = output.get_index()
+            target_point_type = TargetType.POST_LAYER_OPERATION
+        else:
+            RuntimeError(f"Unsupported target point type for statistic aggregator: {target_point.type}")
+        return stat_node_name, port_id, target_point_type
+
     @staticmethod
     def _process_outputs(outputs: Dict[str, np.ndarray]) -> Dict[str, OVNNCFTensor]:
         return {n: OVNNCFTensor(v) for n, v in outputs.items()}
```

### Comparing `nncf-2.8.1/nncf/openvino/statistics/collectors.py` & `nncf-2.9.0/nncf/openvino/statistics/collectors.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -24,23 +24,22 @@
 from nncf.experimental.common.tensor_statistics.collectors import MeanAggregator
 from nncf.experimental.common.tensor_statistics.collectors import MeanPerChReducer
 from nncf.experimental.common.tensor_statistics.collectors import MeanReducer
 from nncf.experimental.common.tensor_statistics.collectors import MinReducer
 from nncf.experimental.common.tensor_statistics.collectors import NoopAggregator
 from nncf.experimental.common.tensor_statistics.collectors import NoopReducer
 from nncf.experimental.common.tensor_statistics.collectors import QuantileReducer
+from nncf.experimental.common.tensor_statistics.collectors import RawReducer
 from nncf.experimental.common.tensor_statistics.collectors import ShapeAggregator
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.openvino.graph.node_utils import get_inplace_batch_mean_op
 from nncf.openvino.graph.node_utils import get_inplace_max_op
 from nncf.openvino.graph.node_utils import get_inplace_mean_op
 from nncf.openvino.graph.node_utils import get_inplace_mean_per_ch
 from nncf.openvino.graph.node_utils import get_inplace_min_op
-from nncf.openvino.graph.node_utils import get_reducer_output_node_names
-from nncf.openvino.graph.node_utils import get_result_node_name
 from nncf.openvino.statistics.statistics import OVMeanTensorStatistic
 from nncf.openvino.statistics.statistics import OVRawTensorStatistic
 from nncf.openvino.tensor import OVNNCFTensor
 from nncf.quantization.advanced_parameters import StatisticsType
 
 
 class OVNNCFCollectorTensorProcessor(NNCFCollectorTensorProcessor):
@@ -186,128 +185,96 @@
     @staticmethod
     def zero_elements(x: NNCFTensor) -> NNCFTensor:
         np_tensor = x.tensor
         eps = np.finfo(np_tensor.dtype).eps
         return NNCFTensor(np.abs(np_tensor) < eps)
 
 
-class OVNoopReducer(NoopReducer):
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return [get_result_node_name(target_node_name, port_id)]
-
-
 class OVMinReducer(MinReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_min_op(self.name, self._reduction_axes)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_min_op(self._reduction_axes)
 
 
 class OVMaxReducer(MaxReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_max_op(self.name, self._reduction_axes, False)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_max_op(self._reduction_axes, False)
 
 
 class OVAbsMaxReducer(AbsMaxReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_max_op(self.name, self._reduction_axes, True)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_max_op(self._reduction_axes, True)
 
 
 class OVMeanReducer(MeanReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_mean_op(self.name, self._reduction_axes)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_mean_op(self._reduction_axes)
 
 
 class OVBatchMeanReducer(BatchMeanReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_batch_mean_op(self.name)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_batch_mean_op()
 
 
 class OVMeanPerChanelReducer(MeanPerChReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self):
-        return get_inplace_mean_per_ch(self.name, self._reduction_axes)
-
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
+        return get_inplace_mean_per_ch(self._channel_axis)
 
 
 class OVQuantileReducer(QuantileReducer):
     def get_inplace_fn(self) -> Optional[InplaceInsertionFNType]:
         return None
 
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
-
 
 class OVAbsQuantileReducer(AbsQuantileReducer):
     def _get_processor(self):
         return OVNNCFCollectorTensorProcessor
 
     def get_inplace_fn(self) -> Optional[InplaceInsertionFNType]:
         return None
 
-    def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
-        return get_reducer_output_node_names(self.name, target_node_name, port_id, self.output_port_id, self.inplace)
-
 
 def get_mean_statistic_collector(
     num_samples: int, channel_axis: int, window_size: Optional[int] = None, inplace: bool = True
 ) -> TensorCollector:
     """
     Mean statistic collector builder.
 
     :param num_samples: Maximum number of samples to collect.
     :param channel_axis: Channel axis to use during reduction phase.
     :param window_size: Number of samples from the end of the list of collected samples to aggregate.
         Aggregates all available collected statistics in case parameter is None.
     :param inplace: Whether the mean reducer should be calculated inplace or out of place.
     :return: Mean statistic collector.
     """
-    # TODO(dlyakhov): use inplace OVBatchMeanReducer and OVMeanPerChanelReducer
-    # after migration on openvino-dev=2023.0
-    inplace = False
     if channel_axis == 0:
         reducer = OVBatchMeanReducer(inplace)
     else:
         reducer = OVMeanPerChanelReducer(channel_axis=channel_axis, inplace=inplace)
-    noop_reducer = OVNoopReducer()
+    noop_reducer = NoopReducer()
 
     kwargs = {
         "tensor_processor": OVNNCFCollectorTensorProcessor,
         "num_samples": num_samples,
         "window_size": window_size,
     }
     aggregate_mean = MeanAggregator(**kwargs)
@@ -316,15 +283,15 @@
     collector = TensorCollector(OVMeanTensorStatistic)
     collector.register_statistic_branch(OVMeanTensorStatistic.MEAN_STAT, reducer, aggregate_mean)
     collector.register_statistic_branch(OVMeanTensorStatistic.SHAPE_STAT, noop_reducer, aggregate_shape)
     return collector
 
 
 def get_raw_stat_collector(num_samples: Optional[int] = None) -> TensorCollector:
-    reducer = OVNoopReducer()
+    reducer = RawReducer()
     aggregator = NoopAggregator(num_samples)
 
     collector = TensorCollector(OVRawTensorStatistic)
     collector.register_statistic_branch(OVRawTensorStatistic.VALUES_STATS, reducer, aggregator)
     return collector
```

### Comparing `nncf-2.8.1/nncf/openvino/statistics/statistics.py` & `nncf-2.9.0/nncf/onnx/statistics/statistics.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,23 +12,23 @@
 import numpy as np
 
 from nncf.common.tensor_statistics.statistics import MeanTensorStatistic
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.tensor_statistics.statistics import RawTensorStatistic
 
 
-class OVMinMaxTensorStatistic(MinMaxTensorStatistic):
+class ONNXMinMaxTensorStatistic(MinMaxTensorStatistic):
     @staticmethod
     def tensor_eq(tensor1: np.ndarray, tensor2: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.allclose(tensor1, tensor2, rtol=rtol))
 
 
-class OVMeanTensorStatistic(MeanTensorStatistic):
+class ONNXMeanTensorStatistic(MeanTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
 
 
-class OVRawTensorStatistic(RawTensorStatistic):
+class ONNXRawTensorStatistic(RawTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
```

### Comparing `nncf-2.8.1/nncf/openvino/tensor.py` & `nncf-2.9.0/nncf/onnx/tensor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,31 +1,30 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 
 from nncf.common.tensor import NNCFTensor
-from nncf.parameters import TargetDevice
 
 
-class OVNNCFTensor(NNCFTensor):
+class ONNXNNCFTensor(NNCFTensor):
     """
-    A realisation of OpenVINO tensor wrapper for common NNCF algorithms.
+    A realisation of ONNX tensors wrapper for common NNCF algorithms.
     """
 
     def __init__(self, tensor: np.ndarray):
         super().__init__(tensor)
 
     @property
     def device(self):
-        return TargetDevice.CPU.value
+        return "CPU"
 
     def is_empty(self) -> bool:
         return self.tensor.size == 0
```

### Comparing `nncf-2.8.1/nncf/parameters.py` & `nncf-2.9.0/nncf/parameters.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,44 +10,49 @@
 # limitations under the License.
 
 from enum import Enum
 
 from nncf.common.utils.api_marker import api
 
 
+class StrEnum(str, Enum):
+    def __str__(self) -> str:
+        return self.value
+
+
 @api(canonical_alias="nncf.TargetDevice")
-class TargetDevice(Enum):
+class TargetDevice(StrEnum):
     """
     Target device architecture for compression.
 
     Compression will take into account the value of this parameter in order to obtain the best performance
     for this type of device.
     """
 
     ANY = "ANY"
     CPU = "CPU"
     GPU = "GPU"
-    VPU = "VPU"
+    NPU = "NPU"
     CPU_SPR = "CPU_SPR"
 
 
 @api(canonical_alias="nncf.ModelType")
-class ModelType(Enum):
+class ModelType(StrEnum):
     """
     Describes the model type the specificity of which will be taken into account during compression.
 
     :param TRANSFORMER: Transformer-based models
         (https://arxiv.org/pdf/1706.03762.pdf)
     """
 
     TRANSFORMER = "transformer"
 
 
 @api(canonical_alias="nncf.DropType")
-class DropType(Enum):
+class DropType(StrEnum):
     """
     Describes the accuracy drop type, which determines how the accuracy drop between
     the original model and the compressed model is calculated.
 
     :param ABSOLUTE: The accuracy drop is calculated as the absolute drop with respect
         to the results of the original model.
     :param RELATIVE: The accuracy drop is calculated relative to the results of
@@ -55,15 +60,15 @@
     """
 
     ABSOLUTE = "absolute"
     RELATIVE = "relative"
 
 
 @api(canonical_alias="nncf.CompressWeightsMode")
-class CompressWeightsMode(Enum):
+class CompressWeightsMode(StrEnum):
     """
     Defines a mode for weight compression.
     :param INT8_SYM: Stands for 8-bit integer symmetric quantization of all weights.
         Weights are quantized symmetrically with a fixed zero point equals to 128.
         https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md#symmetric-quantization
     :param INT8_ASYM: The same as INT8_SYM mode, but weights are quantized to a primary precision asymmetrically
         with a typical non-fixed zero point.
@@ -86,15 +91,15 @@
     INT4_SYM = "int4_sym"
     INT4_ASYM = "int4_asym"
     NF4 = "nf4"
     INT8 = "int8"  # Deprecated mode
 
 
 @api(canonical_alias="nncf.SensitivityMetric")
-class SensitivityMetric(Enum):
+class SensitivityMetric(StrEnum):
     """
     Defines a sensitivity metric for assigning quantization precision to layers. In order to
         preserve the accuracy of the model, the more sensitive layers receives a higher precision.
 
     :param WEIGHT_QUANTIZATION_ERROR: The inverted 8-bit quantization noise. Weights with highest value
         of this metric can be accurately quantized channel-wise to 8-bit. The idea is to leave these weights in 8bit,
         and quantize the rest of layers to 4-bit group-wise. Since group-wise is more accurate than per-channel,
@@ -113,15 +118,15 @@
     HESSIAN_INPUT_ACTIVATION = "hessian_input_activation"
     MEAN_ACTIVATION_VARIANCE = "mean_activation_variance"
     MAX_ACTIVATION_VARIANCE = "max_activation_variance"
     MEAN_ACTIVATION_MAGNITUDE = "mean_activation_magnitude"
 
 
 @api(canonical_alias="nncf.QuantizationMode")
-class QuantizationMode(Enum):
+class QuantizationMode(StrEnum):
     """
     Defines special modes.
     Currently contains only FP8-related modes (https://arxiv.org/pdf/2209.05433.pdf).
 
     :param FP8_E4M3: Mode with 4-bit exponent and 3-bit mantissa.
     :param FP8_E5M2: Mode with 5-bit exponent and 2-bit mantissa.
     """
```

### Comparing `nncf-2.8.1/nncf/quantization/__init__.py` & `nncf-2.9.0/nncf/quantization/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/algorithm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import sys
 from typing import Iterable, List, Optional, Tuple, TypeVar
 
+import nncf
 from nncf.common.factory import NNCFGraphFactory
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.utils import get_number_of_quantized_ops
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_removal import revert_operations_to_floating_point_precision
 from nncf.common.utils.backend import BackendType
@@ -45,15 +46,20 @@
     :return: The backend for accuracy control algorithm.
     """
     if backend == BackendType.OPENVINO:
         from nncf.quantization.algorithms.accuracy_control.openvino_backend import OVAccuracyControlAlgoBackend
 
         return OVAccuracyControlAlgoBackend()
 
-    raise RuntimeError(
+    if backend == BackendType.ONNX:
+        from nncf.quantization.algorithms.accuracy_control.onnx_backend import ONNXAccuracyControlAlgoBackend
+
+        return ONNXAccuracyControlAlgoBackend()
+
+    raise nncf.UnsupportedBackendError(
         f"Cannot create the backend for the accuracy control algorithm because {backend} is not supported."
     )
 
 
 def _create_message(nodes: Iterable[NNCFNode]) -> str:
     names = [f"\t{x.node_name}" for x in nodes]
     return "\n".join(names)
@@ -306,14 +312,16 @@
             current_model = revert_operations_to_floating_point_precision(
                 current_group.operations,
                 current_group.quantizers,
                 previous_model,
                 quantized_model_graph,
                 self.restore_mode,
                 algo_backend.get_op_with_weights_metatypes(),
+                algo_backend.is_node_with_weight,
+                algo_backend.get_weight_tensor_port_ids,
             )
             report.removed_groups.append(current_group)
 
             nncf_logger.debug(
                 f"Removed a block of {len(current_group.quantizers)} quantizers:"
                 f"\n{_create_message(current_group.quantizers)}"
             )
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/backend.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -138,15 +138,15 @@
     @abstractmethod
     def is_node_with_weight(node: NNCFNode) -> bool:
         """
         Checks if the node has a weight or not.
 
         :param node: The node to check.
         :param nncf_graph: The NNCF graph.
-        :return: True` if `node` corresponds to the operation with weights, `False` otherwise.
+        :return: `True` if `node` corresponds to the operation with weights, `False` otherwise.
         """
 
     @staticmethod
     @abstractmethod
     def get_bias_value(node_with_bias: NNCFNode, nncf_graph: NNCFGraph, model: TModel) -> Any:
         """
         Returns the bias value for the biased node.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/evaluator.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/evaluator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Any, Callable, Iterable, List, Optional, Tuple, TypeVar, Union
 
+import nncf
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 from nncf.common.utils.timer import timer
 from nncf.data.dataset import Dataset
 from nncf.quantization.algorithms.accuracy_control.backend import PreparedModel
 
@@ -120,15 +121,22 @@
         backend = get_backend(model)
 
         if backend == BackendType.OPENVINO:
             from nncf.quantization.algorithms.accuracy_control.openvino_backend import OVPreparedModel
 
             return OVPreparedModel(model)
 
-        raise NotImplementedError(f"The `prepare_model()` method is not implemented for the {backend} backend.")
+        if backend == BackendType.ONNX:
+            from nncf.quantization.algorithms.accuracy_control.onnx_backend import ONNXPreparedModel
+
+            return ONNXPreparedModel(model)
+
+        raise NotImplementedError(
+            f"The `prepare_model_for_inference()` method is not implemented for the {backend} backend."
+        )
 
     def validate_prepared_model(
         self, prepared_model: PreparedModel, dataset: Dataset, indices: Optional[List[int]] = None
     ):
         """
         Validates prepared model for inference.
 
@@ -217,15 +225,15 @@
 
         if metric_mode is not None:
             return metric_mode
 
         try:
             metric_value = metric_value if metric_value is None else float(metric_value)
         except Exception as ex:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"Metric value of {type(metric_value)} type was returned from the `validation_fn` "
                 "but the float value is expected."
             ) from ex
 
         convert_to_float_possible = True
         if values_for_each_item is not None:
             try:
@@ -250,15 +258,15 @@
         # | None         | List[List[TTensor]]  | False       |
         # +--------------+----------------------+-------------+
 
         metric_mode = False
         if isinstance(metric_value, float) and (values_for_each_item is None or convert_to_float_possible):
             metric_mode = True
         elif values_for_each_item is not None and not isinstance(values_for_each_item[0], list):
-            raise RuntimeError("Unexpected return value from provided validation function.")
+            raise nncf.InternalError("Unexpected return value from provided validation function.")
 
         return metric_mode
 
     def collect_values_for_each_item_using_prepared_model(
         self, prepared_model: PreparedModel, dataset: Dataset, indices: Optional[List[int]] = None
     ) -> Union[List[float], List[List[TTensor]]]:
         """
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/openvino_backend.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/rank_functions.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/rank_functions.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Callable, List, TypeVar
 
 import numpy as np
 
+import nncf
 from nncf.common.utils.backend import BackendType
 
 TTensor = TypeVar("TTensor")
 
 
 def create_normalized_mse_func(backend: BackendType) -> Callable[[List[TTensor], List[TTensor]], float]:
     """
@@ -24,15 +25,17 @@
 
     :param backend: A backend type.
     :return: The backend-specific implementation of the normalized_nmse.
     """
     if backend == BackendType.OPENVINO:
         return normalized_mse
 
-    raise RuntimeError(f"Could not create backend-specific implementation! {backend} backend is not supported!")
+    raise nncf.UnsupportedBackendError(
+        f"Could not create backend-specific implementation! {backend} backend is not supported!"
+    )
 
 
 def normalized_mse(ref_outputs: List[np.ndarray], approx_outputs: List[np.ndarray]) -> float:
     """
     Calculates normalized mean square error between `ref_outputs` and `approx_outputs`.
     The normalized mean square error is defined as
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/ranker.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/ranker.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 from copy import deepcopy
 from dataclasses import dataclass
 from typing import Any, Callable, List, Optional, TypeVar, Union
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.logging import nncf_logger
+from nncf.common.logging.track_progress import track
 from nncf.common.quantization.quantizer_removal import find_quantizer_nodes_to_cut
 from nncf.common.quantization.quantizer_removal import revert_operations_to_floating_point_precision
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 from nncf.common.utils.timer import timer
 from nncf.data.dataset import Dataset
 from nncf.quantization.advanced_parameters import RestoreMode
@@ -89,21 +90,23 @@
     def find_groups_of_quantizers_to_rank(self, quantized_model_graph: NNCFGraph) -> List[GroupToRank]:
         """
         Finds groups of quantizers to rank.
 
         :param quantized_model_graph: Graph for quantized model.
         :return: List of groups of quantizers to rank.
         """
+        quantizer_metatypes = self._algo_backend.get_quantizer_metatypes()
+
+        if len(quantizer_metatypes) == 2:  # Quantize-Dequantize case
+            # Use only Quantize metatype
+            quantizer_metatypes = quantizer_metatypes[:1]
+
         groups_to_rank = []
         processed = {}
-        quantizers = [
-            x
-            for x in quantized_model_graph.topological_sort()
-            if x.metatype in self._algo_backend.get_quantizer_metatypes()
-        ]
+        quantizers = [x for x in quantized_model_graph.topological_sort() if x.metatype in quantizer_metatypes]
 
         quantized_model_graph_without_shapeof = remove_shapeof_subgraphs(
             deepcopy(quantized_model_graph),
             self._algo_backend.get_shapeof_metatypes(),
             self._algo_backend.get_start_nodes_for_activation_path_tracing(quantized_model_graph),
         )
 
@@ -151,15 +154,14 @@
         ranking_subset_indices = select_subset(
             self._ranking_subset_size,
             reference_values_for_each_item,
             approximate_values_for_each_item,
             self._ranking_fn,
         )
 
-        nncf_logger.info("Calculating ranking score for groups of quantizers")
         with timer():
             # Calculate ranking score for groups of quantizers.
             if self._num_workers > 1:
                 ranking_scores = self._multithreading_calculation_ranking_score(
                     quantized_model,
                     quantized_model_graph,
                     groups_to_rank,
@@ -186,22 +188,24 @@
         quantized_model: TModel,
         quantized_model_graph: NNCFGraph,
         groups_to_rank: List[GroupToRank],
         ranking_subset_indices: List[int],
         reference_values_for_each_item: Union[List[float], List[List[TTensor]]],
     ):
         ranking_scores = []  # ranking_scores[i] is the ranking score for groups_to_rank[i]
-        for current_group in groups_to_rank:
+        for current_group in track(groups_to_rank, description="Calculating ranking scores"):
             modified_model = revert_operations_to_floating_point_precision(
                 current_group.operations,
                 current_group.quantizers,
                 quantized_model,
                 quantized_model_graph,
                 self._restore_mode,
                 self._algo_backend.get_op_with_weights_metatypes(),
+                self._algo_backend.is_node_with_weight,
+                self._algo_backend.get_weight_tensor_port_ids,
             )
 
             prepared_model = self._evaluator.prepare_model(modified_model)
             ranking_score = self._calculate_ranking_score(
                 prepared_model, ranking_subset_indices, reference_values_for_each_item
             )
             ranking_scores.append(float(ranking_score))
@@ -215,22 +219,25 @@
         groups_to_rank: List[GroupToRank],
         ranking_subset_indices: List[int],
         reference_values_for_each_item: Union[List[float], List[List[TTensor]]],
     ):
         ranking_scores = []  # ranking_scores[i] is the ranking score for groups_to_rank[i]
         prepared_model_queue = []
         executor = ThreadPoolExecutor(max_workers=self._num_workers)
+        nncf_logger.info("Calculating ranking scores")
         for idx, current_group in enumerate(groups_to_rank):
             modified_model = revert_operations_to_floating_point_precision(
                 current_group.operations,
                 current_group.quantizers,
                 quantized_model,
                 quantized_model_graph,
                 self._restore_mode,
                 self._algo_backend.get_op_with_weights_metatypes(),
+                self._algo_backend.is_node_with_weight,
+                self._algo_backend.get_weight_tensor_port_ids,
             )
 
             prepared_model_queue.append(executor.submit(self._evaluator.prepare_model, modified_model))
 
             if idx >= (self._num_workers - 1):
                 prepared_model = prepared_model_queue.pop(0).result()
                 ranking_score = self._calculate_ranking_score(
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/accuracy_control/subset_selection.py` & `nncf-2.9.0/nncf/quantization/algorithms/accuracy_control/subset_selection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/algorithm.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/bias_correction/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/bias_correction/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/bias_correction/algorithm.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,20 +10,22 @@
 # limitations under the License.
 
 from collections import defaultdict
 from typing import Any, Dict, List, Optional, Tuple, TypeVar
 
 import numpy as np
 
+import nncf
 from nncf import Dataset
 from nncf import nncf_logger
 from nncf.common.factory import EngineFactory
 from nncf.common.factory import ModelTransformerFactory
 from nncf.common.factory import NNCFGraphFactory
 from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFGraphEdge
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.logging.track_progress import track
 from nncf.common.tensor_statistics.statistic_point import StatisticPoint
@@ -96,15 +98,15 @@
         self.nncf_graph = None
         self._backend_entity = None
         self._collected_stat_inputs_map = {}
         self._fp_inputs = defaultdict(list)
         self._algorithm_key = f"BC_{hash(self)}"
 
         if self.apply_for_all_nodes:
-            raise RuntimeError("BiasCorrection algorithm does not support apply_for_all_nodes=True yet")
+            raise nncf.InternalError("BiasCorrection algorithm does not support apply_for_all_nodes=True yet")
 
     @property
     def available_backends(self) -> List[BackendType]:
         return [BackendType.ONNX, BackendType.OPENVINO]
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
@@ -118,15 +120,15 @@
 
             self._backend_entity = ONNXBiasCorrectionAlgoBackend()
         elif model_backend == BackendType.OPENVINO:
             from nncf.quantization.algorithms.bias_correction.openvino_backend import OVBiasCorrectionAlgoBackend
 
             self._backend_entity = OVBiasCorrectionAlgoBackend()
         else:
-            raise RuntimeError(
+            raise nncf.UnsupportedBackendError(
                 "Cannot return backend-specific entity because {} is not supported!".format(model_backend.value)
             )
 
     def apply(
         self,
         model: TModel,
         graph: NNCFGraph,
@@ -140,17 +142,15 @@
         model_copy = copy_model(model)
         graph_copy = NNCFGraphFactory.create(model_copy)
         model_copy = self._backend_entity.remove_fq_from_inputs(model_copy, graph_copy)
         nncf_graph = NNCFGraphFactory.create(model_copy)
 
         nodes_with_bias = []
         for node in nncf_graph.topological_sort():
-            if self._backend_entity.is_node_with_bias(node, nncf_graph) and self._backend_entity.is_quantized_weights(
-                node, nncf_graph
-            ):
+            if self._is_node_correctable(node, nncf_graph):
                 nodes_with_bias.append(node)
 
         # We pre-collect information about the subgraph we need in order
         # to collect statistics for the change in the bias of each layer.
         # Also here we collect a list of layers that depend on the current one.
 
         # The collected information contains lists of input and output layers,
@@ -200,87 +200,104 @@
 
             # Also, we need to remove unnecessary statistics that we don't need anymore,
             # to reduce memory usage during the algorithm's pipeline.
             self._remove_unnecessary_stats(position, subgraphs_data)
 
         return main_model_transformer.transform(main_transformations_layout)
 
+    def _is_node_correctable(self, node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        """
+        Verify if node bias can be corrected or not.
+
+        :param node: NNCFNode instance to verify.
+        :param nncf_graph: NNCFGraph instance.
+        :return: Boolean value as the result.
+        """
+        return self._backend_entity.is_node_with_bias(node, nncf_graph) and self._backend_entity.is_quantized_weights(
+            node, nncf_graph
+        )
+
     def _get_subgraph_data_for_node(self, node: NNCFNode, nncf_graph: NNCFGraph) -> Dict[str, List[str]]:
         """
         This method collects necessary data for the specified node and its subgraph.
         This data contains the nodes (NNCFNode) for the subgraph building
         and statistics collection (for the next step).
 
         :param node: NNCFNode instance. This is the main node with bias that would be corrected (or not).
         :param nncf_graph: NNCFGraph instance for graph analysis.
         :return: A dict with the list of the nodes for the subgraph input and statistics collection.
         """
-        statistic_nodes, subgraph_input_nodes, subgraph_output_nodes, subgraph_output_ids = [], [], [], []
+        statistic_nodes = []
+        subgraph_input_ids, subgraph_output_ids = [], []
 
-        def fill_statistic_nodes(node):
-            # A small hack to speed up graph traversal.
-            if node in statistic_nodes or node in visited_nodes:
+        def fill_subgraph_output_ids(edge: NNCFGraphEdge):
+            node = edge.to_node
+            if node in visited_nodes:
                 return
             visited_nodes.append(node)
 
+            input_id = (node.node_name, edge.input_port_id)
+            output_id = (edge.from_node.node_name, edge.output_port_id)
+
             # If we found a node with bias, we have to collect it as a statistic node,
             # and its input for _collected_stat_inputs_map,
             # which will be used during the collection of statistics for the next node.
-            if self._backend_entity.is_node_with_bias(node, nncf_graph) and self._backend_entity.is_quantized_weights(
-                node, nncf_graph
-            ):
-                statistic_nodes.append(node)
-                activation_node, output_port_id = self._get_activation_node_and_port(node, nncf_graph)
-                subgraph_output_nodes.append(activation_node)
-
-                output_id = (activation_node.node_name, output_port_id)
+            if self._is_node_correctable(node, nncf_graph):
                 subgraph_output_ids.append(output_id)
-                self._collected_stat_inputs_map[node.node_name] = output_id
+                self._collected_stat_inputs_map[input_id] = output_id
+
+                statistic_nodes.append(edge.from_node)
                 return
 
-            for next_node in nncf_graph.get_next_nodes(node):
-                fill_statistic_nodes(next_node)
+            for output_edge in nncf_graph.get_output_edges(node):
+                fill_subgraph_output_ids(output_edge)
 
-        def fill_subgraph_input_nodes(node):
-            # A small hack to speed up graph traversal.
-            if node in subgraph_input_nodes or node in visited_nodes:
-                return
-            visited_nodes.append(node)
+        def fill_subgraph_input_ids(edge: NNCFGraphEdge):
+            node = edge.from_node
+            input_id = (edge.to_node.node_name, edge.input_port_id)
 
             # Since we need to find the inputs for the subgraph,
             # we can take only those layers for which we have already collected statistics.
-            if node.node_name in self._collected_stat_inputs_map and node not in statistic_nodes:
-                subgraph_input_nodes.append(node)
+            if input_id in self._collected_stat_inputs_map:
+                activation_id = self._collected_stat_inputs_map[input_id]
+                if activation_id not in subgraph_output_ids:
+                    subgraph_input_ids.append(input_id)
+                    return
+
+            if node in visited_nodes:
                 return
+            visited_nodes.append(node)
 
-            for previous_node in nncf_graph.get_previous_nodes(node):
-                fill_subgraph_input_nodes(previous_node)
+            for input_edge in nncf_graph.get_input_edges(node):
+                fill_subgraph_input_ids(input_edge)
 
         # First, we need to find out the nodes with bias that follow by main node.
         # To collect statistics for next nodes.
         visited_nodes = []
-        for next_node in nncf_graph.get_next_nodes(node):
-            fill_statistic_nodes(next_node)
+        for output_edge in nncf_graph.get_output_edges(node):
+            fill_subgraph_output_ids(output_edge)
 
         # We then need to find nodes for which statistics have already been collected,
         # to use them as inputs for the subgraph.
         statistic_nodes = statistic_nodes if statistic_nodes else nncf_graph.get_next_nodes(node)
         visited_nodes = []
         for stat_node in statistic_nodes:
-            fill_subgraph_input_nodes(stat_node)
+            for input_edge in nncf_graph.get_input_edges(stat_node):
+                fill_subgraph_input_ids(input_edge)
+
+        if not subgraph_output_ids:
+            for edge in nncf_graph.get_output_edges(node):
+                next_node_name = edge.to_node.node_name
+                if NNCFGraphNodeType.OUTPUT_NODE not in next_node_name:
+                    subgraph_output_ids.append((edge.to_node.node_name, OUTPUT_PORT_OF_NODE))
 
         # In case the outputs were not found during the collection of statistics nodes,
         # we use the latter as the outputs of the subgraph.
-        subgraph_output_nodes = subgraph_output_nodes if subgraph_output_nodes else statistic_nodes
-        subgraph_output_names = [
-            n.node_name for n in subgraph_output_nodes if NNCFGraphNodeType.OUTPUT_NODE not in n.node_name
-        ]
         subgraph_data = {
-            "subgraph_input_names": set(n.node_name for n in subgraph_input_nodes),
-            "subgraph_output_names": set(subgraph_output_names),
+            "subgraph_input_ids": set(subgraph_input_ids),
             "subgraph_output_ids": set(subgraph_output_ids),
         }
 
         return subgraph_data
 
     def _prepare_subgraph(self, node: NNCFNode, model: TModel, nncf_graph: NNCFGraph, subgraph_data: Dict) -> TModel:
         """
@@ -289,15 +306,15 @@
         :param node: NNCFNode instance for the current layer.
         :param model: Backend-specific model instance.
         :param nncf_graph: Instance of NNCFGraph.
         :param subgraph_data: A dictionary with the layers for the graph building.
         :return: Backend-specific subgraph extracted from the model.
         """
         extracted_model = self.extract_model(
-            model, subgraph_data["subgraph_input_names"], subgraph_data["subgraph_output_names"]
+            model, subgraph_data["subgraph_input_ids"], subgraph_data["subgraph_output_ids"]
         )
 
         transformation_layout = TransformationLayout()
         model_transformer = ModelTransformerFactory.create(extracted_model)
 
         # For layers with weights, there is only one output port - 0.
         statistic_point = self._backend_entity.target_point(
@@ -318,28 +335,28 @@
         :param statistic_points: StatisticPointsContainer instance.
         :return: List of the dictionaries with the input data.
         """
         feed_dicts = []
         statistics_size = self.subset_size
         statistics_per_input = {}
 
-        for input_node_name in subgraph_data["subgraph_input_names"]:
-            input_tensor_name = self._backend_entity.get_input_name(model, input_node_name)
-            activation_name, port_id = self._collected_stat_inputs_map[input_node_name]
-            input_fp = self._get_fp_inputs(statistic_points, node_name=activation_name, port_id=port_id)
+        for input_node_name, input_port_id in subgraph_data["subgraph_input_ids"]:
+            input_tensor_name = self._backend_entity.get_input_name(model, input_node_name, input_port_id)
+            activation_name, output_port_id = self._collected_stat_inputs_map[(input_node_name, input_port_id)]
+            input_fp = self._get_fp_inputs(statistic_points, node_name=activation_name, port_id=output_port_id)
             statistics_per_input[input_tensor_name] = input_fp
             statistics_size = min(statistics_size, len(input_fp))
 
         for stat_id in range(statistics_size):
             feed_dict = {}
-            for input_node_name in subgraph_data["subgraph_input_names"]:
-                input_tensor_name = self._backend_entity.get_input_name(model, input_node_name)
+            for input_node_name, input_port_id in subgraph_data["subgraph_input_ids"]:
+                input_tensor_name = self._backend_entity.get_input_name(model, input_node_name, input_port_id)
                 # Since we do not use as inputs the layers from which the statistics are gathered,
                 # but those that follow them, we need to take this into account when creating feed dicts.
-                activation_name, port_id = self._collected_stat_inputs_map[input_node_name]
+                activation_name, _ = self._collected_stat_inputs_map[(input_node_name, input_port_id)]
                 feed_dict[input_tensor_name] = statistics_per_input[input_tensor_name][stat_id]
             feed_dicts.append(feed_dict)
         return feed_dicts
 
     def _compute_bias_shift(
         self, node: NNCFNode, model: TModel, feed_dicts: List, statistic_points: StatisticPointsContainer
     ) -> np.ndarray:
@@ -371,16 +388,20 @@
         Calculates bias shift magnitude based on the current and updated values.
 
         :param current_bias_value: Initial bias value.
         :param updated_bias_value: Updated bias value.
         :return: Magnitude between original and updated bias values.
         """
         bias_shift_magnitude = np.inf
-        if np.count_nonzero(current_bias_value == 0) == 0:
-            bias_shift_magnitude = np.max(np.abs((updated_bias_value - current_bias_value) / current_bias_value))
+        bias_shift_magnitude = np.max(
+            np.abs(
+                (updated_bias_value - current_bias_value)
+                / (current_bias_value + np.finfo(current_bias_value.dtype).min)
+            )
+        )
         return bias_shift_magnitude
 
     def _correct_bias(self, model: TModel, bias_correction_command: TransformationCommand) -> TModel:
         """
         Returns the model (which can be represented as subgraph) with the updated bias value for the current layer.
 
         :param model: Backend-specific model.
@@ -415,20 +436,20 @@
         :param position: Zero-based position of the current node that was corrected.
         :param subgraphs_data: A dictionary of the data (input & statistic node names) that
             uses for the sub-graphs creation.
         """
         # Collects list of the statistics that needed for the future layers.
         needed_stats_list = []
         for i in range(position + 1, len(subgraphs_data)):
-            input_names = subgraphs_data[i]["subgraph_input_names"]
-            needed_stats_list.extend([self._collected_stat_inputs_map[name][0] for name in input_names])
+            input_ids = subgraphs_data[i]["subgraph_input_ids"]
+            needed_stats_list.extend([self._collected_stat_inputs_map[input_id][0] for input_id in input_ids])
 
-        node_inputs_name = subgraphs_data[position]["subgraph_input_names"]
-        for node_input_name in node_inputs_name:
-            activation_name, port_id = self._collected_stat_inputs_map[node_input_name]
+        node_inputs_ids = subgraphs_data[position]["subgraph_input_ids"]
+        for node_input_id in node_inputs_ids:
+            activation_name, port_id = self._collected_stat_inputs_map[node_input_id]
             input_id = (activation_name, port_id)
             if activation_name not in needed_stats_list and input_id in self._fp_inputs:
                 nncf_logger.debug(f"Dropped {activation_name} output statistics.")
                 self._fp_inputs[input_id] = []
 
     def _get_fp_inputs(self, statistic_points: StatisticPointsContainer, node_name: str, port_id: int) -> np.ndarray:
         """
@@ -483,22 +504,20 @@
             node_name, output_filter_func, self._algorithm_key
         ):
             output_fp.extend(tensor_collector.get_statistics().mean_values)
         return np.array(output_fp)
 
     def get_statistic_points(self, model: TModel, graph: NNCFGraph) -> StatisticPointsContainer:
         self._set_backend_entity(model)
-        model_copy = self._backend_entity.remove_fq_from_inputs(copy_model(model), graph)
-        nncf_graph = NNCFGraphFactory.create(model_copy)
         statistic_container = StatisticPointsContainer()
 
         nodes_with_bias = [
-            node for node in nncf_graph.topological_sort() if self._backend_entity.is_node_with_bias(node, nncf_graph)
+            node for node in graph.topological_sort() if self._backend_entity.is_node_with_bias(node, graph)
         ]
-        model_inputs = nncf_graph.get_input_nodes()
+        model_inputs = graph.get_input_nodes()
 
         # Collection of statistics after layers where biases will be corrected.
         for node in nodes_with_bias:
             node_name = node.node_name
             channel_axis = node.metatype.output_channel_axis
 
             # For layers with weights, there is only one output port - 0.
@@ -511,66 +530,61 @@
             statistic_container.add_statistic_point(
                 StatisticPoint(
                     target_point=statistic_point, tensor_collector=stat_collector, algorithm=self._algorithm_key
                 )
             )
 
         # We must collect the nodes with biases following the model inputs.
-        biased_after_input_nodes = self._get_biased_after_nodes(nncf_graph, model_inputs, model_copy)
+        biased_after_input_nodes = self._get_biased_after_nodes(graph, model_inputs, model)
 
         for biased_after_input_node in biased_after_input_nodes:
             # We need to collect activation input to register it for the biased layer as the layer with statistics.
-            activation_node, output_port_id = self._get_activation_node_and_port(biased_after_input_node, nncf_graph)
-            activation_node_name = activation_node.node_name
+            activation_port = self._backend_entity.get_activation_port_id(node, graph)
+            edge = graph.get_input_edges(biased_after_input_node)[activation_port]
 
-            self._collected_stat_inputs_map[biased_after_input_node.node_name] = (activation_node_name, output_port_id)
+            input_id = (biased_after_input_node.node_name, edge.input_port_id)
+            output_id = (edge.from_node.node_name, edge.output_port_id)
+
+            if edge.from_node.node_name in statistic_container:
+                continue
+
+            self._collected_stat_inputs_map[input_id] = output_id
             statistic_point = self._backend_entity.target_point(
-                TargetType.POST_LAYER_OPERATION, activation_node_name, port_id=output_port_id
+                TargetType.POST_LAYER_OPERATION, edge.from_node.node_name, port_id=edge.output_port_id
             )
             stat_collector = self._backend_entity.raw_statistic_collector(num_samples=self.subset_size)
             statistic_container.add_statistic_point(
                 StatisticPoint(
                     target_point=statistic_point, tensor_collector=stat_collector, algorithm=self._algorithm_key
                 )
             )
 
         # Then we need also to collect model input statistics to prevent cases when nodes with bias have no input data.
         for input_node in model_inputs:
-            # We assume that input node has only one output port
-            input_name = input_node.node_name
-            if input_name in statistic_container:
+            # We need to map all input node outputs as nodes with statistic
+            for edge in graph.get_output_edges(input_node):
+                input_id = (edge.to_node.node_name, edge.input_port_id)
+                output_id = (input_node.node_name, edge.output_port_id)
+                self._collected_stat_inputs_map[input_id] = output_id
+
+            if input_node.node_name in statistic_container:
                 continue
-            for next_layer in nncf_graph.get_next_nodes(input_node):
-                self._collected_stat_inputs_map[next_layer.node_name] = (input_node.node_name, OUTPUT_PORT_OF_NODE)
+
             statistic_point = self._backend_entity.target_point(
                 TargetType.POST_LAYER_OPERATION, input_node.node_name, port_id=OUTPUT_PORT_OF_NODE
             )
             stat_collector = self._backend_entity.raw_statistic_collector(num_samples=self.subset_size)
             statistic_container.add_statistic_point(
                 StatisticPoint(
                     target_point=statistic_point, tensor_collector=stat_collector, algorithm=self._algorithm_key
                 )
             )
 
         return statistic_container
 
-    def _get_activation_node_and_port(self, node: NNCFNode, nncf_graph: NNCFGraph) -> Tuple[NNCFNode, int]:
-        """
-        This method returns the activation layer and corresponding port id for the node.
-
-        :param node: NNCFGraph node for which the activation is sought.
-        :param nncf_graph: NNCFGraph instance with the node.
-        :return: Tuple with the activation node and port id.
-        """
-        activation_port = self._backend_entity.get_activation_port_id(node, nncf_graph)
-        activation_edge = nncf_graph.get_input_edges(node)[activation_port]
-        activation_node = activation_edge.from_node
-        port_id = activation_edge.output_port_id
-        return activation_node, port_id
-
     def _get_biased_after_nodes(self, nncf_graph: NNCFGraph, nodes: List[NNCFNode], model: TModel) -> List[NNCFNode]:
         """
         This method finds and returns nodes with the bias in the model that follows after the input nodes.
 
         :param nncf_graph: NNCFGraph instance.
         :param nodes: List of the model inputs as NNCFNodes.
         :param model: TModel instance.
@@ -607,21 +621,23 @@
             visited_nodes = []
             nncf_logger.debug(f"Filtering biased nodes after {biased_node.node_name} layer.")
             for next_node in nncf_graph.get_next_nodes(biased_node):
                 traverse_to_biased(next_node, condition_container=dependant_nodes)
 
         return list(biased_nodes - dependant_nodes)
 
-    def extract_model(self, model: TModel, input_node_names: List[str], output_node_names: List[str]) -> TModel:
+    def extract_model(
+        self, model: TModel, input_node_ids: List[Tuple[str, int]], output_node_ids: List[Tuple[str, int]]
+    ) -> TModel:
         """
         Returns the backend-specific model that bounded by the specified input & output layers.
 
         :param model: Backend-specific model.
-        :param input_node_names: List with the input node names.
-        :param output_node_names: List with the output node names.
+        :param input_node_ids: List with the input node IDs.
+        :param output_node_ids: List with the output node IDs.
         :return: Extracted backend-specific model.
         """
         transformation_layout = TransformationLayout()
         model_transformer = ModelTransformerFactory.create(model)
-        model_extraction_command = self._backend_entity.model_extraction_command(input_node_names, output_node_names)
+        model_extraction_command = self._backend_entity.model_extraction_command(input_node_ids, output_node_ids)
         transformation_layout.register(model_extraction_command)
         return model_transformer.transform(transformation_layout)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/bias_correction/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/bias_correction/backend.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
-from typing import List, Optional, TypeVar
+from typing import List, Optional, Tuple, TypeVar
 
 import numpy as np
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
@@ -56,20 +56,24 @@
         :param node: The node for which bias should be updated.
         :param bias_value: New value for the bias.
         :return: Backend-specific command to update bias value.
         """
 
     @staticmethod
     @abstractmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> TransformationCommand:
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> TransformationCommand:
         """
         Returns backend-specific command to extract sub-model based on input & output names.
 
-        :param inputs: List of the input names for sub-model beginning.
-        :param outputs: List of the output names for sub-model end.
+        :param input_ids: List of the input IDs: pairs of node names and correspondent input port ids.
+            Each pair denotes the sub-graph beginning.
+        :param output_ids: List of the output IDs: pairs of node names and correspondent output port ids.
+            Each pair denotes the sub-graph ending.
         :return: Backend-specific TransformationCommand for the model extraction.
         """
 
     @staticmethod
     @abstractmethod
     def output_insertion_command(nncf_graph: NNCFGraph, target_point: TargetPoint) -> TransformationCommand:
         """
@@ -143,32 +147,33 @@
         :param model: Backend-specific model for the initializer finding.
         :param nncf_graph: NNCFGraph instance with the node.
         :return: Bias value in the NumPy format.
         """
 
     @staticmethod
     @abstractmethod
-    def get_input_name(model: TModel, node_name: str) -> str:
+    def get_input_name(model: TModel, node_name: str, input_port_id: int) -> str:
         """
         Returns input tensor name for the specific node.
 
         :param model: Backend-specific model for the initializer finding.
         :param node_name: Name of the backend-specific node.
+        :param input_port_id: Port Id for input.
         :return: Input tensor name.
         """
 
     @staticmethod
     @abstractmethod
-    def get_output_name(model: TModel, node_name: str, output_id: int) -> str:
+    def get_output_name(model: TModel, node_name: str, output_port_id: int) -> str:
         """
         Returns output tensor name for the specific node.
 
         :param model: Backend-specific model.
         :param node_name: Name of the backend-specific node.
-        :param output_id: Port Id for output.
+        :param output_port_id: Port Id for output.
         :return: Output tensor name.
         """
 
     @staticmethod
     @abstractmethod
     def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/bias_correction/onnx_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,58 +13,52 @@
 
 import numpy as np
 import onnx
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.onnx.graph.model_utils import remove_fq_from_inputs
 from nncf.onnx.graph.node_utils import get_bias_value
 from nncf.onnx.graph.node_utils import is_any_weight_quantized
 from nncf.onnx.graph.node_utils import is_node_with_bias
 from nncf.onnx.graph.onnx_helper import get_name_to_node_map
 from nncf.onnx.graph.transformations.command_creation import create_bias_correction_command
-from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
+from nncf.onnx.graph.transformations.commands import ONNXInitializerUpdateCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
-from nncf.onnx.graph.transformations.commands import ONNXNullBiasInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXOutputInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXTargetPoint
-from nncf.onnx.statistics.collectors import ONNXMeanStatisticCollector
 from nncf.onnx.statistics.collectors import ONNXNNCFCollectorTensorProcessor
-from nncf.onnx.statistics.collectors import ONNXRawStatisticCollector
+from nncf.onnx.statistics.collectors import get_mean_statistic_collector
+from nncf.onnx.statistics.collectors import get_raw_stat_collector
 from nncf.onnx.tensor import ONNXNNCFTensor
 from nncf.quantization.algorithms.bias_correction.backend import BiasCorrectionAlgoBackend
 
 
 class ONNXBiasCorrectionAlgoBackend(BiasCorrectionAlgoBackend):
     @property
     def tensor_processor(self) -> ONNXNNCFCollectorTensorProcessor:
         return ONNXNNCFCollectorTensorProcessor
 
-    @property
-    def types_to_insert_bias(self):
-        return []
-
     @staticmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> ONNXTargetPoint:
         return ONNXTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
     def create_bias_correction_command(
         node: NNCFNode, bias_value: np.ndarray, nncf_graph: NNCFGraph
-    ) -> ONNXBiasCorrectionCommand:
+    ) -> ONNXInitializerUpdateCommand:
         return create_bias_correction_command(node, bias_value)
 
     @staticmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> ONNXModelExtractionCommand:
-        return ONNXModelExtractionCommand(inputs, outputs)
-
-    @staticmethod
-    def create_bias_insertion_command(node: NNCFNode) -> ONNXNullBiasInsertionCommand:
-        return ONNXNullBiasInsertionCommand(node)
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> ONNXModelExtractionCommand:
+        return ONNXModelExtractionCommand(input_ids, output_ids)
 
     @staticmethod
     def output_insertion_command(nncf_graph: NNCFGraph, target_point: ONNXTargetPoint) -> ONNXOutputInsertionCommand:
         nncf_input_node_next_nodes = {}
         for input_node in nncf_graph.get_input_nodes():
             next_nodes = nncf_graph.get_next_nodes(input_node)
             nncf_input_node_next_nodes[input_node.node_name] = [node.node_name for node in next_nodes]
@@ -72,42 +66,42 @@
 
     @staticmethod
     def mean_statistic_collector(
         channel_axis: int,
         inplace: bool,
         num_samples: Optional[int] = None,
         window_size: Optional[int] = None,
-    ) -> ONNXMeanStatisticCollector:
-        return ONNXMeanStatisticCollector(channel_axis, num_samples, window_size)
+    ) -> TensorCollector:
+        return get_mean_statistic_collector(num_samples, channel_axis, window_size, inplace)
 
     @staticmethod
-    def raw_statistic_collector(num_samples: Optional[int] = None) -> ONNXMeanStatisticCollector:
-        return ONNXRawStatisticCollector(num_samples)
+    def raw_statistic_collector(num_samples: int = None) -> TensorCollector:
+        return get_raw_stat_collector(num_samples)
 
     @staticmethod
     def process_model_output(raw_data: Dict, output_name: str) -> ONNXNNCFTensor:
         return ONNXNNCFTensor(raw_data[output_name])
 
     @staticmethod
     def get_activation_port_id(node: NNCFNode, nncf_graph: NNCFGraph) -> Tuple[int, int]:
         return 0
 
     @staticmethod
     def get_bias_value(node: NNCFNode, model: onnx.ModelProto, nncf_graph: NNCFGraph) -> np.ndarray:
         return get_bias_value(node, model)
 
     @staticmethod
-    def get_input_name(model: onnx.ModelProto, node_name: str) -> str:
+    def get_input_name(model: onnx.ModelProto, node_name: str, input_port_id: int) -> str:
         node_mapping = get_name_to_node_map(model)
-        return node_mapping[node_name].input[0]
+        return node_mapping[node_name].input[input_port_id]
 
     @staticmethod
-    def get_output_name(model: onnx.ModelProto, node_name: str, output_id: int) -> List[str]:
+    def get_output_name(model: onnx.ModelProto, node_name: str, output_port_id: int) -> str:
         node_mapping = get_name_to_node_map(model)
-        return node_mapping[node_name].output[output_id]
+        return node_mapping[node_name].output[output_port_id]
 
     @staticmethod
     def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         return is_any_weight_quantized(node, nncf_graph)
 
     @staticmethod
     def is_node_with_bias(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/bias_correction/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/bias_correction/openvino_backend.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,30 +1,32 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, List, Optional
+from typing import Dict, List, Optional, Tuple
 
 import numpy as np
 import openvino.runtime as ov
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.openvino.graph.metatypes.groups import FAKE_QUANTIZE_OPERATIONS
 from nncf.openvino.graph.model_utils import remove_fq_from_inputs
 from nncf.openvino.graph.node_utils import get_bias_value
+from nncf.openvino.graph.node_utils import get_parameter_node_name
+from nncf.openvino.graph.node_utils import get_result_node_name
 from nncf.openvino.graph.node_utils import is_node_with_bias
 from nncf.openvino.graph.transformations.command_creation import OVCommandCreator
 from nncf.openvino.graph.transformations.commands import OVBiasCorrectionCommand
 from nncf.openvino.graph.transformations.commands import OVModelExtractionCommand
 from nncf.openvino.graph.transformations.commands import OVOutputInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVTargetPoint
 from nncf.openvino.statistics.collectors import OVNNCFCollectorTensorProcessor
@@ -46,16 +48,18 @@
     @staticmethod
     def create_bias_correction_command(
         node: NNCFNode, bias_value: np.ndarray, nncf_graph: NNCFGraph
     ) -> OVBiasCorrectionCommand:
         return OVCommandCreator.create_command_to_update_bias(node, bias_value, nncf_graph)
 
     @staticmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> OVModelExtractionCommand:
-        return OVModelExtractionCommand(inputs, outputs)
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> OVModelExtractionCommand:
+        return OVModelExtractionCommand(input_ids, output_ids)
 
     @staticmethod
     def output_insertion_command(nncf_graph: NNCFGraph, target_point: OVTargetPoint) -> OVOutputInsertionCommand:
         return OVOutputInsertionCommand(target_point)
 
     @staticmethod
     def mean_statistic_collector(
@@ -84,39 +88,20 @@
         return activation_ports[0]
 
     @staticmethod
     def get_bias_value(node: NNCFNode, model: ov.Model, nncf_graph: NNCFGraph) -> np.ndarray:
         return get_bias_value(node, nncf_graph, model)
 
     @staticmethod
-    def get_input_name(model: ov.Model, node_name: str) -> str:
-        ops_dict = {op.get_friendly_name(): op for op in model.get_ops()}
+    def get_input_name(model: ov.Model, node_name: str, input_port_id: int) -> str:
+        return get_parameter_node_name(node_name, input_port_id)
 
-        model_input_names = []
-        for tensor in model.inputs:
-            model_input_names.extend(tensor.get_names())
-        if node_name in model_input_names:
-            return node_name
-
-        for input_port in ops_dict[node_name].inputs():
-            input_node = input_port.get_source_output().get_node()
-            if input_node.get_type_name() == "Parameter":
-                return input_port.get_tensor().get_any_name()
-        raise RuntimeError(f"Input layer not found for {node_name}")
-
-    @staticmethod
-    def get_output_name(model: ov.Model, node_name: str, output_id: int) -> str:
-        ops_dict = {op.get_friendly_name(): op for op in model.get_ops()}
-
-        output_port = ops_dict[node_name].output(output_id)
-        for output_input_port in output_port.get_target_inputs():
-            output_node = output_input_port.get_node()
-            if output_node.get_type_name() == "Result":
-                return output_port.get_any_name()
-        raise RuntimeError(f"Output layer not found for {node_name}")
+    @staticmethod
+    def get_output_name(model: ov.Model, node_name: str, output_port_id: int) -> str:
+        return get_result_node_name(node_name, output_port_id)
 
     @staticmethod
     def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         if node.layer_attributes is None:
             return False
         const_port_ids = node.layer_attributes.get_const_port_ids()
         assert len(const_port_ids) == 1
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/backend.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/channel_alignment/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/channel_alignment/openvino_backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from typing import Any, Tuple
 
 import numpy as np
 import openvino.runtime as ov
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.tensor_statistics.collectors import ReductionAxes
 from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.experimental.common.tensor_statistics.collectors import MedianAggregator
@@ -106,15 +107,15 @@
     @staticmethod
     def get_dims_descriptor(node: NNCFNode) -> LayoutDescriptor:
         if node.metatype in CONV_OPERATIONS:
             weights_layout = get_conv_weights_layout_from_node(node=node)
         elif node.metatype == OVMatMulMetatype:
             weights_layout = get_linear_weights_layout_from_node(node=node)
         else:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"Metatype {node.metatype} of node {node.node_name} dimensions description retrieving is not supported"
             )
 
         if OVLayoutElem.GROUPS in weights_layout:
             # Using groups dim as output channels dim for ChannelAlignment algorithm
             # TODO(dlyakhov) support group convolutions with groups number not in [1, out_channels]
             return LayoutDescriptor(
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/post_training/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from math import inf
 from typing import Any, Dict, List, Optional, Tuple, TypeVar, Union
 
+import nncf
 from nncf import Dataset
 from nncf.common.factory import EngineFactory
 from nncf.common.factory import ModelTransformerFactory
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
@@ -84,15 +85,15 @@
         self.apply_for_all_nodes = apply_for_all_nodes
         self.inplace_statistics = inplace_statistics
         self.backend_params = backend_params
         self._backend_entity = None
         self._algorithm_key = f"FBC_{hash(self)}"
 
         if self.apply_for_all_nodes:
-            raise RuntimeError("FastBiasCorrection algorithm does not support apply_for_all_nodes=True yet")
+            raise nncf.InternalError("FastBiasCorrection algorithm does not support apply_for_all_nodes=True yet")
 
     @property
     def available_backends(self) -> List[BackendType]:
         return [BackendType.ONNX, BackendType.OPENVINO, BackendType.TORCH]
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
@@ -112,15 +113,15 @@
 
             self._backend_entity = OVFastBiasCorrectionAlgoBackend()
         elif model_backend == BackendType.TORCH:
             from nncf.quantization.algorithms.fast_bias_correction.torch_backend import PTFastBiasCorrectionAlgoBackend
 
             self._backend_entity = PTFastBiasCorrectionAlgoBackend()
         else:
-            raise RuntimeError(
+            raise nncf.UnsupportedBackendError(
                 "Cannot return backend-specific entity because {} is not supported!".format(model_backend.value)
             )
 
     def apply(
         self,
         model: TModel,
         graph: NNCFGraph,
@@ -270,15 +271,15 @@
         """
         Extracts sub-model using backend-specific ModelTransformer.
 
         :param model_transformer: Backend-specific ModelTransformer.
         :param node_name: Name of the node that should be a center of the sub-model.
         :return: Backend-specific sub-model.
         """
-        model_extraction_command = self._backend_entity.model_extraction_command([node_name], [node_name])
+        model_extraction_command = self._backend_entity.model_extraction_command([(node_name, 0)], [(node_name, 0)])
         me_transformation_layout = TransformationLayout()
         me_transformation_layout.register(model_extraction_command)
         extracted_model = model_transformer.transform(me_transformation_layout)
         return extracted_model
 
     def _add_statistic_point(self, container: StatisticPointsContainer, point: TargetPoint, axis: int) -> None:
         """
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/backend.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -53,20 +53,24 @@
         :param bias_value: New value for the bias.
         :param nncf_graph: NNCFGraph instance that contains the node.
         :return: Backend-specific command to update bias value.
         """
 
     @staticmethod
     @abstractmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> TransformationCommand:
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> TransformationCommand:
         """
         Returns backend-specific command to extract sub-model based on input & output names.
 
-        :param inputs: List of the input names for sub-model beginning.
-        :param outputs: List of the output names for sub-model end.
+        :param input_ids: List of the input IDs: pairs of node names and correspondent input port ids.
+            Each pair denotes the sub-graph beginning.
+        :param output_ids: List of the output IDs: pairs of node names and correspondent output port ids.
+            Each pair denotes the sub-graph ending.
         :return: Backend-specific TransformationCommand for the model extraction.
         """
 
     @staticmethod
     @abstractmethod
     def mean_statistic_collector(
         channel_axis: int,
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,58 +13,52 @@
 
 import numpy as np
 import onnx
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.experimental.tensor import Tensor
 from nncf.onnx.graph.node_utils import get_bias_value
 from nncf.onnx.graph.node_utils import is_any_weight_quantized
 from nncf.onnx.graph.node_utils import is_node_with_bias
 from nncf.onnx.graph.transformations.command_creation import create_bias_correction_command
-from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
+from nncf.onnx.graph.transformations.commands import ONNXInitializerUpdateCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
-from nncf.onnx.graph.transformations.commands import ONNXNullBiasInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXTargetPoint
-from nncf.onnx.statistics.collectors import ONNXMeanStatisticCollector
+from nncf.onnx.statistics.collectors import get_mean_statistic_collector
 from nncf.quantization.algorithms.fast_bias_correction.backend import FastBiasCorrectionAlgoBackend
 
 
 class ONNXFastBiasCorrectionAlgoBackend(FastBiasCorrectionAlgoBackend):
-    @property
-    def types_to_insert_bias(self):
-        return []
-
     @staticmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> ONNXTargetPoint:
         return ONNXTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
-    def create_bias_insertion_command(node: NNCFNode) -> ONNXNullBiasInsertionCommand:
-        return ONNXNullBiasInsertionCommand(node)
-
-    @staticmethod
     def create_bias_correction_command(
         node: NNCFNode, bias_value: Tensor, nncf_graph: NNCFGraph
-    ) -> ONNXBiasCorrectionCommand:
+    ) -> ONNXInitializerUpdateCommand:
         return create_bias_correction_command(node, bias_value.data)
 
     @staticmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> ONNXModelExtractionCommand:
-        return ONNXModelExtractionCommand(inputs, outputs)
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> ONNXModelExtractionCommand:
+        return ONNXModelExtractionCommand(input_ids, output_ids)
 
     @staticmethod
     def mean_statistic_collector(
         channel_axis: int,
         inplace: bool,
         num_samples: Optional[int] = None,
         window_size: Optional[int] = None,
-    ) -> ONNXMeanStatisticCollector:
-        return ONNXMeanStatisticCollector(channel_axis, num_samples, window_size)
+    ) -> TensorCollector:
+        return get_mean_statistic_collector(num_samples, channel_axis, window_size, inplace)
 
     @staticmethod
     def get_sub_input_output_names(subgraph: onnx.ModelProto) -> Tuple[str, str]:
         return subgraph.graph.input[0].name, subgraph.graph.output[0].name
 
     @staticmethod
     def create_input_data(
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -38,16 +38,18 @@
     @staticmethod
     def create_bias_correction_command(
         node: NNCFNode, bias_value: Tensor, nncf_graph: NNCFGraph
     ) -> OVBiasCorrectionCommand:
         return OVCommandCreator.create_command_to_update_bias(node, bias_value.data, nncf_graph)
 
     @staticmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> OVModelExtractionCommand:
-        return OVModelExtractionCommand(inputs, outputs)
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> OVModelExtractionCommand:
+        return OVModelExtractionCommand(input_ids, output_ids)
 
     @staticmethod
     def mean_statistic_collector(
         channel_axis: int,
         inplace: bool,
         num_samples: Optional[int] = None,
         window_size: Optional[int] = None,
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/fast_bias_correction/torch_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/fast_bias_correction/torch_backend.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -50,16 +50,18 @@
     @staticmethod
     def create_bias_correction_command(
         node: NNCFNode, bias_value: Tensor, nncf_graph: NNCFGraph
     ) -> PTBiasCorrectionCommand:
         return create_bias_correction_command(node, bias_value.data)
 
     @staticmethod
-    def model_extraction_command(inputs: List[str], outputs: List[str]) -> PTModelExtractionWithFusedBiasCommand:
-        return PTModelExtractionWithFusedBiasCommand(inputs[0])
+    def model_extraction_command(
+        input_ids: List[Tuple[str, int]], output_ids: List[Tuple[str, int]]
+    ) -> PTModelExtractionWithFusedBiasCommand:
+        return PTModelExtractionWithFusedBiasCommand(input_ids[0][0])
 
     @staticmethod
     def mean_statistic_collector(
         channel_axis: int,
         inplace: bool,
         num_samples: Optional[int] = None,
         window_size: Optional[int] = None,
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/hyperparameter_tuner/param_grid.py` & `nncf-2.9.0/nncf/quantization/algorithms/hyperparameter_tuner/param_grid.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/__init__.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/algorithm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,28 +12,30 @@
 import collections
 import dataclasses
 from copy import deepcopy
 from typing import Any, Dict, List, Optional, OrderedDict, Set, TypeVar, Union
 
 import numpy as np
 
+import nncf
 from nncf import Dataset
 from nncf.common.factory import ModelTransformerFactory
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.patterns import GraphPattern
 from nncf.common.graph.patterns.manager import PatternsManager
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.hardware.config import get_hw_config_type
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.config_assignment import assign_qconfig_lists_to_modules
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.quantizer_propagation.solver import QuantizerPropagationSolver
 from nncf.common.quantization.quantizer_propagation.structs import IgnoreReason
 from nncf.common.quantization.quantizer_setup import SingleConfigQuantizationPoint
 from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 from nncf.common.quantization.structs import QuantizableWeightedLayerNode
 from nncf.common.quantization.structs import QuantizationConstraints
 from nncf.common.quantization.structs import QuantizationPreset
@@ -192,41 +194,43 @@
     def _review_defaults_based_on_mode(self):
         """
         Reviews default values because mode option doesn't support them.
         """
         nncf_logger.warning(f"You're using experimental option mode with {self._mode} value.")
 
         if self._preset != QuantizationPreset.PERFORMANCE:
-            raise RuntimeError(f"preset option with {self._preset} value is not supported with the mode option!")
+            raise nncf.ParameterNotSupportedError(
+                f"preset option with {self._preset} value is not supported with the mode option!"
+            )
 
         if self._target_device not in [TargetDevice.CPU, TargetDevice.ANY]:
-            raise RuntimeError(
+            raise nncf.ParameterNotSupportedError(
                 f"target_device option with {self._target_device} value is not supported with the mode option!"
             )
 
         if self._overflow_fix != OverflowFix.DISABLE:
-            raise RuntimeError(
+            raise nncf.ParameterNotSupportedError(
                 f"overflow_fix option with {self._overflow_fix} value is not supported with the mode option!"
             )
 
         if self._quantize_outputs:
-            raise RuntimeError("quantize_outputs option is not supported with the mode option!")
+            raise nncf.ParameterNotSupportedError("quantize_outputs option is not supported with the mode option!")
 
         if self._backend_params is not None:
-            raise RuntimeError("backend_params option is not supported with the mode option!")
+            raise nncf.ParameterNotSupportedError("backend_params option is not supported with the mode option!")
 
         if isinstance(self._quantization_params[QuantizerGroup.WEIGHTS], QuantizationParameters):
-            raise RuntimeError(
+            raise nncf.ParameterNotSupportedError(
                 "quantization_params option for weights with "
                 f"{self._quantization_params[QuantizerGroup.WEIGHTS]} "
                 "value is not supported with the mode option!"
             )
 
         if isinstance(self._quantization_params[QuantizerGroup.ACTIVATIONS], QuantizationParameters):
-            raise RuntimeError(
+            raise nncf.ParameterNotSupportedError(
                 "quantization_params option for activations with "
                 f"{self._quantization_params[QuantizerGroup.ACTIVATIONS]} "
                 "value is not supported with the mode option!"
             )
 
     def _set_quantization_params_based_on_mode(self):
         """
@@ -270,15 +274,15 @@
         """
         constraints = {"mode": preset.get_params_configured_by_preset(group)["mode"]}
         if quantization_params is None:
             return QuantizationConstraints(**constraints)
 
         if isinstance(quantization_params, FP8QuantizationParameters):
             if self._mode is None:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"FP8QuantizationParameters for {group.value} can not be used without QuantizationMode option!"
                 )
             return QuantizationConstraints(**constraints)
 
         if quantization_params.mode is not None:
             constraints["mode"] = quantization_params.mode
         if quantization_params.num_bits is not None:
@@ -306,15 +310,15 @@
 
             self._backend_entity = OVMinMaxAlgoBackend()
         elif model_backend == BackendType.TORCH:
             from nncf.quantization.algorithms.min_max.torch_backend import PTMinMaxAlgoBackend
 
             self._backend_entity = PTMinMaxAlgoBackend()
         else:
-            raise RuntimeError(
+            raise nncf.UnsupportedBackendError(
                 "Cannot return backend-specific entity because {} is not supported!".format(model_backend.value)
             )
 
     def _get_range_estimator_parameters(
         self, target_point: TargetPoint, quantizer_config: QuantizerConfig
     ) -> RangeEstimatorParameters:
         """
@@ -362,19 +366,24 @@
         :param quantizer_config: Configuration of a quantizer layer,
         defining the configuration of created statistic collector.
         :param num_samples: Number of samples to collect from the 'target_point'.
         :return: Statistic Collector.
         """
         range_estimator_params = self._get_range_estimator_parameters(target_point, quantizer_config)
 
+        collector_params = RangeInitCollectorParams(
+            is_weights=target_point.is_weight_target_point(),
+            scheme=quantizer_config.mode,
+            per_channel=quantizer_config.per_channel,
+        )
         return self._backend_entity.get_statistic_collector(
             range_estimator_params,
             nncf_graph,
             target_point,
-            quantizer_config,
+            collector_params,
             inplace=self._inplace_statistics,
             num_samples=num_samples,
         )
 
     def _get_default_qconfig(self, constraints: QuantizationConstraints = None) -> QuantizerConfig:
         """
         Returns default quantizer configuration, based on the provided constraints.
@@ -646,15 +655,15 @@
         quantization_points = self._topological_sort_quantization_points(quantization_points, nncf_graph)
         for quantization_point in quantization_points:
             if quantization_point.is_weight_quantization_point():
                 self._add_weight_quantization_target_point(quantization_point, nncf_graph)
             elif quantization_point.is_activation_quantization_point():
                 self._add_activation_quantization_target_point(quantization_point)
             else:
-                raise RuntimeError("Incorrect quantization point")
+                raise nncf.InternalError("Incorrect quantization point")
         return self._quantization_target_points_to_qconfig, self._unified_scale_groups
 
     def _collect_unified_groups(
         self, quantizer_setup: SingleConfigQuantizerSetup, nncf_graph: NNCFGraph
     ) -> List[List[TargetPoint]]:
         """
         Collects the group of quantizers for unification.
@@ -783,15 +792,15 @@
             for quantization_target_point in unified_scale_group:
                 target_node_name = quantization_target_point.target_node_name
                 for tensor_collector in statistic_points.get_algo_statistics_for_node(
                     target_node_name, filter_func, self._algorithm_key
                 ):
                     statistics = tensor_collector.get_statistics()
                     if statistics.min_values is None or statistics.max_values is None:
-                        raise RuntimeError(f"Statistics were not collected for the node {target_node_name}")
+                        raise nncf.InternalError(f"Statistics were not collected for the node {target_node_name}")
                     group_statistics.append(statistics)
 
             unified_values = self._backend_entity.unify_statistics(group_statistics)
             for quantization_target_point in unified_scale_group:
                 qconfig = quantization_target_points[quantization_target_point]
                 q_group = QuantizerGroup.ACTIVATIONS
                 narrow_range = get_quantizer_narrow_range(qconfig, q_group)
@@ -827,15 +836,15 @@
                 else:
                     quant_group = QuantizerGroup.ACTIVATIONS
 
                 half_range = quantization_target_point in quantization_points_overflow_fix
                 narrow_range = get_quantizer_narrow_range(qconfig, quant_group)
                 statistics = tensor_collector.get_statistics()
                 if statistics.min_values is None or statistics.max_values is None:
-                    raise RuntimeError(f"Statistics were not collected for the node {target_node_name}")
+                    raise nncf.InternalError(f"Statistics were not collected for the node {target_node_name}")
                 if self._mode is not None:
                     destination_type = self._quantization_params[quant_group].destination_type
                     parameters = calculate_convert_parameters(
                         statistics, is_per_channel=qconfig.per_channel, destination_type=destination_type
                     )
                     command = self._backend_entity.create_convert_insertion_command(
                         quantization_target_point, parameters
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -16,14 +16,15 @@
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.hardware.config import HWConfig
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
 from nncf.quantization.fake_quantize import FakeConvertParameters
 from nncf.quantization.fake_quantize import FakeQuantizeParameters
@@ -184,25 +185,25 @@
 
     @staticmethod
     @abstractmethod
     def get_statistic_collector(
         range_estimator_params: RangeEstimatorParameters,
         nncf_graph: NNCFGraph,
         target_point: TargetPoint,
-        quantizer_config: QuantizerConfig,
+        collector_params: RangeInitCollectorParams,
         inplace: bool,
         num_samples: int = None,
     ) -> TensorStatisticCollectorBase:
         """
         Returns backend-specific statistic collector.
 
         :param range_estimator_params: Parameters that specify estimators types.
         :param nncf_graph: NNCFGraph to get input/output shapes for the target point.
         :param target_point: Target location for the correction.
-        :param quantizer_config: QuantizerConfig instance for the current layer.
+        :param collector_params: RangeInitCollectorParams instance for the current layer.
         :param inplace: Whether to calculate statistic inplace or not.
         :param num_samples: Maximum number of samples to collect.
         :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
         """
 
     @staticmethod
     @abstractmethod
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/onnx_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/onnx_backend.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,47 +1,49 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, List, Optional, Set, Union
+from typing import Dict, List, Optional, Set
 
 import numpy as np
 
+import nncf
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.hardware.config import HWConfig
-from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.structs import QuantizerConfig
+from nncf.experimental.common.tensor_statistics.collectors import AGGREGATORS_MAP
+from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.onnx.graph.metatypes import onnx_metatypes as om
 from nncf.onnx.graph.metatypes.groups import MATMUL_METATYPES
 from nncf.onnx.graph.node_utils import get_input_edges_mapping
 from nncf.onnx.graph.node_utils import get_quantization_axis
 from nncf.onnx.graph.node_utils import get_quantized_tensor_shape
 from nncf.onnx.graph.node_utils import get_reduction_shape
 from nncf.onnx.graph.transformations.commands import ONNXQuantizerInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXTargetPoint
 from nncf.onnx.hardware.config import ONNXHWConfig
 from nncf.onnx.quantization.default_quantization import DEFAULT_ONNX_QUANT_TRAIT_TO_OP_DICT
 from nncf.onnx.quantization.quantizer_parameters import convert_fq_params_to_onnx_params
-from nncf.onnx.statistics.collectors import ONNXMeanMinMaxStatisticCollector
-from nncf.onnx.statistics.collectors import ONNXMinMaxStatisticCollector
+from nncf.onnx.statistics.collectors import ONNX_REDUCERS_MAP
+from nncf.onnx.statistics.collectors import ONNXNNCFCollectorTensorProcessor
 from nncf.onnx.statistics.statistics import ONNXMinMaxTensorStatistic
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
-from nncf.quantization.advanced_parameters import AggregatorType
 from nncf.quantization.advanced_parameters import StatisticsType
 from nncf.quantization.algorithms.min_max.backend import MinMaxAlgoBackend
 from nncf.quantization.fake_quantize import FakeConvertParameters
 from nncf.quantization.fake_quantize import FakeQuantizeParameters
 from nncf.quantization.range_estimator import RangeEstimatorParameters
 
 
@@ -56,15 +58,19 @@
 
     @property
     def conv_metatypes(self) -> List[OperatorMetatype]:
         return [om.ONNXConvolutionMetatype]
 
     @property
     def overflow_fix_metatypes(self) -> List[OperatorMetatype]:
-        return [om.ONNXConvolutionMetatype, om.ONNXConvolutionTransposeMetatype, *MATMUL_METATYPES]
+        return [
+            om.ONNXConvolutionMetatype,
+            om.ONNXConvolutionTransposeMetatype,
+            *MATMUL_METATYPES,
+        ]
 
     @property
     def add_metatypes(self) -> List[OperatorMetatype]:
         return [om.ONNXAddLayerMetatype]
 
     @property
     def group_conv_metatypes(self) -> List[OperatorMetatype]:
@@ -95,15 +101,17 @@
         return ONNXHWConfig
 
     @property
     def quant_trait_op_dict(self) -> Dict[int, OperatorMetatype]:
         return DEFAULT_ONNX_QUANT_TRAIT_TO_OP_DICT
 
     @staticmethod
-    def get_start_nodes_for_activation_path_tracing(nncf_graph: NNCFGraph) -> List[NNCFNode]:
+    def get_start_nodes_for_activation_path_tracing(
+        nncf_graph: NNCFGraph,
+    ) -> List[NNCFNode]:
         return nncf_graph.get_input_nodes()
 
     @staticmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> ONNXTargetPoint:
         return ONNXTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
@@ -123,18 +131,20 @@
         return ONNXQuantizerInsertionCommand(target_point, nncf_input_node_next_nodes, onnx_parameters)
 
     @staticmethod
     def create_convert_insertion_command(
         target_point: ONNXTargetPoint,
         parameters: FakeConvertParameters,
     ) -> TransformationCommand:
-        raise RuntimeError("FakeConvert insertion not implemented in ONNX backend!")
+        raise nncf.InternalError("FakeConvert insertion not implemented in ONNX backend!")
 
     @staticmethod
-    def unify_statistics(statistics: List[ONNXMinMaxTensorStatistic]) -> ONNXMinMaxTensorStatistic:
+    def unify_statistics(
+        statistics: List[ONNXMinMaxTensorStatistic],
+    ) -> ONNXMinMaxTensorStatistic:
         max_values, min_values = [], []
         for statistic in statistics:
             max_values.append(np.array(statistic.max_values).flatten())
             min_values.append(np.array(statistic.min_values).flatten())
         max_values = np.max(max_values, axis=0)
         min_values = np.min(min_values, axis=0)
         return ONNXMinMaxTensorStatistic(min_values=min_values, max_values=max_values)
@@ -144,52 +154,63 @@
         return get_input_edges_mapping(nncf_graph)
 
     @staticmethod
     def get_statistic_collector(
         range_estimator_params: RangeEstimatorParameters,
         nncf_graph: NNCFGraph,
         target_point: ONNXTargetPoint,
-        quantizer_config: QuantizerConfig,
+        collector_params: RangeInitCollectorParams,
         inplace: bool,
         num_samples: int = None,
-    ) -> Union[ONNXMinMaxStatisticCollector, ONNXMeanMinMaxStatisticCollector]:
-        is_per_channel = quantizer_config.per_channel
+    ) -> TensorCollector:
+        is_per_channel = collector_params.is_per_channel
         node = nncf_graph.get_node_by_name(target_point.target_node_name)
-        use_abs_max = quantizer_config.mode == QuantizationMode.SYMMETRIC
-        reduction_shape = None  # Per-Tensor
+        use_abs_max = collector_params.use_abs_max
         quantization_axis = get_quantization_axis(is_per_channel, node, target_point)
         quantized_tensor_shape = get_quantized_tensor_shape(nncf_graph, node, target_point)
+        reduction_axes = None  # Per-Tensor
         if quantization_axis is not None and quantized_tensor_shape is not None:  # Per-Channel
-            reduction_shape = get_reduction_shape(quantized_tensor_shape, quantization_axis)
-
-        if (
-            range_estimator_params.min.statistics_type == StatisticsType.MIN
-            and range_estimator_params.min.aggregator_type == AggregatorType.MIN
-            and range_estimator_params.max.statistics_type == StatisticsType.MAX
-            and range_estimator_params.max.aggregator_type == AggregatorType.MAX
+            reduction_axes = get_reduction_shape(quantized_tensor_shape, quantization_axis)
+        collector = TensorCollector(ONNXMinMaxTensorStatistic)
+        for params, container_key in zip(
+            [range_estimator_params.min, range_estimator_params.max],
+            [ONNXMinMaxTensorStatistic.MIN_STAT, ONNXMinMaxTensorStatistic.MAX_STAT],
         ):
-            return ONNXMinMaxStatisticCollector(use_abs_max, reduction_shape, num_samples)
-
-        if (
-            range_estimator_params.min.statistics_type == StatisticsType.MIN
-            and range_estimator_params.min.aggregator_type == AggregatorType.MEAN
-            and range_estimator_params.max.statistics_type == StatisticsType.MAX
-            and range_estimator_params.max.aggregator_type == AggregatorType.MEAN
-        ):
-            return ONNXMeanMinMaxStatisticCollector(
-                use_per_sample_stats=False,
-                use_abs_max=use_abs_max,
-                reduction_shape=reduction_shape,
+            if params.statistics_type not in ONNX_REDUCERS_MAP:
+                raise nncf.InternalError(
+                    f"Statistic type: {params.statistics_type} is not supported for ONNX PTQ backend yet."
+                )
+
+            if params.aggregator_type not in AGGREGATORS_MAP:
+                raise nncf.InternalError(
+                    f"Aggregator type: {params.aggregator_type} is not supported for ONNX PTQ backend yet."
+                )
+
+            statistic_type = params.statistics_type
+            kwargs = {"reduction_axes": reduction_axes, "inplace": inplace}
+            if statistic_type in [StatisticsType.QUANTILE, StatisticsType.ABS_QUANTILE]:
+                # TODO(dlyakhov): merge two quantile aggregators in one
+                if container_key == ONNXMinMaxTensorStatistic.MIN_STAT:
+                    quantile = params.quantile_outlier_prob
+                else:
+                    quantile = 1 - params.quantile_outlier_prob
+                kwargs.update({"quantile": [quantile]})
+            if use_abs_max and statistic_type == StatisticsType.MAX:
+                statistic_type = StatisticsType.ABS_MAX
+            reducer = ONNX_REDUCERS_MAP[statistic_type](reduction_axes=reduction_axes)
+
+            aggregation_axes = (0,)
+            aggregator = AGGREGATORS_MAP[params.aggregator_type](
+                aggregation_axes=aggregation_axes,
                 num_samples=num_samples,
-                window_size=None,
+                tensor_processor=ONNXNNCFCollectorTensorProcessor,
             )
-        raise RuntimeError(
-            "The following range estimator parameters are not supported by ONNX backend by now: "
-            f"{str(range_estimator_params)}"
-        )
+
+            collector.register_statistic_branch(container_key, reducer, aggregator)
+        return collector
 
     @staticmethod
     def get_weight_tensor_port_ids(node: NNCFNode) -> List[Optional[int]]:
         return list(node.layer_attributes.weight_attrs.keys())
 
     @staticmethod
     def get_ignored_metatypes(model_type: ModelType, device: TargetDevice) -> List[OperatorMetatype]:
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/openvino_backend.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Dict, List, Optional, Set, Tuple
 
 import numpy as np
 
+import nncf
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.hardware.config import HWConfig
-from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.tensor_statistics.collectors import ReductionAxes
 from nncf.experimental.common.tensor_statistics.collectors import AGGREGATORS_MAP
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.openvino.graph.layer_attributes import OVLayerAttributes
 from nncf.openvino.graph.metatypes import openvino_metatypes as om
 from nncf.openvino.graph.metatypes.groups import OPERATIONS_WITH_WEIGHTS
@@ -136,83 +137,80 @@
             max_values.append(np.array(statistic.max_values).flatten())
             min_values.append(np.array(statistic.min_values).flatten())
         max_values = np.max(max_values, axis=0)
         min_values = np.min(min_values, axis=0)
         return OVMinMaxTensorStatistic(min_values=min_values, max_values=max_values)
 
     @staticmethod
-    def _get_reduction_axes_and_use_abs_max(
-        nncf_graph: NNCFGraph, target_point: OVTargetPoint, quantizer_config: QuantizerConfig
+    def _get_reduction_axes(
+        nncf_graph: NNCFGraph, target_point: OVTargetPoint, collector_params: RangeInitCollectorParams
     ) -> Tuple[ReductionAxes, bool]:
-        use_abs_max = quantizer_config.mode == QuantizationMode.SYMMETRIC
-        if not quantizer_config.per_channel:
-            return None, use_abs_max
+        if not collector_params.is_per_channel:
+            return None
 
         node = nncf_graph.get_node_by_name(target_point.target_node_name)
         if not target_point.is_weight_target_point():
             if target_point.type == TargetType.PRE_LAYER_OPERATION:
                 shape = nncf_graph.get_input_edges(node)[target_point.port_id].tensor_shape
             elif target_point.type == TargetType.POST_LAYER_OPERATION:
                 shape = nncf_graph.get_output_edges(node)[target_point.port_id].tensor_shape
             else:
                 raise NotImplementedError(f"Unsupported target point type {target_point.type}.")
 
             # TODO (l-bat): Disable quantizer propagation through layout changing operations
             channel_axis = 1  # OpenVINO activations have channel first layout: [N, C, Z, Y, X]
             axes = get_channel_agnostic_reduction_axes([channel_axis], shape)
-            return axes, use_abs_max
+            return axes
 
         assert isinstance(node.layer_attributes, OVLayerAttributes)
         const_shape = node.layer_attributes.constant_attributes[target_point.port_id]["shape"]
 
-        if quantizer_config.per_channel:
+        if collector_params.is_per_channel:
             channel_axes = get_weight_channel_axes(node)
             axes = get_channel_agnostic_reduction_axes(channel_axes, const_shape)
         else:
             axes = tuple(range(len(const_shape)))
-        return axes, use_abs_max
+        return axes
 
     @staticmethod
     def get_statistic_collector(
         range_estimator_params: RangeEstimatorParameters,
         nncf_graph: NNCFGraph,
         target_point: OVTargetPoint,
-        quantizer_config: QuantizerConfig,
+        collector_params: RangeInitCollectorParams,
         inplace: bool,
         num_samples: int = None,
     ) -> TensorCollector:
-        reduction_axes, use_abs_max = OVMinMaxAlgoBackend._get_reduction_axes_and_use_abs_max(
-            nncf_graph, target_point, quantizer_config
-        )
+        reduction_axes = OVMinMaxAlgoBackend._get_reduction_axes(nncf_graph, target_point, collector_params)
 
         collector = TensorCollector(OVMinMaxTensorStatistic)
         for params, container_key in zip(
             [range_estimator_params.min, range_estimator_params.max],
             [OVMinMaxTensorStatistic.MIN_STAT, OVMinMaxTensorStatistic.MAX_STAT],
         ):
             if params.statistics_type not in OV_REDUCERS_MAP:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Statistic type: {params.statistics_type} is not supported for OpenVino PTQ backend yet."
                 )
 
             if params.aggregator_type not in AGGREGATORS_MAP:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Aggregator type: {params.aggregator_type} is not supported for OpenVino PTQ backend yet."
                 )
 
             kwargs = {"reduction_axes": reduction_axes, "inplace": inplace}
             if params.statistics_type in [StatisticsType.QUANTILE, StatisticsType.ABS_QUANTILE]:
                 if container_key == OVMinMaxTensorStatistic.MIN_STAT:
                     quantile = params.quantile_outlier_prob
                 else:
                     quantile = 1 - params.quantile_outlier_prob
                 kwargs.update({"quantile": [quantile]})
             # TODO(dlyakhov): merge two quantile aggregators in one
             statistic_type = params.statistics_type
-            if use_abs_max and statistic_type == StatisticsType.MAX:
+            if collector_params.use_abs_max and statistic_type == StatisticsType.MAX:
                 statistic_type = StatisticsType.ABS_MAX
             reducer = OV_REDUCERS_MAP[statistic_type](**kwargs)
 
             kwargs = {"num_samples": num_samples, "tensor_processor": OVNNCFCollectorTensorProcessor}
             aggregator = AGGREGATORS_MAP[params.aggregator_type](**kwargs)
 
             collector.register_statistic_branch(container_key, reducer, aggregator)
@@ -247,17 +245,19 @@
         return types
 
     @staticmethod
     def get_ignored_names_by_layer_attributes(nncf_graph: NNCFGraph) -> List[str]:
         ignored_names = []
         target_nodes = nncf_graph.get_nodes_by_metatypes([om.OVGRUSequenceMetatype])
         for node in target_nodes:
-            if isinstance(node.layer_attributes, OVLayerAttributes):
-                if node.layer_attributes.input_attributes["linear_before_reset"]:
-                    ignored_names.append(node.node_name)
+            if (
+                isinstance(node.layer_attributes, OVLayerAttributes)
+                and node.layer_attributes.input_attributes["linear_before_reset"]
+            ):
+                ignored_names.append(node.node_name)
         return ignored_names
 
     @staticmethod
     def get_weight_nodes(nncf_graph: NNCFGraph) -> List[NNCFNode]:
         return [
             node
             for node in nncf_graph.get_all_nodes()
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/min_max/torch_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/min_max/torch_backend.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,33 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Dict, List, Optional, Set, Tuple
 
 import torch
 
+import nncf
 import nncf.torch.graph.operator_metatypes as om
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.layer_attributes import WeightedLayerAttributes
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.hardware.config import HWConfig
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.experimental.common.tensor_statistics.collectors import AGGREGATORS_MAP
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
 from nncf.quantization.advanced_parameters import StatisticsType
@@ -54,15 +56,15 @@
     TARGET_TYPE_TO_PT_INS_TYPE_MAP = {
         TargetType.PRE_LAYER_OPERATION: TargetType.OPERATOR_PRE_HOOK,
         TargetType.POST_LAYER_OPERATION: TargetType.OPERATOR_POST_HOOK,
     }
 
     @property
     def mat_mul_metatypes(self) -> List[OperatorMetatype]:
-        return [om.PTModuleLinearMetatype]
+        return [om.PTModuleLinearMetatype, om.PTLinearMetatype, om.PTMatMulMetatype]
 
     @property
     def post_processing_metatypes(self) -> List[OperatorMetatype]:
         return []
 
     @property
     def shapeof_metatypes(self) -> List[OperatorMetatype]:
@@ -140,15 +142,15 @@
         )
 
     @staticmethod
     def create_convert_insertion_command(
         target_point: PTTargetPoint,
         parameters: FakeConvertParameters,
     ) -> TransformationCommand:
-        raise RuntimeError("FakeConvert insertion not implemented in PyTorch backend!")
+        raise nncf.InternalError("FakeConvert insertion not implemented in PyTorch backend!")
 
     @staticmethod
     def unify_statistics(statistics: List[PTMinMaxTensorStatistic]) -> PTMinMaxTensorStatistic:
         max_values, min_values = [], []
         for statistic in statistics:
             max_values.append(statistic.max_values.flatten())
             min_values.append(statistic.min_values.flatten())
@@ -157,34 +159,34 @@
         return PTMinMaxTensorStatistic(min_values=min_values, max_values=max_values)
 
     @staticmethod
     def get_statistic_collector(
         range_estimator_params: RangeEstimatorParameters,
         nncf_graph: NNCFGraph,
         target_point: PTTargetPoint,
-        quantizer_config: QuantizerConfig,
+        collector_params: RangeInitCollectorParams,
         inplace: bool,
         num_samples: int = None,
     ) -> TensorCollector:
-        collector_params = PTMinMaxAlgoBackend._default_collector_params(nncf_graph, target_point, quantizer_config)
+        collector_params = PTMinMaxAlgoBackend._default_collector_params(nncf_graph, target_point, collector_params)
         reduction_axes = collector_params.get_reduction_axes(per_sample_stats=False)
         aggregation_axes = collector_params.get_aggregation_axes(per_sample_stats=False)
 
         collector = TensorCollector(PTMinMaxTensorStatistic)
         for params, container_key in zip(
             [range_estimator_params.min, range_estimator_params.max],
             [PTMinMaxTensorStatistic.MIN_STAT, PTMinMaxTensorStatistic.MAX_STAT],
         ):
             if params.statistics_type not in PT_REDUCERS_MAP:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Statistic type: {params.statistics_type} is not supported for Torch PTQ backend yet."
                 )
 
             if params.aggregator_type not in AGGREGATORS_MAP:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Aggregator type: {params.aggregator_type} is not supported for Torch PTQ backend yet."
                 )
 
             statistic_type = params.statistics_type
             if statistic_type in [StatisticsType.QUANTILE, StatisticsType.ABS_QUANTILE]:
                 # TODO(dlyakhov): merge two quantile aggregators in one
                 if container_key == PTMinMaxTensorStatistic.MIN_STAT:
@@ -221,46 +223,44 @@
 
     @staticmethod
     def get_weight_config(config: QuantizerConfig, model: NNCFNetwork) -> QuantizerConfig:
         return config
 
     @staticmethod
     def _get_input_scale_shape(
-        nncf_graph: NNCFGraph, target_point: PTTargetPoint, quantization_config: QuantizerConfig
+        nncf_graph: NNCFGraph, target_point: PTTargetPoint, per_channel: bool
     ) -> Tuple[Tuple[int, ...], Tuple[int, ...], int]:
         is_weights = target_point.is_weight_target_point()
         if is_weights:
             module_node = nncf_graph.get_node_by_name(target_point.target_node_name)
             layer_attributes = module_node.layer_attributes
             assert isinstance(layer_attributes, WeightedLayerAttributes)
             input_shape = layer_attributes.get_weight_shape()
             channel_idx = layer_attributes.get_target_dim_for_compression()
         else:
             input_shape = nncf_graph.get_input_shape_for_insertion_point(target_point)
             channel_idx = 1  # channel dim for activations
 
         scale_shape = tuple(
-            get_scale_shape(
-                input_shape, is_weights=is_weights, per_channel=quantization_config.per_channel, channel_idx=channel_idx
-            )
+            get_scale_shape(input_shape, is_weights=is_weights, per_channel=per_channel, channel_idx=channel_idx)
         )
 
         return input_shape, scale_shape, channel_idx
 
     @staticmethod
     def _default_collector_params(
-        nncf_graph: NNCFGraph, target_point: PTTargetPoint, quantizer_config: QuantizerConfig
+        nncf_graph: NNCFGraph, target_point: PTTargetPoint, collector_params: RangeInitCollectorParams
     ) -> PTRangeInitCollectorParams:
         input_shape, _, channel_idx = PTMinMaxAlgoBackend._get_input_scale_shape(
-            nncf_graph, target_point, quantizer_config
+            nncf_graph, target_point, collector_params.is_per_channel
         )
         return PTRangeInitCollectorParams(
-            is_weights=target_point.is_weight_target_point(),
-            mode=quantizer_config.mode,
-            per_channel=quantizer_config.per_channel,
+            is_weights=collector_params.is_weights,
+            scheme=collector_params.scheme,
+            per_channel=collector_params.is_per_channel,
             input_shape=input_shape,
             channel_idx=channel_idx,
         )
 
     @staticmethod
     def _create_quantizer(
         quantizer_config: QuantizerConfig,
@@ -300,15 +300,17 @@
     @staticmethod
     def _create_quantizer_insertion_command(
         nncf_graph: NNCFGraph,
         target_point: PTTargetPoint,
         quantizer_config: QuantizerConfig,
         parameters: FakeQuantizeParameters,
     ) -> PTQuantizerInsertionCommand:
-        _, scale_shape, _ = PTMinMaxAlgoBackend._get_input_scale_shape(nncf_graph, target_point, quantizer_config)
+        _, scale_shape, _ = PTMinMaxAlgoBackend._get_input_scale_shape(
+            nncf_graph, target_point, quantizer_config.per_channel
+        )
 
         quantizer = PTMinMaxAlgoBackend._create_quantizer(
             quantizer_config, scale_shape, parameters, target_point.target_type
         )
         return PTQuantizerInsertionCommand(target_point, quantizer)
 
     @staticmethod
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/pipeline.py` & `nncf-2.9.0/nncf/quantization/algorithms/pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/post_training/__init__.py` & `nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/post_training/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/post_training/algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/post_training/pipeline.py` & `nncf-2.9.0/nncf/quantization/algorithms/post_training/pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -68,16 +68,14 @@
         in the model. Supported only `transformer` now.
     :param ignored_scope: An ignored scope that defined the list of model control
         flow graph nodes to be ignored during quantization.
     :param advanced_parameters: Advanced quantization parameters for
         fine-tuning the quantization algorithm
     :return: A post-training quantization pipeline.
     """
-    if target_device is TargetDevice.VPU:
-        warning_deprecated("VPU device is deprecated and will no longer be supported in the future.")
 
     if advanced_parameters is None:
         advanced_parameters = AdvancedQuantizationParameters()
 
     # Build the post-training quantization pipeline.
     pipeline_steps = []
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/__init__.py` & `nncf-2.9.0/nncf/tensorflow/api/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/algorithm.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#      http://www.apache.org/licenses/LICENSE-2.0
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -20,27 +10,29 @@
 # limitations under the License.
 
 from collections import Counter
 from collections import defaultdict
 from copy import deepcopy
 from typing import Dict, List, Optional, Tuple, TypeVar
 
+import nncf
 from nncf import Dataset
 from nncf.common.factory import ModelTransformerFactory
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
-from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.track_progress import track
 from nncf.common.tensor_statistics.statistic_point import StatisticPoint
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
+from nncf.experimental.tensor import Tensor
+from nncf.experimental.tensor import functions as fns
 from nncf.quantization.algorithms.algorithm import Algorithm
 
 TModel = TypeVar("TModel")
 TTensor = TypeVar("TTensor")
 STATISTIC_BRANCH_KEY = "abs_max"
 ALPHA_MAP = {"convolution": 0.05, "matmul": 0.95}
 
@@ -73,29 +65,33 @@
         self._backend_entity = None
         self._algorithm_key = f"SQ_{hash(self)}"
         self._cached_multiply_names = Counter()
         self._alpha_map = alpha_map
 
     @property
     def available_backends(self) -> List[BackendType]:
-        return [BackendType.OPENVINO]
+        return [BackendType.OPENVINO, BackendType.TORCH]
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
         Creates a helper class with a backed-specific logic of the algorithm.
 
         :param model: Backend-specific input model.
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.OPENVINO:
             from nncf.quantization.algorithms.smooth_quant.openvino_backend import OVSmoothQuantAlgoBackend
 
             self._backend_entity = OVSmoothQuantAlgoBackend()
+        elif model_backend == BackendType.TORCH:
+            from nncf.quantization.algorithms.smooth_quant.torch_backend import PTSmoothQuantAlgoBackend
+
+            self._backend_entity = PTSmoothQuantAlgoBackend()
         else:
-            raise RuntimeError(
+            raise nncf.UnsupportedBackendError(
                 "Cannot return backend-specific entity because {} is not supported!".format(model_backend.value)
             )
 
     def apply(
         self,
         model: TModel,
         graph: NNCFGraph,
@@ -116,29 +112,34 @@
             best_ratio = 0.0
             empty_statistic = False
             for node_to_smooth in nodes:
                 source_node, input_port_id, source_output_port_id, _ = group_id
                 activations_value = self._get_statistics_for_node(
                     statistic_points, node_to_smooth.node_name, input_port_id
                 )
-                if any(val is None for val in activations_value):
+                if any(val.data is None for val in activations_value):
                     empty_statistic = True
                     break
-                activations_value = self._backend_entity.clip_statistics(activations_value)
+                if len(activations_value) != 1:
+                    raise RuntimeError(
+                        (
+                            "More than one statistic is collected for one node during"
+                            f"Smooth Quanti algorithm: {node_to_smooth.node_name}"
+                        )
+                    )
 
-                weight_port = self._backend_entity.get_weight_tensor_port_id(node_to_smooth)
-                weight_value = self._backend_entity.get_weight_value(node_to_smooth, model, weight_port)
+                activations_value = self._clip_statistics(activations_value)
+
+                weight_value = self._backend_entity.get_weight_value(node_to_smooth, model)
                 weight_statistics = self._process_weight_statistics(node_to_smooth, weight_value)
-                weight_statistics = self._backend_entity.clip_statistics(weight_statistics)
+                weight_statistics = self._clip_statistics([weight_statistics])
 
                 alpha = alpha_map[node_to_smooth.metatype]
 
-                scales, ratio = self._backend_entity.calculate_scale_and_ratio(
-                    activations_value, weight_statistics, alpha
-                )
+                scales, ratio = self._calculate_scale_and_ratio(activations_value, weight_statistics, alpha)
 
                 if ratio > best_ratio:
                     best_ratio = ratio
                     best_scale = deepcopy(scales)
 
             if empty_statistic:
                 nncf_logger.debug(
@@ -149,35 +150,56 @@
             if best_scale is None:
                 nncf_logger.debug(
                     f"Skipped SmoothQuant for nodes after {source_node.node_name} because of the empty scale."
                 )
                 continue
 
             for node_to_smooth in nodes:
-                weights_scale = self._calculate_weight_scale(best_scale, node_to_smooth)
-                weight_port = self._backend_entity.get_weight_tensor_port_id(node_to_smooth)
-                weight_value = self._backend_entity.get_weight_value(node_to_smooth, model, weight_port)
+                weight_value = self._backend_entity.get_weight_value(node_to_smooth, model)
+                weights_scale = self._calculate_weight_scale(best_scale, node_to_smooth, weight_value)
                 scaled_weight = weight_value * weights_scale
-                weight_update_command = self._backend_entity.weight_update_command(
-                    node_to_smooth, scaled_weight, weight_port
-                )
+                weight_update_command = self._backend_entity.weight_update_command(node_to_smooth, scaled_weight.data)
                 transformation_layout.register(weight_update_command)
 
             activations_shape = graph.get_output_edges(source_node)[source_output_port_id].tensor_shape
             activation_scale = self._calculate_activation_scale(best_scale, activations_shape, nodes, graph)
 
             scale_node_name = self._create_scale_node_name(source_node.node_name, source_output_port_id)
             scale_insertion_command = self._backend_entity.scale_insertion_command(
-                source_node, activation_scale, source_output_port_id, nodes, scale_node_name
+                source_node, activation_scale.data, source_output_port_id, nodes, scale_node_name
             )
             transformation_layout.register(scale_insertion_command)
 
         transformed_model = model_transformer.transform(transformation_layout)
         return transformed_model
 
+    @staticmethod
+    def _calculate_scale_and_ratio(
+        activations: Tensor, weights: Tensor, alpha: float, quantile: Optional[float] = 0.1
+    ) -> Tuple[Tensor, float]:
+        """
+        Calculates base scale value and it's ratio.
+
+        :param activations: Activation statistics value.
+        :param weights: Weights statistics value.
+        :param alpha: Base value for exponentiation.
+        :param quantile: Base quantile value.
+        :return: Calculated base scale value & ratio.
+        """
+
+        eps = fns.finfo(activations).eps
+        scales = fns.power(activations, alpha) / (fns.power(weights, 1 - alpha) + eps)
+
+        a_min = fns.quantile(scales, quantile, keepdims=False)
+        a_max = 1e2
+
+        scales = fns.clip(scales, a_min=a_min, a_max=a_max)
+        ratio = scales.min() / (scales.max() + eps)
+        return scales, ratio
+
     def _group_nodes_by_source(self, nodes_to_smooth: List[Dict], nncf_graph: NNCFGraph) -> Dict[tuple, List]:
         """
         Groups nodes that will be smoothed by source (parent node).
 
         :param nodes_to_smooth: List of the nodes that will be smoothed.
         :param nncf_graph: NNCFGraph instance.
         :return: Dictionary with the source info as key and grouped nodes as value.
@@ -204,40 +226,36 @@
 
         :param statistic_points: StatisticPointsContainer instance.
         :param node_name: Name of the node for collection.
         :param act_port: Activation port id.
         :return: List of the TTensor instances.
         """
 
-        def filter_func(point: StatisticPoint) -> bool:
-            return (
-                self._algorithm_key in point.algorithm_to_tensor_collectors
-                and point.target_point.type == TargetType.PRE_LAYER_OPERATION
-                and point.target_point.port_id == act_port
-            )
-
         statistics_for_node = []
         for tensor_collector in statistic_points.get_algo_statistics_for_node(
-            node_name, filter_func, self._algorithm_key
+            node_name,
+            self._backend_entity.get_filter_fn_for_statistics(act_port, self._algorithm_key),
+            self._algorithm_key,
         ):
-            statistics_for_node.append(tensor_collector.get_statistics()[STATISTIC_BRANCH_KEY])
+            statistic = tensor_collector.get_statistics()[STATISTIC_BRANCH_KEY]
+            statistics_for_node.append(Tensor(statistic))
         return statistics_for_node
 
     def get_statistic_points(self, model: TModel, graph: NNCFGraph) -> StatisticPointsContainer:
         statistic_container = StatisticPointsContainer()
 
         self._set_backend_entity(model)
         alpha_map = self._get_alpha_map()
 
         nodes_to_smooth_data = self._get_nodes_to_smooth_data(graph, alpha_map.keys())
 
         for node_data in nodes_to_smooth_data:
             node_to_smooth = node_data["node_to_smooth"]
             target_point = self._backend_entity.target_point(
-                TargetType.PRE_LAYER_OPERATION,
+                target_type=self._backend_entity.pre_layer_target_type(),
                 target_node_name=node_to_smooth.node_name,
                 port_id=node_data["input_act_port"],
             )
             input_reduction_axes = self._calculate_input_reduction_axes(
                 graph, node_to_smooth, node_data["input_act_port"]
             )
             stat_collector = self._backend_entity.get_abs_max_channel_collector(
@@ -263,35 +281,34 @@
         nodes_with_weights = nncf_graph.get_nodes_by_metatypes(node_metatypes)
         nodes_to_smooth_data = []
 
         for node_with_weight in nodes_with_weights:
             if not self._backend_entity.is_node_with_weights(node_with_weight):
                 continue
 
-            ports_map = self._backend_entity.get_input_ports_map(node_with_weight, nncf_graph)
+            activation_port_id = self._backend_entity.get_activations_port_id(node_with_weight, nncf_graph)
             input_edges = nncf_graph.get_input_edges(node_with_weight)
-            weight_node = input_edges[ports_map["weight"]].from_node
-            activation_node = input_edges[ports_map["activation"]].from_node
+            activation_node = input_edges[activation_port_id].from_node
 
             # Skipping agnostic layers as inputs to propagate quantizer
             # Only for Convolution layers
             if (
-                node_with_weight.metatype == self._backend_entity.convolution_metatype
+                node_with_weight.metatype in self._backend_entity.convolution_metatypes
                 and activation_node.metatype in self._backend_entity.quantize_agnostic_metatypes
             ):
                 continue
 
             # Skipping shared weights
-            if len(nncf_graph.get_next_nodes(weight_node)) > 1:
+            if self._backend_entity.is_node_with_shared_weight(node_with_weight, nncf_graph):
                 continue
 
             nodes_to_smooth_data.append(
                 {
                     "node_to_smooth": node_with_weight,
-                    "input_act_port": ports_map["activation"],
+                    "input_act_port": activation_port_id,
                 }
             )
         return nodes_to_smooth_data
 
     def _calculate_activation_scale(
         self, scale_value: TTensor, activations_shape: List[int], nodes: List[NNCFNode], nncf_graph: NNCFGraph
     ) -> TTensor:
@@ -299,41 +316,48 @@
         Calculates activation scales for Smooth node.
 
         :param scale_value: Base scale value.
         :param activations_shape: activation tensor shape.
         :param nodes: List of consumers for Smooth node.
         :return: Calculated per-channel activation scale.
         """
-        activation_ports_map = {
-            node: self._backend_entity.get_input_ports_map(node, nncf_graph)["activation"] for node in nodes
-        }
+        activation_ports_map = {node: self._backend_entity.get_activations_port_id(node, nncf_graph) for node in nodes}
         channel_axes = [
             self._backend_entity.get_activation_channel_axis(node, port) for node, port in activation_ports_map.items()
         ]
         channel_axis = channel_axes[0]
 
         if not all(axis == channel_axis for axis in channel_axes):
-            raise RuntimeError(f"Channel axes for nodes {[n.node_name for n in nodes]} are not identical")
+            raise nncf.InternalError(f"Channel axes for nodes {[n.node_name for n in nodes]} are not identical")
 
         activations_size = len(activations_shape)
-        return self._backend_entity.calculate_activation_scale(scale_value, activations_size, channel_axis)
+        activation_scale = scale_value ** (-1)
+        if activations_size > 1:
+            reshape_shape = [1 for _ in range(activations_size)]
+            reshape_shape[channel_axis] = activation_scale.size
+            activation_scale = activation_scale.reshape(reshape_shape)
+        return activation_scale
 
-    def _calculate_weight_scale(self, scale_value: TTensor, node: NNCFNode) -> TTensor:
+    def _calculate_weight_scale(self, scale_value: Tensor, node: NNCFNode, weights_value: Tensor) -> Tensor:
         """
         Calculates scale for weight tensor.
 
         :param scale_value: Base scale value.
         :param node: Consumer for Smooth node.
         :return: Calculated scale for weights.
         """
-        port_id = self._backend_entity.get_weight_tensor_port_id(node)
-        weights_size = len(node.layer_attributes.constant_attributes[port_id]["shape"])
+        weights_size = len(weights_value.shape)
         if weights_size > 1:
             channel_axis = self._backend_entity.get_weight_channel_axis(node)
-            return self._backend_entity.calculate_weight_scale(scale_value, weights_size, channel_axis)
+            weight_scale = scale_value
+            if weights_size > 1:
+                reshape_shape = [1 for _ in range(weights_size)]
+                reshape_shape[channel_axis] = scale_value.size
+                weight_scale = scale_value.reshape(reshape_shape)
+            return weight_scale
         return scale_value
 
     def _calculate_input_reduction_axes(self, nncf_graph: NNCFGraph, node: NNCFNode, input_port: int) -> Tuple[int]:
         """
         Returns reduction axes for specified input.
 
         :param nncf_graph: NNCFGraph instance.
@@ -344,28 +368,28 @@
         shape = nncf_graph.get_input_edges(node)[input_port].tensor_shape
         reduction_axes = tuple([])
         if len(shape) > 1:
             channel_axis = self._backend_entity.get_activation_channel_axis(node, input_port)
             reduction_axes = self._backend_entity.get_channel_agnostic_reduction_axes(channel_axis, shape)
         return reduction_axes
 
-    def _process_weight_statistics(self, node: NNCFNode, weights: TTensor) -> TTensor:
+    def _process_weight_statistics(self, node: NNCFNode, weights: Tensor) -> Tensor:
         """
         Returns processed weight statistics for node.
 
         :param node: NNCFNode to check.
         :param weights: Backend-specific weights.
         :return: Weight statistic for node.
         """
         channel_axis = 0
         if len(weights.shape) > 1:
             channel_axis = self._backend_entity.get_weight_channel_axis(node)
         reduction_shape = [i for i, _ in enumerate(weights.shape)]
         reduction_shape.pop(channel_axis)
-        return self._backend_entity.process_weight_statistics(weights, tuple(reduction_shape))
+        return fns.max(fns.abs(weights), axis=tuple(reduction_shape))
 
     def _create_scale_node_name(self, source_name: str, source_port_id: int) -> str:
         """
         Returns uniqie scale node name for new layer.
 
         :param source_name: Source layer name.
         :param source_port_id: Source port id.
@@ -380,20 +404,34 @@
         """
         Returns alpha map by metatypes.
 
         :return: Alpha map by metatypes.
         """
         alpha_by_metatype_map = {}
         name_to_metatype = {
-            "convolution": self._backend_entity.convolution_metatype,
-            "matmul": self._backend_entity.matmul_metatype,
+            "convolution": self._backend_entity.convolution_metatypes,
+            "matmul": self._backend_entity.matmul_metatypes,
         }
         for type_name, alpha_value in self._alpha_map.items():
             if alpha_value < 0:
                 nncf_logger.debug(
                     f"Smooth Quant algorithm does not support negative parameter for {type_name}! "
                     "Skipping these layers."
                 )
                 continue
-            metatype = name_to_metatype[type_name]
-            alpha_by_metatype_map[metatype] = alpha_value
+            metatypes = name_to_metatype[type_name]
+            for metatype in metatypes:
+                alpha_by_metatype_map[metatype] = alpha_value
         return alpha_by_metatype_map
+
+    @staticmethod
+    def _clip_statistics(statistics: List[Tensor]) -> Tensor:
+        """
+        Clips statistics for further calculation.
+        :param statistics: Input statistics.
+        :return: Clipped statistics.
+        """
+        a_min = 1e-5
+
+        statistics = fns.stack(statistics)
+        squeezed = fns.squeeze(statistics)
+        return fns.clip(squeezed, a_min=a_min, a_max=None)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/backend.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,64 +1,75 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
-from typing import Dict, List, Optional, Tuple, TypeVar
+from typing import Callable, List, Tuple, TypeVar
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
+from nncf.common.tensor_statistics.statistic_point import StatisticPoint
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
+from nncf.experimental.tensor import Tensor
 
 TModel = TypeVar("TModel")
 TTensor = TypeVar("TTensor")
 
 
 class SmoothQuantAlgoBackend(ABC):
     @property
     @abstractmethod
-    def convolution_metatype(self) -> OperatorMetatype:
+    def convolution_metatypes(self) -> List[OperatorMetatype]:
         """
-        Parameter for backend-specific metatype for Convolution.
+        Parameter for backend-specific metatypes for Convolution.
 
-        :return: OperatorMetatype
+        :return: OperatorMetatype list.
         """
 
     @property
     @abstractmethod
-    def matmul_metatype(self) -> OperatorMetatype:
+    def matmul_metatypes(self) -> List[OperatorMetatype]:
         """
-        Parameter for backend-specific metatype for MatMul.
+        Parameter for backend-specific metatypes for MatMul.
 
-        :return: OperatorMetatype
+        :return: OperatorMetatype list.
         """
 
     @property
     @abstractmethod
     def quantize_agnostic_metatypes(self) -> List[OperatorMetatype]:
         """
         Parameter for backend-specific quantize agnostic metatypes.
 
         :return: List of OperatorMetatype.
         """
 
     @staticmethod
     @abstractmethod
+    def pre_layer_target_type() -> TargetType:
+        """
+        Returns backend-specific pre layer target type.
+
+        :returns: Backend-specific pre layer target type.
+        """
+
+    @staticmethod
+    @abstractmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> TargetPoint:
         """
         Returns backend-specific target point.
 
         :param target_type: Type of the location that should be modified.
         :param target_node_name: Name of the located node.
         :param port_id: Port ID of the tensor for the statistics distribution.
@@ -73,15 +84,15 @@
 
         :param node: NNCFNode to check.
         :return: Boolean indicating whether the node has weights or not.
         """
 
     @staticmethod
     @abstractmethod
-    def get_input_ports_map(node: NNCFNode, nncf_graph: NNCFGraph) -> Dict[str, int]:
+    def get_activations_port_id(node: NNCFNode, nncf_graph: NNCFGraph) -> int:
         """
         Returns map with activation & weighted ports.
 
         :param node: NNCFNode to check.
         :param nncf_graph: NNCFGraph instance.
         :return: Map with the activation & weighted ports.
         """
@@ -109,26 +120,15 @@
         :param inplace: Whether to calculate statistic inplace or not.
         :param branch_key: Specific string for branch key.
         :return: TensorCollector instance.
         """
 
     @staticmethod
     @abstractmethod
-    def process_weight_statistics(weights: TTensor, channel_axis: int) -> TTensor:
-        """
-        Returns processed weight statistics for node.
-
-        :param weights: Weights tensor.
-        :param channel_axis: Channel axis for calculation.
-        :return: Weight statistics.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def get_weight_value(node_with_weight: NNCFNode, model: TModel, port_id: int) -> TTensor:
+    def get_weight_value(node_with_weight: NNCFNode, model: TModel, port_id: int) -> Tensor:
         """
         Returns the weight value for the node with weight.
 
         :param node_with_weight: The node with weight.
         :param model: The model that contains this operation.
         :param port_id: The input port ID to get weight input.
         :return: The weight value.
@@ -142,63 +142,14 @@
 
         :param node: NNCFNode to find its weights input port indices.
         :return: Weights input port indices.
         """
 
     @staticmethod
     @abstractmethod
-    def clip_statistics(statistics: TTensor) -> TTensor:
-        """
-        Clips statistics for further calculation.
-
-        :param statistics: Input statistics.
-        :return: Clipped statistics.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def calculate_scale_and_ratio(
-        activations: TTensor, weights: TTensor, alpha: float, quantile: Optional[float]
-    ) -> Tuple[TTensor, TTensor]:
-        """
-        Calculates base scale value and it's ratio.
-
-        :param activations: Activation statistics value.
-        :param weights: Weights statistics value.
-        :param alpha: Base value for exponentiation.
-        :param quantile: Base quantile value.
-        :return: Calculated base scale value & ratio.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def calculate_activation_scale(scale_value: TTensor, activations_size: int, channel_axis: int) -> TTensor:
-        """
-        Calculates activation scales for Smooth node.
-
-        :param scale_value: Base scale value.
-        :param activations_size: Size of the activation shape.
-        :param channel_axis: Axis for shape calculation.
-        :return: Calculated activation scale.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def calculate_weight_scale(scale_value: TTensor, weights_size: int, channel_axis: int) -> TTensor:
-        """
-        Calculates scale for weight tensor.
-
-        :param scale_value: Base scale value.
-        :param weights_size: Size of the weights shape.
-        :param channel_axis: Axis for shape calculation.
-        :return: Calculated scale for weights.
-        """
-
-    @staticmethod
-    @abstractmethod
     def weight_update_command(
         node_with_weight: NNCFNode, weight_value: TTensor, weight_port_id: int
     ) -> TransformationCommand:
         """
         Returns command to update weights.
 
         :param node_with_weight: NNCFNode instance.
@@ -206,22 +157,22 @@
         :param weight_port_id: Weight port id.
         :return: TransformationCommand instance.
         """
 
     @staticmethod
     @abstractmethod
     def scale_insertion_command(
-        source_node: NNCFNode, scale_value: TTensor, port_id: int, nodes: List[NNCFNode]
+        source_node: NNCFNode, scale_value: TTensor, source_output_port_id: int, nodes: List[NNCFNode]
     ) -> TransformationCommand:
         """
         Returns command to insert Smooth Quant node.
 
         :param source_node: NNCFNode instance.
         :param scale_value: Smooth Quant value.
-        :param port_id: Output port for source node.
+        :param source_output_port_id: Output port for source node.
         :param nodes: List of consumers for Smooth node.
         :return: TransformationCommand instance.
         """
 
     @staticmethod
     @abstractmethod
     def get_activation_channel_axis(node: NNCFNode, port_id: int) -> int:
@@ -241,15 +192,26 @@
 
         :param node: NNCFNode instance.
         :return: Channel axis number.
         """
 
     @staticmethod
     @abstractmethod
-    def calculate_port_based_channel_axis(port_id: int, transpose: bool) -> int:
+    def is_node_with_shared_weight(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
-        Returns port-based channel axis.
+        Returns true if given node shares constant with a different node.
 
-        :param port_id: Specified input port id.
-        :param transpose: Transpose position.
-        :return: Channel axis.
+        :param node: NNCFNode instance.
+        :param nncf_graph: NNCFGraph instance.
+        :return: Whether the given node is shares weights with a different node or not.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def get_filter_fn_for_statistics(activation_port_id: int, algorithm_key: str) -> Callable[[StatisticPoint], bool]:
+        """
+        Returns backend-specific callable to filter statistic containers according to its statistic point.
+
+        :param activation_port_id: Activation port id for the statistic collection target node.
+        :param algorithm_key: Current algorithm key.
+        :return: Backend-specific callable to filter statistic containers according to its statistic point.
         """
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/smooth_quant/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/smooth_quant/openvino_backend.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,32 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, List, Optional, Tuple
+from typing import Callable, List, Tuple
 
 import numpy as np
 import openvino.runtime as ov
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.tensor_statistics.statistic_point import StatisticPoint
 from nncf.experimental.common.tensor_statistics.collectors import MaxAggregator
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
+from nncf.experimental.tensor import Tensor
 from nncf.openvino.graph.layout import OVLayoutElem
 from nncf.openvino.graph.layout import get_linear_weights_layout_from_node
 from nncf.openvino.graph.metatypes.groups import QUANTIZE_AGNOSTIC_OPERATIONS
 from nncf.openvino.graph.metatypes.openvino_metatypes import OVConvolutionMetatype
 from nncf.openvino.graph.metatypes.openvino_metatypes import OVMatMulMetatype
 from nncf.openvino.graph.node_utils import get_channel_agnostic_reduction_axes
 from nncf.openvino.graph.node_utils import get_weight_value
@@ -31,47 +34,52 @@
 from nncf.openvino.graph.transformations.commands import OVMultiplyInsertionCommand
 from nncf.openvino.graph.transformations.commands import OVTargetPoint
 from nncf.openvino.graph.transformations.commands import OVWeightUpdateCommand
 from nncf.openvino.statistics.collectors import OVAbsMaxReducer
 from nncf.openvino.statistics.collectors import OVNNCFCollectorTensorProcessor
 from nncf.quantization.algorithms.smooth_quant.backend import SmoothQuantAlgoBackend
 
+OV_PRE_LAYER_TARGET_TYPE = TargetType.PRE_LAYER_OPERATION
+
 
 class OVSmoothQuantAlgoBackend(SmoothQuantAlgoBackend):
     @property
-    def convolution_metatype(self) -> OperatorMetatype:
-        return OVConvolutionMetatype
+    def convolution_metatypes(self) -> List[OperatorMetatype]:
+        return [OVConvolutionMetatype]
 
     @property
-    def matmul_metatype(self) -> OperatorMetatype:
-        return OVMatMulMetatype
+    def matmul_metatypes(self) -> List[OperatorMetatype]:
+        return [OVMatMulMetatype]
 
     @property
     def quantize_agnostic_metatypes(self) -> List[OperatorMetatype]:
         return QUANTIZE_AGNOSTIC_OPERATIONS
 
     @staticmethod
+    def pre_layer_target_type() -> TargetType:
+        return OV_PRE_LAYER_TARGET_TYPE
+
+    @staticmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> OVTargetPoint:
         return OVTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
     def is_node_with_weights(node: NNCFNode) -> bool:
         return node.layer_attributes and node.layer_attributes.constant_attributes
 
     @staticmethod
-    def get_input_ports_map(node: NNCFNode, nncf_graph: NNCFGraph) -> Dict[str, int]:
+    def get_activations_port_id(node: NNCFNode, nncf_graph: NNCFGraph) -> int:
         weight_ports = node.layer_attributes.get_const_port_ids()
         activation_ports = [
             e.input_port_id for e in nncf_graph.get_input_edges(node) if e.input_port_id not in weight_ports
         ]
 
-        if len(weight_ports) != 1 or len(activation_ports) != 1:
-            raise RuntimeError(f"Too many weight or activation ports for {node.node_name} node")
-
-        return {"activation": activation_ports[0], "weight": weight_ports[0]}
+        if len(activation_ports) != 1:
+            raise nncf.InternalError(f"Too many weight or activation ports for {node.node_name} node")
+        return activation_ports[0]
 
     @staticmethod
     def get_channel_agnostic_reduction_axes(channel_axis: int, shape: Tuple[int]) -> Tuple[int]:
         return get_channel_agnostic_reduction_axes([channel_axis], shape)
 
     @staticmethod
     def get_abs_max_channel_collector(
@@ -80,99 +88,81 @@
         collector = TensorCollector()
         reducer = OVAbsMaxReducer(reduction_axes=stats_reduction_axes, inplace=inplace)
         aggregator = MaxAggregator(tensor_processor=OVNNCFCollectorTensorProcessor, num_samples=num_samples)
         collector.register_statistic_branch(branch_key, reducer, aggregator)
         return collector
 
     @staticmethod
-    def process_weight_statistics(weights: np.ndarray, reduction_shape: Tuple[int]) -> np.ndarray:
-        return np.max(np.abs(weights), axis=reduction_shape)
-
-    @staticmethod
-    def get_weight_value(node_with_weight: NNCFNode, model: ov.Model, port_id: int) -> np.ndarray:
-        return get_weight_value(node_with_weight, model, port_id)
+    def get_weight_value(node_with_weight: NNCFNode, model: ov.Model) -> Tensor:
+        port_id = OVSmoothQuantAlgoBackend.get_weight_tensor_port_id(node_with_weight)
+        return Tensor(get_weight_value(node_with_weight, model, port_id))
 
     @staticmethod
     def get_weight_tensor_port_id(node: NNCFNode) -> int:
         const_ids = node.layer_attributes.get_const_port_ids()
         if len(const_ids) != 1:
-            raise RuntimeError(f"Found more than 1 port for {node.node_name} node")
+            raise nncf.InternalError(f"Found more than 1 port for {node.node_name} node")
         return const_ids[0]
 
     @staticmethod
-    def clip_statistics(statistics: np.ndarray) -> np.ndarray:
-        a_min = 1e-5
-        squeezed = np.squeeze(statistics)
-        return np.clip(squeezed, a_min=a_min, a_max=None)
-
-    @staticmethod
-    def calculate_scale_and_ratio(
-        activations: np.ndarray, weights: np.ndarray, alpha: float, quantile: Optional[float] = 0.1
-    ) -> np.ndarray:
-        scales = np.power(activations, alpha) / (np.power(weights, 1 - alpha) + np.finfo(float).eps)
-
-        a_min = np.quantile(scales, quantile)
-        a_max = 1e2
-
-        scales = np.clip(scales, a_min=a_min, a_max=a_max)
-        ratio = scales.min() / (scales.max() + np.finfo(float).eps)
-        return scales, ratio
-
-    @staticmethod
-    def calculate_activation_scale(scale_value: np.ndarray, activations_size: int, channel_axis: int) -> np.ndarray:
-        activation_scale = scale_value ** (-1)
-        if activations_size > 1:
-            reshape_shape = np.ones(activations_size, dtype=np.int64)
-            reshape_shape[channel_axis] = activation_scale.size
-            activation_scale = np.reshape(activation_scale, reshape_shape)
-        return activation_scale
-
-    @staticmethod
-    def calculate_weight_scale(scale_value: np.ndarray, weights_size: int, channel_axis: int) -> np.ndarray:
-        weight_scale = scale_value
-        if weights_size > 1:
-            reshape_shape = np.ones(weights_size, dtype=np.int64)
-            reshape_shape[channel_axis] = scale_value.size
-            weight_scale = np.reshape(scale_value, reshape_shape)
-        return weight_scale
-
-    @staticmethod
-    def weight_update_command(
-        node_with_weight: NNCFNode, weight_value: np.ndarray, weight_port_id: int
-    ) -> OVWeightUpdateCommand:
+    def weight_update_command(node_with_weight: NNCFNode, weight_value: np.ndarray) -> OVWeightUpdateCommand:
+        weight_port_id = OVSmoothQuantAlgoBackend.get_weight_tensor_port_id(node_with_weight)
         return OVCommandCreator.create_command_to_update_weight(node_with_weight, weight_value, weight_port_id)
 
     @staticmethod
     def scale_insertion_command(
-        source_node: NNCFNode, scale_value: np.ndarray, port_id: int, nodes: List[NNCFNode], scale_node_name: str
+        source_node: NNCFNode,
+        scale_value: np.ndarray,
+        source_output_port_id: int,
+        nodes: List[NNCFNode],
+        scale_node_name: str,
     ) -> OVMultiplyInsertionCommand:
-        return OVCommandCreator.multiply_insertion_command(source_node, nodes, port_id, scale_value, scale_node_name)
+        return OVCommandCreator.multiply_insertion_command(
+            source_node, nodes, source_output_port_id, scale_value, scale_node_name
+        )
 
     @staticmethod
     def get_activation_channel_axis(node: NNCFNode, port_id: int) -> int:
         channel_axis = 1
 
         if port_id > 1:
-            raise RuntimeError(f"{node.metatype.name} can not take more than 2 input tensors.")
+            raise nncf.InternalError(f"{node.metatype.name} can not take more than 2 input tensors.")
 
-        if node.metatype == OVMatMulMetatype:
-            if (
-                node.layer_attributes is not None
-                and node.layer_attributes.input_attributes is not None
-                and "transpose" in node.layer_attributes.input_attributes
-            ):
-                transpose = node.layer_attributes.input_attributes["transpose"]
-                channel_axis = OVSmoothQuantAlgoBackend.calculate_port_based_channel_axis(port_id, transpose)
+        if (
+            node.metatype == OVMatMulMetatype
+            and node.layer_attributes is not None
+            and node.layer_attributes.input_attributes is not None
+            and "transpose" in node.layer_attributes.input_attributes
+        ):
+            transpose = node.layer_attributes.input_attributes["transpose"]
+            channel_axis = OVSmoothQuantAlgoBackend.calculate_port_based_channel_axis(port_id, transpose)
 
         return channel_axis
 
     @staticmethod
     def get_weight_channel_axis(node: NNCFNode) -> int:
         if node.metatype != OVMatMulMetatype:
             return 1
 
         weights_layout = get_linear_weights_layout_from_node(node)
         return weights_layout.index(OVLayoutElem.C_IN)
 
     @staticmethod
     def calculate_port_based_channel_axis(port_id: int, transpose: bool) -> int:
         return -2 + port_id if transpose else -1 - port_id
+
+    @staticmethod
+    def is_node_with_shared_weight(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        weight_port_id = OVSmoothQuantAlgoBackend.get_weight_tensor_port_id(node)
+        weight_node = nncf_graph.get_input_edges(node)[weight_port_id].from_node
+        return len(nncf_graph.get_next_nodes(weight_node)) > 1
+
+    @staticmethod
+    def get_filter_fn_for_statistics(activation_port_id: int, algorithm_key: str) -> Callable[[StatisticPoint], bool]:
+        def filter_func(point: StatisticPoint) -> bool:
+            return (
+                algorithm_key in point.algorithm_to_tensor_collectors
+                and point.target_point.type == OV_PRE_LAYER_TARGET_TYPE
+                and point.target_point.port_id == activation_port_id
+            )
+
+        return filter_func
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/__init__.py` & `nncf-2.9.0/nncf/tensorflow/callbacks/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/algorithm.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/algorithm.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#      http://www.apache.org/licenses/LICENSE-2.0
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from collections import defaultdict
 from typing import Dict, List, Optional, OrderedDict, Tuple, TypeVar
 
+import nncf
 from nncf import Dataset
 from nncf.common.factory import StatisticsAggregatorFactory
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.track_progress import track
@@ -36,14 +27,15 @@
 from nncf.common.utils.backend import get_backend
 from nncf.common.utils.helpers import create_table
 from nncf.experimental.tensor import Tensor
 from nncf.experimental.tensor.definitions import TensorDataType
 from nncf.parameters import CompressWeightsMode
 from nncf.parameters import SensitivityMetric
 from nncf.quantization.algorithms.algorithm import Algorithm
+from nncf.quantization.algorithms.weight_compression.awq import AWQ
 from nncf.quantization.algorithms.weight_compression.config import WeightCompressionParameters
 from nncf.quantization.algorithms.weight_compression.mixed_precision import MIXED_PRECISION_CRITERIA
 from nncf.quantization.algorithms.weight_compression.weight_lowering import WeightCompressionConfig
 from nncf.scopes import IgnoredScope
 from nncf.scopes import get_ignored_node_names_from_ignored_scope
 
 TModel = TypeVar("TModel")
@@ -62,14 +54,16 @@
         self,
         mode: CompressWeightsMode,
         ratio: float,
         group_size: int,
         ignored_scope: IgnoredScope,
         all_layers: bool,
         sensitivity_metric: SensitivityMetric,
+        awq: bool,
+        subset_size: int,
     ):
         """
         :param mode: Defines a mode for weight compression.
             INT8_SYM stands for 8-bit integer symmetric quantization of all weights.
                 Weights are quantized symmetrically with a fixed zero point equals to 128.
             INT8_ASYM is the same as INT8_SYM mode, but weights are quantized to a primary precision asymmetrically
                 with a typical non-fixed zero point.
@@ -83,29 +77,34 @@
             NF4 is the same as INT4_SYM mode, but primary precision is NF4 data type without zero point.
         :param ratio: the ratio between primary and backup precisions (e.g. 0.9 means 90% of layers quantized to NF4
             and the rest to INT8_ASYM).
         :param group_size: number of weights (e.g. 128) in the channel dimension
             that share quantization parameters (scale). The value -1 means no grouping.
         :param ignored_scope: An ignored scope that defined the list of model control
             flow graph nodes to be ignored during quantization.
-        :param all_layers: Indicates whether embeddings and last layers should be compressed to a primary
-            precision. By default, the backup precision is assigned for the embeddings and last layers.
+        :param all_layers: Indicates whether embeddings and last MatMul layers should be compressed to a primary
+            precision. By default, the backup precision is assigned for the embeddings and last MatMul layers.
         :param sensitivity_metric: The sensitivity metric for assigning quantization precision to layers. In order to
             preserve the accuracy of the model, the more sensitive layers receives a higher precision.
+        :param awq: determines whether to use or not modified AWQ algorithm.
+        :param subset_size: Number of data samples to calculate activation statistics used for assigning different
+            quantization precision.
         """
         super().__init__()
         self._mode = mode
         self._group_size = group_size
         self._ratio = ratio
         self._ignored_scope = ignored_scope
         self._backend_entity = None
         self._algorithm_key = f"CW_{hash(self)}"
         self._fp_inputs = defaultdict(list)
         self._all_layers = all_layers
         self._sensitivity_metric = sensitivity_metric
+        self._awq = awq
+        self._subset_size = subset_size
 
     @property
     def available_backends(self) -> List[BackendType]:
         return [BackendType.OPENVINO, BackendType.TORCH]
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
@@ -119,26 +118,31 @@
 
             self._backend_entity = OVWeightCompressionAlgoBackend(model)
         elif model_backend == BackendType.TORCH:
             from nncf.quantization.algorithms.weight_compression.torch_backend import PTWeightCompressionAlgoBackend
 
             self._backend_entity = PTWeightCompressionAlgoBackend()
         else:
-            raise RuntimeError(
+            raise nncf.UnsupportedBackendError(
                 "Cannot return backend-specific entity because {} is not supported!".format(model_backend.value)
             )
 
     def _get_nodes_to_compress(self, nncf_graph: NNCFGraph) -> List[NNCFNode]:
         """
         Collects nodes in the model's graph corresponding to the layers for weight compression.
 
         :param nncf_graph: NNCFGraph instance.
         :return: List with the data for each layer.
         """
-        weighted_metatypes = self._backend_entity.matmul_metatypes + self._backend_entity.embedding_metatypes
+        weighted_metatypes = (
+            self._backend_entity.matmul_metatypes
+            + self._backend_entity.embedding_metatypes
+            + self._backend_entity.convolution_metatypes
+        )
+
         ordered_nodes_to_compress = []
         ignored_names = get_ignored_node_names_from_ignored_scope(
             self._ignored_scope, nncf_graph, strict=self._ignored_scope.validate
         )
         for node in nncf_graph.topological_sort():
             is_node_with_weights = self._backend_entity.is_node_with_weights(node, nncf_graph)
             is_within_scope = should_consider_scope(node.node_name, ignored_names)
@@ -154,24 +158,38 @@
         and backup precisions.
 
         :param all_weight_params: List of all weight parameters.
         :param is_last_layer_shared: Indicates whether the last layer which shares the weight
             should be quantized or not.
         :return: Information about each weight node that is considered for mixed precision.
         """
-        if self._mode in [CompressWeightsMode.INT8_SYM, CompressWeightsMode.INT8_ASYM] or self._all_layers:
+        if self._mode in [CompressWeightsMode.INT8_SYM, CompressWeightsMode.INT8_ASYM]:
             return all_weight_params
 
+        if self._all_layers:
+            return list(filter(lambda wp: len(wp.reduction_axes) == 1, all_weight_params))
+
         ratio_defining_params = list(
             filter(
-                lambda wp: wp.node_with_weight.metatype not in self._backend_entity.embedding_metatypes,
+                lambda wp: wp.node_with_weight.metatype in self._backend_entity.matmul_metatypes,
                 all_weight_params,
             )
         )
-        if not is_last_layer_shared:
+
+        # Embedding layers are quantized to 4-bits only if all_layers=True.
+        if self._all_layers:
+            embedding_params = list(
+                filter(
+                    lambda wp: wp.node_with_weight.metatype in self._backend_entity.embedding_metatypes,
+                    all_weight_params,
+                )
+            )
+            ratio_defining_params.extend(embedding_params)
+
+        if not self._all_layers and not is_last_layer_shared:
             ratio_defining_params = ratio_defining_params[:-1]
         return ratio_defining_params
 
     def _set_weight_compression_config(
         self,
         ratio_defining_params: List[WeightCompressionParameters],
         model: TModel,
@@ -261,15 +279,15 @@
         dataset: Optional[Dataset] = None,
     ) -> TModel:
         self._set_backend_entity(model)
         nodes_to_compress = self._get_nodes_to_compress(graph)
 
         activations = {}
         if dataset is not None and self._sensitivity_metric != SensitivityMetric.WEIGHT_QUANTIZATION_ERROR:
-            activations = self._get_activations(dataset, nodes_to_compress, graph, model)
+            activations = self._get_activations(dataset, self._subset_size, nodes_to_compress, graph, model)
 
         transformed_model = self.do_compression(model, graph, nodes_to_compress, activations)
         return transformed_model
 
     def do_compression(
         self,
         model: TModel,
@@ -289,47 +307,62 @@
                         is_last_layer_shared = True
                     continue
 
                 weight = self._backend_entity.get_weight(node, weight_port_id, model, graph)
                 if weight.dtype not in [TensorDataType.float32, TensorDataType.float16, TensorDataType.float64]:
                     continue
                 reduction_axes = self._backend_entity.get_channel_agnostic_reduction_axes(node, weight_port_id, graph)
-                if isinstance(reduction_axes, tuple) and len(reduction_axes) != 1:
+                if (
+                    self._group_size != -1
+                    and self._all_layers
+                    and node.metatype in self._backend_entity.embedding_metatypes
+                    and isinstance(reduction_axes, tuple)
+                    and len(reduction_axes) != 1
+                ):
+                    # NNCF supports multiple reduction axes only for ops with group_size != -1.
+                    # Convolution ops are always quantized to 8-bits (without groups).
+                    # Embedding layers are quantized to 4-bits only if all_layers=True.
+                    # MatMul ops can't have multiple reduction axes.
                     nncf_logger.warning(
                         f"Weight compression expects a single reduction axis, but {len(reduction_axes)} given. "
                         f"Weight shape: {weight.shape}, reduction axes: {reduction_axes}, "
-                        f"node name: {node.node_name}. The node won't be quantized."
+                        f"node name: {node.node_name}. The node will be asymmetrically quantized to 8 bits."
                     )
-                    continue
-                reduction_axis = reduction_axes[0] if isinstance(reduction_axes, tuple) else reduction_axes
 
                 weight_params = WeightCompressionParameters(
-                    weight_name, node, weight_port_id, weight.size, reduction_axis
+                    weight_name, node, weight_port_id, weight.size, reduction_axes
                 )
                 all_weight_params.append(weight_params)
                 weight_names.add(weight_name)
 
         ratio_defining_params = self._get_ratio_defining_params(all_weight_params, is_last_layer_shared)
         self._set_weight_compression_config(ratio_defining_params, model, graph, activations)
         nncf_logger.info(self._get_bitwidth_distribution_str(all_weight_params, ratio_defining_params))
 
+        if self._awq and activations is not None and self._mode != CompressWeightsMode.NF4:
+            awq_algo = AWQ(
+                model, self._backend_entity.name_to_node_mapping, all_weight_params, nodes_to_compress, activations
+            )
+            awq_algo.apply(model, graph)
+
         # Compress model using weight compression parameters
         transformed_model = self._backend_entity.transform_model(
             model, graph, track(all_weight_params, description="Applying Weight Compression")
         )
 
         self._backend_entity.dump_parameters(
             model,
             parameters={
                 "mode": self._mode.value,
                 "group_size": self._group_size,
                 "ratio": self._ratio,
                 "all_layers": self._all_layers,
                 "ignored_scope": self._ignored_scope,
                 "sensitivity_metric": self._sensitivity_metric.value,
+                "awq": self._awq,
             },
             algo_name="weight_compression",
         )
         return transformed_model
 
     def get_statistic_points(self, model: TModel, graph: NNCFGraph) -> StatisticPointsContainer:
         pass
@@ -378,27 +411,27 @@
         ):
             for value in tensor_collector.get_statistics().values:
                 input_fp.append(Tensor(value))
         self._fp_inputs[input_id] = input_fp
         return self._fp_inputs[input_id]
 
     def _get_activations(
-        self, dataset: Dataset, nodes_to_compress: List[NNCFNode], graph: NNCFGraph, model: TModel
+        self, dataset: Dataset, subset_size: int, nodes_to_compress: List[NNCFNode], graph: NNCFGraph, model: TModel
     ) -> Dict[str, List[Tensor]]:
         """
         Collects input activations for the given nodes on the dataset.
 
         :param dataset: Dataset to collect values.
+        :param subset_size: Number of data samples to calculate activation statistics used for assigning different
+            quantization precision.
         :param nodes_to_compress: List of nodes, whose inputs are collected.
         :param model: Model for statistics collection.
         :param graph: Model graph.
         :return: statistics values itself per node name.
         """
-        subset_size = 128
-
         activations = {}
         _collected_stat_inputs_map = {}
         statistic_container = StatisticPointsContainer()
         all_act_nodes = set()
         act_vs_shared_node_names_mapping = defaultdict(list)
         matmul_metatypes = self._backend_entity.matmul_metatypes
         filtered_nodes = filter(lambda node: node.metatype in matmul_metatypes, nodes_to_compress)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/backend.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -31,14 +31,21 @@
     def matmul_metatypes(self) -> List[OperatorMetatype]:
         """
         Property for the backend-specific metatypes for matmul layers.
         """
 
     @property
     @abstractmethod
+    def convolution_metatypes(self) -> List[OperatorMetatype]:
+        """
+        Property for the backend-specific metatypes for convolution layers.
+        """
+
+    @property
+    @abstractmethod
     def embedding_metatypes(self) -> List[OperatorMetatype]:
         """
         Property for the backend-specific metatypes for embedding layers.
         """
 
     @staticmethod
     @abstractmethod
@@ -85,14 +92,28 @@
         :param weight_port_id: The weight port id for given node with weight.
         :param model: The model.
         :param graph: The model graph associated with the model.
         :return: The weight tensor.
         """
 
     @abstractmethod
+    def set_weight(
+        self, node_with_weight: NNCFNode, weight_port_id: int, model: TModel, graph: NNCFGraph, weight: Tensor
+    ) -> None:
+        """
+        Update a weight associated with the given node on the given port id.
+
+        :param node_with_weight: The node with weight.
+        :param weight_port_id: The weight port id for given node with weight.
+        :param model: The model.
+        :param graph: The model graph associated with the model.
+        :param weight: The weight tensor.
+        """
+
+    @abstractmethod
     def transform_model(
         self, model: TModel, graph: NNCFGraph, weight_compression_parameters: Iterable[WeightCompressionParameters]
     ) -> TModel:
         """
         Applies weight compression transformations to the model.
 
         :param model: Model in which the weights will be compressed according to the weight compression description.
@@ -144,7 +165,15 @@
         Dumps the given parameters into Model's meta section.
 
         :param model: ov.Model instance.
         :param algo_name: Name of the algorithm to which the parameters refer.
         :param parameters: Incoming dictionary with parameters to save.
         :param path: Optional list of the paths.
         """
+
+
+class AWQAlgoBackend(WeightCompressionAlgoBackend):
+    @staticmethod
+    def get_awq_patterns() -> Dict:
+        """
+        Returns patterns of nodes in network graph for applying AWQ algorithm.
+        """
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/config.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/config.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from dataclasses import dataclass
-from typing import Optional, TypeVar
+from typing import Optional, Tuple, TypeVar
 
 import numpy as np
 
 from nncf.common.graph.graph import NNCFNode
 from nncf.parameters import CompressWeightsMode
 
 TWeightType = TypeVar("TWeightType")
@@ -45,22 +45,22 @@
     """
     Weight compression parameters determine how and what weight should be compressed.
 
     :param weight_name: Unique weight name.
     :param node_with_weight: Node with weight in the NNCF graph.
     :param weight_port_id: Number of elements in the weight array.
     :param num_weights: Number of elements in the weight array.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
     :param compression_config: Configuration of weight compression for the weight node.
     """
 
     weight_name: str
     node_with_weight: NNCFNode
     weight_port_id: int
     num_weights: np.uint64
-    reduction_axis: int
+    reduction_axes: Tuple[int, ...]
     compression_config = WeightCompressionConfig()
 
     def __post_init__(self):
         # Explicitly cast num_weights to avoid overflow on finding total number of weights.
         # The issue happens on Windows, because np.ndarray.size() returns np.int32 and sum of weights is more than 2^32.
         self.num_weights = np.uint64(self.num_weights)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/mixed_precision.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/mixed_precision.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -98,26 +98,26 @@
     """
 
     def _calc_weight_sensitivity(self, weight_param: WeightCompressionParameters) -> float:
         weight = self._backend_entity.get_weight(
             weight_param.node_with_weight, weight_param.weight_port_id, self._model, self._graph
         )
         backup_config = weight_param.compression_config
-        reduction_axis = weight_param.reduction_axis
-        int_error = get_integer_quantization_error(weight, reduction_axis, backup_config)
+        reduction_axes = weight_param.reduction_axes
+        int_error = get_integer_quantization_error(weight, reduction_axes, backup_config)
         eps = fns.finfo(weight).eps
         return 1 / (int_error + eps)
 
     def _calc_score_per_node(self, weight_param: WeightCompressionParameters) -> float:
         weight_score = self._calc_weight_sensitivity(weight_param)
         return weight_score
 
     def _calc_sensitivity(self) -> List[float]:
         scores = []
-        for weight_param in track(self._weight_params, description="Searching for Mixed-Precision Configuration"):
+        for weight_param in track(self._weight_params, description="Mixed-Precision assignment"):
             scores.append(self._calc_score_per_node(weight_param))
         return scores
 
 
 class DataBasedCriterion(DataFreeCriterion):
     """
     Data-based mixed precision criterion that takes into account outliers in the input activations.
@@ -164,22 +164,22 @@
         return htrace
 
     def _calc_weight_sensitivity(self, weight_param: WeightCompressionParameters) -> float:
         weight = self._backend_entity.get_weight(
             weight_param.node_with_weight, weight_param.weight_port_id, self._model, self._graph
         )
         backup_config = weight_param.compression_config
-        reduction_axis = weight_param.reduction_axis
+        reduction_axes = weight_param.reduction_axes
 
         orig_shape = weight.shape
 
         if weight.dtype != TensorDataType.float32:
             weight = weight.astype(TensorDataType.float32)
 
-        compressed_weights, scale, zero_point = do_integer_quantization(weight, reduction_axis, backup_config)
+        compressed_weights, scale, zero_point = do_integer_quantization(weight, reduction_axes, backup_config)
         decompressed_weight = (compressed_weights - zero_point).astype(weight.dtype) * scale
         decompressed_weight = decompressed_weight.reshape(orig_shape)
         return fns.linalg.norm(decompressed_weight - weight, ord="fro").item()
 
 
 @MIXED_PRECISION_CRITERIA.register(SensitivityMetric.MEAN_ACTIVATION_VARIANCE)
 class MeanVarianceCriterion(DataBasedCriterion):
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/openvino_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/openvino_backend.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -15,40 +15,50 @@
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.experimental.tensor.tensor import Tensor
-from nncf.openvino.graph.metatypes.openvino_metatypes import OVEmbeddingMetatype
-from nncf.openvino.graph.metatypes.openvino_metatypes import OVMatMulMetatype
+from nncf.openvino.graph.metatypes import openvino_metatypes as om
 from nncf.openvino.graph.model_transformer import OVModelTransformer
 from nncf.openvino.graph.node_utils import get_channel_agnostic_reduction_axes
 from nncf.openvino.graph.node_utils import get_const_value
 from nncf.openvino.graph.node_utils import get_weight_channel_axes
 from nncf.openvino.graph.transformations.commands import OVTargetPoint
 from nncf.openvino.rt_info import dump_parameters
 from nncf.openvino.statistics.collectors import get_raw_stat_collector
 from nncf.parameters import CompressWeightsMode
+from nncf.quantization.algorithms.weight_compression.awq_patterns import get_awq_patterns
 from nncf.quantization.algorithms.weight_compression.backend import WeightCompressionAlgoBackend
 from nncf.quantization.algorithms.weight_compression.config import WeightCompressionParameters
 from nncf.quantization.algorithms.weight_compression.weight_lowering import compress_weight
 
 
 class OVWeightCompressionAlgoBackend(WeightCompressionAlgoBackend):
     def __init__(self, model: ov.Model):
         self.name_to_node_mapping = OVModelTransformer._get_name_to_node_mapping(model)
 
     @property
     def matmul_metatypes(self) -> List[OperatorMetatype]:
-        return [OVMatMulMetatype]
+        return [om.OVMatMulMetatype]
+
+    @property
+    def convolution_metatypes(self) -> List[OperatorMetatype]:
+        return [
+            om.OVConvolutionMetatype,
+            om.OVDepthwiseConvolutionMetatype,
+            om.OVConvolutionBackpropDataMetatype,
+            om.OVGroupConvolutionMetatype,
+            om.OVGroupConvolutionBackpropDataMetatype,
+        ]
 
     @property
     def embedding_metatypes(self) -> List[OperatorMetatype]:
-        return [OVEmbeddingMetatype]
+        return [om.OVEmbeddingMetatype]
 
     @staticmethod
     def is_node_with_weights(node: NNCFNode, graph: NNCFGraph) -> bool:
         return node.layer_attributes and node.layer_attributes.constant_attributes
 
     @staticmethod
     def get_channel_agnostic_reduction_axes(
@@ -85,14 +95,35 @@
 
     def get_weight(self, node_with_weight: NNCFNode, weight_port_id: int, model: ov.Model, graph: NNCFGraph) -> Tensor:
         weight_name = node_with_weight.layer_attributes.constant_attributes[weight_port_id]["name"]
         weight_node = self.name_to_node_mapping[weight_name]
         weight_tensor = get_const_value(weight_node)
         return Tensor(weight_tensor)
 
+    def set_weight(
+        self, node_with_weight: NNCFNode, weight_port_id: int, model: ov.Model, graph: NNCFGraph, weight: Tensor
+    ):
+        node_with_const = self.name_to_node_mapping[node_with_weight.node_name]
+
+        const_port = node_with_const.input(weight_port_id)
+        const_node = node_with_const.input_value(weight_port_id).get_node()
+
+        new_const_node = ov.runtime.op.Constant(weight.data, shared_memory=True)
+        new_const_node.set_friendly_name(const_node.get_friendly_name())
+        const_port.replace_source_output(new_const_node.output(0))
+
+        const_name = node_with_weight.layer_attributes.constant_attributes[weight_port_id]["name"]
+        self.name_to_node_mapping[const_name] = new_const_node
+
+        new_output = new_const_node.output(0)
+        for target_input in const_node.output(0).get_target_inputs():
+            target_input.replace_source_output(new_output)
+
+        del const_node
+
     def transform_model(
         self, model: ov.Model, graph: NNCFGraph, weight_compression_parameters: Iterable[WeightCompressionParameters]
     ) -> ov.Model:
         for wc_params in weight_compression_parameters:
             compression_config = wc_params.compression_config
             if compression_config.mode == CompressWeightsMode.NF4:
                 compression_dtype = ov.Type.nf4
@@ -113,33 +144,35 @@
             const_attributes = wc_params.node_with_weight.layer_attributes.constant_attributes[wc_params.weight_port_id]
             const_node_name = const_attributes["name"]
             const_node = self.name_to_node_mapping[const_node_name]
             const_dtype = const_node.output(0).get_element_type().to_dtype()
 
             weight = Tensor(get_const_value(const_node))
             original_shape = weight.shape
-            compressed_weight = compress_weight(weight, wc_params.reduction_axis, compression_config)
+            compressed_weight = compress_weight(weight, wc_params.reduction_axes, compression_config)
 
             compressed_const = opset.constant(
                 compressed_weight.tensor.data, dtype=compression_dtype, name=const_node_name
             )
             converted_const = opset.convert(compressed_const, const_dtype)
             if compressed_weight.zero_point is not None:
                 zero_point_const = opset.constant(
                     compressed_weight.zero_point.data,
                     dtype=compression_dtype,
                     name=f"{const_node_name}/zero_point",
                 )
                 converted_zero_point = opset.convert(zero_point_const, const_dtype)
                 converted_const = opset.subtract(converted_const, converted_zero_point)
 
-            scale_data = compressed_weight.scale.data
+            scale_const = opset.constant(
+                compressed_weight.scale.data, dtype=const_dtype, name=f"{const_node_name}/scale"
+            )
             mul = opset.multiply(
                 converted_const,
-                scale_data.astype(const_dtype),
+                scale_const,
                 name=f"{const_node_name}/fq_weights_{wc_params.weight_port_id}",
             )
 
             if compression_config.group_size != -1:
                 mul = opset.reshape(mul, output_shape=original_shape, special_zero=False)
 
             mul_output = mul.output(0)
@@ -152,7 +185,13 @@
         return model
 
     @staticmethod
     def dump_parameters(
         model: ov.Model, parameters: Dict, algo_name: Optional[str] = "quantization", path: Optional[List] = None
     ) -> None:
         dump_parameters(model, parameters, algo_name, path)
+
+
+class OVAWQAlgoAlgoBackend(OVWeightCompressionAlgoBackend):
+    @staticmethod
+    def get_awq_patterns():
+        return get_awq_patterns(om.OVMatMulMetatype, om.OVMultiplyMetatype)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/torch_backend.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/torch_backend.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Iterable, List, Optional, Tuple, Union
 
 import torch
 
+import nncf
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import CONST_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
@@ -35,28 +36,32 @@
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.layers import WeightsDecompressor
 from nncf.torch.tensor_statistics.collectors import get_raw_stat_collector
 
 
 def split_weight_name(weight_name: str) -> Tuple[str, str]:
     index = weight_name.rfind(".")
+    if index == -1:
+        return str(), weight_name
     module_name = weight_name[:index]
     weight_attr_name = weight_name[index + 1 :]
     return module_name, weight_attr_name
 
 
 def get_module_by_name(module_name: str, model: torch.nn.Module) -> torch.nn.Module:
+    if not module_name:
+        return model
     curr_module = model
     for name in module_name.split("."):
         for child_name, child_module in curr_module.named_children():
             if child_name == name:
                 curr_module = child_module
                 break
         else:
-            raise RuntimeError(f"Could not find the {module_name} module in the model.")
+            raise nncf.ModuleNotFoundError(f"Could not find the {module_name} module in the model.")
     return curr_module
 
 
 def find_weight_node_in_constant_subgraph(node: NNCFNode, graph: NNCFGraph) -> Union[NNCFNode, None]:
     if node.metatype == om.PTNoopMetatype:
         prev_nodes = graph.get_previous_nodes(node)
         if len(prev_nodes) != 1:
@@ -69,39 +74,55 @@
 
 def get_weight_node(node_with_weight: NNCFNode, weight_port_id: int, graph: NNCFGraph) -> NNCFNode:
     for prev_node in graph.get_previous_nodes(node_with_weight):
         edge = graph.get_edge(prev_node, node_with_weight)
         if edge.input_port_id == weight_port_id:
             weight_node = find_weight_node_in_constant_subgraph(prev_node, graph)
             if weight_node is None:
-                raise RuntimeError("Could not find a constant node in the model graph.")
+                raise nncf.InternalError("Could not find a constant node in the model graph.")
             return weight_node
 
 
 class PTWeightCompressionAlgoBackend(WeightCompressionAlgoBackend):
     TARGET_TYPE_TO_PT_INS_TYPE_MAP = {
         TargetType.PRE_LAYER_OPERATION: TargetType.OPERATOR_PRE_HOOK,
         TargetType.POST_LAYER_OPERATION: TargetType.OPERATOR_POST_HOOK,
     }
     MATMUL_METATYPES = [om.PTLinearMetatype, om.PTMatMulMetatype, om.PTAddmmMetatype]
     EMBEDDING_METATYPES = [om.PTEmbeddingMetatype]
+    CONVOLUTION_METATYPES = [
+        om.PTConv1dMetatype,
+        om.PTConv2dMetatype,
+        om.PTConv3dMetatype,
+        om.PTDepthwiseConv1dSubtype,
+        om.PTDepthwiseConv2dSubtype,
+        om.PTDepthwiseConv3dSubtype,
+        om.PTConvTranspose1dMetatype,
+        om.PTConvTranspose2dMetatype,
+        om.PTConvTranspose3dMetatype,
+    ]
 
     @property
     def matmul_metatypes(self) -> List[OperatorMetatype]:
         return PTWeightCompressionAlgoBackend.MATMUL_METATYPES
 
     @property
     def embedding_metatypes(self) -> List[OperatorMetatype]:
         return PTWeightCompressionAlgoBackend.EMBEDDING_METATYPES
 
+    @property
+    def convolution_metatypes(self) -> List[OperatorMetatype]:
+        return PTWeightCompressionAlgoBackend.CONVOLUTION_METATYPES
+
     @staticmethod
     def is_node_with_weights(node: NNCFNode, graph: NNCFGraph) -> bool:
         if (
             node.metatype not in PTWeightCompressionAlgoBackend.MATMUL_METATYPES
             and node.metatype not in PTWeightCompressionAlgoBackend.EMBEDDING_METATYPES
+            and node.metatype not in PTWeightCompressionAlgoBackend.CONVOLUTION_METATYPES
         ):
             return False
         for prev_node in graph.get_previous_nodes(node):
             edge = graph.get_edge(prev_node, node)
             if edge.input_port_id not in node.metatype.weight_port_ids:
                 continue
             weight_node = find_weight_node_in_constant_subgraph(prev_node, graph)
@@ -124,30 +145,38 @@
     @staticmethod
     def get_channel_agnostic_reduction_axes(
         node_with_weight: NNCFNode, weight_port_id: int, graph: NNCFGraph
     ) -> Optional[Tuple[int]]:
         weight_node = get_weight_node(node_with_weight, weight_port_id, graph)
 
         ndims = len(weight_node.layer_attributes.shape)
-        reduction_axis = None
+        reduction_axes = None
         if node_with_weight.metatype == om.PTEmbeddingMetatype:
-            reduction_axis = [1]
+            reduction_axes = [1]
         elif node_with_weight.metatype == om.PTLinearMetatype:
-            reduction_axis = [ndims - 1]
+            reduction_axes = [ndims - 1]
         elif node_with_weight.metatype == om.PTMatMulMetatype:
             if weight_port_id == 0:
-                reduction_axis = [ndims - 1]
+                reduction_axes = [ndims - 1]
             elif weight_port_id == 1:
-                reduction_axis = [max(0, ndims - 2)]
+                reduction_axes = [max(0, ndims - 2)]
         elif node_with_weight.metatype == om.PTAddmmMetatype:
             if weight_port_id == 1:
-                reduction_axis = [ndims - 1]
+                reduction_axes = [ndims - 1]
             elif weight_port_id == 2:
-                reduction_axis = [max(0, ndims - 2)]
-        return reduction_axis
+                reduction_axes = [max(0, ndims - 2)]
+        elif node_with_weight.metatype in PTWeightCompressionAlgoBackend.CONVOLUTION_METATYPES:
+            channel_idx = (
+                1
+                if node_with_weight.metatype
+                in [om.PTConvTranspose1dMetatype, om.PTConvTranspose2dMetatype, om.PTConvTranspose3dMetatype]
+                else 0
+            )
+            reduction_axes = [i for i in range(ndims) if i != channel_idx]
+        return tuple(reduction_axes)
 
     @staticmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> PTTargetPoint:
         if NNCFGraphNodeType.INPUT_NODE in target_node_name or target_type == TargetType.POST_LAYER_OPERATION:
             port_id = None
         if target_type in PTWeightCompressionAlgoBackend.TARGET_TYPE_TO_PT_INS_TYPE_MAP:
             target_type = PTWeightCompressionAlgoBackend.TARGET_TYPE_TO_PT_INS_TYPE_MAP[target_type]
@@ -173,18 +202,23 @@
     ) -> Tensor:
         weight_node = get_weight_node(node_with_weight, weight_port_id, graph)
         weight_name = weight_node.layer_attributes.name
         module_name, weight_attr_name = split_weight_name(weight_name)
         module = get_module_by_name(module_name, model)
         weight = getattr(module, weight_attr_name)
         if weight is None or not isinstance(weight, torch.nn.Parameter):
-            raise RuntimeError(f"Could not find a torch.nn.Parameter in the model by name {weight_name}.")
+            raise nncf.InternalError(f"Could not find a torch.nn.Parameter in the model by name {weight_name}.")
 
         return Tensor(weight)
 
+    def set_weight(
+        self, node_with_weight: NNCFNode, weight_port_id: int, model: torch.nn.Module, graph: NNCFGraph, weight: Tensor
+    ):
+        pass
+
     def transform_model(
         self, model: NNCFNetwork, graph: NNCFGraph, weight_compression_parameters: Iterable[WeightCompressionParameters]
     ) -> NNCFNetwork:
         transformation_layout = TransformationLayout()
 
         for wc_params in weight_compression_parameters:
             compression_config = wc_params.compression_config
@@ -197,18 +231,18 @@
 
             weight_node = get_weight_node(wc_params.node_with_weight, wc_params.weight_port_id, graph)
             weight_name = weight_node.layer_attributes.name
             module_name, weight_attr_name = split_weight_name(weight_name)
             module = get_module_by_name(module_name, model)
             weight = getattr(module, weight_attr_name)
             if weight is None or not isinstance(weight, torch.nn.Parameter):
-                raise RuntimeError(f"Could not find a torch.nn.Parameter in the model by name {weight_name}.")
+                raise nncf.InternalError(f"Could not find a torch.nn.Parameter in the model by name {weight_name}.")
 
             # calculates compressed weights and decompression parameters
-            compressed_weight = compress_weight(Tensor(weight), wc_params.reduction_axis, compression_config)
+            compressed_weight = compress_weight(Tensor(weight), wc_params.reduction_axes, compression_config)
 
             # pack compressed tensor
             packed_tensor = compressed_weight.tensor.astype(TensorDataType.uint8)
 
             # sets compressed tensor
             compressed_parameter = torch.nn.Parameter(packed_tensor.data, requires_grad=False)
             setattr(module, weight_attr_name, compressed_parameter)
```

### Comparing `nncf-2.8.1/nncf/quantization/algorithms/weight_compression/weight_lowering.py` & `nncf-2.9.0/nncf/quantization/algorithms/weight_compression/weight_lowering.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,31 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Optional, Tuple
 
+import nncf
 from nncf.experimental.tensor import Tensor
 from nncf.experimental.tensor.definitions import TensorDataType
 from nncf.experimental.tensor.functions import numeric as fns
 from nncf.parameters import CompressWeightsMode
 from nncf.quantization.algorithms.weight_compression.config import WeightCompressionConfig
 from nncf.quantization.fake_quantize import calculate_scale_zero_point
 
+ReductionAxes = Tuple[int, ...]
+
 
 @dataclass
 class CompressedWeight:
     """
     Compressed weight and decompression parameters.
 
     :param tensor: The tensor with compressed weight.
@@ -32,70 +35,77 @@
     """
 
     tensor: Tensor
     scale: Tensor
     zero_point: Optional[Tensor] = None
 
 
-def reshape_weight_for_grouped_quantization(weight: Tensor, reduction_axis: int, group_size: int) -> Tuple[Tensor, int]:
+def reshape_weight_for_grouped_quantization(
+    weight: Tensor, reduction_axes: ReductionAxes, group_size: int
+) -> Tuple[Tensor, int]:
     """
     Reshapes weight for group-wise quantization and return a new reduction axis for collecting statistics per group
     dimension. Having weight with shapes [c_out, c_in] and group size = 128, shape of reshaped weight is
     [c_out, c_in // 128, 128].
 
     :param weight: Weight array to compress.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
     :param group_size: Number of weights (e.g. 128) in the channel dimension that share quantization parameters (scale).
     :return: reshaped weight and new reduction axis.
     """
     assert group_size != -1
-    assert isinstance(reduction_axis, int)
-    channel_size = weight.shape[reduction_axis]
+    if isinstance(reduction_axes, tuple) and len(reduction_axes) == 1:
+        reduction_axes = reduction_axes[0]
+    if not isinstance(reduction_axes, int):
+        raise NotImplementedError(
+            f"Group-wise quantization expects a single reduction axis, but given: {reduction_axes}."
+        )
+    channel_size = weight.shape[reduction_axes]
     if channel_size % group_size != 0:
-        raise RuntimeError(f"Channel size {channel_size} should be divisible by size of group {group_size}")
+        raise nncf.ValidationError(f"Channel size {channel_size} should be divisible by size of group {group_size}")
 
     num_groups_per_channel = channel_size // group_size
     shape = list(weight.shape)  # [a1, r, a2] - "r" refers to number of channels along reduction axis
-    shape[reduction_axis : reduction_axis + 1] = (num_groups_per_channel, group_size)
+    shape[reduction_axes : reduction_axes + 1] = (num_groups_per_channel, group_size)
     reshaped_weight = weight.reshape(shape)
-    reduction_axis += 1
-    return reshaped_weight, reduction_axis
+    reduction_axes += 1
+    return reshaped_weight, reduction_axes
 
 
 def calculate_normalized_weight_and_nf4_scale(
-    weight: Tensor, reduction_axis: int, group_size: int = -1
+    weight: Tensor, reduction_axes: ReductionAxes, group_size: int = -1
 ) -> Tuple[Tensor, Tensor]:
     """
     Calculates scale for nf4 quantization and normalizes weights by the scale.
     Weights are reshaped in case of positive value of group size.
 
     :param weight: Weight array to compress.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
     :param group_size: Number of weights (e.g. 128) in the channel dimension that share quantization parameters (scale).
         The value -1 means no grouping. Defaults to -1.
     :return: Normalized weight tensor of float32 type and nf4 scale tensor of float32 type.
     """
     if weight.dtype != TensorDataType.float32:
         weight = weight.astype(TensorDataType.float32)
 
     if group_size != -1:
         # weights are reshaped: [a1, r, a2] -> [a1, r//gs, gs, a2]
-        weight, reduction_axis = reshape_weight_for_grouped_quantization(weight, reduction_axis, group_size)
-        scale = fns.max(fns.abs(weight), axis=reduction_axis, keepdims=True)  # [a1, r//gs, 1, a2]
+        weight, reduction_axes = reshape_weight_for_grouped_quantization(weight, reduction_axes, group_size)
+        scale = fns.max(fns.abs(weight), axis=reduction_axes, keepdims=True)  # [a1, r//gs, 1, a2]
     else:
-        scale = fns.max(fns.abs(weight), axis=reduction_axis, keepdims=True)  # [a1, 1, a2]
+        scale = fns.max(fns.abs(weight), axis=reduction_axes, keepdims=True)  # [a1, 1, a2]
     eps = fns.finfo(weight).eps
     # NOTE: adding machine epsilon to avoid division by zero
     scale = fns.where(fns.abs(scale) < eps, eps, scale)
     norm_weight = weight / scale
     return norm_weight, scale
 
 
 def do_integer_quantization(
-    weight: Tensor, reduction_axis: int, config: WeightCompressionConfig
+    weight: Tensor, reduction_axes: ReductionAxes, config: WeightCompressionConfig
 ) -> Tuple[Tensor, Tensor, Tensor]:
     """
     The method quantizes the given weights to integer data type in accordance with the compression config.
     The config defines a quantization mode:
         INT8_SYM mode refers to unsigned int8 symmetric weight compression with a fixed zero point equals to 128 -
             quantization to [0, 255] range.
         INT8_ASYM mode refers to unsigned int8 asymmetric weight compression with a typical non-fixed zero-point -
@@ -106,15 +116,15 @@
             quantization to [0, 15] range.
         NF4 mode requires a dedicated procedure and it is not supported in this method.
     One of the parameter of compression config is a group size. Quantization is per-channel, if group size equals to -1,
     otherwise it's per-group, i.e. group size number of weights in the channel dimension share quantization parameters
     (scales).
 
     :param weight: Weight array to compress.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
     :param config: Information on how to compress (quantize) a specific weight.
     :return: The compressed weights tensor of uint8 type, scale tensor of float32 type and
         zero point tensor of int32 type that was used for its quantization.
     """
     mode = config.mode
     assert mode != CompressWeightsMode.NF4, "The function supports integer quantization only"
     group_size = config.group_size
@@ -124,71 +134,88 @@
     level_high = 2**num_bits - 1
 
     if weight.dtype != TensorDataType.float32:
         weight = weight.astype(TensorDataType.float32)
 
     if group_size != -1:
         # weights are reshaped from [a1, r, a2] to [a1, r//gs, gs, a2]
-        weight, reduction_axis = reshape_weight_for_grouped_quantization(weight, reduction_axis, group_size)
+        weight, reduction_axes = reshape_weight_for_grouped_quantization(weight, reduction_axes, group_size)
 
     if mode in [CompressWeightsMode.INT8_ASYM, CompressWeightsMode.INT4_ASYM]:
-        min_values = fns.min(weight, axis=reduction_axis, keepdims=True)  # [a1, r, a2] -> [a1, 1, a2]
-        max_values = fns.max(weight, axis=reduction_axis, keepdims=True)  # [a1, r, a2] -> [a1, 1, a2]
+        min_values = fns.min(weight, axis=reduction_axes, keepdims=True)  # [a1, r, a2] -> [a1, 1, a2]
+        max_values = fns.max(weight, axis=reduction_axes, keepdims=True)  # [a1, r, a2] -> [a1, 1, a2]
         scale, zero_point = calculate_scale_zero_point(
             min_values, max_values, level_low, level_high, narrow_range=False
         )
     else:
-        scale = fns.max(fns.abs(weight), axis=reduction_axis, keepdims=True)  # [a1, r//gs, 1, a2]
+        scale = fns.max(fns.abs(weight), axis=reduction_axes, keepdims=True)  # [a1, r//gs, 1, a2]
         level_low_sym = -(2 ** (num_bits - 1))
         level_high_sym = 2 ** (num_bits - 1) - 1
         scale = scale / level_high_sym
         zero_point = fns.as_tensor_like(scale, [-level_low_sym])
         eps = fns.finfo(scale).eps
         # NOTE: adding machine epsilon to avoid division by zero
         scale = fns.where(fns.abs(scale) < eps, eps, scale)
 
     compressed_weights = fns.round(weight / scale + zero_point.astype(weight.dtype))
     compressed_weights = fns.clip(compressed_weights, level_low, level_high).astype(TensorDataType.uint8)
     return compressed_weights, scale, zero_point
 
 
-def get_integer_quantization_error(weight: Tensor, reduction_axis: int, config: WeightCompressionConfig) -> float:
+def get_integer_quantization_error(
+    weight: Tensor, reduction_axes: ReductionAxes, config: WeightCompressionConfig
+) -> float:
     """
     Calculates a quantity characterizing the difference between floating point weights and fake quantized
     (compressed and decompressed) to integer ones.
 
     :param weight: Weight array to compress.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
     :param config: Information on how to compress (quantize) a specific weight.
     :return: The quantity characterizing the error of integer quantization.
     """
     orig_shape = weight.shape
 
     if weight.dtype != TensorDataType.float32:
         weight = weight.astype(TensorDataType.float32)
 
-    compressed_weights, scale, zero_point = do_integer_quantization(weight, reduction_axis, config)
+    compressed_weights, scale, zero_point = do_integer_quantization(weight, reduction_axes, config)
 
     decompressed_weight = (compressed_weights - zero_point).astype(weight.dtype) * scale
 
     decompressed_weight = decompressed_weight.reshape(orig_shape)
     diff = (decompressed_weight - weight) ** 2
-    layer_err = fns.mean(diff, axis=reduction_axis)
+    layer_err = fns.mean(diff, axis=reduction_axes)
     val = fns.max(layer_err)
     return val.item()
 
 
-def compress_weight(weight: Tensor, reduction_axis: int, config: WeightCompressionConfig):
+def compress_weight(weight: Tensor, reduction_axes: ReductionAxes, config: WeightCompressionConfig):
     """
     Compress weight using compression configuration.
 
     :param weight: The weight to compress.
-    :param reduction_axis: Axis, along which to reduce (collect) different statistics (e.g. min, max).
-    :param config: Compresssion configuration.
+    :param reduction_axes: Axes, along which to reduce (collect) different statistics (e.g. min, max).
+    :param config: Compression configuration.
     :return: The compressed weight and decompression parameters as instance of CompressedWeight
     """
     if config.mode == CompressWeightsMode.NF4:
-        compressed_weight, scale = calculate_normalized_weight_and_nf4_scale(weight, reduction_axis, config.group_size)
+        compressed_weight, scale = calculate_normalized_weight_and_nf4_scale(weight, reduction_axes, config.group_size)
         return CompressedWeight(compressed_weight, scale)
 
-    compressed_weight, scale, zero_point = do_integer_quantization(weight, reduction_axis, config)
+    compressed_weight, scale, zero_point = do_integer_quantization(weight, reduction_axes, config)
     return CompressedWeight(compressed_weight, scale, zero_point)
+
+
+def do_dequantization(compressed_weights: Tensor, scale: Tensor, zero_point: Tensor) -> Tensor:
+    """
+    The method dequantizes the given weights to float point data type in accordance with the scale and
+    zero_point data type.
+
+    :param compressed_weights: compressed weights.
+    :param scale: scale in compression/quantization.
+    :param zero_point: zero point in compression/quantization.
+    :return: dequantized/decompressed weights.
+    """
+    decompressed_weight = compressed_weights.astype(dtype=scale.dtype)
+    decompressed_weight = (decompressed_weight - zero_point) * scale
+    return decompressed_weight
```

### Comparing `nncf-2.8.1/nncf/quantization/fake_quantize.py` & `nncf-2.9.0/nncf/quantization/fake_quantize.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Tuple
 
+import nncf
 from nncf.common.quantization.quantizers import calculate_asymmetric_level_ranges
 from nncf.common.quantization.quantizers import calculate_symmetric_level_ranges
 from nncf.common.quantization.quantizers import get_num_levels
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
@@ -179,15 +180,15 @@
     """
     Calculates the numbers of the low and high quant for the asymmetric quantization scheme.
 
     :param min_values: Collected min values for the quantized insertion.
     :param max_values: Collected max values for the quantized insertion.
     :param quantizer_config: Config of the quantization configuration.
     :param unify_zp: Whether to unify the zero point.
-        It is `True` for the per-tensor zero point constrain on KMB (vpu2p0).
+        It is `True` for the per-tensor zero point constrain on KMB.
     :return: A Tuple
         level_low - the low quant number
         level_high - the high quant number
     """
     level_low, level_high = fix_zero_filters_asymmetric(min_values, max_values)
     level_low = fns.where(level_low < 0.0, level_low, 0.0)
     level_high = fns.where(level_high > 0.0, level_high, 0.0)
@@ -309,17 +310,17 @@
         naive case, False otherwise.
     :return: A Tuple
         input_low: Tensor with minimum limit for input value.
         input_high: Tensor with maximum limit for input value.
         levels: Number of quantization levels.
     """
     if quantizer_config.mode == QuantizationMode.ASYMMETRIC:
-        raise RuntimeError("half_range is only applied to symmetric quantization mode.")
+        raise nncf.ValidationError("half_range is only applied to symmetric quantization mode.")
     if quant_group != QuantizerGroup.WEIGHTS:
-        raise RuntimeError("half_range is only applied to weight quantizers.")
+        raise nncf.ValidationError("half_range is only applied to weight quantizers.")
 
     num_bits = quantizer_config.num_bits
     level_low, level_high = calculate_symmetric_level_ranges(num_bits - 1, signed=True, narrow_range=False)
     levels = get_num_levels(level_low, level_high)
     input_low, input_high = symmetric_range(min_values, max_values, levels, quantizer_config, quant_group)
 
     export_level_low, export_level_high = calculate_symmetric_level_ranges(
```

### Comparing `nncf-2.8.1/nncf/quantization/passes.py` & `nncf-2.9.0/nncf/quantization/passes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/quantization/quantize_model.py` & `nncf-2.9.0/nncf/quantization/quantize_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Callable, Iterable, List, Optional, Tuple, TypeVar, Union
 
+import nncf
 from nncf.api.compression import TModel
 from nncf.common.deprecation import warning_deprecated
-from nncf.common.factory import NNCFGraphFactory
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 from nncf.data import Dataset
 from nncf.parameters import CompressWeightsMode
 from nncf.parameters import DropType
@@ -27,15 +27,14 @@
 from nncf.parameters import TargetDevice
 from nncf.quantization.advanced_parameters import AdvancedAccuracyRestorerParameters
 from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
 from nncf.quantization.algorithms.accuracy_control.evaluator import MetricResults
 from nncf.quantization.algorithms.hyperparameter_tuner.algorithm import HyperparameterTuner
 from nncf.quantization.algorithms.hyperparameter_tuner.param_grid import get_quantization_param_grids
 from nncf.quantization.algorithms.post_training.pipeline import create_ptq_pipeline
-from nncf.quantization.algorithms.weight_compression.algorithm import WeightCompression
 from nncf.scopes import IgnoredScope
 
 TTensor = TypeVar("TTensor")
 
 
 @api(canonical_alias="nncf.quantize")
 def quantize(
@@ -152,15 +151,15 @@
             subset_size=subset_size,
             fast_bias_correction=fast_bias_correction,
             model_type=model_type,
             ignored_scope=ignored_scope,
             advanced_parameters=advanced_parameters,
         )
 
-    raise RuntimeError(f"Unsupported type of backend: {backend}")
+    raise nncf.UnsupportedBackendError(f"Unsupported type of backend: {backend}")
 
 
 @api(canonical_alias="nncf.quantize_with_accuracy_control")
 def quantize_with_accuracy_control(
     model: TModel,
     calibration_dataset: Dataset,
     validation_dataset: Dataset,
@@ -240,28 +239,50 @@
             subset_size,
             fast_bias_correction,
             model_type,
             ignored_scope,
             advanced_quantization_parameters,
             advanced_accuracy_restorer_parameters,
         )
+    if backend == BackendType.ONNX:
+        from nncf.onnx.quantization.quantize_model import quantize_with_accuracy_control_impl
 
-    raise RuntimeError(f"Unsupported type of backend: {backend}")
+        return quantize_with_accuracy_control_impl(
+            model,
+            calibration_dataset,
+            validation_dataset,
+            validation_fn,
+            max_drop,
+            drop_type,
+            preset,
+            target_device,
+            subset_size,
+            fast_bias_correction,
+            model_type,
+            ignored_scope,
+            advanced_quantization_parameters,
+            advanced_accuracy_restorer_parameters,
+        )
+
+    raise nncf.UnsupportedBackendError(f"Unsupported type of backend: {backend}")
 
 
 @api(canonical_alias="nncf.compress_weights")
 def compress_weights(
     model: TModel,
     mode=CompressWeightsMode.INT8_ASYM,
     ratio: Optional[float] = None,
     group_size: Optional[int] = None,
     ignored_scope: Optional[IgnoredScope] = None,
     all_layers: Optional[bool] = None,
     dataset: Optional[Dataset] = None,
     sensitivity_metric: Optional[SensitivityMetric] = None,
+    *,
+    subset_size: Optional[int] = 128,
+    awq: Optional[bool] = None,
 ) -> TModel:
     """
     Compress model weights.
 
     :param model: A model to be compressed.
     :param mode: Defines a mode for weight compression.
         INT8_SYM stands for 8-bit integer symmetric quantization of all weights.
@@ -277,96 +298,117 @@
         NF4 is the same as INT4_SYM mode, but primary precision is NF4 data type without zero point.
     :param ratio: the ratio between baseline and backup precisions (e.g. 0.9 means 90% of layers quantized to NF4
         and the rest to INT8_ASYM).
     :param group_size: number of weights (e.g. 128) in the channel dimension that share quantization parameters (scale).
         The value -1 means no grouping.
     :param ignored_scope: An ignored scope that defined the list of model control
         flow graph nodes to be ignored during quantization.
-    :param all_layers: Indicates whether embeddings and last layers should be compressed to a primary
-        precision. By default, the backup precision is assigned for the embeddings and last layers.
+    :param all_layers: Indicates whether embeddings and last MatMul layers should be compressed to a primary
+        precision. By default, the backup precision is assigned for the embeddings and last MatMul layers.
     :param dataset: Dataset used for assigning different quantization precision by finding outliers in activations.
     :param sensitivity_metric: The sensitivity metric for assigning quantization precision to layers. In order to
         preserve the accuracy of the model, the more sensitive layers receives a higher precision.
+    :param subset_size: Number of data samples to calculate activation statistics used for assigning different
+        quantization precision. Defaults to 128.
+    :param awq: Indicates whether use AWQ weights correction.
     :return: The non-trainable model with compressed weights.
     """
     if mode == CompressWeightsMode.INT8:
         warning_deprecated(
             "`CompressWeightsMode.INT8` is deprecated." "Please, use `CompressWeightsMode.INT8_ASYM` as value instead."
         )
         mode = CompressWeightsMode.INT8_ASYM
 
     backend = get_backend(model)
+    compression_weights_impl = None
+
     if backend == BackendType.TORCH:
         from nncf.torch.model_creation import is_wrapped_model
         from nncf.torch.model_creation import wrap_model
+        from nncf.torch.quantization.quantize_model import compress_weights_impl as pt_compression_weights_impl
 
         if mode not in [CompressWeightsMode.INT8_ASYM, CompressWeightsMode.INT8_SYM]:
             raise AttributeError(
                 "Torch backend supports only INT8_ASYM, INT8_SYM modes for weight compression, "
                 f"but given {mode.value} mode."
             )
 
+        if awq is True:
+            raise AttributeError("Torch backend doesn`t supports AWQ algorithm, but awq=True is specified.")
+
         if is_wrapped_model(model):
             if not model.nncf.trace_parameters:
                 raise ValueError(
                     "Tracing capabilities with tracing parameters are required in the PyTorch model "
                     "for nncf.compress_weights(). Please wrap the model using "
                     "nncf.torch.wrap_model(model, example_input, trace_parameters=True) before calling "
                     "nncf.compress_weights()."
                 )
         elif dataset is None:
             raise AttributeError("Please provide a dataset of at least one element for PyTorch model tracing.")
         else:
             example_input = next(iter(dataset.get_inference_data()))
             model = wrap_model(model, example_input=example_input, trace_parameters=True)
             dataset = None
+        compression_weights_impl = pt_compression_weights_impl
+
+    if backend == BackendType.OPENVINO:
+        from nncf.openvino.quantization.quantize_model import compress_weights_impl as ov_compress_weights_impl
+
+        compression_weights_impl = ov_compress_weights_impl
 
     if mode in [CompressWeightsMode.INT8_ASYM, CompressWeightsMode.INT8_SYM]:
         if ratio is None:
             ratio = 1
         if group_size is None:
             group_size = -1
         if ratio != 1 or group_size != -1:
             raise AttributeError(
                 "INT8 mode assumes per-channel quantization of all layers in 8 bit. "
                 "Default values of `ratio` (1) and `group_size` (-1) parameters can not be overridden"
             )
-        options = [all_layers, sensitivity_metric, dataset]
+        options = [all_layers, sensitivity_metric, dataset, awq]
         if any(option is not None for option in options):
             raise AttributeError(
-                "INT8 modes do not support `all_layers`, `sensitivity_metric` and `dataset` options."
+                "INT8 modes do not support `all_layers`, `sensitivity_metric`, `awq` and `dataset` options. "
                 "Set them to None."
             )
 
     if ratio is None:
         ratio = 1
     if group_size is None:
         group_size = 128
     if all_layers is None:
         all_layers = False
+    if awq is None:
+        awq = False
     if ignored_scope is None:
         ignored_scope = IgnoredScope()
     if sensitivity_metric is None:
         sensitivity_metric = (
             SensitivityMetric.WEIGHT_QUANTIZATION_ERROR
             if dataset is None
             else SensitivityMetric.MAX_ACTIVATION_VARIANCE
         )
     if ratio != 1 and dataset is None and sensitivity_metric != SensitivityMetric.WEIGHT_QUANTIZATION_ERROR:
         raise AttributeError(
             f"Mixed precision selection based on the given sensitivity metric={sensitivity_metric.value} requires "
             "a dataset, but it's not provided."
         )
-
     if ratio < 0 or ratio > 1:
-        raise ValueError(f"The ratio should be between 0 and 1, but ration={ratio} is specified.")
-
-    compression_algorithm = WeightCompression(mode, ratio, group_size, ignored_scope, all_layers, sensitivity_metric)
-    graph = NNCFGraphFactory.create(model)
-    return compression_algorithm.apply(model, graph, dataset=dataset)
+        raise ValueError(f"The ratio should be between 0 and 1, but ratio={ratio} is specified.")
+    if subset_size is None or subset_size <= 0:
+        raise ValueError(f"The subset_size value should be positive, but subset_size={subset_size} is given.")
+
+    if compression_weights_impl is None:
+        raise nncf.UnsupportedBackendError(f"Unsupported type of backend: {backend}")
+
+    return compression_weights_impl(
+        model, dataset, mode, ratio, group_size, ignored_scope, all_layers, sensitivity_metric, awq, subset_size
+    )
 
 
 def quantize_with_tune_hyperparams(
     model: TModel,
     calibration_dataset: Dataset,
     validation_dataset: Dataset,
     validation_fn: Callable[[Any, Iterable[Any]], Tuple[float, Union[None, List[float], List[List[TTensor]]]]],
```

### Comparing `nncf-2.8.1/nncf/quantization/range_estimator.py` & `nncf-2.9.0/nncf/quantization/range_estimator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/scopes.py` & `nncf-2.9.0/nncf/scopes.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 import re
 from dataclasses import dataclass
 from dataclasses import field
 from typing import List, Optional, Set
 
+import nncf
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.api_marker import api
 
 
 @api(canonical_alias="nncf.IgnoredScope")
 @dataclass
@@ -82,15 +83,15 @@
     results = []
     if ignored_scope is None:
         return results
     results.extend(ignored_scope.names)
     for p in ignored_scope.patterns:
         results.append("{re}" + p)
     if ignored_scope.types:
-        raise RuntimeError("Legacy ignored scope format does not support operation types")
+        raise nncf.InternalError("Legacy ignored scope format does not support operation types")
     return results
 
 
 def get_ignored_node_names_from_ignored_scope(
     ignored_scope: IgnoredScope, nncf_graph: NNCFGraph, strict: bool = True
 ) -> Set[str]:
     """
@@ -114,40 +115,42 @@
     matched_by_names = []
     if ignored_scope.names:
         for ignored_node_name in ignored_scope.names:
             if ignored_node_name in node_names:
                 matched_by_names.append(ignored_node_name)
         if strict and len(ignored_scope.names) != len(matched_by_names):
             skipped_names = set(ignored_scope.names) - set(matched_by_names)
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 f"Ignored nodes with name {list(skipped_names)} were not found in the NNCFGraph. " + error_msg
             )
         nncf_logger.info(f"{len(matched_by_names)} ignored nodes were found by name in the NNCFGraph")
 
     matched_by_patterns = []
     if ignored_scope.patterns:
         not_matched_patterns = []
         for str_pattern in ignored_scope.patterns:
             pattern = re.compile(str_pattern)
             matches = list(filter(pattern.match, node_names))
             if not matches:
                 not_matched_patterns.append(str_pattern)
             matched_by_patterns.extend(matches)
         if strict and not_matched_patterns:
-            raise RuntimeError(f"No matches for ignored patterns {not_matched_patterns} in the NNCFGraph. " + error_msg)
+            raise nncf.ValidationError(
+                f"No matches for ignored patterns {not_matched_patterns} in the NNCFGraph. " + error_msg
+            )
         nncf_logger.info(f"{len(matched_by_patterns)} ignored nodes were found by patterns in the NNCFGraph")
 
     matched_by_types = []
     if ignored_scope.types:
         types_found = set()
         for node in nncf_graph.get_all_nodes():
             if node.node_type in ignored_scope.types:
                 types_found.add(node.node_type)
                 matched_by_types.append(node.node_name)
         not_matched_types = set(ignored_scope.types) - types_found
         if strict and not_matched_types:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 f"Nodes with ignored types {list(not_matched_types)} were not found in the NNCFGraph. " + error_msg
             )
         nncf_logger.info(f"{len(matched_by_types)} ignored nodes were found by types in the NNCFGraph")
 
     return set(matched_by_names + matched_by_types + matched_by_patterns)
```

### Comparing `nncf-2.8.1/nncf/telemetry/__init__.py` & `nncf-2.9.0/nncf/telemetry/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/telemetry/decorator.py` & `nncf-2.9.0/nncf/telemetry/decorator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/telemetry/events.py` & `nncf-2.9.0/nncf/telemetry/events.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/telemetry/extractors.py` & `nncf-2.9.0/nncf/telemetry/extractors.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/telemetry/wrapper.py` & `nncf-2.9.0/nncf/telemetry/wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/__init__.py` & `nncf-2.9.0/nncf/tensorflow/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 """
 Base subpackage for NNCF TensorFlow functionality.
 """
 
 import tensorflow
 from packaging import version
 
+import nncf
 from nncf import nncf_logger
 from nncf.common.logging.logger import warn_bkc_version_mismatch
 from nncf.version import BKC_TF_VERSION
 
 try:
     _tf_version = tensorflow.__version__
     tensorflow_version = version.parse(_tf_version).base_version
@@ -26,15 +27,15 @@
     nncf_logger.debug("Could not parse tensorflow version")
     _tf_version = "0.0.0"
     tensorflow_version = version.parse(_tf_version).base_version
 tensorflow_version_major, tensorflow_version_minor = tuple(map(int, tensorflow_version.split(".")))[:2]
 if not tensorflow_version.startswith(BKC_TF_VERSION[:-2]):
     warn_bkc_version_mismatch("tensorflow", BKC_TF_VERSION, _tf_version)
 elif not (tensorflow_version_major == 2 and 8 <= tensorflow_version_minor <= 13):
-    raise RuntimeError(
+    raise nncf.UnsupportedVersionError(
         f"NNCF only supports 2.8.4 <= tensorflow <= 2.13.*, while current tensorflow version is {_tf_version}"
     )
 
 
 from nncf.common.accuracy_aware_training.training_loop import (
     AdaptiveCompressionTrainingLoop as AdaptiveCompressionTrainingLoop,
 )
```

### Comparing `nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/__init__.py` & `nncf-2.9.0/nncf/tensorflow/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py` & `nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/accuracy_aware_training/runner.py` & `nncf-2.9.0/nncf/tensorflow/accuracy_aware_training/runner.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/algorithm_selector.py` & `nncf-2.9.0/nncf/tensorflow/algorithm_selector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/api/__init__.py` & `nncf-2.9.0/nncf/tensorflow/graph/metatypes/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/api/composite_compression.py` & `nncf-2.9.0/nncf/tensorflow/api/composite_compression.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import TypeVar
 
+import nncf
 from nncf import NNCFConfig
 from nncf.common.composite_compression import CompositeCompressionAlgorithmBuilder
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
 from nncf.config.extractors import extract_algorithm_names
 from nncf.tensorflow.algorithm_selector import get_compression_algorithm_builder
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.transformations.layout import TFTransformationLayout
@@ -24,15 +25,15 @@
 
 class TFCompositeCompressionAlgorithmBuilder(CompositeCompressionAlgorithmBuilder, TFCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
 
         algo_names = extract_algorithm_names(config)
         if len(algo_names) < 2:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "Composite algorithm builder must be supplied with a config with more than one "
                 "compression algo specified!"
             )
         for algo_name in algo_names:
             algo_builder_cls = get_compression_algorithm_builder(algo_name)
             self._child_builders.append(algo_builder_cls(config, should_init=should_init))
```

### Comparing `nncf-2.8.1/nncf/tensorflow/api/compression.py` & `nncf-2.9.0/nncf/tensorflow/api/compression.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/batchnorm_adaptation.py` & `nncf-2.9.0/nncf/tensorflow/batchnorm_adaptation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/callbacks/__init__.py` & `nncf-2.9.0/nncf/tensorflow/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/callbacks/checkpoint_callback.py` & `nncf-2.9.0/nncf/tensorflow/callbacks/checkpoint_callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/callbacks/statistics_callback.py` & `nncf-2.9.0/nncf/tensorflow/callbacks/statistics_callback.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/exporter.py` & `nncf-2.9.0/nncf/tensorflow/exporter.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/functions.py` & `nncf-2.9.0/nncf/tensorflow/functions.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/__init__.py` & `nncf-2.9.0/nncf/tensorflow/hardware/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/converter.py` & `nncf-2.9.0/nncf/tensorflow/graph/converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 from collections import defaultdict
 from typing import Dict, List, Set, Tuple, Type
 
 import tensorflow as tf
 from tensorflow.core.framework.node_def_pb2 import NodeDef
 from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import ConvertDtypeLayerAttributes
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
@@ -253,15 +254,15 @@
                     if len(splits) == 1:
                         input_graphdef_node_name = splits[0]
                         output_port_id = 0
                     elif len(splits) == 2:
                         input_graphdef_node_name = splits[0]
                         output_port_id = int(splits[1])
                     else:
-                        raise RuntimeError("Could not parse NodeDef's input field!")
+                        raise nncf.InternalError("Could not parse NodeDef's input field!")
 
                     pretty_input_node_name = custom_layer_info.graphdef_node_name_to_pretty_node_name[
                         input_graphdef_node_name
                     ]
 
                     # TODO (vshampor): add proper tensor_shape, will probably involve
                     #                  running as_graph_def(add_shapes=True)
@@ -355,15 +356,15 @@
             weight_node_name = get_node_name(previous_node_names[-1])
             weight_node = graphdef_nodes[weight_node_name]  # TODO (vshampor): how correct is this actually?
             previous_node_names = [get_node_name(node_input) for node_input in weight_node.input]
 
             # Filter control inputs, whatever these are
             previous_node_names = list(filter(lambda x: "^" not in x, previous_node_names))
         if weight_node_name is None:
-            raise RuntimeError("Could not find a weight node for a weighted node {}".format(weighted_node.name))
+            raise nncf.InternalError("Could not find a weight node for a weighted node {}".format(weighted_node.name))
         return weight_node_name
 
     @staticmethod
     def _prepare_shape(shape) -> List:
         if not isinstance(shape, list):
             return [shape]
         return shape
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/metatypes/__init__.py` & `nncf-2.9.0/nncf/tensorflow/layers/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/metatypes/common.py` & `nncf-2.9.0/nncf/tensorflow/graph/metatypes/common.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/metatypes/keras_layers.py` & `nncf-2.9.0/nncf/tensorflow/graph/metatypes/keras_layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from collections import namedtuple
 from typing import List, Optional, Type
 
 import tensorflow as tf
 
+import nncf
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 from nncf.tensorflow.graph.metatypes.tf_ops import TF_OPERATION_METATYPES
 
@@ -61,15 +62,15 @@
             if subtype.matches(layer, wrapper):
                 subtype_matches = subtype.determine_subtype_wrapped_layer(layer, wrapper)
                 if subtype_matches is not None:
                     matches.extend(subtype_matches)
                 else:
                     matches.append(subtype)
         if len(matches) > 1:
-            raise RuntimeError("Multiple subtypes match operator call - cannot determine single subtype.")
+            raise nncf.InternalError("Multiple subtypes match operator call - cannot determine single subtype.")
         if not matches:
             return None
         return matches[0]
 
 
 class TFLayerWithWeightsMetatype(TFLayerMetatype):
     weight_definitions: List[WeightDef] = []
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/metatypes/matcher.py` & `nncf-2.9.0/nncf/tensorflow/graph/metatypes/matcher.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/metatypes/tf_ops.py` & `nncf-2.9.0/nncf/tensorflow/graph/metatypes/tf_ops.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/model_transformer.py` & `nncf-2.9.0/nncf/tensorflow/graph/model_transformer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 import copy
 from collections import OrderedDict
 from collections import namedtuple
 from typing import Callable, Dict, List, Set, Union
 
 import tensorflow as tf
 
+import nncf
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationType
 from nncf.tensorflow.graph.transformations.commands import TFAfterLayer
 from nncf.tensorflow.graph.transformations.commands import TFBeforeLayer
@@ -158,15 +159,15 @@
 
     def _shared_insert_layers(self, target_points: List[TargetPoint], layers_to_insert: List[Callable]):
         functional_model = is_functional_model(self._model)
         if functional_model:
             for layer in self._model_config["input_layers"]:
                 for tp in target_points:
                     if isinstance(tp, TFBeforeLayer) and tp.layer_name == layer[0]:
-                        raise RuntimeError(f"Insertion before input layer: {tp.layer_name} is not supported")
+                        raise nncf.InternalError(f"Insertion before input layer: {tp.layer_name} is not supported")
 
         layer_configs = []
         for layer in layers_to_insert:
             config = tf.keras.utils.serialize_keras_object(layer)
             if functional_model:
                 config["name"] = config["config"]["name"]
                 config["inbound_nodes"] = []
@@ -205,15 +206,15 @@
                                 inbound_node, tp.layer_name, tp.instance_idx, layer_out_ports, replace_layer_name, i
                             )
 
                     self._insert_after_model_outputs(
                         tp.layer_name, tp.instance_idx, layer_out_ports, replace_layer_name, i
                     )
                     if len(layer_out_ports) > 1:
-                        raise RuntimeError(
+                        raise nncf.InternalError(
                             "Insertion after layer ({}) with multiple ports is not supported".format(tp.layer_name)
                         )
 
             layer_name = target_points[0].layer_name
             self._insert_layer_after_sequential(layer_name, config)
 
     def _multi_insertion(self, target_point: TargetPoint, commands: List[TransformationCommand]):
@@ -320,15 +321,15 @@
         :param layers_to_insert: List of the layers, which will be inserted into the graph.
         """
         functional_model = is_functional_model(self._model)
 
         if functional_model:
             for layer in self._model_config["input_layers"]:
                 if layer_name == layer[0]:
-                    raise RuntimeError("Insertion before input layer: {} is not supported".format(layer_name))
+                    raise nncf.InternalError("Insertion before input layer: {} is not supported".format(layer_name))
 
         layer_configs = []
         idx, downstream_layer_cfg = self._find_layer_config(layer_name)
 
         for layer in layers_to_insert:
             config = tf.keras.utils.serialize_keras_object(layer)
             if functional_model:
@@ -405,21 +406,23 @@
             for inbound_node in inbound_nodes:
                 self._process_insertion_after(
                     inbound_node, layer_name, instance_idx, layer_out_ports, replace_layer_name
                 )
 
         self._insert_after_model_outputs(layer_name, instance_idx, layer_out_ports, replace_layer_name)
         if len(layer_out_ports) > 1:
-            raise RuntimeError("Insertion after layer ({}) with multiple ports is not supported".format(layer_name))
+            raise nncf.InternalError(
+                "Insertion after layer ({}) with multiple ports is not supported".format(layer_name)
+            )
         self._insert_layer_after_sequential(layer_name, layer_to_insert_config)
 
     def _insert_layer_after_sequential(self, layer_name: str, layer_configs):
         idx, _ = self._find_layer_config(layer_name)
         if idx is None:
-            raise RuntimeError("Layer is not found: {}".format(layer_name))
+            raise nncf.InternalError("Layer is not found: {}".format(layer_name))
         self._model_config["layers"].insert(idx + 1, layer_configs)
 
     @staticmethod
     def _process_insertion_after(
         connection_infos,
         layer_name: str,
         instance_idx: int,
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/pattern_operations.py` & `nncf-2.9.0/nncf/tensorflow/graph/pattern_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/patterns.py` & `nncf-2.9.0/nncf/tensorflow/graph/patterns.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/tensorflow/tensor_statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/transformations/commands.py` & `nncf-2.9.0/nncf/tensorflow/graph/transformations/commands.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/transformations/layout.py` & `nncf-2.9.0/nncf/tensorflow/graph/transformations/layout.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/graph/utils.py` & `nncf-2.9.0/nncf/tensorflow/graph/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 import inspect
 import sys
 from typing import List, Tuple
 
 import tensorflow as tf
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.tensorflow.graph.metatypes.keras_layers import TFNNCFWrapperLayerMetatype
 from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
 from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.tensorflow.layers.wrapper import NNCFWrapper
@@ -76,17 +77,16 @@
     keras_activations = get_keras_activation_names()
     custom_objects = {}
     for layer in model.submodules:
         if layer.__class__.__name__ == "NNCFWrapper":
             layer = layer.layer
         if layer.__class__.__name__ not in keras_layers:
             custom_objects[layer.__class__.__name__] = layer.__class__
-        if layer.__class__.__name__ == "Activation":
-            if layer.activation.__name__ not in keras_activations:
-                custom_objects[layer.activation.__name__] = layer.activation
+        if layer.__class__.__name__ == "Activation" and layer.activation.__name__ not in keras_activations:
+            custom_objects[layer.activation.__name__] = layer.activation
     return custom_objects
 
 
 def get_weight_name(name, layer_name=None):
     if layer_name and layer_name in name:
         return name.split(layer_name + "/")[-1]
     return name
@@ -125,15 +125,15 @@
 
 
 def get_layer_to_graph_nodes_map(model, node_names):
     layer_to_nodes_map = {layer.name: {"type": layer.__class__.__name__, "nodes": []} for layer in model.layers}
     for node in node_names:
         parent_layer_name = node.split("/")[1]  # model_name/layer_name/layer_op_name/...
         if parent_layer_name not in layer_to_nodes_map:
-            raise RuntimeError("Could not find {} layer in Model".format(parent_layer_name))
+            raise nncf.ValidationError("Could not find {} layer in Model".format(parent_layer_name))
         layer_to_nodes_map[parent_layer_name]["nodes"].append(node)
     return layer_to_nodes_map
 
 
 def get_weight_node_name(graph: NNCFGraph, node_name: NNCFNodeName) -> NNCFNodeName:
     node = graph.get_node_by_name(node_name)
     while list(graph.get_previous_nodes(node)):
```

### Comparing `nncf-2.8.1/nncf/tensorflow/hardware/__init__.py` & `nncf-2.9.0/nncf/tensorflow/utils/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/hardware/config.py` & `nncf-2.9.0/nncf/tensorflow/hardware/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/hardware/fused_patterns.py` & `nncf-2.9.0/nncf/tensorflow/hardware/fused_patterns.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/helpers/__init__.py` & `nncf-2.9.0/nncf/tensorflow/helpers/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/helpers/callback_creation.py` & `nncf-2.9.0/nncf/tensorflow/helpers/callback_creation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/helpers/model_creation.py` & `nncf-2.9.0/nncf/tensorflow/helpers/model_creation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 import types
 from typing import Any, Dict, Optional, Tuple
 
 import tensorflow as tf
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
 from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algorithm_names
 from nncf.config.telemetry_extractors import CompressionStartedFromConfig
 from nncf.config.utils import is_experimental_quantization
@@ -113,15 +114,15 @@
         sample_size = input_info["sample_size"]
         samples_sizes.append(sample_size)
     elif isinstance(input_info, list):
         for info in input_info:
             sample_size = info["sample_size"]
             samples_sizes.append(sample_size)
     else:
-        raise RuntimeError("sample_size must be provided in configuration file")
+        raise nncf.ValidationError("sample_size must be provided in configuration file")
 
     input_signature = []
     for sample_size in samples_sizes:
         shape = [None] + list(sample_size[1:])
         input_signature.append(tf.TensorSpec(shape=shape, dtype=tf.float32))
 
     return input_signature if len(input_signature) > 1 else input_signature[0]
```

### Comparing `nncf-2.8.1/nncf/tensorflow/helpers/model_manager.py` & `nncf-2.9.0/nncf/tensorflow/helpers/model_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/helpers/utils.py` & `nncf-2.9.0/nncf/tensorflow/helpers/utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import nncf
+
 
 def get_built_model(model, config):
     if not model.built:
         input_info = config.get("input_info", {})
         if isinstance(input_info, dict):
             sample_size = input_info.get("sample_size", None)
         else:
             sample_size = input_info[0].get("sample_size", None) if input_info else None
         if not sample_size:
-            raise RuntimeError("sample_size must be provided in configuration file")
+            raise nncf.ValidationError("sample_size must be provided in configuration file")
         model.build([None] + list(sample_size[1:]))
 
     return model
```

### Comparing `nncf-2.8.1/nncf/tensorflow/initialization.py` & `nncf-2.9.0/nncf/tensorflow/initialization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/layers/__init__.py` & `nncf-2.9.0/nncf/torch/accuracy_aware_training/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/layers/custom_objects.py` & `nncf-2.9.0/nncf/tensorflow/layers/custom_objects.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/layers/data_layout.py` & `nncf-2.9.0/nncf/tensorflow/layers/data_layout.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/layers/operation.py` & `nncf-2.9.0/nncf/tensorflow/layers/operation.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from collections import OrderedDict
 
-from nncf.tensorflow.utils.hook_handle import HookHandle
+from nncf.common.hook_handle import add_op_to_registry
 
 
 class InputType:
     INPUTS = "inputs"
     WEIGHTS = "weights"
 
 
@@ -79,17 +79,15 @@
         The NNCF operation does not support serialization of the registered hooks.
 
         :param hook: callable object with following signatures:
                      hook(inputs) -> None or modified input
         :return: a handle that can be used to remove the hook form
                  the NNCF operation by calling handle.remove()
         """
-        handle = HookHandle(self._call_pre_hooks)
-        self._call_pre_hooks[handle.hook_id] = hook
-        return handle
+        return add_op_to_registry(self._call_pre_hooks, hook)
 
     def __call__(self, *args, **kwargs):
         inputs = args[0]
         for hook in self._call_pre_hooks.values():
             result = hook(inputs)
             if result is not None:
                 inputs = result
```

### Comparing `nncf-2.8.1/nncf/tensorflow/layers/wrapper.py` & `nncf-2.9.0/nncf/tensorflow/layers/wrapper.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from collections import OrderedDict
 from inspect import getfullargspec
 from typing import Dict
 
 import tensorflow as tf
 
+import nncf
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
 from nncf.tensorflow.layers.custom_objects import get_nncf_custom_objects
 from nncf.tensorflow.layers.operation import InputType
 from nncf.tensorflow.layers.operation import NNCFOperation
 
 
 @NNCF_CUSTOM_OBJECTS.register()
@@ -197,15 +198,17 @@
             self.set_layer_weight(weight_attr, layer_weight)
 
     def registry_weight_operation(self, weights_attr: str, op: NNCFOperation):
         if weights_attr not in self.weights_attr_ops:
             self.weights_attr_ops[weights_attr] = OrderedDict()
 
         if op.name in self.weights_attr_ops[weights_attr]:
-            raise RuntimeError(f"Attempt to apply an operation with the same name {op.name} on layer weight twice")
+            raise nncf.InternalError(
+                f"Attempt to apply an operation with the same name {op.name} on layer weight twice"
+            )
 
         self.weights_attr_ops[weights_attr][op.name] = op
 
     def get_operation_weights(self, operation_name):
         return self._ops_weights[operation_name]
 
     def get_layer_weight(self, weight_attr):
```

### Comparing `nncf-2.8.1/nncf/tensorflow/loss.py` & `nncf-2.9.0/nncf/tensorflow/loss.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/__init__.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementations of pruning algorithms.
+Backend-specific implementations of sparsity algorithms.
 """
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/base_algorithm.py` & `nncf-2.9.0/nncf/tensorflow/pruning/base_algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/callbacks.py` & `nncf-2.9.0/nncf/tensorflow/pruning/callbacks.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/__init__.py` & `nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/algorithm.py` & `nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from math import floor
 from math import isclose
 from typing import List, Set
 
 import tensorflow as tf
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionStage
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph import NNCFGraph
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.logging import nncf_logger
@@ -433,15 +434,15 @@
                 self.current_params_num = params_num
                 nncf_sorted_nodes = self._original_graph.topological_sort()
                 for layer in wrapped_layers:
                     nncf_node = [n for n in nncf_sorted_nodes if layer.name == n.layer_name][0]
                     if nncf_node.attributes["output_mask"] is not None:
                         self._set_operation_masks([layer], nncf_node.attributes["output_mask"].tensor)
                 return
-        raise RuntimeError(f"Unable to prune model to required flops pruning level: {target_flops_pruning_level}")
+        raise nncf.InternalError(f"Unable to prune model to required flops pruning level: {target_flops_pruning_level}")
 
     def _set_operation_masks(self, layers: List[NNCFWrapper], filter_mask):
         for layer in layers:
             for weight_attr, ops in layer.weights_attr_ops.items():
                 weight_shape = layer.layer_weights[weight_attr].shape
                 for op_name, op in ops.items():
                     if isinstance(op, BinaryMask):
@@ -461,15 +462,15 @@
             else:
                 left = middle
         flops, params_num = self._calculate_flops_and_weights_in_uniformly_pruned_model(right)
         if flops <= target_flops:
             self.current_flops = flops
             self.current_params_num = params_num
             return right
-        raise RuntimeError(
+        raise nncf.ParameterNotSupportedError(
             f"Unable to prune the model to get the required pruning level in flops = {target_flops_pruning_level}"
         )
 
     def _calculate_flops_and_weights_in_uniformly_pruned_model(self, pruning_level):
         (
             tmp_in_channels,
             tmp_out_channels,
@@ -531,15 +532,15 @@
             input_channels=tmp_in_channels,
             output_channels=tmp_out_channels,
         )
 
     def _layer_filter_importance(self, layer: NNCFWrapper):
         layer_metatype = get_keras_layer_metatype(layer)
         if len(layer_metatype.weight_definitions) != 1:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 f"The layer {layer.layer.name} does not support by the pruning "
                 f"algorithm because it contains several weight attributes."
             )
         weight_attr = layer_metatype.weight_definitions[0].weight_attr_name
         weight = layer.layer_weights[weight_attr]
         if self.all_weights:
             weight = self._weights_normalizer(weight)
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/filter_pruning/functions.py` & `nncf-2.9.0/nncf/tensorflow/pruning/filter_pruning/functions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/operations.py` & `nncf-2.9.0/nncf/tensorflow/pruning/operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/tensor_processor.py` & `nncf-2.9.0/nncf/tensorflow/pruning/tensor_processor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/pruning/utils.py` & `nncf-2.9.0/nncf/tensorflow/pruning/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from typing import Dict, List
 
 import numpy as np
 import tensorflow as tf
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import LINEAR_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.keras_layers import TFBatchNormalizationLayerMetatype
 from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
@@ -90,23 +91,23 @@
             if layer.data_format == "channels_last"
             else slice(channel_axis + 1, None)
         )
         in_shape = layer.get_input_shape_at(node_index)[dims_slice]
         out_shape = layer.get_output_shape_at(node_index)[dims_slice]
 
         if not is_valid_shape(in_shape) or not is_valid_shape(out_shape):
-            raise RuntimeError(f"Input/output shape is not defined for layer `{layer.name}` ")
+            raise nncf.ValidationError(f"Input/output shape is not defined for layer `{layer.name}` ")
 
         layers_out_shapes[node.node_name] = out_shape
 
     for node in graph.get_nodes_by_metatypes(LINEAR_LAYER_METATYPES):
         node_name, node_index = get_original_name_and_instance_idx(node.node_name)
         layer = model.get_layer(node_name)
 
         in_shape = layer.get_input_shape_at(node_index)[1:]
         out_shape = layer.get_output_shape_at(node_index)[1:]
 
         if not is_valid_shape(in_shape) or not is_valid_shape(out_shape):
-            raise RuntimeError(f"Input/output shape is not defined for layer `{layer.name}` ")
+            raise nncf.ValidationError(f"Input/output shape is not defined for layer `{layer.name}` ")
 
         layers_out_shapes[node.node_name] = out_shape
     return layers_out_shapes
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/__init__.py` & `nncf-2.9.0/nncf/tensorflow/quantization/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/algorithm.py` & `nncf-2.9.0/nncf/tensorflow/quantization/algorithm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from copy import deepcopy
 from typing import Any, Dict, List, Tuple
 
 import tensorflow as tf
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.compression import BaseCompressionAlgorithmController
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
@@ -250,18 +251,18 @@
         super().__init__(config, should_init)
 
         self.quantize_inputs = self._algo_config.get("quantize_inputs", QUANTIZE_INPUTS)
         self.quantize_outputs = self._algo_config.get("quantize_outputs", QUANTIZE_OUTPUTS)
         self._overflow_fix = self._algo_config.get("overflow_fix", QUANTIZATION_OVERFLOW_FIX)
         self._target_device = config.get("target_device", TARGET_DEVICE)
         algo_config = self._get_algo_specific_config_section()
-        if self._target_device == "VPU" and "preset" in algo_config:
-            raise RuntimeError("The VPU target device does not support presets.")
+        if self._target_device == "NPU" and "preset" in algo_config:
+            raise nncf.ValidationError("The NPU target device does not support presets.")
         if self._target_device == "CPU_SPR":
-            raise RuntimeError("The CPU_SPR target device does not supported.")
+            raise nncf.ValidationError("The CPU_SPR target device does not supported.")
 
         self.global_quantizer_constraints = {}
         self.ignored_scopes_per_group = {}
         self.target_scopes_per_group = {}
         self._op_names = []
 
         for quantizer_group in QuantizerGroup:
@@ -337,17 +338,16 @@
 
     def _get_half_range(
         self, qconfig: QuantizerConfig, target_node: NNCFNode, first_conv_nodes: List[NNCFNode]
     ) -> bool:
         if self._target_device in ["CPU", "ANY"] and qconfig.num_bits == 8:
             if self._overflow_fix == "enable":
                 return True
-            if self._overflow_fix == "first_layer_only":
-                if target_node in first_conv_nodes:
-                    return True
+            if self._overflow_fix == "first_layer_only" and target_node in first_conv_nodes:
+                return True
         return False
 
     def _create_quantizer(self, name: str, qspec: TFQuantizerSpec) -> Quantizer:
         quantizer_cls = NNCF_QUANTIZATION_OPERATIONS.get(qspec.mode)
         return quantizer_cls(name, qspec)
 
     def _build_insertion_commands_for_quantizer_setup(
@@ -464,21 +464,21 @@
         applied_overflow_fix = False
         first_conv_nodes = get_first_nodes_of_type(nncf_graph, ["Conv2D", "Conv3D"])
         for qp_id, qp in quantizer_setup.quantization_points.items():
             if qp.is_weight_quantization_point():
                 target_node = nncf_graph.get_node_by_name(qp.insertion_point.target_node_name)
                 is_custom, layer_info = converter.get_layer_info_for_node(target_node.node_name)
                 if is_custom:
-                    raise RuntimeError("Quantizing custom layer weights is currently unsupported!")
+                    raise nncf.InternalError("Quantizing custom layer weights is currently unsupported!")
                 layer_name = layer_info.layer_name
                 qconfig = qp.qconfig
                 if layer_name in quantized_layer_names_vs_qconfigs:
                     assigned_qconfig = quantized_layer_names_vs_qconfigs[layer_name]
                     if qconfig != assigned_qconfig:
-                        raise RuntimeError(
+                        raise nncf.InternalError(
                             f"Inconsistent quantizer configurations selected by solver for one and the "
                             f"same quantizable layer! Tried to assign {qconfig} to {layer_name} as "
                             f"specified by QP {qp_id}, but the layer already has quantizer "
                             f"config {assigned_qconfig} assigned to it!"
                         )
                     continue  # The layer has already been quantized
                 quantized_layer_names_vs_qconfigs[layer_name] = qconfig
@@ -504,15 +504,15 @@
                 fake_quantize_name = self._get_fake_quantize_name(target_node_name, input_port_id)
                 quantizer_spec = TFQuantizerSpec.from_config(qp.qconfig, narrow_range=False, half_range=False)
                 fake_quantize_layer = FakeQuantize(quantizer_spec, name=fake_quantize_name)
                 self._op_names.append(fake_quantize_layer.op_name)
 
                 is_custom, layer_info = converter.get_layer_info_for_node(target_node_name)
                 if is_custom:
-                    raise RuntimeError("Quantizing custom layer activations is currently unsupported!")
+                    raise nncf.InternalError("Quantizing custom layer activations is currently unsupported!")
                 if input_port_id is not None:
                     target_point = TFBeforeLayer(
                         layer_info.layer_name, instance_idx=layer_info.instance_idx, input_port_id=input_port_id
                     )
                 else:
                     target_point = TFAfterLayer(
                         layer_info.layer_name, instance_idx=layer_info.instance_idx, output_port_id=0
@@ -541,15 +541,15 @@
     def _log_if_overflow_fix_was_applied(self, applied_overflow_fix: bool):
         if applied_overflow_fix:
             if self._overflow_fix == "enable":
                 quantizers_with_overflow_fix_str = "all weight quantizers"
             elif self._overflow_fix == "first_layer_only":
                 quantizers_with_overflow_fix_str = "first convolution weight quantizers"
             elif self._overflow_fix != "disable":
-                raise RuntimeError(f"Unknown overflow fix type: {self._overflow_fix}")
+                raise nncf.InternalError(f"Unknown overflow fix type: {self._overflow_fix}")
             nncf_logger.info(f"Overflow issue fix was applied to {quantizers_with_overflow_fix_str}.")
 
     def _generate_unified_scale_groups(
         self,
         model: tf.keras.Model,
         quantizer_setup: SingleConfigQuantizerSetup,
         qp_id_to_index: Dict[QuantizationPointId, int],
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/collectors.py` & `nncf-2.9.0/nncf/tensorflow/quantization/collectors.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/default_quantization.py` & `nncf-2.9.0/nncf/tensorflow/quantization/default_quantization.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/functions.py` & `nncf-2.9.0/nncf/tensorflow/quantization/functions.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/init_range.py` & `nncf-2.9.0/nncf/tensorflow/quantization/init_range.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -61,23 +61,23 @@
             node_name = layer.name.replace("/fake_quantize", "")
             group = QuantizerGroup.ACTIVATIONS
         return self.get_init_config_for_scope_and_group(node_name, group)
 
     def get_init_config_for_scope_and_group(self, node_name: str, group: QuantizerGroup) -> RangeInitConfig:
         matches: List[RangeInitConfig] = []
         for pl_config in self.per_layer_range_init_configs:
-            if should_consider_scope(
+            should_be_considered = should_consider_scope(
                 node_name, ignored_scopes=pl_config.ignored_scopes, target_scopes=pl_config.target_scopes
-            ):
-                if group == pl_config.target_group or pl_config.target_group is None:
-                    matches.append(
-                        RangeInitConfig(
-                            pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
-                        )
+            )
+            if should_be_considered and (group == pl_config.target_group or pl_config.target_group is None):
+                matches.append(
+                    RangeInitConfig(
+                        pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
                     )
+                )
         if len(matches) > 1:
             raise ValueError(
                 "Location {} matches more than one per-layer initialization parameter "
                 "definition!".format(str(node_name))
             )
         if len(matches) == 1:
             return matches[0]
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/layers.py` & `nncf-2.9.0/nncf/tensorflow/quantization/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/quantize_model.py` & `nncf-2.9.0/nncf/tensorflow/quantization/quantize_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Dict, Optional
 
 import tensorflow as tf
 
+import nncf
 from nncf.common.initialization.dataloader import NNCFDataLoader
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.config import NNCFConfig
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import QuantizationRangeInitArgs
 from nncf.data import Dataset
 from nncf.data.dataset import DataProvider
@@ -147,21 +148,21 @@
     Implementation of the `quantize()` method for the TensorFlow backend.
     """
     if model_type is not None:
         raise ValueError(f"model_type={model_type} is not supported")
     if fast_bias_correction is False:
         raise ValueError(f"fast_bias_correction={fast_bias_correction} is not supported")
     if ignored_scope is not None and ignored_scope.types:
-        raise RuntimeError(
+        raise nncf.InternalError(
             "Quantization algorithm form the TensorFlow backend "
             "does not support operation types in the ignored "
             "scopes yet"
         )
     if target_device == TargetDevice.CPU_SPR:
-        raise RuntimeError("target_device == CPU_SPR is not supported.")
+        raise nncf.InternalError("target_device == CPU_SPR is not supported.")
 
     if mode is not None:
         raise ValueError(f"mode={mode} is not supported")
 
     if preset is None:
         preset = QuantizationPreset.PERFORMANCE
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/quantizers.py` & `nncf-2.9.0/nncf/tensorflow/quantization/quantizers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from functools import partial
 from typing import Any, Dict, Optional
 
 import tensorflow as tf
 
+import nncf
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerSpec
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
 from nncf.tensorflow.layers.custom_objects import NNCF_QUANTIZATION_OPERATIONS
 from nncf.tensorflow.layers.data_layout import get_channel_axis
 from nncf.tensorflow.layers.data_layout import get_channel_size
@@ -322,15 +323,17 @@
             initializer=tf.keras.initializers.Constant(-1.0 if self.signedness_to_force in (True, None) else 0.0),
             trainable=False,
         )
         return {"scale_var": scale, "signed_var": signed}
 
     def apply_overflow_fix(self, weights):
         if self.num_bits != 8 or not self._half_range:
-            raise RuntimeError("Attempt to apply overflow issue fix to quantizer which is not configured for that.")
+            raise nncf.InternalError(
+                "Attempt to apply overflow issue fix to quantizer which is not configured for that."
+            )
 
         # Multiplier to expand scale from 7 bit to 8 bit
         multiplier = 127 / 63 if self.narrow_range else 255 / 127
         weights["scale_var"].assign(multiplier * weights["scale_var"])
         self._eps *= multiplier
         self._half_range = False
 
@@ -446,15 +449,17 @@
         input_range = layer.add_weight(
             name + "_input_range", shape=shape, initializer=tf.keras.initializers.Constant(1.0), trainable=True
         )
         return {"input_low_var": input_low, "input_range_var": input_range}
 
     def apply_overflow_fix(self, weights):
         if self.num_bits != 8 or not self._half_range:
-            raise RuntimeError("Attempt to apply overflow issue fix to quantizer which is not configured for that.")
+            raise nncf.InternalError(
+                "Attempt to apply overflow issue fix to quantizer which is not configured for that."
+            )
 
         # Low value shift to expand quantize range from 7 bit to 8 bit properly
         weights["input_low_var"].assign(
             weights["input_low_var"]
             + self._min_adj(7, weights["input_low_var"], weights["input_range_var"] + self._eps, self.narrow_range)
         )
         # Multiplier to expand scale from 7 bit to 8 bit
```

### Comparing `nncf-2.8.1/nncf/tensorflow/quantization/utils.py` & `nncf-2.9.0/nncf/tensorflow/quantization/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/__init__.py` & `nncf-2.9.0/nncf/torch/sparsity/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/base_algorithm.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/base_algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/callbacks.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/callbacks.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/collector.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/collector.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/__init__.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/algorithm.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/algorithm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,24 +11,24 @@
 from copy import deepcopy
 from typing import Set
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
-from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.scopes import check_scopes_in_graph
 from nncf.common.scopes import should_consider_scope
 from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
+from nncf.common.sparsity.schedulers import SparsityScheduler
 from nncf.common.sparsity.statistics import LayerThreshold
 from nncf.common.sparsity.statistics import MagnitudeSparsityStatistics
 from nncf.common.statistics import NNCFStatistics
 from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.config.schemata.defaults import MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE
@@ -153,25 +153,29 @@
         params["sparsity_init"] = sparsity_init
         scheduler_type = params.get("schedule", "polynomial")
 
         if scheduler_type == "adaptive":
             raise ValueError("Magnitude sparsity algorithm do not support adaptive scheduler")
 
         scheduler_cls = SPARSITY_SCHEDULERS.get(scheduler_type)
-        self._scheduler = scheduler_cls(self, params)
+        self._scheduler: SparsityScheduler = scheduler_cls(self, params)
         self._loss = TFZeroCompressionLoss()
         self._bn_adaptation = None
         self._config = config
         self.set_sparsity_level(sparsity_init)
 
     @property
-    def scheduler(self) -> CompressionScheduler:
+    def scheduler(self) -> SparsityScheduler:
         return self._scheduler
 
     @property
+    def current_sparsity_level(self) -> float:
+        return self._scheduler.current_sparsity_level
+
+    @property
     def loss(self) -> CompressionLoss:
         return self._loss
 
     def freeze(self, freeze: bool = True):
         self._frozen = freeze
 
     def set_sparsity_level(self, sparsity_level, run_batchnorm_adaptation: bool = False):
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/functions.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/functions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/magnitude/operation.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/magnitude/operation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/rb/__init__.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/rb/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/rb/algorithm.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/rb/algorithm.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -146,14 +146,20 @@
     @property
     def loss(self) -> SparseLoss:
         return self._loss
 
     def set_sparsity_level(self, sparsity_level):
         self._loss.set_target_sparsity_loss(sparsity_level)
 
+    @property
+    def current_sparsity_level(self) -> float:
+        # TODO: align with torch where it currently shows the sparsity level as reported by loss object.
+        #  TF does not seem to have this functionality in its SparseLoss right now.
+        return self.scheduler.current_sparsity_level
+
     def freeze(self):
         self._loss.disable()
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         collector = TFSparseModelStatisticsCollector(self.model, self._op_names)
         model_stats = collector.collect()
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/rb/functions.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/rb/functions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/rb/loss.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/rb/loss.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/rb/operation.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/rb/operation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/sparsity/utils.py` & `nncf-2.9.0/nncf/tensorflow/sparsity/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tensor.py` & `nncf-2.9.0/nncf/torch/tensor.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,35 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+import torch
 
 from nncf.common.tensor import NNCFTensor
 
 
-class TFNNCFTensor(NNCFTensor):
+class PTNNCFTensor(NNCFTensor):
     """
-    A realisation of tensorflow tensors wrapper for common NNCF algorithms.
+    A realisation of torch tensors wrapper for common NNCF algorithms.
     """
 
-    def __init__(self, tensor: tf.Tensor):
+    def __init__(self, tensor: torch.tensor):
         # In case somebody attempts to wrap
         # tensor twice
         if isinstance(tensor, self.__class__):
             tensor = tensor.tensor
 
         super().__init__(tensor)
 
     @property
-    def device(self) -> tf.device:
+    def device(self) -> torch.device:
         return self._tensor.device
+
+    def is_empty(self) -> bool:
+        return self.tensor.numel() == 0
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tensor_statistics/__init__.py` & `nncf-2.9.0/nncf/torch/automl/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tensor_statistics/collectors.py` & `nncf-2.9.0/nncf/tensorflow/tensor_statistics/collectors.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tensor_statistics/reduction.py` & `nncf-2.9.0/nncf/tensorflow/tensor_statistics/reduction.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tensor_statistics/statistics.py` & `nncf-2.9.0/nncf/tensorflow/tensor_statistics/statistics.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/tf_internals.py` & `nncf-2.9.0/nncf/tensorflow/tf_internals.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/utils/__init__.py` & `nncf-2.9.0/nncf/torch/automl/agent/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/utils/node.py` & `nncf-2.9.0/nncf/tensorflow/utils/node.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/utils/scopes_handle.py` & `nncf-2.9.0/nncf/tensorflow/utils/scopes_handle.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/tensorflow/utils/state.py` & `nncf-2.9.0/nncf/tensorflow/utils/state.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/__init__.py` & `nncf-2.9.0/nncf/torch/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/accuracy_aware_training/__init__.py` & `nncf-2.9.0/nncf/torch/automl/agent/ddpg/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/accuracy_aware_training/runner.py` & `nncf-2.9.0/nncf/torch/accuracy_aware_training/runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -127,22 +127,20 @@
             load_state(model, resuming_model_state_dict, is_resume=True)
 
     def _make_checkpoint_path(self, is_best, compression_rate=None):
         extension = ".pth"
         return osp.join(self._checkpoint_save_dir, f'acc_aware_checkpoint_{"best" if is_best else "last"}{extension}')
 
     def add_tensorboard_scalar(self, key, data, step):
-        if is_main_process():
-            if self.verbose and self._tensorboard_writer is not None:
-                self._tensorboard_writer.add_scalar(key, data, step)
+        if is_main_process() and self.verbose and self._tensorboard_writer is not None:
+            self._tensorboard_writer.add_scalar(key, data, step)
 
     def add_tensorboard_image(self, key, data, step):
-        if is_main_process():
-            if self.verbose and self._tensorboard_writer is not None:
-                self._tensorboard_writer.add_image(key, ToTensor()(data), step)
+        if is_main_process() and self.verbose and self._tensorboard_writer is not None:
+            self._tensorboard_writer.add_image(key, ToTensor()(data), step)
 
 
 class PTAdaptiveCompressionLevelTrainingRunner(
     BaseAdaptiveCompressionLevelTrainingRunner, PTAccuracyAwareTrainingRunner
 ):
     def __init__(
         self,
```

### Comparing `nncf-2.8.1/nncf/torch/accuracy_aware_training/utils.py` & `nncf-2.9.0/nncf/torch/accuracy_aware_training/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/algo_selector.py` & `nncf-2.9.0/nncf/torch/algo_selector.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/__init__.py` & `nncf-2.9.0/nncf/torch/automl/environment/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/agent/__init__.py` & `nncf-2.9.0/nncf/torch/binarization/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/agent/ddpg/__init__.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/agent/ddpg/ddpg.py` & `nncf-2.9.0/nncf/torch/automl/agent/ddpg/ddpg.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/agent/ddpg/memory.py` & `nncf-2.9.0/nncf/torch/automl/agent/ddpg/memory.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 
 import random
 from collections import deque
 from collections import namedtuple
 
 import numpy as np
 
+import nncf
 from nncf import nncf_logger
 
 # [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py
 
 # This is to be understood as a transition: Given `state0`, performing `action`
 # yields `reward` and results in `state1`, which might be `terminal`.
 Experience = namedtuple("Experience", "state0, action, reward, state1, terminal1")
@@ -77,15 +78,15 @@
 
         elif self.length == self.maxlen:
             # No space, remove the first item then append
             self.data.pop(0)
             self.data.append(v)
         else:
             # This should never happen.
-            raise RuntimeError()
+            raise nncf.BufferFullError()
 
 
 def zeroed_observation(observation):
     if hasattr(observation, "shape"):
         return np.zeros(observation.shape)
     if hasattr(observation, "__iter__"):
         out = []
```

### Comparing `nncf-2.8.1/nncf/torch/automl/environment/__init__.py` & `nncf-2.9.0/nncf/torch/graph/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/automl/environment/quantization_env.py` & `nncf-2.9.0/nncf/torch/automl/environment/quantization_env.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -144,15 +144,15 @@
         self._hw_precision_constraints = hw_precision_constraints
         self._bn_adaptation = None
 
         self.model_name = self.qmodel.__class__.__name__
 
         # Check and only proceed if target device is supported by Q.Env
         self.hw_cfg_type = hw_config_type
-        assert self.hw_cfg_type in [None, HWConfigType.VPU]
+        assert self.hw_cfg_type in [None, HWConfigType.NPU]
 
         # Set target compression ratio
         self.compression_ratio = params.compression_ratio
 
         self.eval_loader = PartialDataLoader(self.eval_loader, iter_ratio=params.eval_subset_ratio)
 
         # Bool to disable hard resource constraint
@@ -166,24 +166,24 @@
 
         # Counter for number of evaluate_strategy calls
         self._n_eval = 0
 
         # Configure search space for precision according to target device
         if self.hw_cfg_type is None:
             self.model_bitwidth_space = params.bits
-        elif self.hw_cfg_type is HWConfigType.VPU:
+        elif self.hw_cfg_type is HWConfigType.NPU:
             self.model_bitwidth_space = self._hw_precision_constraints.get_all_unique_bitwidths()
         self.model_bitwidth_space = sorted(list(self.model_bitwidth_space))
 
         # Create mapping of QuantizerId to the space of the corresponding quantizer's allowed qconfigs
         self.qconfig_space_map: Dict[QuantizerId, List[QuantizerConfig]] = OrderedDict.fromkeys(
             self.qctrl.all_quantizations.keys()
         )
         if self.hw_cfg_type is None:
-            for qid in self.qconfig_space_map.keys():
+            for qid in self.qconfig_space_map:
                 conf = self.qctrl.all_quantizations[qid].get_quantizer_config()
                 conf_list_to_set = []
                 for bit in self.model_bitwidth_space:
                     bit_adjusted_conf = deepcopy(conf)
                     bit_adjusted_conf.num_bits = bit
                     conf_list_to_set.append(bit_adjusted_conf)
                 self.qconfig_space_map[qid] = conf_list_to_set
```

### Comparing `nncf-2.8.1/nncf/torch/batchnorm_adaptation.py` & `nncf-2.9.0/nncf/torch/batchnorm_adaptation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/__init__.py` & `nncf-2.9.0/nncf/torch/graph/transformations/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/algo.py` & `nncf-2.9.0/nncf/torch/binarization/algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,14 +13,15 @@
 
 from texttable import Texttable
 from torch import nn
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
+from nncf.common.deprecation import warning_deprecated
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.logging import nncf_logger
 from nncf.common.statistics import NNCFStatistics
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.schemata.defaults import BINARIZATION_MODE
@@ -45,14 +46,15 @@
 
 
 @PT_COMPRESSION_ALGORITHMS.register("binarization")
 class BinarizationBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config, should_init: bool = True):
         super().__init__(config, should_init)
         self.mode = self._algo_config.get("mode", BINARIZATION_MODE)
+        warning_deprecated("`binarization` algorithm is deprecated and will no longer be supported in the future.")
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         layout = PTTransformationLayout()
         commands = self._binarize_weights_and_module_inputs(target_model)
         for command in commands:
             layout.register(command)
         return layout
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/binarize_functions.py` & `nncf-2.9.0/nncf/torch/binarization/binarize_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -123,18 +123,17 @@
             ctx.save_for_backward(input_, scale, output)
         ctx.save_for_backward(input_, scale, output)
         return output
 
     @staticmethod
     def backward(ctx: Any, *grad_outputs: Any) -> Any:
         grad_output = grad_outputs[0]
-        if grad_output.is_cuda:
-            if not grad_output.is_contiguous():
-                nncf_logger.debug("grad_output is not contiguous!")
-                grad_output = grad_output.contiguous()
+        if grad_output.is_cuda and not grad_output.is_contiguous():
+            nncf_logger.debug("grad_output is not contiguous!")
+            grad_output = grad_output.contiguous()
 
         input_, scale, output = ctx.saved_variables
 
         if input_.is_cuda:
             grad_input, grad_scale, grad_threshold = BinarizedFunctionsCUDA.get("ActivationBinarize_backward")(
                 grad_output, input_, scale, output
             )
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/extensions.py` & `nncf-2.9.0/nncf/torch/binarization/extensions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 import os.path
 import subprocess
 
 import torch
 
+import nncf
 from nncf import nncf_logger
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
 from nncf.torch.binarization.reference import ReferenceBinarizedFunctions
 from nncf.torch.extensions import EXTENSIONS
 from nncf.torch.extensions import CudaNotAvailableStub
 from nncf.torch.extensions import ExtensionLoader
 from nncf.torch.extensions import ExtensionLoaderTimeoutException
@@ -94,15 +95,15 @@
                 build_directory=cls.get_build_dir(),
                 verbose=False,
             )
         except ExtensionLoaderTimeoutException as e:
             raise e
         except (subprocess.CalledProcessError, OSError, RuntimeError) as e:
             assert torch.cuda.is_available()
-            raise RuntimeError(
+            raise nncf.InstallationError(
                 "CUDA is available for PyTorch, but NNCF could not compile "
                 "GPU binarization extensions. Make sure that you have installed CUDA development "
                 "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
                 "guidance) and that 'nvcc' is available on your system's PATH variable.\n"
             ) from e
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/layers.py` & `nncf-2.9.0/nncf/torch/binarization/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/binarization/reference.py` & `nncf-2.9.0/nncf/torch/binarization/reference.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2022 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,16 @@
 
 from enum import Enum
 from typing import TypeVar
 
 import numpy as np
 import torch
 
+import nncf
+
 GeneralizedTensor = TypeVar("GeneralizedTensor", torch.Tensor, np.ndarray)
 
 
 class ReferenceBackendType(Enum):
     NUMPY = "numpy"
     TORCH = "torch"
 
@@ -26,15 +28,15 @@
 class ReferenceBase:
     def __init__(self, backend_type: ReferenceBackendType):
         if backend_type is ReferenceBackendType.NUMPY:
             self.backend = np
         elif backend_type is ReferenceBackendType.TORCH:
             self.backend = torch
         else:
-            raise RuntimeError("Unknown backend for ReferenceQuantize")
+            raise nncf.UnsupportedBackendError("Unknown backend for ReferenceQuantize")
 
     def _astype(self, tensor: GeneralizedTensor, dtype) -> GeneralizedTensor:
         if self.backend is np:
             return tensor.astype(dtype)
         return tensor.type(dtype)
```

### Comparing `nncf-2.8.1/nncf/torch/checkpoint_loading.py` & `nncf-2.9.0/nncf/torch/checkpoint_loading.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 import re
 from enum import Enum
 from typing import Dict, List, Set, Tuple
 
 import torch
 
+import nncf
 from nncf.common.deprecation import warning_deprecated
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.api_marker import api
 
 
 @api(canonical_alias="nncf.torch.load_state")
 def load_state(
@@ -105,15 +106,15 @@
     def add_skipped_and_missing_keys(self, model_state_dict: Dict[str, torch.Tensor]):
         all_processed_keys = []
         optional_param_names = OPTIONAL_PARAMETERS_REGISTRY.get_parameters_names()
         params_to_skip = tuple("." + name for name in optional_param_names)
         for keys in self._keys.values():
             all_processed_keys.extend(keys)
 
-        for key in model_state_dict.keys():
+        for key in model_state_dict:
             if key not in all_processed_keys:
                 if key.endswith(params_to_skip) or key in optional_param_names:
                     self.add_key(key, ProcessedKeyStatus.SKIPPED)
                     nncf_logger.warning(f"The optional parameter {key} is missing in the loaded state.")
                 else:
                     self.add_key(key, ProcessedKeyStatus.MISSING)
 
@@ -136,15 +137,15 @@
             is_missing = key_status == ProcessedKeyStatus.MISSING
             erroneous = key_status in (ProcessedKeyStatus.SIZE_MISMATCHED, ProcessedKeyStatus.UNEXPECTED)
             if keys and (erroneous or is_missing and (is_resume or not are_all_loaded_params_matched)):
                 add_error_msg(key_status.value, keys)
         if error_msgs:
             error_msg = "Error(s) when loading model parameters:\n\t{}".format("\n\t".join(error_msgs))
             if is_resume:
-                raise RuntimeError(error_msg)
+                raise nncf.InternalError(error_msg)
             nncf_logger.error(error_msg)
 
 
 class NormalizedKeys:
     """
     Contains normalized form of parameters. It helps to discard irrelevant prefixes added during wrapping in
     NNCFNetwork or DataParallel/DistributedDataParallel objects, to handle legacy parameters' names and to match
```

### Comparing `nncf-2.8.1/nncf/torch/composite_compression.py` & `nncf-2.9.0/nncf/torch/composite_compression.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import TypeVar
 
 import torch.nn
 
+import nncf
 from nncf import NNCFConfig
 from nncf.common.composite_compression import CompositeCompressionAlgorithmBuilder
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
 from nncf.common.composite_compression import CompositeCompressionLoss
 from nncf.config.extractors import extract_algorithm_names
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
@@ -40,15 +41,15 @@
 
 class PTCompositeCompressionAlgorithmBuilder(CompositeCompressionAlgorithmBuilder, PTCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
 
         algo_names = extract_algorithm_names(config)
         if len(algo_names) < 2:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "Composite algorithm builder must be supplied with a config with more than one "
                 "compression algo specified!"
             )
         for algo_name in algo_names:
             algo_builder = PT_COMPRESSION_ALGORITHMS.get(algo_name)
             self._child_builders.append(algo_builder(config, should_init=should_init))
```

### Comparing `nncf-2.8.1/nncf/torch/compression_method_api.py` & `nncf-2.9.0/nncf/torch/compression_method_api.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,14 +17,15 @@
 """
 from abc import abstractmethod
 from typing import Any, Dict, List, Tuple, TypeVar
 
 import torch
 from torch import nn
 
+import nncf
 from nncf.api.compression import CompressionLoss
 from nncf.common.compression import BaseCompressionAlgorithmBuilder
 from nncf.common.compression import BaseCompressionAlgorithmController
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.scopes import check_scopes_in_graph
 from nncf.common.scopes import should_consider_scope
@@ -162,15 +163,15 @@
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         :return: The instance of the `PTCompressionAlgorithmController`.
         """
         ctrl = self._build_controller(model)
         if not isinstance(ctrl, PTCompressionAlgorithmController):
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "Internal error: builder must create controller inherited from "
                 "`PTCompressionAlgorithmController` class"
             )
         ctrl.set_builder_state_with_name(self.name, self.get_state())
         return ctrl
 
     def _get_state_without_name(self) -> Dict[str, Any]:
@@ -191,26 +192,26 @@
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         raise NotImplementedError()
 
     def _handle_frozen_layers(self, target_model: NNCFNetwork):
         scopes_of_frozen_layers = []
         for weighted_node in target_model.nncf.get_weighted_original_graph_nodes():
-            if not weighted_node.layer_attributes.weight_requires_grad:
-                if self._should_consider_scope(weighted_node.node_name):
-                    scopes_of_frozen_layers.append(weighted_node.node_name)
+            should_be_considered = self._should_consider_scope(weighted_node.node_name)
+            if not weighted_node.layer_attributes.weight_requires_grad and should_be_considered:
+                scopes_of_frozen_layers.append(weighted_node.node_name)
         scopes_to_print = "\n".join(scopes_of_frozen_layers)
         if len(scopes_of_frozen_layers) > 0:
             is_allowed, reason = self._are_frozen_layers_allowed()
             if is_allowed:
                 nncf_logger.warning(
                     f"{reason}, compressing them without tuning weights.\nFrozen layers:\n{scopes_to_print}"
                 )
             else:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"{reason}.\n"
                     f"Please unfreeze them or put into the Ignored Scope.\n"
                     f"Frozen Layers:\n"
                     f"{scopes_to_print}"
                 )
 
     def _should_consider_scope(self, node_name: NNCFNodeName) -> bool:
```

### Comparing `nncf-2.8.1/nncf/torch/debug.py` & `nncf-2.9.0/nncf/torch/debug.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/__init__.py` & `nncf-2.9.0/nncf/torch/hardware/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/context.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/context.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,31 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import threading
 import weakref
+from collections import OrderedDict
 from collections import defaultdict
 from collections import deque
 from contextlib import contextmanager
-from typing import Callable, DefaultDict, List, Optional, Union
+from typing import Callable, DefaultDict, Dict, List, Optional, Union
 
 import torch
 
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
+from nncf.common.hook_handle import HookHandle
+from nncf.common.hook_handle import add_op_to_registry
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.debug import is_debug
 from nncf.common.utils.patcher import PATCHER
 from nncf.torch.dynamic_graph.graph import DynamicGraph
 from nncf.torch.dynamic_graph.graph import DynamicGraphNode
 from nncf.torch.dynamic_graph.graph import DynamicGraphNodeParameters
 from nncf.torch.dynamic_graph.graph import TensorMetaComparator
@@ -90,16 +93,16 @@
         return CopySafeThreadingVars()
 
 
 class TracingContext:
     def __init__(self):
         self.graph = DynamicGraph()
 
-        self._post_hooks: DefaultDict[OperationAddress, List[Callable]] = defaultdict(list)
-        self._pre_hooks: DefaultDict[PreHookId, List[Callable]] = defaultdict(list)
+        self._post_hooks: DefaultDict[OperationAddress, Dict[str, Callable]] = defaultdict(OrderedDict)
+        self._pre_hooks: DefaultDict[PreHookId, Dict[str, Callable]] = defaultdict(OrderedDict)
         self._num_nested_hooks = 0
         self.reused_parameters = []
 
         self._threading = CopySafeThreadingVars()
 
         self._n_instances_searching_graph = 0
 
@@ -278,43 +281,43 @@
         self.module_call_stack.append(called_module)
         self.relative_scopes_stack.append(relative_scopes_list)
 
     def pop_scope(self):
         self.relative_scopes_stack.pop()
         self.module_call_stack.pop()
 
-    def register_pre_hooks(self, fn_list: List[Callable], op_address: OperationAddress, input_port_id: int):
+    def register_pre_hook(self, fn: Callable, op_address: OperationAddress, input_port_id: int) -> HookHandle:
         pre_hook_id = PreHookId(op_address, input_port_id)
-        self._pre_hooks[pre_hook_id].extend(fn_list)
+        return add_op_to_registry(self._pre_hooks[pre_hook_id], fn)
 
     def execute_pre_hooks(self, op_address: OperationAddress, op_inputs: OperatorInput) -> OperatorInput:
         in_op = getattr(self, "in_operator", False)
         self.in_operator = False
         self._threading.thread_local.num_nested_hooks += 1
 
         pre_hook_ids_for_curr_op = [x for x in self._pre_hooks if x.op_address == op_address]
         pre_hook_ids_for_curr_op = sorted(pre_hook_ids_for_curr_op, key=lambda x: x.input_port_id)
         for pre_hook_id in pre_hook_ids_for_curr_op:
-            hook_list_for_current_input_port = self._pre_hooks[pre_hook_id]
+            hook_list_for_current_input_port = self._pre_hooks[pre_hook_id].values()
             input_arg_to_process = pre_hook_id.input_port_id
             for hook in hook_list_for_current_input_port:
                 op_inputs[input_arg_to_process] = hook(op_inputs[input_arg_to_process])
         self._threading.thread_local.num_nested_hooks -= 1
         self.in_operator = in_op
         return op_inputs
 
-    def register_post_hooks(self, fn_list: List[Callable], op_address: OperationAddress):
-        self._post_hooks[op_address].extend(fn_list)
+    def register_post_hook(self, fn: Callable, op_address: OperationAddress) -> HookHandle:
+        return add_op_to_registry(self._post_hooks[op_address], fn)
 
     def execute_post_hooks(self, op_address: OperationAddress, outputs):
         in_op = getattr(self, "in_operator", False)
         self.in_operator = False
         self._threading.thread_local.num_nested_hooks += 1
         if op_address in self._post_hooks:
-            for hook in self._post_hooks[op_address]:
+            for hook in self._post_hooks[op_address].values():
                 outputs = hook(outputs)
         self._threading.thread_local.num_nested_hooks -= 1
         self.in_operator = in_op
         return outputs
 
     @property
     def is_tracing(self) -> bool:
@@ -450,22 +453,21 @@
         self.active_block_indexes = block_indexes
         if self.elastic_depth and len(block_indexes) > 0:
             for block_index in block_indexes:
                 self.start_node_name_of_skipped_block.append(self.skipped_blocks[block_index].start_node_name)
                 self.end_node_name_of_skipped_block.append(self.skipped_blocks[block_index].end_node_name)
 
     def set_elastic_blocks(self, blocks: List["BuildingBlock"] = None):  # noqa: F821
-        if blocks is not None:
-            if isinstance(blocks, list):
-                if len(blocks) == 0:
-                    self.skipped_blocks = []
-                elif isinstance(blocks[0], str):
-                    self.skipped_blocks = [blocks]
-                else:
-                    self.skipped_blocks = blocks
+        if blocks is not None and isinstance(blocks, list):
+            if len(blocks) == 0:
+                self.skipped_blocks = []
+            elif isinstance(blocks[0], str):
+                self.skipped_blocks = [blocks]
+            else:
+                self.skipped_blocks = blocks
 
 
 def set_current_context(c: TracingContext):
     _CURRENT_CONTEXT.context = c
 
 
 @api(canonical_alias="nncf.torch.no_nncf_trace")
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/graph.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/graph.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/graph_tracer.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/graph_tracer.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/io_handling.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/io_handling.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,14 +13,15 @@
 from functools import partial
 from inspect import Parameter
 from inspect import Signature
 from typing import Any, Dict, List, Optional, Protocol, Set, Tuple, Type, Union
 
 import torch
 
+import nncf
 from nncf import Dataset
 from nncf import NNCFConfig
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
 from nncf.common.initialization.dataloader import NNCFDataLoader
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.api_marker import api
@@ -174,15 +175,15 @@
         otherwise raises a RuntimeError. The "input_info" field structure must conform to the NNCF config jsonschema.
 
         :param config: An NNCFConfig instance.
         :return: FillerInputInfo object initialized according to config.
         """
         input_infos = config.get("input_info")
         if input_infos is None:
-            raise RuntimeError("Passed NNCFConfig does not have an 'input_info' field")
+            raise nncf.ValidationError("Passed NNCFConfig does not have an 'input_info' field")
         if isinstance(input_infos, dict):
             return FillerInputInfo(
                 [
                     FillerInputElement(
                         input_infos.get("sample_size"),
                         input_infos.get("type"),
                         input_infos.get("keyword"),
@@ -198,15 +199,15 @@
                         info_dict.get("sample_size"),
                         info_dict.get("type"),
                         info_dict.get("keyword"),
                         info_dict.get("filler"),
                     )
                 )
             return FillerInputInfo(elements)
-        raise RuntimeError("Invalid input_infos specified in config - should be either dict or list of dicts")
+        raise nncf.ValidationError("Invalid input_infos specified in config - should be either dict or list of dicts")
 
     def get_forward_inputs(
         self, device: Optional[Union[str, torch.device]] = None
     ) -> Tuple[Tuple[torch.Tensor, ...], Dict[str, torch.Tensor]]:
         args_list = []
         kwargs = {}
         for fe in self.elements:
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/layer_attributes_handlers.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/layer_attributes_handlers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/op_input_processing.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/op_input_processing.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/operation_address.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/operation_address.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/patch_pytorch.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/patch_pytorch.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,14 +17,15 @@
 import torch
 import torch.utils.cpp_extension
 from torch._jit_internal import createResolutionCallbackFromFrame
 from torch.jit import is_tracing
 from torch.nn import DataParallel
 from torch.nn.parallel import DistributedDataParallel
 
+import nncf
 from nncf import nncf_logger
 from nncf.common.utils.api_marker import api
 from nncf.torch.dynamic_graph.structs import NamespaceTarget
 from nncf.torch.dynamic_graph.structs import PatchedOperatorInfo
 from nncf.torch.dynamic_graph.trace_tensor import TracedParameter
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
 from nncf.torch.dynamic_graph.wrappers import ignore_scope
@@ -35,25 +36,25 @@
 def get_namespaces_to_patch(namespace_target: NamespaceTarget) -> Tuple[object, ...]:
     if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:
         return (torch.nn.functional,)
     if namespace_target == NamespaceTarget.TORCH_TENSOR:
         return (TracedTensor, TracedParameter)
     if namespace_target == NamespaceTarget.TORCH:
         return (torch,)
-    raise RuntimeError("{} namespace wasn't found in {}".format(namespace_target, NamespaceTarget))
+    raise nncf.ValidationError("{} namespace wasn't found in {}".format(namespace_target, NamespaceTarget))
 
 
 def get_namespace_to_extract_functions_from(namespace_target: NamespaceTarget) -> object:
     if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:
         return torch.nn.functional
     if namespace_target == NamespaceTarget.TORCH_TENSOR:
         return torch.Tensor
     if namespace_target == NamespaceTarget.TORCH:
         return torch._C._VariableFunctions
-    raise RuntimeError("{} namespace wasn't found in {}".format(namespace_target, NamespaceTarget))
+    raise nncf.ValidationError("{} namespace wasn't found in {}".format(namespace_target, NamespaceTarget))
 
 
 class FunctionsToPatchWithoutTracing:
     TENSOR_CREATING_FUNCTIONS = [
         "arange",
         "as_subclass",
         "as_tensor",
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/scope.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/scope.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import re
 from copy import deepcopy
 from typing import List
 
+import nncf
+
 
 class ScopeElement:
     def __init__(self, calling_module_class_name: str, calling_field_name: str = None):
         self.calling_module_class_name = calling_module_class_name
         self.calling_field_name = calling_field_name
 
     def __str__(self):
@@ -31,20 +33,20 @@
     def __hash__(self):
         return hash((self.calling_module_class_name, self.calling_field_name))
 
     @staticmethod
     def from_str(string: str):
         matches = re.search(r"(.*)\[(.*)\]|(.*)", string)
         if matches is None:
-            raise RuntimeError("Invalid scope element string")
+            raise nncf.InternalError("Invalid scope element string")
         if matches.groups()[0] is None and matches.groups()[1] is None:
             return ScopeElement(matches.groups()[2])
         if matches.groups()[0] is not None and matches.groups()[1] is not None:
             return ScopeElement(matches.groups()[0], matches.groups()[1])
-        raise RuntimeError("Could not parse the scope element string")
+        raise nncf.InternalError("Could not parse the scope element string")
 
 
 class Scope:
     def __init__(self, scope_elements: List[ScopeElement] = None):
         if scope_elements is not None:
             self.scope_elements = scope_elements
         else:
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/scope_access.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/scope_access.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,36 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Optional
 
 import torch.nn
 
+import nncf
 from nncf.torch.dynamic_graph.scope import Scope
 
 
 def get_module_by_scope(model: torch.nn.Module, scope: Scope) -> Optional[torch.nn.Module]:
     curr_module = model
     for scope_element in scope[1:]:  # omit first scope element which corresponds to base module
         if scope_element.calling_field_name is None:
             # The module used is being created in-place every time and never stored in the model,
             # happens for nn.Softmax in BERT implementations.
             return None
 
         next_module = curr_module._modules.get(scope_element.calling_field_name)
         if next_module is None:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "Could not find a {} module member in {} module of scope {} during node search".format(
                     scope_element.calling_field_name, scope_element.calling_module_class_name, str(scope)
                 )
             )
         curr_module = next_module
     return curr_module
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/structs.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/structs.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/trace_functions.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/trace_functions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from copy import deepcopy
 from typing import Callable, Dict, Iterable, List, Optional, Tuple, TypeVar
 
 import numpy as np
 import torch
 
+import nncf
 from nncf import nncf_logger
 from nncf.common.graph.layer_attributes import Dtype
 from nncf.torch.dynamic_graph.context import TracingContext
 from nncf.torch.dynamic_graph.graph import DynamicGraphNode
 from nncf.torch.dynamic_graph.op_input_processing import OperatorInput
 from nncf.torch.dynamic_graph.trace_tensor import TensorMeta
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
@@ -77,29 +78,29 @@
             # Broadcast one and the same creator ID of input to all outputs
             for out_idx in output_tensors_to_be_traced_indices:
                 forwarded_meta = deepcopy(fargs[input_traced_tensor_indices[0]].tensor_meta)
                 if forwarded_meta is not None:
                     forwarded_meta.shape = tuple(result[out_idx].shape)
                 result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx], forwarded_meta)
         elif len(input_traced_tensor_indices) != len(output_tensors_to_be_traced_indices):
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "Unable to forward trace through operator {} - "
                 "input and output tensor count mismatch!".format(operator.__name__)
             )
         else:
             # Assume that output tensor order corresponds to input tensor order
             for in_idx, out_idx in zip(input_traced_tensor_indices, output_tensors_to_be_traced_indices):
                 forwarded_meta = deepcopy(fargs[in_idx].tensor_meta)
                 if forwarded_meta is not None:
                     forwarded_meta.shape = tuple(result[out_idx].shape)
                 result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx], forwarded_meta)
         if was_tuple:
             result = tuple(result)
     elif len(input_traced_tensor_indices) > 1:
-        raise RuntimeError(
+        raise nncf.ValidationError(
             "Unable to forward trace through operator {} - "
             "input and output tensor count mismatch!".format(operator.__name__)
         )
     elif input_traced_tensor_indices:
         forwarded_meta = deepcopy(fargs[input_traced_tensor_indices[0]].tensor_meta)
         if forwarded_meta is not None:
             forwarded_meta.shape = tuple(result.shape)
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/trace_tensor.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/trace_tensor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/dynamic_graph/wrappers.py` & `nncf-2.9.0/nncf/torch/dynamic_graph/wrappers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 import functools
 from copy import deepcopy
 from typing import Callable, List, Tuple
 
 import torch
 from torch.nn import DataParallel
 
+import nncf
 from nncf.common.graph.definitions import MODEL_CONST_OP_NAME
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.debug import is_debug
 from nncf.torch.dynamic_graph.context import TracingContext
 from nncf.torch.dynamic_graph.context import get_current_context
 from nncf.torch.dynamic_graph.layer_attributes_handlers import OP_NAMES_REQUIRING_ATTRS_FROM_ARGS_KWARGS
@@ -30,16 +31,14 @@
 from nncf.torch.dynamic_graph.structs import PatchedOperatorInfo
 from nncf.torch.dynamic_graph.trace_functions import forward_trace_only
 from nncf.torch.dynamic_graph.trace_functions import make_tensor_metas
 from nncf.torch.dynamic_graph.trace_functions import trace_tensors
 from nncf.torch.dynamic_graph.trace_tensor import TracedParameter
 from nncf.torch.layer_utils import _NNCFModuleMixin
 from nncf.torch.layers import ITERATION_MODULES
-from nncf.torch.return_types import maybe_unwrap_from_torch_return_type
-from nncf.torch.return_types import maybe_wrap_to_torch_return_type
 
 _IGNORED_SCOPES = []
 
 
 def _warn_data_parallel():
     if getattr(_warn_data_parallel, "warned_once", False):
         return
@@ -57,15 +56,15 @@
 
 
 def wrap_operator(operator, operator_info: PatchedOperatorInfo):
     """
     Wraps the input callable object (`operator`) with the functionality that allows the calls to this object
     to be tracked by the currently set global TracingContext. The wrapped functions can be then intercepted,
     their arguments and return values modified arbitrarily and, for functions that correspond to operations on
-    tensors in a DNN,  their general position and address in the DNN's model control flow graph can be established.
+    tensors in a DNN, their general position and address in the DNN's model control flow graph can be established.
 
     :param: operator: A callable object to be wrapped.
     :param: operator_info (PatchedOperatorInfo): An informational struct containing the specifics of wrapping
             the `operator` in question.
 
     :return: The wrapped version of `operator` that, without a TracingContext, performs functionally the same as
              the unwrapped version, but within a TracingContext is able to be tracked and hooked.
@@ -191,32 +190,30 @@
             is_called_inside_nncf_module = isinstance(ctx.get_current_module(), _NNCFModuleMixin)
             node = ctx.maybe_add_node(
                 processed_input, tensor_metas, op_address, layer_attrs, ignored_algos, is_called_inside_nncf_module
             )
         if is_debug() and node is not None:
             ctx.register_node_call(node)
 
-    unwrapped_result = maybe_unwrap_from_torch_return_type(result)
-    unwrapped_result = trace_tensors(unwrapped_result, node, ctx)
-    unwrapped_result = ctx.execute_post_hooks(op_address, unwrapped_result)
-    result = maybe_wrap_to_torch_return_type(unwrapped_result, result)
+    result = trace_tensors(result, node, ctx)
+    result = ctx.execute_post_hooks(op_address, result)
     return result
 
 
 def _collect_module_attrs_and_ignored_algorithms(
     ctx: TracingContext, op_name: str, args, kwargs
 ) -> Tuple[BaseLayerAttributes, List[str]]:
     layer_attrs = None
     ignored_algos = []
     from nncf.torch.graph.operator_metatypes import OP_NAMES_WITH_WEIGHTS
 
     if op_name in OP_NAMES_WITH_WEIGHTS:
         curr_module = ctx.get_current_module()
         if curr_module is None:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 f"Operation {op_name} requires module attributes, but it was executed outside any module"
             )
         layer_attrs = get_layer_attributes_from_module(curr_module, op_name)
         if isinstance(curr_module, _NNCFModuleMixin):
             ignored_algos = deepcopy(curr_module.ignored_algorithms)
     elif op_name in OP_NAMES_REQUIRING_ATTRS_FROM_ARGS_KWARGS:
         layer_attrs = get_layer_attributes_from_args_and_kwargs(op_name, args, kwargs)
```

### Comparing `nncf-2.8.1/nncf/torch/engine.py` & `nncf-2.9.0/nncf/torch/engine.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/exporter.py` & `nncf-2.9.0/nncf/torch/exporter.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/extensions/__init__.py` & `nncf-2.9.0/nncf/torch/extensions/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,14 +18,15 @@
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 from typing import Callable
 
 import torch
 from torch.utils.cpp_extension import _get_build_directory
 
+import nncf
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.logger import extension_is_loading_info_log
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
 
 EXTENSIONS = Registry("extensions")
 
@@ -134,11 +135,11 @@
 @api(canonical_alias="nncf.torch.force_build_cuda_extensions")
 def force_build_cuda_extensions():
     _force_build_extensions(ExtensionsType.CUDA)
 
 
 class CudaNotAvailableStub:
     def __getattr__(self, item):
-        raise RuntimeError(
+        raise nncf.InstallationError(
             f"CUDA is not available on this machine. Check that the machine has a GPU and a proper "
             f"driver supporting CUDA {torch.version.cuda} is installed."
         )
```

### Comparing `nncf-2.8.1/nncf/torch/extensions/include/common_cuda_defs.cuh` & `nncf-2.9.0/nncf/torch/extensions/include/common_cuda_defs.cuh`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/include/common_cuda_funcs.cuh` & `nncf-2.9.0/nncf/torch/extensions/include/common_cuda_funcs.cuh`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/include/dispatch.h` & `nncf-2.9.0/nncf/torch/extensions/include/dispatch.h`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp` & `nncf-2.9.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp` & `nncf-2.9.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu` & `nncf-2.9.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp` & `nncf-2.9.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp` & `nncf-2.9.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp` & `nncf-2.9.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu` & `nncf-2.9.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu`

 * *Files identical despite different names*

### Comparing `nncf-2.8.1/nncf/torch/external_hook.py` & `nncf-2.9.0/nncf/torch/external_hook.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/functions.py` & `nncf-2.9.0/nncf/torch/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/__init__.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/graph.py` & `nncf-2.9.0/nncf/torch/graph/graph.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Dict, List, Tuple
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import MultipleInputLayerAttributes
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 
@@ -63,15 +64,15 @@
         matches = []
         for node_id, scope_str in self._node_ids_vs_layer_names.items():
             node = self.get_node_by_id(node_id)
             if node.node_name == node_name:
                 matches.append(Scope.from_str(scope_str))
         assert len(matches) <= 1
         if not matches:
-            raise RuntimeError("Node name {} not found in the node-vs-scope dict!".format(node_name))
+            raise nncf.InternalError("Node name {} not found in the node-vs-scope dict!".format(node_name))
         return matches[0]
 
     def get_nodes_with_missed_input_edges(self) -> List[NNCFNode]:
         """
         Returns a list of NNCFNodes that have at least one expected input edge missed.
         Requires MultipleInputLayerAttributes for nodes with several inputs and
         right `num_expected_input_edges` parameter setted for nncf nodes metatypes.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/graph_builder.py` & `nncf-2.9.0/nncf/torch/graph/graph_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/operator_metatypes.py` & `nncf-2.9.0/nncf/torch/graph/operator_metatypes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/pattern_operations.py` & `nncf-2.9.0/nncf/torch/graph/pattern_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/transformations/__init__.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/graph/transformations/command_creation.py` & `nncf-2.9.0/nncf/torch/graph/transformations/command_creation.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,19 +11,32 @@
 
 from torch import Tensor
 
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.torch.graph.transformations.commands import PTBiasCorrectionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
+from nncf.torch.graph.transformations.commands import PTWeightUpdateCommand
 
 
 def create_bias_correction_command(node: NNCFNode, bias_value: Tensor) -> PTBiasCorrectionCommand:
     """
      Creates bias correction command.
 
     :param node: The node in the NNCF graph that corresponds to operation with bias.
     :param bias_value: The new bias value that will be set.
     :return: The `PTBiasCorrectionCommand` command to update bias.
     """
     target_point = PTTargetPoint(TargetType.LAYER, node.node_name)
     return PTBiasCorrectionCommand(target_point, bias_value)
+
+
+def create_command_to_update_weight(node: NNCFNode, weight_value: Tensor) -> PTWeightUpdateCommand:
+    """
+     Creates weight update command.
+
+    :param node: The node in the NNCF graph that corresponds to operation with weight.
+    :param weight_value: The new weight value that will be set.
+    :return: The `PTWeightUpdateCommand` command to update weight.
+    """
+    target_point = PTTargetPoint(TargetType.LAYER, node.node_name)
+    return PTWeightUpdateCommand(target_point, weight_value)
```

### Comparing `nncf-2.8.1/nncf/torch/graph/transformations/commands.py` & `nncf-2.9.0/nncf/torch/graph/transformations/commands.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,14 +17,16 @@
 from nncf.common.graph.transformations.commands import Command
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.graph.transformations.commands import TransformationType
 
+DEFAULT_HOOKS_GROUP_NAME = "default_hooks_group"
+
 
 class PTTargetPointStateNames:
     TARGET_NODE_NAME = "target_node_name"
     INPUT_PORT = "input_port_id"
     TARGET_TYPE = "target_type"
 
 
@@ -132,54 +134,60 @@
     """
 
     def __init__(
         self,
         point: PTTargetPoint,
         fn: Callable,
         priority: TransformationPriority = TransformationPriority.DEFAULT_PRIORITY,
+        hooks_group_name: str = DEFAULT_HOOKS_GROUP_NAME,
     ):
         super().__init__(TransformationType.INSERT, point)
         self.fn: Callable = fn
         self.priority: TransformationPriority = priority
+        self.hooks_group_name = hooks_group_name
 
     def requires_graph_rebuild(self):
         # Rebuild graph when adding quantization nodes.
         return self.priority == TransformationPriority.QUANTIZATION_PRIORITY
 
 
 class PTSharedFnInsertionCommand(PTTransformationCommand):
     def __init__(
         self,
         target_points: List[PTTargetPoint],
         fn: Callable,
         op_unique_name: str,
         priority: TransformationPriority = TransformationPriority.DEFAULT_PRIORITY,
+        hooks_group_name: str = DEFAULT_HOOKS_GROUP_NAME,
     ):
         super().__init__(TransformationType.INSERT, None)
         self.target_points = target_points
         self.fn = fn
         self.op_name = op_unique_name
         self.priority = priority
+        self.hooks_group_name = hooks_group_name
 
     def requires_graph_rebuild(self):
         return True
 
 
 class PTQuantizerInsertionCommand(PTTransformationCommand):
     """
     Insertion quantizer operation to the models.
     """
 
     def __init__(
         self,
         point: PTTargetPoint,
         quantizer: "BaseQuantizer",  # noqa: F821
+        hooks_group_name: str = DEFAULT_HOOKS_GROUP_NAME,
     ):
         super().__init__(TransformationType.INSERT, point)
         self.quantizer = quantizer
+        self.hooks_group_name = hooks_group_name
 
     def requires_graph_rebuild(self):
         return True
 
 
 class PTModelExtractionWithFusedBiasCommand(PTCommand):
     """
```

### Comparing `nncf-2.8.1/nncf/torch/graph/transformations/layout.py` & `nncf-2.9.0/nncf/torch/graph/transformations/layout.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/hardware/__init__.py` & `nncf-2.9.0/nncf/torch/statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/hardware/config.py` & `nncf-2.9.0/nncf/torch/hardware/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/hardware/fused_patterns.py` & `nncf-2.9.0/nncf/torch/hardware/fused_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/initialization.py` & `nncf-2.9.0/nncf/torch/initialization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/knowledge_distillation/__init__.py` & `nncf-2.9.0/nncf/torch/knowledge_distillation/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/knowledge_distillation/algo.py` & `nncf-2.9.0/nncf/torch/knowledge_distillation/algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -36,15 +36,15 @@
 @PT_COMPRESSION_ALGORITHMS.register("knowledge_distillation")
 class KnowledgeDistillationBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
         self.kd_type = self._algo_config.get("type")
         self.scale = self._algo_config.get("scale", KNOWLEDGE_DISTILLATION_SCALE)
         self.temperature = self._algo_config.get("temperature", KNOWLEDGE_DISTILLATION_TEMPERATURE)
-        if "temperature" in self._algo_config.keys() and self.kd_type == "mse":
+        if "temperature" in self._algo_config and self.kd_type == "mse":
             raise ValueError("Temperature shouldn't be stated for MSE Loss (softmax only feature)")
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         self.original_model = deepcopy(target_model).nncf.get_clean_shallow_copy()
         for param in self.original_model.parameters():
             param.requires_grad = False
         return PTTransformationLayout()
```

### Comparing `nncf-2.8.1/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py` & `nncf-2.9.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py` & `nncf-2.9.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/layer_utils.py` & `nncf-2.9.0/nncf/torch/layer_utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,23 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import torch
 from torch import nn
 
+from nncf.common.hook_handle import HookHandle
+from nncf.common.hook_handle import add_op_to_registry
 from nncf.common.utils.registry import Registry
 
 COMPRESSION_MODULES = Registry("compression modules")
 
 
 class ProxyModule:
     def __init__(self, module):
@@ -57,26 +59,22 @@
 
     def get_pre_op(self, key):
         return self.pre_ops[key]
 
     def get_post_op(self, key):
         return self.post_ops[key]
 
-    def register_pre_forward_operation(self, op):
-        key = str(len(self.pre_ops))
-        self.pre_ops[key] = op
-        return key
+    def register_pre_forward_operation(self, op) -> HookHandle:
+        return add_op_to_registry(self.pre_ops, op)
 
     def remove_pre_forward_operation(self, key):
         return self.pre_ops.pop(key)
 
-    def register_post_forward_operation(self, op):
-        key = str(len(self.post_ops))
-        self.post_ops[key] = op
-        return key
+    def register_post_forward_operation(self, op) -> HookHandle:
+        return add_op_to_registry(self.post_ops, op)
 
     def remove_post_forward_operation(self, key):
         return self.post_ops.pop(key)
 
     def reset(self):
         self.pre_ops.clear()
         self.post_ops.clear()
```

### Comparing `nncf-2.8.1/nncf/torch/layers.py` & `nncf-2.9.0/nncf/torch/layers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -15,14 +15,15 @@
 import torch
 import torch.nn.functional as F
 from torch import nn
 from torch.nn import init
 from torch.nn.utils.rnn import PackedSequence
 from torch.nn.utils.weight_norm import WeightNorm
 
+import nncf
 from nncf import nncf_logger
 from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
 from nncf.torch.checkpoint_loading import OPTIONAL_PARAMETERS_REGISTRY
 from nncf.torch.dynamic_graph.context import forward_nncf_trace
 from nncf.torch.layer_utils import _NNCFModuleMixin
@@ -504,28 +505,28 @@
             s += ", bias={bias}"
         if "nonlinearity" in self.__dict__ and self.nonlinearity != "tanh":
             s += ", nonlinearity={nonlinearity}"
         return s.format(**self.__dict__)
 
     def check_forward_input(self, input_):
         if input_.size(1) != self.input_size:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "input_ has inconsistent input_size: got {}, expected {}".format(input_.size(1), self.input_size)
             )
 
     def check_forward_hidden(self, input_: torch.Tensor, hx: torch.Tensor, hidden_label: str = ""):
         if input_.size(0) != hx.size(0):
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "Input batch size {} doesn't match hidden{} batch size {}".format(
                     input_.size(0), hidden_label, hx.size(0)
                 )
             )
 
         if hx.size(1) != self.hidden_size:
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "hidden{} has inconsistent hidden_size: got {}, expected {}".format(
                     hidden_label, hx.size(1), self.hidden_size
                 )
             )
 
     def reset_parameters(self):
         stdv = 1.0 / math.sqrt(self.hidden_size)
@@ -851,17 +852,19 @@
             inners.extend(layer_inners)
         return StackedRNN(inners, self.num_layers, (self.mode == "LSTM"), dropout=self.dropout)
 
     def check_forward_args(self, input_, hidden, batch_sizes):
         is_input_packed = batch_sizes is not None
         expected_input_dim = 2 if is_input_packed else 3
         if input_.dim() != expected_input_dim:
-            raise RuntimeError("input_ must have {} dimensions, got {}".format(expected_input_dim, input_.dim()))
+            raise nncf.ValidationError(
+                "input_ must have {} dimensions, got {}".format(expected_input_dim, input_.dim())
+            )
         if self.input_size != input_.size(-1):
-            raise RuntimeError(
+            raise nncf.ValidationError(
                 "input_.size(-1) must be equal to input_size. Expected {}, got {}".format(
                     self.input_size, input_.size(-1)
                 )
             )
 
         if is_input_packed:
             mini_batch = int(batch_sizes[0])
@@ -869,18 +872,18 @@
             mini_batch = input_.size(0) if self.batch_first else input_.size(1)
 
         expected_hidden_size = (mini_batch, self.hidden_size)
 
         def check_hidden_size(hx, expected_hidden_size, msg="Expected hidden size {}, got {}"):
             expected_size = self.num_layers * self.num_directions
             if expected_size != len(hx):
-                raise RuntimeError("Expected number of hidden states {}, got {}".format(expected_size, len(hx)))
+                raise nncf.InternalError("Expected number of hidden states {}, got {}".format(expected_size, len(hx)))
             for element in hx:
                 if tuple(element.size()) != expected_hidden_size:
-                    raise RuntimeError(msg.format(expected_hidden_size, tuple(element.size())))
+                    raise nncf.InternalError(msg.format(expected_hidden_size, tuple(element.size())))
 
         if self.mode == "LSTM":
             check_hidden_size(hidden[0], expected_hidden_size, "Expected hidden[0] size {}, got {}")
             check_hidden_size(hidden[1], expected_hidden_size, "Expected hidden[1] size {}, got {}")
         else:
             check_hidden_size(hidden, expected_hidden_size)
```

### Comparing `nncf-2.8.1/nncf/torch/model_analyzer.py` & `nncf-2.9.0/nncf/torch/model_analyzer.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/model_creation.py` & `nncf-2.9.0/nncf/torch/model_creation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,17 +12,17 @@
 from os import path as osp
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from torch.distributed import barrier
 from torch.nn import Module
 
+import nncf
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
-from nncf.common.deprecation import warning_deprecated
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.debug import set_debug_log_dir
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algorithm_names
 from nncf.config.extractors import has_input_info_field
 from nncf.config.telemetry_extractors import CompressionStartedFromConfig
@@ -95,29 +95,26 @@
     :param dump_graphs: Whether to dump the internal graph representation of the
         original and compressed models in the .dot format into the log directory.
     :return: A controller for the compression algorithm (or algorithms, in which case the controller
         is an instance of CompositeCompressionController) and the model ready for compression parameter training wrapped
         as an object of NNCFNetwork.
     """
     if isinstance(model, NNCFNetwork):
-        raise RuntimeError(
+        raise nncf.InternalError(
             "The model object has already been compressed.\n"
             "NNCF for PyTorch modifies the model object in-place, and repeat calls to "
             "`nncf.torch.create_compressed_model` with the same model object passed as argument "
             "will lead to an incorrect attempt to compress the model twice.\n"
             "Make sure that the model object you are passing has not already been compressed (for "
             "instance, by testing `if isinstance(model, nncf.torch.nncf_network.NNCFNetwork)`).\n"
             "If you are encountering this in a Jupyter notebook context - make sure that when "
             "re-running cells involving `nncf.torch.create_compressed_model` the original model object "
             "is also re-created (via constructor call)."
         )
 
-    if config.get("target_device") == "VPU":
-        warning_deprecated("VPU device is deprecated and will no longer be supported in the future.")
-
     set_debug_log_dir(config.get("log_dir", "."))
 
     is_legacy_model_state_dict = (
         compression_state is not None
         and BaseController.BUILDER_STATE not in compression_state
         and BaseController.CONTROLLER_STATE not in compression_state
     )
@@ -167,15 +164,15 @@
 
     nncf_logger.debug(
         "Config has no 'input_info' section, trying to use dataloader output as model inputs " "for graph building."
     )
     exact_info = LoaderInputInfo.from_nncf_config_dataloaders(config)
     if exact_info is not None:
         return exact_info
-    raise RuntimeError(
+    raise nncf.ValidationError(
         "Could not determine tensor inputs for the model's forward call.\n"
         "If you are using the `nncf.quantize` API, make sure that you supply the "
         "calibration dataloader to the `nncf.quantize` call.\n"
         "If you are using the `create_compressed_model` API, either specify the "
         "inputs by using the 'input_info' section in the NNCFConfig, or attach an "
         "initialization dataloader to the NNCFConfig by calling "
         "`NNCFConfig.register_extra_structs(...)` with one of the following extra "
```

### Comparing `nncf-2.8.1/nncf/torch/model_transformer.py` & `nncf-2.9.0/nncf/torch/model_transformer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,28 +18,28 @@
 from torch.nn.parameter import Parameter
 
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.torch.external_hook import EXTERNAL_OP_STORAGE_NAME
-from nncf.torch.external_hook import ExternalOpCallHook
 from nncf.torch.graph.transformations.commands import PTBiasCorrectionCommand
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTModelExtractionWithFusedBiasCommand
 from nncf.torch.graph.transformations.commands import PTQuantizerInsertionCommand
 from nncf.torch.graph.transformations.commands import PTSharedFnInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import PTWeightUpdateCommand
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.model_analyzer import get_potential_fused_node
 from nncf.torch.module_operations import UpdateWeight
 from nncf.torch.nncf_network import ExtraCompressionModuleType
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.nncf_network import PTInsertionPoint
+from nncf.torch.quantization.external_quantizer import ExternalOpCallHook
 from nncf.torch.quantization.external_quantizer import ExternalQuantizerCallHook
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import is_multidevice
 
 
 class PTModelTransformer(ModelTransformer):
     """
@@ -80,38 +80,38 @@
     @staticmethod
     def _apply_insertion_transformations(model: NNCFNetwork, transformations: List[PTInsertionCommand]) -> NNCFNetwork:
         """
         Applies insertion transformations to the model.
 
         :param model: Model to apply transformations.
         :param transformations: List of the bias correction transformations.
+        :return: A modified NNCFNetwork.
         """
         node_to_op_address_mapping = model.nncf.get_node_to_op_address_mapping()
-        fns_grouped_by_points: Dict[PTInsertionPoint, List[Tuple[Callable, TransformationPriority]]] = {}
+        fns_grouped_by_points: Dict[PTInsertionPoint, List[Tuple[Callable, TransformationPriority]]] = defaultdict(list)
 
         for transformation_command in transformations:
             target_point: PTTargetPoint = transformation_command.target_point
             target_node_name = target_point.target_node_name
             pt_ip = PTInsertionPoint(
                 target_type=target_point.target_type,
                 op_address=node_to_op_address_mapping[target_node_name],
                 input_port_id=target_point.input_port_id,
             )
             fn = transformation_command.fn
             if target_point.type is TargetType.OPERATION_WITH_WEIGHTS:
                 fn = UpdateWeight(fn)
-            tup = (fn, transformation_command.priority)
-            if pt_ip not in fns_grouped_by_points:
-                fns_grouped_by_points[pt_ip] = [tup]
-            else:
-                fns_grouped_by_points[pt_ip].append(tup)
+            tup = (fn, transformation_command)
+            fns_grouped_by_points[pt_ip].append(tup)
 
         for pt_ip, fn_list_with_priority in fns_grouped_by_points.items():
-            fn_list_with_priority = sorted(fn_list_with_priority, key=lambda x: x[1])
-            model.nncf.insert_at_point(pt_ip, [x[0] for x in fn_list_with_priority])
+            fn_list_with_priority = sorted(fn_list_with_priority, key=lambda x: x[1].priority)
+            for fn, command in fn_list_with_priority:
+                model.nncf.insert_at_point(pt_ip, fn, command.hooks_group_name)
+
         return model
 
     @staticmethod
     def _apply_shared_nodes_insertion(
         model: NNCFNetwork, transformations: List[PTSharedFnInsertionCommand]
     ) -> NNCFNetwork:
         compression_model_type = ExtraCompressionModuleType.EXTERNAL_OP
@@ -124,15 +124,22 @@
         for shared_command in transformations:
             model.nncf.add_compression_module(shared_command.op_name, shared_command.fn, compression_model_type)
 
             for target_point in shared_command.target_points:
                 fn = ExternalOpCallHook(
                     EXTERNAL_OP_STORAGE_NAME, model.nncf.get_tracing_context(), shared_command.op_name
                 )
-                insertion_commands.append(PTInsertionCommand(target_point, fn, priority=shared_command.priority))
+                insertion_commands.append(
+                    PTInsertionCommand(
+                        target_point,
+                        fn,
+                        priority=shared_command.priority,
+                        hooks_group_name=shared_command.hooks_group_name,
+                    )
+                )
 
         return PTModelTransformer._apply_insertion_transformations(model, insertion_commands)
 
     @staticmethod
     def _apply_quantizer_insertion_transformations(
         model: NNCFNetwork, transformations: List[PTQuantizerInsertionCommand]
     ) -> NNCFNetwork:
@@ -252,15 +259,14 @@
 
 def extraction_potential_fused_modules(node_name: str, model: NNCFNetwork) -> nn.Sequential:
     """
     Return Sequential from the copy of module by node_name and potential fused node if exists.
 
     :param node_name: The node name.
     :param model: The model.
-
     :return nn.Sequential: Copy of the modules.
     """
     extracted_node_names = [node_name]
     nncf_graph = model.nncf.get_graph()
     fused_node = get_potential_fused_node(node_name, nncf_graph)
     if fused_node:
         extracted_node_names.append(fused_node.node_name)
```

### Comparing `nncf-2.8.1/nncf/torch/module_operations.py` & `nncf-2.9.0/nncf/torch/module_operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/nested_objects_traversal.py` & `nncf-2.9.0/nncf/torch/nested_objects_traversal.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/nncf_module_replacement.py` & `nncf-2.9.0/nncf/torch/nncf_module_replacement.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from copy import deepcopy
 from typing import Callable, Dict, List, Optional, Set, Tuple, Type
 
 import torch
 from torch import nn
 
+import nncf
 from nncf.common.logging import nncf_logger
 from nncf.common.scopes import matches_any
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.dynamic_graph.scope import ScopeElement
 from nncf.torch.layers import NNCF_MODULES
 from nncf.torch.layers import NNCF_MODULES_DICT
 from nncf.torch.layers import NNCF_MODULES_MAP
@@ -127,15 +128,15 @@
         if module.__class__ == original_module_class:
             return nncf_module_class.from_module(module)
     for user_module_class in UNWRAPPED_USER_MODULES.registry_dict.values():
         if module.__class__ == user_module_class:
             nncf_module = deepcopy(module)
             nncf_module = add_nncf_functionality_to_user_module(nncf_module)
             return nncf_module
-    raise RuntimeError(f"Could not extend module {module} with NNCF functionality!")
+    raise nncf.InternalError(f"Could not extend module {module} with NNCF functionality!")
 
 
 def replace_modules_by_nncf_modules(
     model: nn.Module,
     ignored_scopes: Optional[List[str]] = None,
     target_scopes: Optional[List[str]] = None,
     eval_op_scopes: Optional[List[Scope]] = None,
@@ -248,15 +249,15 @@
     :param reset: Whether the module should be reset.
     """
     curr_module = base_model
     owning_module = base_model
     for scope_element in scope[1:]:  # omit first scope element which corresponds to base module
         child_module = curr_module._modules.get(scope_element.calling_field_name)
         if child_module is None:
-            raise RuntimeError(
+            raise nncf.InternalError(
                 "Could not find a {} module member in {} module of scope {} during module replacement".format(
                     scope_element.calling_field_name, scope_element.calling_module_class_name, str(scope)
                 )
             )
         owning_module = curr_module
         curr_module = child_module
```

### Comparing `nncf-2.8.1/nncf/torch/nncf_network.py` & `nncf-2.9.0/nncf/torch/nncf_network.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,42 +1,45 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import functools
 import inspect
 import types
 from collections import OrderedDict
+from collections import defaultdict
 from contextlib import contextmanager
 from copy import deepcopy
 from dataclasses import dataclass
 from enum import Enum
 from enum import IntEnum
 from typing import Callable, Dict, Iterator, List, Optional, Tuple, TypeVar
 
 import torch
 from torch import nn
 
+import nncf
 from nncf import nncf_logger
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.operator_metatypes import CONST_NOOP_METATYPES
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
+from nncf.common.hook_handle import HookHandle
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.insertion_point_graph import PostHookInsertionPoint
 from nncf.common.insertion_point_graph import PreHookInsertionPoint
 from nncf.common.utils.debug import is_debug
 from nncf.torch.debug import CombinedDebugInterface
 from nncf.torch.debug import debuggable_forward
 from nncf.torch.dynamic_graph.context import TracingContext
@@ -59,14 +62,15 @@
 from nncf.torch.dynamic_graph.wrappers import wrap_parameters
 from nncf.torch.external_hook import EXTERNAL_OP_STORAGE_NAME
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.graph.graph_builder import GraphBuilder
 from nncf.torch.graph.graph_builder import GraphConverter
 from nncf.torch.graph.operator_metatypes import OPERATORS_WITH_WEIGHTS_METATYPES
 from nncf.torch.graph.operator_metatypes import PTSplitMetatype
+from nncf.torch.graph.transformations.commands import DEFAULT_HOOKS_GROUP_NAME
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.knowledge_distillation.knowledge_distillation_handler import KnowledgeDistillationLossHandler
 from nncf.torch.layer_utils import _NNCFModuleMixin
 from nncf.torch.nncf_module_replacement import replace_modules_by_nncf_modules
 from nncf.torch.quantization.external_quantizer import EXTERNAL_QUANTIZERS_STORAGE_NAME
 from nncf.torch.utils import compute_FLOPs_hook
 from nncf.torch.utils import get_all_modules_by_type
@@ -93,15 +97,15 @@
     }
 
     def _get_pt_insertion_type(self, target_type: TargetType) -> PTInsertionType:
         if (
             not isinstance(target_type, TargetType)
             or target_type not in PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT
         ):
-            raise RuntimeError("Unsupported target type for PyTorch: {}".format(target_type))
+            raise nncf.InternalError("Unsupported target type for PyTorch: {}".format(target_type))
         return PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT[target_type]
 
     def __init__(self, target_type: TargetType, op_address: OperationAddress, input_port_id: int = None):
         self.insertion_type = self._get_pt_insertion_type(target_type)
         self.op_address = op_address
         self.module_scope = op_address.scope_in_model
         self.input_port_id = input_port_id
@@ -240,14 +244,15 @@
         self._forward_signature = inspect.signature(self.get_original_forward())
         self._input_info = input_info
 
         self._ignored_scopes = ignored_scopes
         self._target_scopes = target_scopes
         self._user_dummy_forward_fn = dummy_forward_fn
         self._kd_loss_handler = None
+        self._groups_vs_hooks_handlers: Dict[str, List[HookHandle]] = defaultdict(list)
 
         if wrap_inputs_fn is not None:
             self._wrap_inputs_fn = wrap_inputs_fn
         elif self._input_info is not None:
             self.__input_info_based_input_wrapper = InputInfoWrapManager(
                 self._input_info, self._forward_signature, module_ref_for_device=model
             )
@@ -396,44 +401,70 @@
         retval = {}
         for nncf_module, nncf_module_scope in nncf_modules.items():
             nncf_module_scope.pop()
             for relative_scope, target_module in get_all_modules_by_type(nncf_module, class_names).items():
                 retval[nncf_module_scope + relative_scope] = target_module
         return retval
 
-    def insert_at_point(self, point: PTInsertionPoint, fn_list: List[Callable]):
+    def insert_at_point(
+        self,
+        point: PTInsertionPoint,
+        fn: Callable,
+        hooks_group_name: Optional[str] = DEFAULT_HOOKS_GROUP_NAME,
+    ) -> List[HookHandle]:
+        """
+        Inserts given function to the point in the NNCFNetwork, creates hook handle for the inserted function and
+        stores created hook handle in a group with the given name. A group name could be used late
+        to remove all hooks from the NNCFNewtork which belongs to the group.
+
+        :param point: Target point to insert function.
+        :param fn: Function to insert to the NNCFNetwork.
+        :param hooks_group_name: Name of hooks group for hook handle associated with the inserted function.
+        :return: Hook handle associated with the inserted function.
+        """
+        handle = None
         if point.insertion_type == PTInsertionType.OPERATOR_PRE_HOOK:
-            self._compressed_context.register_pre_hooks(fn_list, point.op_address, point.input_port_id)
+            handle = self._compressed_context.register_pre_hook(fn, point.op_address, point.input_port_id)
         elif point.insertion_type == PTInsertionType.OPERATOR_POST_HOOK:
-            self._compressed_context.register_post_hooks(fn_list, point.op_address)
+            handle = self._compressed_context.register_post_hook(fn, point.op_address)
         elif point.insertion_type in [PTInsertionType.NNCF_MODULE_PRE_OP, PTInsertionType.NNCF_MODULE_POST_OP]:
             nncf_module = self.get_module_by_scope(point.module_scope)
             if not isinstance(nncf_module, _NNCFModuleMixin):
-                raise RuntimeError(
+                raise nncf.ValidationError(
                     f"Failed to insert pre/post op for not registered custom module {point.module_scope}. NNCF only "
                     f"supports native PyTorch modules with respect to trainable parameter (weight) compressed, such "
                     f"as `torch.nn.Conv2d`. If your model contains a custom, non-PyTorch standard module with trainable"
                     f" weights that should be compressed, you can register it using the "
                     f"`@nncf.register_module` decorator. Please refer to `Compression of custom modules` section in "
                     f"docs/Usage.md for more details."
                 )
 
             norm_target_scope = self._normalize_variable_recurrent_scope(point.module_scope)
             norm_nncf_scopes = []
             for scope_list_for_module in self.get_nncf_module_scopes():
                 norm_nncf_scopes.extend([self._normalize_variable_recurrent_scope(x) for x in scope_list_for_module])
             assert norm_target_scope in norm_nncf_scopes  # Required for proper Recurrent/VariableRecurrent addressing
             if point.insertion_type == PTInsertionType.NNCF_MODULE_PRE_OP:
-                for fn in fn_list:
-                    nncf_module.register_pre_forward_operation(fn)
+                handle = nncf_module.register_pre_forward_operation(fn)
             elif point.insertion_type == PTInsertionType.NNCF_MODULE_POST_OP:
-                for fn in fn_list:
-                    nncf_module.register_post_forward_operation(fn)
+                handle = nncf_module.register_post_forward_operation(fn)
         else:
-            raise RuntimeError("Unsupported insertion type: {}".format(point.insertion_type))
+            raise nncf.ValidationError("Unsupported insertion type: {}".format(point.insertion_type))
+        self._groups_vs_hooks_handlers[hooks_group_name].append(handle)
+        return handle
+
+    def remove_hooks_group(self, hooks_group_name: str) -> None:
+        """
+        Removes all hooks of given hooks group from the nncf interface.
+
+        :param group: Target hooks group name to remove all hooks from.
+        """
+        for handle in self._groups_vs_hooks_handlers[hooks_group_name]:
+            handle.remove()
+        del self._groups_vs_hooks_handlers[hooks_group_name]
 
     def get_graph(self) -> PTNNCFGraph:
         if self._compressed_context.graph.get_nodes_count() == 0 or self._compressed_graphs_pair.nncf_graph is None:
             self.rebuild_graph()
         return self._compressed_graphs_pair.nncf_graph
 
     def get_dynamic_graph(self) -> DynamicGraph:
@@ -526,34 +557,34 @@
             if norm_op_scope in nncf_scope:
                 return True
         return False
 
     def register_compression_module_type(self, compression_module_type: ExtraCompressionModuleType):
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type in self._extra_module_types:
-            raise RuntimeError(f"Module type {compression_module_type} is already registered")
+            raise nncf.ValidationError(f"Module type {compression_module_type} is already registered")
 
         self.__setattr__(attr_name, nn.ModuleDict())
         self._extra_module_types.append(compression_module_type)
 
     def add_compression_module(
         self, module_key: str, module: nn.Module, compression_module_type: ExtraCompressionModuleType
     ):
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type not in self._extra_module_types:
-            raise RuntimeError(f"Module type {compression_module_type} was not registered")
+            raise nncf.InternalError(f"Module type {compression_module_type} was not registered")
         storage = self.__getattr__(attr_name)
         if module_key in storage:
-            raise RuntimeError(f"Module {module_key} is already registered under {attr_name}")
+            raise nncf.InternalError(f"Module {module_key} is already registered under {attr_name}")
         storage[module_key] = module
 
     def get_compression_modules_by_type(self, compression_module_type: ExtraCompressionModuleType) -> nn.ModuleDict:
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type not in self._extra_module_types:
-            raise RuntimeError(f"Module type {compression_module_type} was not registered")
+            raise nncf.InternalError(f"Module type {compression_module_type} was not registered")
         return self.__getattr__(attr_name)
 
     def is_compression_module_registered(self, compression_module_type: ExtraCompressionModuleType) -> bool:
         """
         Check that extra compression module was registered.
 
         :param compression_module_type: Type of the extra compression module.
@@ -567,20 +598,20 @@
         Required for backward compatibility with checkpoints that store function and activation
         quantizers directly under corresponding attributes of NNCFNetwork.
         """
         if compression_module_type == ExtraCompressionModuleType.EXTERNAL_QUANTIZER:
             return EXTERNAL_QUANTIZERS_STORAGE_NAME
         if compression_module_type == ExtraCompressionModuleType.EXTERNAL_OP:
             return EXTERNAL_OP_STORAGE_NAME
-        raise RuntimeError("Unknown extra module type")
+        raise nncf.ValidationError("Unknown extra module type")
 
     def sort_compression_modules(self, compression_module_type: ExtraCompressionModuleType):
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type not in self._extra_module_types:
-            raise RuntimeError("Module type {} was not registered".format(compression_module_type))
+            raise nncf.InternalError("Module type {} was not registered".format(compression_module_type))
         module_dict = self.__getattr__(attr_name)
 
         module_dict._modules = OrderedDict(sorted(module_dict._modules.items()))
         self.__setattr__(attr_name, module_dict)
 
     @staticmethod
     def _normalize_variable_recurrent_scope(scope: Scope):
@@ -606,17 +637,16 @@
         if force_eval:
             train_mode = self._model_ref.training
             self._model_ref.eval()
         with torch.no_grad():
             with self._compressed_context as ctx:
                 ctx.base_module_thread_local_replica = self._model_ref
                 self._dummy_forward_fn(self._model_ref)
-        if force_eval:
-            if train_mode:
-                self._model_ref.train()
+        if force_eval and train_mode:
+            self._model_ref.train()
 
     def get_original_insertion_point_graph(self) -> InsertionPointGraph:
         # Set up a pre- and post-hooks on almost every op in PyTorch
         nncf_graph = self.get_original_graph()
         pre_hooks: List[PreHookInsertionPoint] = []
         post_hooks: List[PostHookInsertionPoint] = []
         for node in nncf_graph.get_all_nodes():
@@ -961,15 +991,15 @@
 
     def __init__(self, *args, **kwargs):
         """
         In normal situations, the __init__ of the NNCFNetwork will never be called. The constructor-like syntax is
         achieved by a __call__ method defined in the metaclass `NNCFNetworkMeta`.
         """
         super().__init__()
-        raise RuntimeError("Direct instantiation of NNCFNetwork objects using __init__ is prohibited.")
+        raise nncf.InternalError("Direct instantiation of NNCFNetwork objects using __init__ is prohibited.")
 
     def __call__(self, *args, **kwargs):
         """
         Ensures that functor-like calls of the processed model object will directly trigger the NNCF-specific
         forward call.
         """
         return ORIGINAL_CALL(self, *args, **kwargs)
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/__init__.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementations of pruning algorithms.
+Backend-specific implementations of filter-wise pruning.
 """
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/base_algo.py` & `nncf-2.9.0/nncf/torch/pruning/base_algo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/export_utils.py` & `nncf-2.9.0/nncf/torch/pruning/export_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/__init__.py` & `nncf-2.9.0/nncf/common/initialization/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementations of filter-wise pruning.
+Functions and classes utilized during the user dataset-driven initialization of the compression algorithms.
 """
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/algo.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -13,14 +13,15 @@
 from math import isclose
 from pathlib import Path
 from typing import Dict, List, Tuple, Union
 
 import numpy as np
 import torch
 
+import nncf
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionStage
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
@@ -348,15 +349,15 @@
             else:
                 left = middle
         flops, params_num = self._calculate_flops_and_weights_in_uniformly_pruned_model(right)
         if flops <= target_flops:
             self.current_flops = flops
             self.current_params_num = params_num
             return right
-        raise RuntimeError(
+        raise nncf.InternalError(
             "Can't prune the model to get the required "
             "pruning level in flops = {}".format(target_flops_pruning_level)
         )
 
     def set_pruning_level(
         self, pruning_level: Union[float, Dict[int, float]], run_batchnorm_adaptation: bool = False
     ) -> None:
@@ -373,26 +374,28 @@
         passed_pruning_level = pruning_level
 
         if not self.frozen:
             nncf_logger.info("Computing filter importance scores and binary masks...")
             with torch.no_grad():
                 if self.all_weights:
                     if groupwise_pruning_levels_set:
-                        raise RuntimeError("Cannot set group-wise pruning levels with all_weights=True")
+                        raise nncf.InternalError("Cannot set group-wise pruning levels with all_weights=True")
                     # Non-uniform (global) importance-score-based pruning according
                     # to the global pruning level
                     if self.prune_flops:
                         self._set_binary_masks_for_pruned_modules_globally_by_flops_target(pruning_level)
                     else:
                         self._set_binary_masks_for_pruned_modules_globally(pruning_level)
                 else:
                     if groupwise_pruning_levels_set:
                         group_ids = [group.id for group in self.pruned_module_groups_info.get_all_clusters()]
                         if set(pruning_level.keys()) != set(group_ids):
-                            raise RuntimeError("Groupwise pruning level dict keys do not correspond to layer group ids")
+                            raise nncf.InternalError(
+                                "Groupwise pruning level dict keys do not correspond to layer group ids"
+                            )
                     else:
                         # Pruning uniformly with the same pruning level across layers
                         if self.prune_flops:
                             # Looking for layerwise pruning level needed for the required flops pruning level
                             pruning_level = self._find_uniform_pruning_level_for_target_flops(pruning_level)
                     self._set_binary_masks_for_pruned_modules_groupwise(pruning_level)
 
@@ -588,15 +591,15 @@
                 output_channels=tmp_out_channels,
             )
             if flops < target_flops:
                 self.current_flops = flops
                 self.current_params_num = params_num
                 return
             cur_num += 1
-        raise RuntimeError("Can't prune model to asked flops pruning level")
+        raise nncf.InternalError("Can't prune model to asked flops pruning level")
 
     def _propagate_masks(self):
         nncf_logger.debug("Propagating pruning masks")
         # 1. Propagate masks for all modules
         graph = self.model.nncf.get_original_graph()
 
         init_output_masks_in_graph(graph, self.pruned_module_groups_info.get_all_nodes())
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/functions.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py` & `nncf-2.9.0/nncf/torch/tensor_statistics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/global_ranking/legr.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/filter_pruning/layers.py` & `nncf-2.9.0/nncf/torch/pruning/filter_pruning/layers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import numpy as np
 import torch
 from torch import nn
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.torch.layer_utils import COMPRESSION_MODULES
 
 
 @COMPRESSION_MODULES.register()
 class FilterPruningMask(nn.Module):
     """
@@ -75,13 +76,13 @@
     :param filter_mask: binary mask (should have the same shape as conv weight on the given dimension)
     :param module_parameter: a tensor representing a module parameter (e.g. weight or bias of convolution)
     :param node_name_for_logging: name of the module to which the mask is applied
     :param dim: a dimension to apply the mask (0 by default)
     :return: result with applied mask
     """
     if filter_mask.size(0) != module_parameter.size(dim):
-        raise RuntimeError(
+        raise nncf.InternalError(
             "Shape of mask = {} for module {} isn't broadcastable to weight shape={}."
             " ".format(filter_mask.shape, node_name_for_logging, module_parameter.shape)
         )
     broadcasted_filter_mask = broadcast_filter_mask(filter_mask, module_parameter.shape, dim)
     return module_parameter.mul(broadcasted_filter_mask)
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/operations.py` & `nncf-2.9.0/nncf/torch/pruning/operations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 from enum import Enum
 from enum import auto
 
 import torch
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import UnknownMetatype
 from nncf.common.logging import nncf_logger
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.operations import BatchNormPruningOp
 from nncf.common.pruning.operations import ConcatPruningOp
@@ -570,15 +571,15 @@
 
         if isinstance(input_mask, PTNNCFTensor):
             input_mask = input_mask.tensor
 
         node_module = model.nncf.get_containing_module(node.node_name)
 
         if prun_type == PrunType.CUT_WEIGHTS:
-            raise RuntimeError("LayerNorm does not support pruning by cutting channels")
+            raise nncf.InternalError("LayerNorm does not support pruning by cutting channels")
 
         node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
         node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.bias))
 
 
 @PT_PRUNING_OPERATOR_METATYPES.register("elementwise")
 class PTElementwisePruningOp(ElementwisePruningOp, PTPruner):
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/structs.py` & `nncf-2.9.0/nncf/torch/pruning/structs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/tensor_processor.py` & `nncf-2.9.0/nncf/torch/pruning/tensor_processor.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/pruning/utils.py` & `nncf-2.9.0/nncf/torch/pruning/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Dict, List, Optional, Tuple
 
 import torch
 
+import nncf
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.logging import nncf_logger
 from nncf.torch.graph.graph import NNCFNode
 from nncf.torch.layers import NNCF_GENERAL_CONV_MODULES_DICT
@@ -83,15 +84,15 @@
             if attrs.transpose:
                 shape[i] = (shape[i] - 1) * attrs.stride[i] - 2 * attrs.padding_values[i] + attrs.kernel_size[i]
             else:
                 shape[i] = (shape[i] + 2 * attrs.padding_values[i] - attrs.kernel_size[i]) // attrs.stride[i] + 1
     elif isinstance(attrs, LinearLayerAttributes):
         shape = shape[:-1] + [attrs.out_features]
     else:
-        raise RuntimeError(f"Unexpected node type {node.node_type} is fed to _calculate_output_shape")
+        raise nncf.ValidationError(f"Unexpected node type {node.node_type} is fed to _calculate_output_shape")
     return tuple(shape)
 
 
 def collect_output_shapes(graph: NNCFGraph) -> Dict[NNCFNodeName, List[int]]:
     """
     Collects output dimension shapes for convolutions and fully connected layers
     from the connected edges in the NNCFGraph.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/__init__.py` & `nncf-2.9.0/nncf/version.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Backend-specific implementations of quantization algorithms.
-"""
 
-# Required for correct QUANTIZATION_MODULES registry functioning
-from . import layers as layers
+__version__ = "2.9.0"
+
+BKC_TORCH_VERSION = "2.1.2"
+BKC_TORCHVISION_VERSION = "0.16.2"
+BKC_TF_VERSION = "2.12.*"
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/adjust_padding.py` & `nncf-2.9.0/nncf/torch/quantization/adjust_padding.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 from collections import namedtuple
 from typing import NamedTuple
 
 import networkx as nx
 import torch
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
 from nncf.torch.layers import NNCFConv2d
 from nncf.torch.module_operations import UpdatePaddingValue
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.layers import BaseQuantizer
 from nncf.torch.quantization.layers import QuantizerConfig
@@ -28,26 +29,26 @@
     weight_bitwidth: int
     activation_quantizer: BaseQuantizer
     module_op_node_name: NNCFNodeName
 
 
 class CalculatePaddingAdjustment:
     """
-    Calculates padding value to perform a workaround for U4 support on VPU.
-    VPU supports only i4 for weights and activations with zero-point=0 and padding=0. This imposes some limitations on
+    Calculates padding value to perform a workaround for U4 support on NPU.
+    NPU supports only i4 for weights and activations with zero-point=0 and padding=0. This imposes some limitations on
     the quantization scheme we can apply. In case of unsigned input for a quantizer (e.g. output of ReLU) half of
     i4 range (8 values) is insufficient to preserve the accuracy. To overcome the problem it is proposed
-    to transform u4 to i4 in the VPU plugin by shifting the input by half of the quantization range to left. Padding
+    to transform u4 to i4 in the NPU plugin by shifting the input by half of the quantization range to left. Padding
     value should be shifted as well. And to make it zero after the shift (non-zero padding values are not
     supported), the model should be trained with padding value equal to the half of the quantization range.
     """
 
     def __init__(self, activation_quantizer: SymmetricQuantizer):
         if not isinstance(activation_quantizer, SymmetricQuantizer):
-            raise RuntimeError("Padding adjustment is not supported for not symmetric quantization")
+            raise nncf.InternalError("Padding adjustment is not supported for not symmetric quantization")
         self._activation_quantizer = activation_quantizer
         self._is_enabled = True
 
     def __call__(self, previous_padding_value) -> torch.Tensor:
         if self._is_enabled:
             scale = self._activation_quantizer.scale
             eps = self._activation_quantizer.eps
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/algo.py` & `nncf-2.9.0/nncf/torch/quantization/algo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,14 +18,15 @@
 from copy import deepcopy
 from enum import IntEnum
 from string import Template
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 import torch
 
+import nncf
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.deprecation import warning_deprecated
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
@@ -458,18 +459,18 @@
         self._target_device = self.config.get("target_device", "ANY")
         hw_config_type = get_hw_config_type(self._target_device)
         if hw_config_type is not None:
             hw_config_path = PTHWConfig.get_path_to_hw_config(hw_config_type)
             self.hw_config = PTHWConfig.from_json(hw_config_path)
 
         algo_config = self._get_algo_specific_config_section()
-        if self._target_device == "VPU" and "preset" in algo_config:
-            raise RuntimeError("The VPU target device does not support presets.")
+        if self._target_device == "NPU" and "preset" in algo_config:
+            raise nncf.InternalError("The NPU target device does not support presets.")
         if self._target_device == "CPU_SPR":
-            raise RuntimeError("The CPU_SPR target device does not supported.")
+            raise nncf.InternalError("The CPU_SPR target device does not supported.")
 
         self._range_init_params = None
         self._precision_init_type = None
         self._precision_init_params = None
         if self.should_init:
             self._parse_init_params()
 
@@ -534,31 +535,31 @@
 
     def _parse_precision_init_params(self, initializer_config: Dict) -> Tuple[str, BasePrecisionInitParams]:
         init_precision_config = initializer_config.get("precision", None)
         if not init_precision_config:
             return None, None
         precision_init_type = init_precision_config.get("type", "manual")
         if precision_init_type not in PRECISION_INIT_TYPES_VS_DESCRIPTION:
-            raise RuntimeError(f"Unrecognized precision init type: {precision_init_type}")
+            raise nncf.InternalError(f"Unrecognized precision init type: {precision_init_type}")
         if precision_init_type == "hawq":
             try:
                 precision_init_args = self.config.get_extra_struct(QuantizationPrecisionInitArgs)
             except KeyError as e:
                 raise ValueError(
                     "Specified non-manual precision initialization in the NNCF config, "
                     "but the initializing data loader and loss criterion are not provided as an extra struct. "
                     "Refer to `NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs` "
                     "class"
                 ) from e
             precision_init_params = HAWQPrecisionInitParams.from_config(init_precision_config, precision_init_args)
         elif precision_init_type == "autoq":
-            if self.hw_config is not None and self.hw_config.target_device != HWConfigType.VPU.value:
+            if self.hw_config is not None and self.hw_config.target_device != HWConfigType.NPU.value:
                 raise ValueError(
                     "Unsupported device ({}). Automatic Precision Initialization only supports for "
-                    "target_device NONE or VPU".format(self.hw_config.target_device)
+                    "target_device NONE or NPU".format(self.hw_config.target_device)
                 )
             try:
                 precision_init_args = self.config.get_extra_struct(AutoQPrecisionInitArgs)
             except KeyError as e:
                 raise ValueError(
                     "Specified Automated precision initialization in the NNCF config, "
                     "but the initializing data loader and loss criterion are not provided as an extra "
@@ -748,15 +749,15 @@
                         half_range = True
                         quantizers_with_overflow_fix_str = "all weight quantizers"
                     elif self._overflow_fix == "first_layer_only":
                         if target_node in get_first_nodes_of_type(target_model_graph, ["conv2d", "conv3d"]):
                             half_range = True
                             quantizers_with_overflow_fix_str = "first convolution weight quantizers"
                     elif self._overflow_fix != "disable":
-                        raise RuntimeError(f"Unknown overflow fix type: {self._overflow_fix}")
+                        raise nncf.InternalError(f"Unknown overflow fix type: {self._overflow_fix}")
                     if half_range:
                         nncf_logger.debug(f"Overflow issue fix will be applied to {quantizers_with_overflow_fix_str}")
 
             if qp.is_weight_quantization_point():
                 use_logarithm_scale = self._use_logarithm_scale_per_group[QuantizerGroup.WEIGHTS]
                 narrow_range = qconfig.num_bits == 8 and not half_range
             else:
@@ -858,15 +859,15 @@
 
     @staticmethod
     def _check_and_log_missing_stats_for_setup(
         quantizer_setup: SingleConfigQuantizerSetup,
         minmax_values_for_range_init: Dict[QuantizationPointId, MinMaxTensorStatistic],
     ):
         tps_with_uncollected_stats = set()
-        for qp_id in quantizer_setup.quantization_points.keys():
+        for qp_id in quantizer_setup.quantization_points:
             if qp_id not in minmax_values_for_range_init:
                 tps_with_uncollected_stats.add(quantizer_setup.quantization_points[qp_id].insertion_point)
         if tps_with_uncollected_stats:
             nncf_logger.error("Tensor statistics for the following locations were not collected:")
             for tp in tps_with_uncollected_stats:
                 nncf_logger.error(f"\t{tp}")
             nncf_logger.error(
@@ -901,15 +902,15 @@
 
             quant_module_id, commands = self._build_commands_for_single_unified_scale_group(
                 target_model, quantizer_setup, filtered_unified_scales_group, minmax_values_for_range_init
             )
 
             for layer_name in shared_weight_quantized_layers_in_group:
                 if layer_name in already_weight_quantized_shared_layers:
-                    raise RuntimeError(
+                    raise nncf.InternalError(
                         "Attempted to assign a unified-scale quantizer to a shared layer node that has "
                         "already had its weights quantized by another unified-scale quantizer!"
                     )
                 already_weight_quantized_shared_layers[layer_name] = quant_module_id
 
             for us_qp_id in unified_scales_group:
                 qp_id_vs_quant_module_id_dict[us_qp_id] = quant_module_id
@@ -1070,15 +1071,15 @@
 
         primary_qp_id = sorted_qp_ids[0]
         linked_qp_ids = sorted_qp_ids[1:]
         qspec = quantizer_setup.quantization_points[primary_qp_id].qspec
         linked_qspecs = [quantizer_setup.quantization_points[qp_id].qspec for qp_id in linked_qp_ids]
         for linked_qspec in linked_qspecs:
             if not qspec == linked_qspec:
-                raise RuntimeError("The qspecs for unified scale quantization points should be identical!")
+                raise nncf.InternalError("The qspecs for unified scale quantization points should be identical!")
 
         range_init_minmax_values = None
         if minmax_values_for_range_init:
             # Hopefully this will suffice.
             # TODO: gather unified statistic by linking stat collectors_and_modules_to_init instead
             min_values = None
             max_values = None
@@ -1132,15 +1133,15 @@
         :return: A tuple with the identifier of the new quantizer module and a list of
         insertion commands registering this module for quantization at spots described by
         insertion_points.
         """
 
         target_model_graph = target_model.nncf.get_original_graph()
         if not insertion_points:
-            raise RuntimeError("No insertion points to put quantizers into!")
+            raise nncf.InternalError("No insertion points to put quantizers into!")
 
         def is_weights(ip: PTTargetPoint) -> bool:
             return ip.target_type is TargetType.OPERATION_WITH_WEIGHTS
 
         primary_ip = insertion_points[0]
 
         quantizer = self.__create_quantize_module(qspec).to(self._device_for_callable_obj_creation)
@@ -1190,15 +1191,15 @@
             target_model.nncf.add_compression_module(
                 external_quantizer_storage_key, quantizer, ExtraCompressionModuleType.EXTERNAL_QUANTIZER
             )
 
         insertion_commands = []
         for curr_insertion_point in insertion_points:
             if curr_insertion_point in self._processed_insertion_points:
-                raise RuntimeError("Insertion point {} already quantized!".format(str(curr_insertion_point)))
+                raise nncf.InternalError("Insertion point {} already quantized!".format(str(curr_insertion_point)))
             self._processed_insertion_points.add(curr_insertion_point)
 
             if is_weights(curr_insertion_point):
                 if len(insertion_points) == 1:
                     # For backward compatibility, if only one weight is quantized by a single quantizer,
                     # insert UpdateWeight ops with a genuine quantizer module
                     callable_obj = quantizer
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/base_ctrl.py` & `nncf-2.9.0/nncf/torch/quantization/base_ctrl.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/debug_interface.py` & `nncf-2.9.0/nncf/torch/quantization/debug_interface.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -14,14 +14,15 @@
 from pathlib import Path
 from typing import TYPE_CHECKING, Dict, List
 
 import networkx as nx
 import numpy as np
 import torch
 
+import nncf
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.insertion_point_graph import InsertionPointGraphNodeType
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.dot_file_rw import write_dot_graph
 from nncf.common.utils.os import safe_open
 from nncf.torch.debug import CallCountTracker
 from nncf.torch.debug import DebugInterface
@@ -58,15 +59,15 @@
         self._strict_forward = False
 
     def init_actual(self, owner_model: NNCFNetwork):
         from nncf.torch.nncf_network import ExtraCompressionModuleType
 
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
         quantizers_in_nncf_modules = owner_model.nncf.get_modules_in_nncf_modules_by_type(quantization_types)
-        nncf_module_quantizations_id_list: List[str] = [str(scope) for scope in quantizers_in_nncf_modules.keys()]
+        nncf_module_quantizations_id_list: List[str] = [str(scope) for scope in quantizers_in_nncf_modules]
 
         activation_quantizer_id_list: List[str] = owner_model.nncf.get_compression_modules_by_type(
             ExtraCompressionModuleType.EXTERNAL_QUANTIZER
         ).keys()
         self.call_trackers[self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME].init_with_key_list(
             nncf_module_quantizations_id_list
         )
@@ -99,15 +100,17 @@
         total_calls = sum(call_dict.values())
         nncf_logger.debug(f"{total_calls} nodes called out of total {ctx.graph.get_nodes_count()}")
         if self._strict_forward:
             for tracker in self.call_trackers.values():
                 if tracker.get_never_called_keys():
                     # This will always trigger for DataParallel - disregard or disable debug mode
                     # for DataParallel runs
-                    raise RuntimeError(f"{tracker.name} has never called modules: {tracker.get_never_called_keys()}!")
+                    raise nncf.InternalError(
+                        f"{tracker.name} has never called modules: {tracker.get_never_called_keys()}!"
+                    )
 
     def dump_scale(self, quantizer_scale_params: Dict[str, torch.Tensor], quantizer_name: str):
         import re
 
         quantizer_normalized_name = re.sub(r"[^\w\-_\. ]", "_", quantizer_name)
         for scale_param_name, scale_param in quantizer_scale_params.items():
             fname = "{}_{}.txt".format(quantizer_normalized_name, scale_param_name)
@@ -159,15 +162,15 @@
             ]:
                 target_point_data = node[InsertionPointGraph.INSERTION_POINT_NODE_ATTR]
                 label = "TP: {}".format(str(target_point_data))
                 out_graph.add_node(node_key, label=label, color="red")
             elif node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:
                 out_graph.add_node(node_key)
             else:
-                raise RuntimeError("Invalid InsertionPointGraph node!")
+                raise nncf.InternalError("Invalid InsertionPointGraph node!")
         for u, v in insertion_point_graph.edges:
             out_graph.add_edge(u, v)
 
         for node_key, node in insertion_point_graph.nodes.items():
             if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:
                 for ip_node_key in node[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR]:
                     out_graph.add_edge(node_key, ip_node_key, style="dashed", headport="e", tailport="e")
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/default_quantization.py` & `nncf-2.9.0/nncf/torch/quantization/default_quantization.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/extensions.py` & `nncf-2.9.0/nncf/torch/quantization/extensions.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 import os.path
 import subprocess
 
 import torch
 
+import nncf
 from nncf import nncf_logger
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
 from nncf.torch.extensions import EXTENSIONS
 from nncf.torch.extensions import CudaNotAvailableStub
 from nncf.torch.extensions import ExtensionLoader
 from nncf.torch.extensions import ExtensionLoaderTimeoutException
 from nncf.torch.extensions import ExtensionNamespace
@@ -90,15 +91,15 @@
                 build_directory=cls.get_build_dir(),
                 verbose=False,
             )
         except ExtensionLoaderTimeoutException as e:
             raise e
         except (subprocess.CalledProcessError, OSError, RuntimeError) as e:
             assert torch.cuda.is_available()
-            raise RuntimeError(
+            raise nncf.InstallationError(
                 "CUDA is available for PyTorch, but NNCF could not compile "
                 "GPU quantization extensions. Make sure that you have installed CUDA development "
                 "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
                 "guidance) and that 'nvcc' is available on your system's PATH variable.\n"
             ) from e
 
     @classmethod
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/external_quantizer.py` & `nncf-2.9.0/nncf/torch/quantization/external_quantizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/hessian_trace.py` & `nncf-2.9.0/nncf/torch/quantization/hessian_trace.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/ignored_patterns.py` & `nncf-2.9.0/nncf/torch/quantization/ignored_patterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/init_precision.py` & `nncf-2.9.0/nncf/torch/quantization/init_precision.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/init_range.py` & `nncf-2.9.0/nncf/torch/quantization/init_range.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,22 +12,23 @@
 from collections import OrderedDict
 from copy import deepcopy
 from typing import Callable, Dict, List, Tuple
 
 import numpy as np
 import torch
 
+import nncf
 from nncf.common.graph.layer_attributes import WeightedLayerAttributes
 from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.initialization.range import RangeInitConfig
 from nncf.common.quantization.initialization.range import RangeInitParams
 from nncf.common.quantization.quantizer_setup import QuantizationPointBase
 from nncf.common.quantization.quantizer_setup import QuantizerSetupBase
 from nncf.common.quantization.structs import NonWeightQuantizerId
-from nncf.common.quantization.structs import QuantizationScheme as QuantizationMode
+from nncf.common.quantization.structs import QuantizationScheme
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.quantization.structs import QuantizerId
 from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.common.scopes import should_consider_scope
 from nncf.common.tensor_statistics.collectors import ReductionAxes
 from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.config.schemata.algo.quantization import RANGE_INIT_TYPES_VS_DESCRIPTIONS
@@ -67,21 +68,21 @@
             qid = NonWeightQuantizerId(qp.insertion_point.target_node_name, qp.insertion_point.input_port_id)
             group = QuantizerGroup.ACTIVATIONS
         return self.get_init_config_for_scope_and_group(qid, group)
 
     def get_init_config_for_scope_and_group(self, qid: QuantizerId, group: QuantizerGroup) -> RangeInitConfig:
         matches: List[RangeInitConfig] = []
         for pl_config in self.per_layer_range_init_configs:
-            if should_consider_scope(qid, pl_config.ignored_scopes, pl_config.target_scopes):
-                if group == pl_config.target_group or pl_config.target_group is None:
-                    matches.append(
-                        RangeInitConfig(
-                            pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
-                        )
+            should_be_considered = should_consider_scope(qid, pl_config.ignored_scopes, pl_config.target_scopes)
+            if should_be_considered and (group == pl_config.target_group or pl_config.target_group is None):
+                matches.append(
+                    RangeInitConfig(
+                        pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
                     )
+                )
         if len(matches) > 1:
             raise ValueError(
                 "Location {} matches more than one per-layer initialization parameter definition!".format(str(qid))
             )
         if len(matches) == 1:
             return matches[0]
         if not matches and self.global_init_config is not None:
@@ -90,39 +91,40 @@
         raise ValueError(
             "Location {} does not match any per-layer initialization parameter definition!".format(str(qid))
         )
 
 
 class PTRangeInitCollectorParams(RangeInitCollectorParams):
     def __init__(
-        self, is_weights: bool, mode: QuantizationMode, per_channel: bool, input_shape: tuple, channel_idx: int
+        self, is_weights: bool, scheme: QuantizationScheme, per_channel: bool, input_shape: tuple, channel_idx: int
     ):
         """
-
+        :param is_weights: Boolean that defines tensor type. True for Weights, False for Activations.
+        :param scheme: Quantization scheme: symmetric or asymmetric.
         :param input_shape: Shape of the input tensor.
         :param channel_idx: Channel dimension.
         """
-        super().__init__(is_weights, mode, per_channel)
+        super().__init__(is_weights, scheme, per_channel)
         self._input_shape = input_shape
         self._channel_idx = channel_idx
 
     def get_reduction_axes(self, per_sample_stats: bool) -> ReductionAxes:
         """
         Calculates the reduction axes of the tensor.
 
         :param per_sample_stats: Boolean flag that indicated whether statistics are collected per-sample or per-batch.
         :return: Shape to reduce to.
         """
         ndims = len(self._input_shape)
         reduction_axes: List[int] = list(range(ndims))
-        if self._per_channel:
+        if self.is_per_channel:
             val = (ndims + self._channel_idx) % ndims
             reduction_axes.remove(val)
             if not val and self.use_per_sample_stats(per_sample_stats):
-                raise RuntimeError("Batch dimension should be equal to zero")
+                raise nncf.InternalError("Batch dimension should be equal to zero")
         if self.use_per_sample_stats(per_sample_stats):
             reduction_axes = reduction_axes[1:]  # Assumes batch is the first dimension
         return tuple(reduction_axes)
 
     def get_aggregation_axes(self, per_sample_stats: bool) -> AggregationAxes:
         """
         Calculates the aggregation axes of the tensor.
@@ -171,15 +173,15 @@
         collector_params: PTRangeInitCollectorParams = None,
         num_samples_to_collect_override: int = None,
     ) -> TensorStatisticCollectorBase:
         num_samples = init_config.num_init_samples
         if num_samples_to_collect_override is not None:
             num_samples = num_samples_to_collect_override
         if init_config.init_type not in RANGE_INIT_TYPES_VS_DESCRIPTIONS:
-            raise RuntimeError("Unknown range init type: {}".format(init_config.init_type))
+            raise nncf.InternalError("Unknown range init type: {}".format(init_config.init_type))
 
         use_per_sample_stats = collector_params.use_per_sample_stats(init_config.init_type == "mixed_min_max")
         reduction_axes = collector_params.get_reduction_axes(use_per_sample_stats)
         aggregation_axes = collector_params.get_aggregation_axes(use_per_sample_stats)
 
         if init_config.init_type == "min_max":
             return get_min_max_statistic_collector(
@@ -303,17 +305,17 @@
             quantizer_module, init_config, is_weights, input_shape = data
             num_samples_override = None
             if self.batch_size is not None:
                 num_batches = np.ceil(init_config.num_init_samples / self.batch_size)
                 num_samples_override = num_batches
 
             if isinstance(quantizer_module, SymmetricQuantizer):
-                mode = QuantizationMode.SYMMETRIC
+                mode = QuantizationScheme.SYMMETRIC
             else:
-                mode = QuantizationMode.ASYMMETRIC
+                mode = QuantizationScheme.ASYMMETRIC
 
             shape = quantizer_module.scale_shape
             if shape == (1,):  # Per-tensor
                 channel_idx = None
             elif len(shape) > 1 and all(item == 1 for item in shape):
                 channel_idx = 0  # (1, 1, 1, 1) - doest not matter which dim is channel_idx
             else:
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/layers.py` & `nncf-2.9.0/nncf/torch/quantization/layers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,21 +10,22 @@
 # limitations under the License.
 
 
 from abc import ABC
 from abc import abstractmethod
 from enum import Enum
 from functools import partial
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import torch
 from torch import distributed
 from torch import nn
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
 from nncf.common.quantization.quantizer_setup import QuantizerSetupBase
 from nncf.common.quantization.quantizers import calculate_asymmetric_level_ranges
 from nncf.common.quantization.quantizers import calculate_symmetric_level_ranges
 from nncf.common.quantization.quantizers import get_num_levels
@@ -43,14 +44,16 @@
 from nncf.torch.quantization.quantize_functions import ExportQuantizeToFakeQuantize
 from nncf.torch.quantization.quantize_functions import ExportQuantizeToONNXQuantDequant
 from nncf.torch.quantization.quantize_functions import TuneRange
 from nncf.torch.quantization.quantize_functions import asymmetric_quantize
 from nncf.torch.quantization.quantize_functions import decompress
 from nncf.torch.quantization.quantize_functions import get_scale_zp_from_input_low_input_high
 from nncf.torch.quantization.quantize_functions import symmetric_quantize
+from nncf.torch.return_types import maybe_get_values_from_torch_return_type
+from nncf.torch.return_types import maybe_wrap_to_torch_return_type
 from nncf.torch.utils import get_flat_tensor_contents_string
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import is_tracing_state
 from nncf.torch.utils import no_jit_trace
 
 QUANTIZATION_MODULES = Registry("quantization_modules")
 INITIALIZABLE_MODULES = Registry("initializable_modules")
@@ -315,15 +318,15 @@
 
             def __init__(self, module):
                 self.hook = module._register_load_state_dict_pre_hook(partial(self.hook_fn, module=module))
 
             def hook_fn(
                 self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, module
             ):
-                for module_key in module.state_dict().keys():
+                for module_key in module.state_dict():
                     candidate = prefix + module_key
                     if candidate in state_dict:
                         module.initialized = True
 
             def close(self):
                 self.hook.remove()
 
@@ -365,15 +368,24 @@
         self.enabled[0] = 1
         self.enable_gradients()
 
     def disable_quantization(self):
         self.enabled[0] = 0
         self.disable_gradients()
 
-    def forward(self, x):
+    def forward(self, x: Union[torch.Tensor, tuple]):
+        """
+        Method that unwraps return types if it is needed
+        before acutal quantization forward impl
+        """
+        x_unwrapped = maybe_get_values_from_torch_return_type(x)
+        result = self._forward_impl(x_unwrapped)
+        return maybe_wrap_to_torch_return_type(result, x)
+
+    def _forward_impl(self, x: torch.Tensor):
         if is_debug():
             self.call_count += 1
         # TODO: refactor to get rid of extra if's and calls on each forward
         if not self.is_enabled_quantization():
             return x
         is_exporting = is_tracing_state()
         if is_exporting:
@@ -486,15 +498,15 @@
         with no_jit_trace():
             levels = level_high - level_low + 1
             assert levels in [255, 256], "Can only export to INT8 256-level ONNX Quantize/Dequantize pairs"
 
             y_scale, y_zero_point = get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high)
             possible_axes = self._possible_per_channel_dimensions()
             if len(possible_axes) > 1:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     f"Impossible to determine the per-channel axis for a scale shape {self.scale_shape} - "
                     f"more than one dimension is >1"
                 )
             if not possible_axes:
                 # Impossible to determine proper axis for per-channel quantization because we have
                 # scale shape ~ [1, 1, 1, 1], therefore falling back to per-tensor style export
                 axis = 1  # default value by opset, ignored in per-tensor quantization anyway
@@ -518,15 +530,15 @@
 
                 return ExportQuantizeToFakeQuantize.apply(
                     x, levels, input_low, input_high, input_low, input_high, scale, zero_point, q_min, q_max, ch_axis
                 )
             if self._export_mode == QuantizerExportMode.ONNX_QUANTIZE_DEQUANTIZE_PAIRS:
                 x, y_scale, y_zero_point, axis = self._prepare_qdq_export_quantization(x)
                 return ExportQuantizeToONNXQuantDequant.apply(x, y_scale, y_zero_point, axis)
-        raise RuntimeError("Unknown export mode")
+        raise nncf.InternalError("Unknown export mode")
 
     def extra_repr(self):
         return "bit={}, ch={}".format(self.num_bits, self.per_channel)
 
     @abstractmethod
     def get_quantizer_config(self) -> QuantizerConfig:
         pass
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/metrics.py` & `nncf-2.9.0/nncf/torch/quantization/metrics.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -279,15 +279,15 @@
                         self._marking_edges(merged_original_graph, node_key, queue)
                     else:
                         self._marking_edges(merged_original_graph, node_key, queue, False)
                 else:
                     node_name = str(node[NNCFNode.NODE_NAME_ATTR])
 
                     matched = False
-                    for aq_key in self._compressed_model.nncf.external_quantizers.keys():
+                    for aq_key in self._compressed_model.nncf.external_quantizers:
                         if node_name in aq_key:
                             matched = True
                             break
                     if matched:
                         self._marking_edges(merged_original_graph, node_key, queue)
                     else:
                         is_op_non_change_precision_activation_tensor = True
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_constraints.py` & `nncf-2.9.0/nncf/torch/quantization/precision_constraints.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/__init__.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/definitions.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,16 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import List
+
+from nncf.common.quantization.structs import QuantizerConfig
+
+QConfigSequenceForHAWQToEvaluate = List[QuantizerConfig]
+CoveringQConfigSequenceForQuantNoiseCalculation = List[QuantizerConfig]
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/adjacent_quantizers.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/autoq_init.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/autoq_init.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/base_init.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/base_init.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -95,15 +95,15 @@
 
     def __init__(
         self,
         model: NNCFNetwork,
         weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
         constraints: HardwareQuantizationConstraints,
     ):
-        self._wq_affected_module_node_name_vs_qid_dict = {k.target_node_name: k for k in weight_quantizers.keys()}
+        self._wq_affected_module_node_name_vs_qid_dict = {k.target_node_name: k for k in weight_quantizers}
         self._quantizer_module_scope_vs_qid_dict: Dict[Scope, WeightQuantizerId] = {}
         self._skipped_quantized_weight_node_names = []
         self._skipped_weight_quantizers: Dict[WeightQuantizerId, BaseQuantizer] = {}
         self._weight_quantizers_in_execution_order_per_scope: Dict[Scope, BaseQuantizer] = OrderedDict()
         self._weight_quantizers_in_execution_order: Dict[WeightQuantizerId, BaseQuantizer] = OrderedDict()
 
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/bitwidth_graph.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/bitwidth_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/compression_ratio.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/compression_ratio.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/definitions.py` & `nncf-2.9.0/nncf/torch/sparsity/functions.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import List
+from nncf.torch.dynamic_graph.patch_pytorch import register_operator
 
-from nncf.common.quantization.structs import QuantizerConfig
 
-QConfigSequenceForHAWQToEvaluate = List[QuantizerConfig]
-CoveringQConfigSequenceForQuantNoiseCalculation = List[QuantizerConfig]
+@register_operator()
+def apply_binary_mask(mask, weight):
+    return mask * weight
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/hawq_debug.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/hawq_debug.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/hawq_init.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/hawq_init.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,14 +19,15 @@
 from typing import Any, Callable, Dict, List, NamedTuple, Set, Tuple
 
 import torch
 from torch import Tensor
 from torch import nn
 from torch.nn.modules.loss import _Loss
 
+import nncf
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
 from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerId
 from nncf.common.quantization.structs import WeightQuantizerId
@@ -152,15 +153,15 @@
     def _generate_covering_qconfig_sequences(observed_qconfs: List[Dict[QuantizerConfig, QuantizerConfig]]):
         covering_qconfig_sequences: List[CoveringQConfigSequenceForQuantNoiseCalculation] = []
         # For each index, put the largest qconf subset that only varies in bitwidth on top
         # so that the associated covering configurations would not require model regeneration
         optimized_observed_qconfs: List[List[QuantizerConfig]] = []
         for qconf_oset in observed_qconfs:
             variants: List[List[QuantizerConfig]] = []
-            for qconf in qconf_oset.keys():
+            for qconf in qconf_oset:
                 variants.append(list(filter(qconf.is_a_bitwidth_variant, qconf_oset.keys())))
             max_bw_varying_variant = max(variants, key=len)
             other_qconfs = list(filter(lambda x: x not in max_bw_varying_variant, qconf_oset.keys()))
             optimized_observed_qconfs.append(max_bw_varying_variant + other_qconfs)
 
         max_depth = max([len(qconfs_for_trace_idx) for qconfs_for_trace_idx in optimized_observed_qconfs])
         for i in range(max_depth):
@@ -261,15 +262,15 @@
             return self._algo.get_quantizer_setup_for_current_state()
 
         original_device = get_model_device(self._model)
         self._model.to(self._init_device)
 
         traces_per_layer = self._calc_traces(self._criterion_fn, self._criterion, self._iter_number, self._tolerance)
         if not traces_per_layer:
-            raise RuntimeError("Failed to calculate hessian traces!")
+            raise nncf.InternalError("Failed to calculate hessian traces!")
 
         traces_order = traces_per_layer.traces_order
         (
             weight_qconfig_sequences_in_trace_order,
             covering_qconfig_sequences,
         ) = self.get_qconfig_sequences_constrained_by_traces_order(traces_order)
 
@@ -377,15 +378,15 @@
             for quantizer_id, _ in all_quantizers:
                 bitwidths_vs_qconfig_sequence = retval.get_bitwidth_vs_qconfigs_dict(quantizer_id)
                 bitwidths = set(bitwidths_vs_qconfig_sequence.keys())
                 all_bitwidths_sets.append(bitwidths)
                 quantizer_ids.append(quantizer_id)
             minimal_set_bitwidths = set.intersection(*all_bitwidths_sets)
             if not minimal_set_bitwidths:
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "No bitwidths configurations are left after removing inconsistent groups of weight quantizers"
                     " with adjacent activation quantizers!"
                 )
             for quantizer_id in quantizer_ids:
                 qconfig_sequence = retval.get(quantizer_id)
                 filtered_qconfig_sequence = []
                 for qconf in qconfig_sequence:
@@ -487,15 +488,15 @@
         trace_estimator = HessianTraceEstimator(
             self._model, criterion_fn, criterion, self._init_device, self._data_loader, self._num_data_points
         )
         try:
             avg_traces = trace_estimator.get_average_traces(max_iter=iter_number, tolerance=tolerance)
         except RuntimeError as error:
             if "cuda out of memory" in error.args[0].lower():
-                raise RuntimeError(
+                raise nncf.InternalError(
                     "Failed to estimate average Hessian traces within precision initialization. Specify "
                     "a smaller batch size via --batch-size-init option in the NNCF samples or register "
                     "a data loader with a smaller batch size. Refer to "
                     "`NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs`"
                     " class"
                 ) from error
             raise error
@@ -724,29 +725,29 @@
     def _set_activations_bitwidth_strictly(
         self,
         quantizer_setup_to_set: SingleConfigQuantizerSetup,
         act_qp_ids: List[QuantizationPointId],
         weight_bitwidth_set: Set[int],
     ) -> SingleConfigQuantizerSetup:
         if len(weight_bitwidth_set) > 1:
-            raise RuntimeError("Invalid grouping of weight quantizers")
+            raise nncf.InternalError("Invalid grouping of weight quantizers")
         all_constraints = set()
         original_quant_module_ids = [
             self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id] for act_qp_id in act_qp_ids
         ]
         for act_quant_module_id in original_quant_module_ids:
             all_constraints.update(self._hw_precision_constraints.get_all_unique_bitwidths(act_quant_module_id))
         common_constraints = set(all_constraints)
         for act_quant_module_id in original_quant_module_ids:
             constraint = self._hw_precision_constraints.get_all_unique_bitwidths(act_quant_module_id)
             common_constraints = common_constraints.intersection(constraint)
         if weight_bitwidth_set:
             common_constraints = common_constraints.intersection(weight_bitwidth_set)
         if not common_constraints:
-            raise RuntimeError("No hardware compatible bitwidth for activation quantizers")
+            raise nncf.InternalError("No hardware compatible bitwidth for activation quantizers")
         for act_qp_id in act_qp_ids:
             quant_id = self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id]
             target_bitwidth = sorted(list(common_constraints))[0]
             bitwidths_vs_qconfig_sequence = self._hw_precision_constraints.get_bitwidth_vs_qconfigs_dict(quant_id)
             qconfig_to_select = bitwidths_vs_qconfig_sequence[target_bitwidth][0]
             quantizer_setup_to_set.quantization_points[act_qp_id].qconfig = qconfig_to_select
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/manual_init.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/manual_init.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/perturbations.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/perturbations.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/precision_init/traces_order.py` & `nncf-2.9.0/nncf/torch/quantization/precision_init/traces_order.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/quantize_functions.py` & `nncf-2.9.0/nncf/torch/quantization/quantize_functions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -220,15 +220,15 @@
     def forward(ctx, input_low, input_range, levels):
         input_high = input_range + input_low
         input_low_copy = input_low.clone()
         input_low_copy[input_low_copy > 0] = 0
         input_high[input_high < 0] = 0
         n = levels - 1
         # Need a cast here because fp16 division yields fp32 results sometimes
-        scale = (levels / (input_high - input_low_copy)).to(dtype=input_high.dtype)
+        scale = (n / (input_high - input_low_copy)).to(dtype=input_high.dtype)
         zp = torch.round(-input_low_copy * scale)
 
         new_input_low = torch.where(zp < n, zp / (zp - n) * input_high, input_low_copy)
         new_input_high = torch.where(zp > 0.0, (zp - n) / zp * input_low_copy, input_high)
 
         range_1 = input_high - new_input_low
         range_2 = new_input_high - input_low_copy
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/quantize_model.py` & `nncf-2.9.0/nncf/torch/quantization/quantize_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,21 +10,26 @@
 # limitations under the License.
 
 from copy import deepcopy
 from typing import Optional
 
 import torch
 
+import nncf
+from nncf.common.factory import NNCFGraphFactory
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.data import Dataset
+from nncf.parameters import CompressWeightsMode
 from nncf.parameters import ModelType
 from nncf.parameters import QuantizationMode
+from nncf.parameters import SensitivityMetric
 from nncf.parameters import TargetDevice
 from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
 from nncf.quantization.algorithms.post_training.algorithm import PostTrainingQuantization
+from nncf.quantization.algorithms.weight_compression.algorithm import WeightCompression
 from nncf.scopes import IgnoredScope
 from nncf.torch.model_creation import wrap_model
 
 DEFAULT_RANGE_TYPE = "mean_min_max"
 
 
 def quantize_impl(
@@ -41,15 +46,15 @@
 ) -> torch.nn.Module:
     """
     Implementation of the `quantize()` method for the PyTorch backend.
     """
     if fast_bias_correction is False:
         raise ValueError(f"fast_bias_correction={fast_bias_correction} is not supported")
     if target_device == TargetDevice.CPU_SPR:
-        raise RuntimeError("target_device == CPU_SPR is not supported")
+        raise nncf.InternalError("target_device == CPU_SPR is not supported")
     if mode is not None:
         raise ValueError(f"mode={mode} is not supported")
 
     copied_model = deepcopy(model)
 
     example_input = next(iter(calibration_dataset.get_inference_data()))
     nncf_network = wrap_model(copied_model.eval(), example_input)
@@ -67,7 +72,30 @@
     quantized_model = quantization_algorithm.apply(
         nncf_network, nncf_network.nncf.get_graph(), dataset=calibration_dataset
     )
 
     quantized_model.nncf.disable_dynamic_graph_building()
 
     return quantized_model
+
+
+def compress_weights_impl(
+    model: torch.nn.Module,
+    dataset: Dataset,
+    mode: CompressWeightsMode,
+    ratio: float,
+    group_size: int,
+    ignored_scope: IgnoredScope,
+    all_layers: bool,
+    sensitivity_metric: SensitivityMetric,
+    awq: bool,
+    subset_size: int,
+) -> torch.nn.Module:
+    """
+    Implementation of the `compress_weights()` method for the PyTorch backend.
+    """
+
+    compression_algorithm = WeightCompression(
+        mode, ratio, group_size, ignored_scope, all_layers, sensitivity_metric, awq, subset_size
+    )
+    graph = NNCFGraphFactory.create(model)
+    return compression_algorithm.apply(model, graph, dataset=dataset)
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/reference.py` & `nncf-2.9.0/nncf/torch/quantization/reference.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -11,14 +11,15 @@
 
 from enum import Enum
 from typing import List, Tuple, TypeVar
 
 import numpy as np
 import torch
 
+import nncf
 from nncf.torch.utils import sum_like
 
 GeneralizedTensor = TypeVar("GeneralizedTensor", torch.Tensor, np.ndarray)
 
 
 class ReferenceBackendType(Enum):
     NUMPY = "numpy"
@@ -28,15 +29,15 @@
 class ReferenceQuantize:
     def __init__(self, backend_type: ReferenceBackendType):
         if backend_type is ReferenceBackendType.NUMPY:
             self.backend = np
         elif backend_type is ReferenceBackendType.TORCH:
             self.backend = torch
         else:
-            raise RuntimeError("Unknown backend for ReferenceQuantize")
+            raise nncf.UnsupportedBackendError("Unknown backend for ReferenceQuantize")
 
     def _astype(self, tensor: GeneralizedTensor, dtype) -> GeneralizedTensor:
         if self.backend is np:
             return tensor.astype(dtype)
         return tensor.type(dtype)
 
     def forward(
@@ -84,15 +85,15 @@
     def tune_range(
         self, input_low: GeneralizedTensor, input_range: GeneralizedTensor, levels: int
     ) -> Tuple[GeneralizedTensor, GeneralizedTensor]:
         input_high = input_range + input_low
         input_low[input_low > 0] = 0
         input_high[input_high < 0] = 0
         n = levels - 1
-        scale = levels / (input_high - input_low)
+        scale = n / (input_high - input_low)
         scale = self._astype(scale, input_high.dtype)
         zp = self.backend.round(-input_low * scale)
 
         new_input_low = self.backend.where(zp < n, zp / (zp - n) * input_high, input_low)
         new_input_high = self.backend.where(zp > 0.0, (zp - n) / zp * input_low, input_high)
 
         range_1 = input_high - new_input_low
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/schedulers.py` & `nncf-2.9.0/nncf/torch/quantization/schedulers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/statistics.py` & `nncf-2.9.0/nncf/torch/quantization/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/strip.py` & `nncf-2.9.0/nncf/torch/quantization/strip.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -10,14 +10,15 @@
 # limitations under the License.
 
 
 import numpy as np
 import torch
 from torch.quantization.fake_quantize import FakeQuantize
 
+import nncf
 from nncf.torch.nncf_network import ExtraCompressionModuleType
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.layers import AsymmetricQuantizer
 from nncf.torch.quantization.layers import BaseQuantizer
 from nncf.torch.quantization.layers import SymmetricQuantizer
 
 SUPPORTED_NUM_BITS_FOR_STRIP_MODEL = [8]
@@ -29,15 +30,15 @@
 
     :param model: Target model.
     :return: The modified NNCF network.
     """
     compression_module_type = ExtraCompressionModuleType.EXTERNAL_QUANTIZER
     if model.nncf.is_compression_module_registered(compression_module_type):
         external_quantizers = model.nncf.get_compression_modules_by_type(compression_module_type)
-        for key in external_quantizers.keys():
+        for key in external_quantizers:
             if external_quantizers[key].is_enabled_quantization():
                 external_quantizers[key] = convert_to_torch_fakequantizer(external_quantizers[key])
 
     for node in model.nncf.get_original_graph().get_all_nodes():
         if node.node_type in ["nncf_model_input", "nncf_model_output"]:
             continue
 
@@ -75,15 +76,15 @@
     :return: Instance of FakeQuantize similar to the input quantizer.
     """
 
     # Call set_ranges in case the basic parameters impacting levels had changed
     nncf_quantizer.set_levels()
 
     if nncf_quantizer.num_bits not in SUPPORTED_NUM_BITS_FOR_STRIP_MODEL:
-        raise RuntimeError(
+        raise nncf.InternalError(
             "Converting nncf quantizer module to torch native only supports "
             f"for num_bits in {SUPPORTED_NUM_BITS_FOR_STRIP_MODEL}."
         )
     per_channel = nncf_quantizer.per_channel
     scale_shape = nncf_quantizer.scale_shape
     ch_axis = int(np.argmax(scale_shape))
     dtype = torch.qint8 if nncf_quantizer.level_low < 0 else torch.quint8
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/structs.py` & `nncf-2.9.0/nncf/torch/quantization/structs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/quantization/translator.py` & `nncf-2.9.0/nncf/torch/quantization/translator.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/return_types.py` & `nncf-2.9.0/nncf/torch/return_types.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import inspect
-from typing import Any, Optional, Tuple, Type, Union
+from typing import Any, Tuple, Type, Union
 
 import torch
 
 
 def __get_supported_torch_return_types() -> Tuple[Type[tuple], ...]:
     """
     Collects types from torch.return_type which can be wrapped/unwrapped by NNCF.
@@ -26,28 +26,30 @@
     retval = [t for _, t in inspect.getmembers(torch.return_types) if inspect.isclass(t) and hasattr(t, "values")]
     return tuple(t for t in retval if t.n_fields == 2)
 
 
 _TORCH_RETURN_TYPES = __get_supported_torch_return_types()
 
 
-def maybe_unwrap_from_torch_return_type(tensor: Any) -> torch.Tensor:
+def maybe_get_values_from_torch_return_type(tensor: Any) -> torch.Tensor:
     """
     Attempts to unwrap the tensor value from one of torch.return_types instances
     in case torch operation output is wrapped by a torch return_type.
 
     :param tensor: Torch tensor or torch return type instance to unwrap values from.
     :return: Unwrapped torch tensor.
     """
     if isinstance(tensor, _TORCH_RETURN_TYPES):
         return tensor.values
     return tensor
 
 
-def maybe_wrap_to_torch_return_type(tensor: torch.Tensor, wrapped_input: Optional[Union[tuple, torch.Tensor]]) -> Any:
+def maybe_wrap_to_torch_return_type(
+    tensor: torch.Tensor, wrapped_input: Union[tuple, torch.Tensor]
+) -> Union[tuple, torch.Tensor]:
     """
     Wraps tensor to wrapped_input wrapper in case wrapped_input is wrapped by a torch.return_value container.
 
     :param tensor: Torch tensor to wrap.
     :param wrapped_tensor: Instance of the tensor before it was unwrapped.
     :return: Wrapped tensor in case wrapped_input is wrapped by a torch.return_value container else the tensor.
     """
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/__init__.py` & `nncf-2.9.0/nncf/torch/sparsity/const/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementations of sparsity algorithms.
+Backend-specific implementation of the auxiliary "constant" sparsity algorithm.
 """
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/base_algo.py` & `nncf-2.9.0/nncf/torch/sparsity/base_algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -12,23 +12,23 @@
 Base classes for NNCF PyTorch sparsity algorithm builder and controller objects.
 """
 from typing import List
 
 import torch
 
 from nncf.api.compression import CompressionLoss
-from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.logging import nncf_logger
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.sparsity.controller import SparsityController
+from nncf.common.sparsity.schedulers import SparsityScheduler
 from nncf.common.utils.api_marker import api
 from nncf.common.utils.backend import copy_model
 from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
@@ -104,19 +104,23 @@
     def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo]):
         super().__init__(target_model)
         self._loss = ZeroCompressionLoss(get_model_device(target_model))
         self._scheduler = BaseCompressionScheduler()
         self.sparsified_module_info = sparsified_module_info
 
     @property
+    def current_sparsity_level(self) -> float:
+        return self._scheduler.current_sparsity_level
+
+    @property
     def loss(self) -> CompressionLoss:
         return self._loss
 
     @property
-    def scheduler(self) -> CompressionScheduler:
+    def scheduler(self) -> SparsityScheduler:
         return self._scheduler
 
     def disable_scheduler(self):
         self._scheduler = StubCompressionScheduler()
         self._scheduler.target_level = 0.0
         self._scheduler.current_sparsity_level = 0.0
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/collector.py` & `nncf-2.9.0/nncf/torch/sparsity/collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/const/__init__.py` & `nncf-2.9.0/nncf/torch/sparsity/rb/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementation of the auxiliary "constant" sparsity algorithm.
+Backend-specific implementations of regularization-based (RB) sparsity.
 """
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/const/algo.py` & `nncf-2.9.0/nncf/torch/sparsity/const/algo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/functions.py` & `nncf-2.9.0/nncf/torch/quantization/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from nncf.torch.dynamic_graph.patch_pytorch import register_operator
+"""
+Backend-specific implementations of quantization algorithms.
+"""
 
-
-@register_operator()
-def apply_binary_mask(mask, weight):
-    return mask * weight
+# Required for correct QUANTIZATION_MODULES registry functioning
+from . import layers as layers
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/layers.py` & `nncf-2.9.0/nncf/torch/sparsity/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/magnitude/__init__.py` & `nncf-2.9.0/nncf/torch/sparsity/magnitude/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/magnitude/algo.py` & `nncf-2.9.0/nncf/torch/sparsity/magnitude/algo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/magnitude/functions.py` & `nncf-2.9.0/nncf/torch/sparsity/magnitude/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/rb/__init__.py` & `nncf-2.9.0/nncf/tensorflow/pruning/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Backend-specific implementations of regularization-based (RB) sparsity.
+Backend-specific implementations of pruning algorithms.
 """
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/rb/algo.py` & `nncf-2.9.0/nncf/torch/sparsity/rb/algo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -85,14 +85,18 @@
     def set_sparsity_level(self, sparsity_level, target_sparsified_module_info: SparseModuleInfo = None):
         if target_sparsified_module_info is None:
             self._loss.set_target_sparsity_loss(sparsity_level)
         else:
             sparse_op = target_sparsified_module_info.operand
             self._loss.set_target_sparsity_loss(sparsity_level, sparse_op)
 
+    @property
+    def current_sparsity_level(self) -> float:
+        return self._loss.current_sparsity
+
     def compression_stage(self) -> CompressionStage:
         if self._mode == "local":
             return CompressionStage.FULLY_COMPRESSED
 
         if self.scheduler.current_sparsity_level == 0:
             return CompressionStage.UNCOMPRESSED
         if self.scheduler.current_sparsity_level >= self.scheduler.target_level:
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/rb/functions.py` & `nncf-2.9.0/nncf/torch/sparsity/rb/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/rb/layers.py` & `nncf-2.9.0/nncf/torch/sparsity/rb/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/sparsity/rb/loss.py` & `nncf-2.9.0/nncf/torch/sparsity/rb/loss.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,16 +18,16 @@
 class SparseLoss(PTCompressionLoss):
     def __init__(self, sparse_layers=None, target=1.0, p=0.05):
         super().__init__()
         self._sparse_layers = sparse_layers
         self.target = target
         self.p = p
         self.disabled = False
-        self.current_sparsity = 0
-        self.mean_sparse_prob = 0
+        self.current_sparsity: float = 0.0
+        self.mean_sparse_prob = 0.0
 
     def set_layers(self, sparse_layers):
         self._sparse_layers = sparse_layers
 
     def disable(self):
         if not self.disabled:
             self.disabled = True
```

### Comparing `nncf-2.8.1/nncf/torch/statistics/aggregator.py` & `nncf-2.9.0/nncf/torch/statistics/aggregator.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -23,18 +23,20 @@
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.tensor import PTNNCFTensor
 from nncf.torch.tensor_statistics.algo import create_register_input_hook
 
 
 class PTStatisticsAggregator(StatisticsAggregator):
+    HOOKS_GROUP_NAME = "statistics_hooks"
+
     def collect_statistics(self, model: NNCFNetwork, graph: NNCFGraph) -> None:
         with torch.no_grad():
-            with model.nncf.temporary_clean_view() as intermediate_model:
-                super().collect_statistics(intermediate_model, graph)
+            super().collect_statistics(model, graph)
+        model.nncf.remove_hooks_group(self.HOOKS_GROUP_NAME)
 
     def _register_statistics(
         self, outputs: Dict[str, PTNNCFTensor], statistic_points: StatisticPointsContainer
     ) -> None:
         return
 
     def _get_transformation_layout_extra_outputs(
@@ -48,14 +50,15 @@
                 for collectors in _statistic_point.algorithm_to_tensor_collectors.values():
                     for collector in collectors:
                         transformation_commands.append(
                             PTInsertionCommand(
                                 _statistic_point.target_point,
                                 create_register_input_hook(collector=collector),
                                 TransformationPriority.FP32_TENSOR_STATISTICS_OBSERVATION,
+                                hooks_group_name=self.HOOKS_GROUP_NAME,
                             )
                         )
 
         for transformation_command in transformation_commands:
             transformation_layout.register(transformation_command)
 
         return transformation_layout
```

### Comparing `nncf-2.8.1/nncf/torch/strip.py` & `nncf-2.9.0/nncf/torch/strip.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/structures.py` & `nncf-2.9.0/nncf/torch/structures.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/tensor.py` & `nncf-2.9.0/nncf/tensorflow/tensor.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import torch
+import tensorflow as tf
 
 from nncf.common.tensor import NNCFTensor
 
 
-class PTNNCFTensor(NNCFTensor):
+class TFNNCFTensor(NNCFTensor):
     """
-    A realisation of torch tensors wrapper for common NNCF algorithms.
+    A realisation of tensorflow tensors wrapper for common NNCF algorithms.
     """
 
-    def __init__(self, tensor: torch.tensor):
+    def __init__(self, tensor: tf.Tensor):
         # In case somebody attempts to wrap
         # tensor twice
         if isinstance(tensor, self.__class__):
             tensor = tensor.tensor
 
         super().__init__(tensor)
 
     @property
-    def device(self) -> torch.device:
+    def device(self) -> tf.device:
         return self._tensor.device
 
     def is_empty(self) -> bool:
-        return self.tensor.numel() == 0
+        return tf.equal(tf.size(self._tensor), 0)
```

### Comparing `nncf-2.8.1/nncf/torch/tensor_statistics/algo.py` & `nncf-2.9.0/nncf/torch/tensor_statistics/algo.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Callable, Dict, Set
+from typing import Callable, Dict, Set, Union
 
 import torch
 
 from nncf.api.compression import CompressionStage
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
 from nncf.common.tensor_statistics.collectors import ReductionAxes
@@ -24,14 +24,15 @@
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.dynamic_graph.context import no_nncf_trace
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import TransformationPriority
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.return_types import maybe_get_values_from_torch_return_type
 from nncf.torch.tensor import PTNNCFTensor
 
 
 class TensorStatisticObservationPoint:
     def __init__(self, target_point: PTTargetPoint, reduction_shapes: Set[ReductionAxes] = None):
         self.target_point = target_point
         self.reduction_shapes = reduction_shapes
@@ -47,23 +48,24 @@
     """
     Function to create regiter inputs hook function.
 
     :param collector: Collector to use in resulting hook.
     :return: Register inputs hook function.
     """
 
-    def register_inputs_hook(x: torch.Tensor) -> torch.Tensor:
+    def register_inputs_hook(x: Union[torch.Tensor, tuple]) -> torch.Tensor:
         """
         Register inputs hook function.
 
         :parameter x: tensor to register in hook.
         :return: tensor to register in hook.
         """
         with no_nncf_trace():
-            collector.register_input_for_all_reducers(PTNNCFTensor(x))
+            x_unwrapped = maybe_get_values_from_torch_return_type(x)
+            collector.register_input_for_all_reducers(PTNNCFTensor(x_unwrapped))
         return x
 
     return register_inputs_hook
 
 
 class TensorStatisticsCollectionBuilder(PTCompressionAlgorithmBuilder):
     def __init__(
```

### Comparing `nncf-2.8.1/nncf/torch/tensor_statistics/collectors.py` & `nncf-2.9.0/nncf/torch/tensor_statistics/collectors.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -30,14 +30,15 @@
 from nncf.experimental.common.tensor_statistics.collectors import MedianAbsoluteDeviationAggregator
 from nncf.experimental.common.tensor_statistics.collectors import MinAggregator
 from nncf.experimental.common.tensor_statistics.collectors import MinReducer
 from nncf.experimental.common.tensor_statistics.collectors import NoopAggregator
 from nncf.experimental.common.tensor_statistics.collectors import NoopReducer
 from nncf.experimental.common.tensor_statistics.collectors import PercentileAggregator
 from nncf.experimental.common.tensor_statistics.collectors import QuantileReducer
+from nncf.experimental.common.tensor_statistics.collectors import RawReducer
 from nncf.experimental.common.tensor_statistics.collectors import ShapeAggregator
 from nncf.experimental.common.tensor_statistics.collectors import TensorCollector
 from nncf.quantization.advanced_parameters import StatisticsType
 from nncf.torch.tensor import PTNNCFTensor
 from nncf.torch.tensor_statistics.statistics import PTMeanTensorStatistic
 from nncf.torch.tensor_statistics.statistics import PTMedianMADTensorStatistic
 from nncf.torch.tensor_statistics.statistics import PTMinMaxTensorStatistic
@@ -83,28 +84,28 @@
             device = x.tensor.device
             result = torch.tensor(np.median(x.tensor.detach().cpu().numpy(), axis=axis, keepdims=keepdims))
             return PTNNCFTensor(result.type(x.tensor.dtype).to(device))
         return PTNNCFCollectorTensorProcessor.quantile(x, quantile=[0.5], axis=axis, keepdims=keepdims)[0]
 
     @classmethod
     def masked_mean(
-        cls, x: NNCFTensor, axis: Union[int, Tuple[int, ...], List[int]], mask: NNCFTensor, keepdims=False
+        cls, x: NNCFTensor, axis: Union[int, Tuple[int, ...]], mask: NNCFTensor, keepdims: bool = False
     ) -> NNCFTensor:
         if mask is None:
             return cls.mean(x, axis=axis, keepdims=keepdims)
         device = x.tensor.device
         masked_x = np.ma.array(x.tensor.detach().cpu().numpy(), mask=mask.tensor.detach().cpu().numpy())
         result = np.ma.mean(masked_x, axis=axis, keepdims=keepdims).astype(masked_x.dtype)
         if isinstance(result, np.ma.MaskedArray):
             result = result.data
         return PTNNCFTensor(torch.tensor(result).to(device=device))
 
     @classmethod
     def masked_median(
-        cls, x: NNCFTensor, axis: Union[int, Tuple[int, ...], List[int]], mask: NNCFTensor, keepdims=False
+        cls, x: NNCFTensor, axis: Union[int, Tuple[int, ...]], mask: NNCFTensor, keepdims: bool = False
     ) -> NNCFTensor:
         # Implemented in numy as torch.masked.median is not implemented yet
         if mask is None:
             return cls.median(x, axis=axis, keepdims=keepdims)
         device = x.tensor.device
         masked_x = np.ma.array(x.tensor.detach().cpu().numpy(), mask=mask.tensor.detach().cpu().numpy())
         result = np.ma.median(masked_x, axis=axis, keepdims=keepdims).astype(masked_x.dtype)
@@ -219,18 +220,14 @@
     def get_inplace_fn(self):
         return None
 
     def get_output_names(self, target_node_name: str, port_id: int) -> List[str]:
         return []
 
 
-class PTNoopReducer(PTReducerMixIn, NoopReducer):
-    pass
-
-
 class PTMinReducer(PTReducerMixIn, MinReducer):
     pass
 
 
 class PTMaxReducer(PTReducerMixIn, MaxReducer):
     pass
 
@@ -461,15 +458,15 @@
     :param aggregation_axes: Axes to use in aggregation functions.
     :param num_samples: Maximum number of samples to collect.
     :param window_size: Number of samples from the end of the list of collected samples to aggregate.
         Aggregates all available collected statistics in case parameter is None.
     :return: Target statistic collector.
     """
     tensor_collector = TensorCollector(statistic_cls)
-    reducer = PTNoopReducer()
+    reducer = NoopReducer()
     aggregation_axes = list(set(list(aggregation_axes) + [dim + 1 for dim in reduction_axes]))
     aggregator = aggregator_cls(
         PTNNCFCollectorTensorProcessor,
         aggregation_axes=aggregation_axes,
         window_size=window_size,
         num_samples=num_samples,
     )
@@ -528,15 +525,15 @@
         Aggregates all available collected statistics in case parameter is None.
     :return: Mean statistic collector.
     """
     if channel_axis == 0:
         reducer = PTBatchMeanReducer()
     else:
         reducer = PTMeanPerChanelReducer(channel_axis=channel_axis)
-    noop_reducer = PTNoopReducer()
+    noop_reducer = NoopReducer()
 
     kwargs = {
         "tensor_processor": PTNNCFCollectorTensorProcessor,
         "num_samples": num_samples,
         "window_size": window_size,
     }
     aggregate_mean = MeanAggregator(**kwargs)
@@ -551,15 +548,15 @@
 def get_raw_stat_collector(num_samples: Optional[int] = None) -> TensorCollector:
     """
     Raw statistic collector builder.
 
     :param num_samples: Maximum number of samples to collect.
     :return: Raw statistic collector.
     """
-    reducer = PTNoopReducer()
+    reducer = RawReducer()
     aggregator = NoopAggregator(num_samples)
 
     collector = TensorCollector(PTRawTensorStatistic)
     collector.register_statistic_branch(PTRawTensorStatistic.VALUES_STATS, reducer, aggregator)
     return collector
```

### Comparing `nncf-2.8.1/nncf/torch/tensor_statistics/statistics.py` & `nncf-2.9.0/nncf/torch/tensor_statistics/statistics.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

### Comparing `nncf-2.8.1/nncf/torch/utils.py` & `nncf-2.9.0/nncf/torch/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -15,14 +15,15 @@
 
 import numpy as np
 import torch
 from torch import distributed as dist
 from torch import nn
 from torch.nn import Module
 
+import nncf
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
 from nncf.common.deprecation import warning_deprecated
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.scopes import matches_any
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.dynamic_graph.scope import ScopeElement
@@ -241,15 +242,15 @@
             module.train(state.training_state[name])
         except KeyError as e:
             # KeyError could happen if the modules name were changed during forward
             # (e.g. LSTM block in NNCF examples)
             msg = f"Could not find a module to restore state: {name}"
             nncf_logger.debug(msg)
             if strict:
-                raise RuntimeError(msg) from e
+                raise nncf.InternalError(msg) from e
 
     for name, param in base_module.named_parameters():
         param.requires_grad = state.requires_grad_state[name]
 
 
 @contextmanager
 def training_mode_switcher(model: Module, is_training: bool = True):
```

### Comparing `nncf-2.8.1/nncf/version.py` & `nncf-2.9.0/nncf/torch/pruning/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,16 +1,13 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-__version__ = "2.8.1"
-
-BKC_TORCH_VERSION = "2.1.2"
-BKC_TORCHVISION_VERSION = "0.16.2"
-BKC_TF_VERSION = "2.12.*"
+"""
+Backend-specific implementations of pruning algorithms.
+"""
```

### Comparing `nncf-2.8.1/nncf.egg-info/PKG-INFO` & `nncf-2.9.0/nncf.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nncf
-Version: 2.8.1
+Version: 2.9.0
 Summary: Neural Networks Compression Framework
 Home-page: https://github.com/openvinotoolkit/nncf
 Author: Intel
 Author-email: alexander.kozlov@intel.com
 License: Apache-2.0
 Keywords: compression,quantization,sparsity,mixed-precision-training,quantization-aware-training,hawq,classification,pruning,object-detection,semantic-segmentation,nas,nlp,bert,transformers,mmdetection
 Classifier: Programming Language :: Python :: 3
@@ -53,22 +53,22 @@
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "torch"
 Provides-Extra: pytorch
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "pytorch"
 Provides-Extra: onnx
 Requires-Dist: onnx~=1.13.1; extra == "onnx"
 Requires-Dist: onnxruntime~=1.14.1; python_version < "3.11" and extra == "onnx"
 Provides-Extra: openvino
-Requires-Dist: openvino==2023.3; extra == "openvino"
+Requires-Dist: openvino==2024.0; extra == "openvino"
 Provides-Extra: all
 Requires-Dist: tensorflow~=2.12.0; extra == "all"
 Requires-Dist: tensorflow-metadata<=1.13.0; extra == "all"
 Requires-Dist: torch<2.2,>=2.0; python_version < "3.11" and extra == "all"
 Requires-Dist: onnx~=1.13.1; extra == "all"
 Requires-Dist: onnxruntime~=1.14.1; python_version < "3.11" and extra == "all"
-Requires-Dist: openvino==2023.3; extra == "all"
+Requires-Dist: openvino==2024.0; extra == "all"
 
 <div align="center">
 
 # Neural Network Compression Framework (NNCF)
 
 [Key Features](#key-features) 
 [Installation](#installation-guide) 
@@ -338,49 +338,53 @@
 
 For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
 
 ### Jupyter* Notebook Tutorials and Demos
 
 A collection of ready-to-run Jupyter* notebooks tutorials and demos are available to explain and display NNCF compression algorithms for optimizing models for inference with the OpenVINO Toolkit.
 
-| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                                   |                                  Compression Algorithm                                  |  Backend   |               Domain                |
-|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
-| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                                           |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
-| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                             |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
-| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                                      |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
-| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                              |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
-| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb)             |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
-| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                               |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
-| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                       |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
-| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                             |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
+| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                                                       |                                  Compression Algorithm                                  |  Backend   |               Domain                |
+|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|
+| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/105-language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert/105-language-quantize-bert.ipynb)                               |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |
+| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2F110-ct-segmentation-quantize%2F110-ct-scan-live-inference.ipynb)                                 |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |
+| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)                                                                                                                                                                                                                                          |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                                  |                               Post-Training Quantization                                | Tensorflow |        Image Classification         |
+| [Migrating from POT to NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/111-yolov5-quantization-migration)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration/111-yolov5-quantization-migration.ipynb) |                               Post-Training Quantization                                |  OpenVINO  |          Object detection           |
+| [Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/122-quantizing-model-with-accuracy-control)                                                                                                                                                                                                                                   |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |
+| [PyTorch Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)                                                                                                                                                                                                                                       |                                Training-Time Compression                                |  PyTorch   |        Image Classification         |
+| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/301-tensorflow-training-openvino)                                                                                                                                                                                                                                           |                                Training-Time Compression                                | Tensorflow |        Image Classification         |
+| [Joint Pruning, Quantization and Distillation for BERT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)                                                                                                                                                                                                                                 |                      Joint Pruning, Quantization and Distillation                       |  OpenVINO  |                 NLP                 |
 
 Below is a list of notebooks demonstrating OpenVINO conversion and inference together with NNCF compression for models from various domains.
 
-| Demo Model                                                                                                                                                                                                                                                                                                                                                |                                                                                                                 Compression Algorithm                                                                                                                  |  Backend  |                              Domain                               |
-|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------:|:-----------------------------------------------------------------:|
-| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)            |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  | Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation |
-| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                               |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                         Object Detection                          |
-| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                  |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                         |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Panoptic Segmentation                       |
-| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                           |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Image-to-Image                           |
-| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Image-to-Text                           |
-| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                      |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                           Text-to-Image                           |
-| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                    |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                |                                                                                                   Post-Training Quantization,<br>Weight Compression                                                                                                    | OpenVINO  |                 Text-to-Image,<br>Image-to-Image                  |
-| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                     |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                       Multi-Modal Retrieval                       |
-| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                        |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb) |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                 |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                          Speech-to-Text                           |
-| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                              |                                                                                                               Post-Training Quantization                                                                                                               | OpenVINO  |                      NLP, Grammar Correction                      |
-| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                         |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                  |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                    NLP, Instruction Following                     |
-| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                |                                                                                                                   Weight Compression                                                                                                                   | OpenVINO  |                           NLP, Chat Bot                           |
+| Demo Model                                                                                                                                                                                                                                                                                                                                                                                  |               Compression Algorithm               |  Backend  |                                Domain                                |
+|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------:|:---------:|:--------------------------------------------------------------------:|
+| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization/230-yolov8-object-detection.ipynb)                                              |            Post-Training Quantization             | OpenVINO  |  Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation   |
+| [YOLOv7](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                           Object Detection                           |
+| [EfficientSAM](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/274-efficient-sam)                                                                                                                                                                                                                                                                                 |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)                                                                                                                                                                                                                                                                    |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/249-oneformer-segmentation)                                                                                                                                                                                                                                                                           |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |
+| [InstructPix2Pix](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/231-instruct-pix2pix-image-editing)                                                                                                                                                                                                                                                             |            Post-Training Quantization             | OpenVINO  |                            Image-to-Image                            |
+| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/228-clip-zero-shot-image-classification)                                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/233-blip-visual-language-processing)                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |
+| [Segmind-VegaRT](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/248-stable-diffusion-xl/248-segmind-vegart.ipynb)                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/263-latent-consistency-models-image-generation)                                                                                                                                                                                                                                        |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [Wrstchen](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/265-wuerstchen-image-generation)                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [ControlNet QR Code Monster](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/264-qrcode-monster)                                                                                                                                                                                                                                                                  |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |
+| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/271-sdxl-turbo)                                                                                                                                                                                                                                                                                      |            Post-Training Quantization             | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [DeepFloyd IF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/238-deepfloyd-if)                                                                                                                                                                                                                                                                                  | Post-Training Quantization,<br>Weight Compression | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |
+| [ImageBind](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/239-image-bind)                                                                                                                                                                                                                                                                                       |            Post-Training Quantization             | OpenVINO  |                        Multi-Modal Retrieval                         |
+| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/267-distil-whisper-asr)                                                                                                                                                                                                                                                                          |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/227-whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/227-whisper-subtitles-generation/227-whisper-convert.ipynb)                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/255-mms-massively-multilingual-speech)                                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |
+| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/214-grammar-correction)                                                                                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                       NLP, Grammar Correction                        |
+| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/275-llm-question-answering)                                                                                                                                                                                                                                                           |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Dolly 2.0](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/240-dolly-2-instruction-following)                                                                                                                                                                                                                                                                    |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |
+| [Stable-Zephyr-3b](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/273-stable-zephyr-3b-chatbot)                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
+| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)                                                                                                                                                                                                                                                                                  |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |
 
 ### Post-Training Quantization Examples
 
 Compact scripts demonstrating quantization and corresponding inference speed boost:
 
 | Example Name                                                                                                                             |              Compression Algorithm               |  Backend   |         Domain         |
 |:-----------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------:|:----------:|:----------------------:|
@@ -416,21 +420,14 @@
 
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
 - [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
 
   NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
 
-### Git patches for third-party repository
-
-See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
-
-- [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
-**NOTE**: this patch is deprecated and will be removed from NNCF repository in future releases.
-
 ## Installation Guide
 
 For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 
 ```bash
@@ -447,16 +444,14 @@
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 
 ```bash
 conda install -c conda-forge nncf
 ```
 
-You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#demos-tutorials-and-samples).
-
 ### System requirements
 
 - Ubuntu\* 18.04 or later (64-bit)
 - Python\* 3.7 or later
 - Supported frameworks:
   - PyTorch\* >=2.0, <2.2
   - TensorFlow\* >=2.8.4, <=2.12.1
```

### Comparing `nncf-2.8.1/nncf.egg-info/SOURCES.txt` & `nncf-2.9.0/nncf.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 LICENSE
 MANIFEST.in
 README.md
 setup.py
 licensing/third-party-programs.txt
 nncf/__init__.py
 nncf/definitions.py
+nncf/errors.py
 nncf/parameters.py
 nncf/scopes.py
 nncf/version.py
 nncf.egg-info/PKG-INFO
 nncf.egg-info/SOURCES.txt
 nncf.egg-info/dependency_links.txt
 nncf.egg-info/requires.txt
@@ -20,14 +21,15 @@
 nncf/common/collector.py
 nncf/common/composite_compression.py
 nncf/common/compression.py
 nncf/common/deprecation.py
 nncf/common/engine.py
 nncf/common/exporter.py
 nncf/common/factory.py
+nncf/common/hook_handle.py
 nncf/common/insertion_point_graph.py
 nncf/common/plotting.py
 nncf/common/schedulers.py
 nncf/common/scopes.py
 nncf/common/stateful_classes_registry.py
 nncf/common/statistics.py
 nncf/common/strip.py
@@ -53,16 +55,16 @@
 nncf/common/graph/transformations/commands.py
 nncf/common/graph/transformations/layout.py
 nncf/common/hardware/__init__.py
 nncf/common/hardware/config.py
 nncf/common/hardware/opset.py
 nncf/common/hardware/configs/cpu.json
 nncf/common/hardware/configs/gpu.json
+nncf/common/hardware/configs/npu.json
 nncf/common/hardware/configs/template.json
-nncf/common/hardware/configs/vpu.json
 nncf/common/initialization/__init__.py
 nncf/common/initialization/batchnorm_adaptation.py
 nncf/common/initialization/dataloader.py
 nncf/common/logging/__init__.py
 nncf/common/logging/logger.py
 nncf/common/logging/progress_bar.py
 nncf/common/logging/track_progress.py
@@ -274,20 +276,14 @@
 nncf/openvino/graph/metatypes/openvino_metatypes.py
 nncf/openvino/graph/transformations/__init__.py
 nncf/openvino/graph/transformations/command_creation.py
 nncf/openvino/graph/transformations/commands.py
 nncf/openvino/hardware/__init__.py
 nncf/openvino/hardware/config.py
 nncf/openvino/hardware/fused_patterns.py
-nncf/openvino/pot/__init__.py
-nncf/openvino/pot/engine.py
-nncf/openvino/pot/telemetry_extractors.py
-nncf/openvino/pot/quantization/__init__.py
-nncf/openvino/pot/quantization/accuracy_aware.py
-nncf/openvino/pot/quantization/quantize_model.py
 nncf/openvino/quantization/__init__.py
 nncf/openvino/quantization/backend_parameters.py
 nncf/openvino/quantization/default_quantization.py
 nncf/openvino/quantization/ignored_patterns.py
 nncf/openvino/quantization/quantize_ifmodel.py
 nncf/openvino/quantization/quantize_model.py
 nncf/openvino/statistics/__init__.py
@@ -304,14 +300,15 @@
 nncf/quantization/algorithms/__init__.py
 nncf/quantization/algorithms/algorithm.py
 nncf/quantization/algorithms/pipeline.py
 nncf/quantization/algorithms/accuracy_control/__init__.py
 nncf/quantization/algorithms/accuracy_control/algorithm.py
 nncf/quantization/algorithms/accuracy_control/backend.py
 nncf/quantization/algorithms/accuracy_control/evaluator.py
+nncf/quantization/algorithms/accuracy_control/onnx_backend.py
 nncf/quantization/algorithms/accuracy_control/openvino_backend.py
 nncf/quantization/algorithms/accuracy_control/rank_functions.py
 nncf/quantization/algorithms/accuracy_control/ranker.py
 nncf/quantization/algorithms/accuracy_control/subset_selection.py
 nncf/quantization/algorithms/bias_correction/__init__.py
 nncf/quantization/algorithms/bias_correction/algorithm.py
 nncf/quantization/algorithms/bias_correction/backend.py
@@ -339,16 +336,19 @@
 nncf/quantization/algorithms/post_training/__init__.py
 nncf/quantization/algorithms/post_training/algorithm.py
 nncf/quantization/algorithms/post_training/pipeline.py
 nncf/quantization/algorithms/smooth_quant/__init__.py
 nncf/quantization/algorithms/smooth_quant/algorithm.py
 nncf/quantization/algorithms/smooth_quant/backend.py
 nncf/quantization/algorithms/smooth_quant/openvino_backend.py
+nncf/quantization/algorithms/smooth_quant/torch_backend.py
 nncf/quantization/algorithms/weight_compression/__init__.py
 nncf/quantization/algorithms/weight_compression/algorithm.py
+nncf/quantization/algorithms/weight_compression/awq.py
+nncf/quantization/algorithms/weight_compression/awq_patterns.py
 nncf/quantization/algorithms/weight_compression/backend.py
 nncf/quantization/algorithms/weight_compression/config.py
 nncf/quantization/algorithms/weight_compression/mixed_precision.py
 nncf/quantization/algorithms/weight_compression/openvino_backend.py
 nncf/quantization/algorithms/weight_compression/torch_backend.py
 nncf/quantization/algorithms/weight_compression/weight_lowering.py
 nncf/telemetry/__init__.py
@@ -435,15 +435,14 @@
 nncf/tensorflow/sparsity/rb/loss.py
 nncf/tensorflow/sparsity/rb/operation.py
 nncf/tensorflow/tensor_statistics/__init__.py
 nncf/tensorflow/tensor_statistics/collectors.py
 nncf/tensorflow/tensor_statistics/reduction.py
 nncf/tensorflow/tensor_statistics/statistics.py
 nncf/tensorflow/utils/__init__.py
-nncf/tensorflow/utils/hook_handle.py
 nncf/tensorflow/utils/node.py
 nncf/tensorflow/utils/scopes_handle.py
 nncf/tensorflow/utils/state.py
 nncf/torch/__init__.py
 nncf/torch/algo_selector.py
 nncf/torch/batchnorm_adaptation.py
 nncf/torch/checkpoint_loading.py
```

### Comparing `nncf-2.8.1/nncf.egg-info/requires.txt` & `nncf-2.9.0/nncf.egg-info/requires.txt`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 texttable>=1.6.3
 tqdm>=4.54.1
 
 [all]
 tensorflow~=2.12.0
 tensorflow-metadata<=1.13.0
 onnx~=1.13.1
-openvino==2023.3
+openvino==2024.0
 
 [all:python_version < "3.11"]
 torch<2.2,>=2.0
 onnxruntime~=1.14.1
 
 [dev]
 black==23.3.0
@@ -40,15 +40,15 @@
 [onnx]
 onnx~=1.13.1
 
 [onnx:python_version < "3.11"]
 onnxruntime~=1.14.1
 
 [openvino]
-openvino==2023.3
+openvino==2024.0
 
 [pytorch]
 
 [pytorch:python_version < "3.11"]
 torch<2.2,>=2.0
 
 [tensorflow]
```

### Comparing `nncf-2.8.1/setup.py` & `nncf-2.9.0/setup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2023 Intel Corporation
+# Copyright (c) 2024 Intel Corporation
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #      http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -129,15 +129,15 @@
 
 TORCH_EXTRAS = [
     "torch>=2.0,<2.2;python_version < '3.11'",
 ]
 
 ONNX_EXTRAS = ["onnx~=1.13.1", "onnxruntime~=1.14.1;python_version < '3.11'"]
 
-OPENVINO_EXTRAS = ["openvino==2023.3"]
+OPENVINO_EXTRAS = ["openvino==2024.0"]
 
 
 EXTRAS_REQUIRE = {
     "dev": [
         "black==23.3.0",
         "isort==5.12.0",
         "kaleido>=0.2.1",
```

