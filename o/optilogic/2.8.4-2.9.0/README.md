# Comparing `tmp/optilogic-2.8.4-py3-none-any.whl.zip` & `tmp/optilogic-2.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,20 @@
-Zip file size: 60678 bytes, number of entries: 18
--rw-r--r--  2.0 unx       22 b- defN 23-Nov-08 04:34 optilogic/__init__.py
--rw-r--r--  2.0 unx       48 b- defN 23-Nov-08 04:35 optilogic/pioneer/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/__init__.py
--rw-r--r--  2.0 unx    88216 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/api.py
--rw-r--r--  2.0 unx   197541 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/api_tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/__init__.py
--rw-r--r--  2.0 unx     5111 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
--rw-r--r--  2.0 unx      317 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/bash.py
--rw-r--r--  2.0 unx      209 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/quick.py
--rw-r--r--  2.0 unx      701 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/sidecar.py
--rw-r--r--  2.0 unx      577 b- defN 23-Nov-08 04:35 optilogic/pioneer/api/quick_tests/sleep.py
--rw-r--r--  2.0 unx      131 b- defN 23-Nov-08 04:35 optilogic/pioneer/job_utils/__init__.py
--rw-r--r--  2.0 unx     3707 b- defN 23-Nov-08 04:35 optilogic/pioneer/job_utils/job_utils.py
--rw-r--r--  2.0 unx     1070 b- defN 23-Nov-08 04:35 optilogic-2.8.4.dist-info/LICENSE
--rw-r--r--  2.0 unx      759 b- defN 23-Nov-08 04:35 optilogic-2.8.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Nov-08 04:35 optilogic-2.8.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Nov-08 04:35 optilogic-2.8.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1624 b- defN 23-Nov-08 04:35 optilogic-2.8.4.dist-info/RECORD
-18 files, 300155 bytes uncompressed, 57952 bytes compressed:  80.7%
+Zip file size: 61506 bytes, number of entries: 18
+-rw-r--r--  2.0 unx       22 b- defN 23-Nov-15 22:27 optilogic/__init__.py
+-rw-r--r--  2.0 unx       48 b- defN 23-Nov-15 22:27 optilogic/pioneer/__init__.py
+-rw-r--r--  2.0 unx       20 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/__init__.py
+-rw-r--r--  2.0 unx    91380 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/api.py
+-rw-r--r--  2.0 unx   198263 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/api_tests.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/__init__.py
+-rw-r--r--  2.0 unx     5111 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
+-rw-r--r--  2.0 unx      317 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/bash.py
+-rw-r--r--  2.0 unx      209 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/quick.py
+-rw-r--r--  2.0 unx      701 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/sidecar.py
+-rw-r--r--  2.0 unx      577 b- defN 23-Nov-15 22:27 optilogic/pioneer/api/quick_tests/sleep.py
+-rw-r--r--  2.0 unx      131 b- defN 23-Nov-15 22:27 optilogic/pioneer/job_utils/__init__.py
+-rw-r--r--  2.0 unx     3707 b- defN 23-Nov-15 22:27 optilogic/pioneer/job_utils/job_utils.py
+-rw-r--r--  2.0 unx     1070 b- defN 23-Nov-15 22:28 optilogic-2.9.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      759 b- defN 23-Nov-15 22:28 optilogic-2.9.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Nov-15 22:28 optilogic-2.9.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Nov-15 22:28 optilogic-2.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1624 b- defN 23-Nov-15 22:28 optilogic-2.9.0.dist-info/RECORD
+18 files, 304041 bytes uncompressed, 58780 bytes compressed:  80.7%
```

## zipnote {}

```diff
@@ -33,23 +33,23 @@
 
 Filename: optilogic/pioneer/job_utils/__init__.py
 Comment: 
 
 Filename: optilogic/pioneer/job_utils/job_utils.py
 Comment: 
 
-Filename: optilogic-2.8.4.dist-info/LICENSE
+Filename: optilogic-2.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: optilogic-2.8.4.dist-info/METADATA
+Filename: optilogic-2.9.0.dist-info/METADATA
 Comment: 
 
-Filename: optilogic-2.8.4.dist-info/WHEEL
+Filename: optilogic-2.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: optilogic-2.8.4.dist-info/top_level.txt
+Filename: optilogic-2.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: optilogic-2.8.4.dist-info/RECORD
+Filename: optilogic-2.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optilogic/pioneer/api/api.py

```diff
@@ -70,14 +70,15 @@
 import pickle
 import time
 from cachetools import cached, TTLCache
 from datetime import datetime
 from getpass import getpass
 from json import dumps, loads
 from os import getenv
+from random import randint
 from re import fullmatch, search
 from requests import ConnectionError, delete, get, HTTPError, Response, request
 from sys import platform
 from tempfile import gettempdir
 from types import FrameType
 from typing import Any, cast, Dict, List, Literal, Optional, Set, Tuple, TypedDict, Union
 from warnings import warn, warn_explicit
@@ -160,15 +161,15 @@
         'Global Sourcing - Cost To Serve',
         'Global Supply Chain Strategy (2.7.14)',
         'Global Supply Chain Strategy',
         'Global Sustainability Analysis (2.7.14)',
         'Global Sustainability Analysis',
         'Lookups example for Warehousing Policies (2.7.14)',
         'Lookups example for Warehousing Policies',
-        'Multi-echelon Inventory Optimization',
+        'Multi-echelon Inventory Optimization (2.7.14)',
         'Multi-echelon Inventory Optimization',
         'Multi-Year Capacity Planning (2.7.14)',
         'Multi-Year Capacity Planning',
         'Outbound Distribution Simulation (2.7.14)',
         'Outbound Distribution Simulation',
         'Sensitivity At Scale (2.7.14)',
         'Sensitivity At Scale',
@@ -229,15 +230,15 @@
         {'id': '36.20230921130338', 'name': 'Outbound Distribution Simulation'},
         {'id': '37.2', 'name': 'Lookups example for Warehousing Policies (2.7.14)'},
         {'id': '37.20230926142439', 'name': 'Lookups example for Warehousing Policies'},
         {'id': '38.1', 'name': 'Sensitivity At Scale'},
         {'id': '38.3', 'name': 'Sensitivity At Scale (2.7.14)'},
         {'id': '39.2', 'name': 'Transportation Optimization'},
         {'id': '40.1', 'name': 'Multi-echelon Inventory Optimization'},
-        {'id': '40.2', 'name': 'Multi-echelon Inventory Optimization'},
+        {'id': '40.2', 'name': 'Multi-echelon Inventory Optimization (2.7.14)'},
     )
     DATABASE_TEMPLATES_NAMEID: Dict[str, str] = {
         'Advanced United States Greenfield Facility Selection (2.7.14)': '9877e82e-7749-11ee-a0df-0d9d492442f8',
         'Advanced United States Greenfield Facility Selection': '0c294262-394e-402c-80c2-cbdc2fba1cbc',
         'Anura New Model v2.7.14.1': 'anura_2_7_14_1_clean',
         'Anura New Model': 'anura_2_6_31_3_1_clean',
         'China Exit Risk Strategy (2.7.14)': '7ab48a82-7761-11ee-a0df-0d9d492442f8',
@@ -257,16 +258,16 @@
         'Global Sourcing - Cost To Serve': '23462acf-f6b2-43a5-bc45-78c3e29bf752',
         'Global Supply Chain Strategy (2.7.14)': 'd753d7ee-776f-11ee-a0df-0d9d492442f8',
         'Global Supply Chain Strategy': '93517785-3447-43c7-abf1-3f4545a41c83',
         'Global Sustainability Analysis (2.7.14)': 'e56669fa-776f-11ee-a0df-0d9d492442f8',
         'Global Sustainability Analysis': '71f9da77-edb9-4fad-b18e-684065109cac',
         'Lookups example for Warehousing Policies (2.7.14)': 'f5a1442a-776f-11ee-a0df-0d9d492442f8',
         'Lookups example for Warehousing Policies': '753501e5-395f-4411-8015-eb178484f356',
+        'Multi-echelon Inventory Optimization (2.7.14)': 'db4ebece-7a97-11ee-a0df-0d9d492442f8',
         'Multi-echelon Inventory Optimization': '12af7134-7a97-11ee-a0df-0d9d492442f8',
-        'Multi-echelon Inventory Optimization': 'db4ebece-7a97-11ee-a0df-0d9d492442f8',
         'Multi-Year Capacity Planning (2.7.14)': '06237a98-7770-11ee-a0df-0d9d492442f8',
         'Multi-Year Capacity Planning': '4903d266-7448-11ee-824e-0716e005474b',
         'Outbound Distribution Simulation (2.7.14)': '1837eb6a-7770-11ee-a0df-0d9d492442f8',
         'Outbound Distribution Simulation': 'f0dfdfff-85cd-4b13-a485-2ff61856234e',
         'Sensitivity At Scale (2.7.14)': 'f11740ce-7744-11ee-a0df-0d9d492442f8',
         'Sensitivity At Scale': 'ee051ce0-7441-11ee-824e-0716e005474b',
         'Tactical Capacity Optimization (2.7.14)': '2bbec910-7770-11ee-a0df-0d9d492442f8',
@@ -476,14 +477,35 @@
         try:
             with open(self.file_cache_store, 'wb') as f:
                 pickle.dump(self.__cache, f)
             return True
         except (FileNotFoundError, PermissionError, IOError, pickle.PicklingError):
             return False
 
+    def __calculate_sleep_time(self, secs: float) -> Tuple[int, int]:
+        '''Determine fast and slow polling rate in seconds.
+
+        :param secs: elapsed duration to determine to back off polling frequency
+        :type secs: float
+        :return: min and max seconds to wait for
+        '''
+
+        if secs < 30:
+            return 1, 3
+        elif secs < 60:
+            return 2, 4
+        elif secs < 180:
+            return 3, 6
+        elif secs < 300:
+            return 4, 8
+        elif secs < 600:
+            return 5, 10
+        else:
+            return 5, 15
+
     def __does_storage_exist(
         self,
         type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db', None],
         name: Optional[str] = None,
     ) -> bool:
         '''Verify the account have a specific storage type and or storage name.
 
@@ -737,15 +759,15 @@
         '''Add or update a cache entry to the cache store.
 
         :param key: cache key name to add or update
         :param value: bool, float, int, str: value to store
         :return: True if saved, False if not saved
         '''
 
-        if type(key) != str:
+        if isinstance(key, str) is False:
             warn(f'expected key {key} of type str, but got {type(key)}', stacklevel=2)
             return False
         elif len(key) == 0:
             warn(f'key {key} cannot be an empty str', stacklevel=2)
             return False
         if type(value) not in [bool, float, int, str]:
             warn(f'value must be of type bool|float|int|str, but got {type(key)}', stacklevel=2)
@@ -953,15 +975,15 @@
         :param name: name of secret, defaults to None
         :param category: type of secret such as geocode, ssh, custom, defaults to None
         :param desc: substring to match a secret description, defaults to None
         :return: true if param(s) provided match any secret.
         '''
 
         if name is None and category is None and desc is None:
-            warn(f'Must provide at least one param for name, category, or desc', stacklevel=2)
+            warn('Must provide at least one param for name, category, or desc', stacklevel=2)
             return False
 
         secret_list: list = self._secret_select_all(name, category, desc)
         return True if len(secret_list) >= 1 else False
 
     def _secret_select_all(
         self, name: Optional[str] = None, category: Optional[str] = None, desc: Optional[str] = None
@@ -1188,28 +1210,27 @@
         resp: Dict[str, Any] = self._fetch_json('delete', url)
         return resp
 
     def database_create(
         self,
         name: str,
         desc: Optional[str] = None,
-        template='anura_2_6_31_3_1_clean',
+        template='anura_2_7_14_1_clean',
         backups=True,
         labels: Optional[Dict[str, Any]] = None,
         tags: Optional[str] = None,
     ) -> Dict[str, Any]:
         '''Create a postgres database.
 
         :param name: name of postgres database to create
         :param desc: describe the purpose, defaults to None
-        :param template: database template id, defaults to anura_2_6_31_3_1_clean
+        :param template: database template id, defaults to anura_2_7_14_1_clean
             call `database_templates` method to list all templates available. Common template ids are:
             `empty`,
-            `anura_2_6_31_3_1_clean`,
-            `anura_2_7_14_1_clean`(For Tech Preview Users)
+            `anura_2_7_14_1_clean`
         :param backups: system will perform routine backups of the database, defaults to True
         :param labels: dict[str, Any]: key/value pairs, defaults to None
         :param tags: comma separated list of tags, defaults to None
         '''
 
         if template == 'empty':
             template = 'template1'
@@ -1710,14 +1731,90 @@
             elif time_delta > 200:
                 secs = 30
             if time_delta >= secs_max:
                 return False
             elif resp['status'] in self.JOBSTATES_TERMINAL or resp['status'] == stop_when:
                 return True
 
+    def util_job_monitor_set(
+        self,
+        wksp: str,
+        tag: str,
+        stop_when: Literal[
+            'underway',
+            'terminal',
+        ] = 'underway',
+        secs_max: int = 600,
+    ) -> bool:
+        '''Poll jobs by tag name until jobs status satisfies stop_when.
+
+        :param tag: tag name that applies to all jobs to be monitored
+        :param stop_when: stop polling when all jobs satisfy the target run state, defaults to underway
+        :param secs_max: hard stop when wait time is exhausted, defaults to 300
+
+        JOB TAG
+        -------
+        - All jobs submitted in last seven days are considered before applying the tag filter
+
+        STOP WHEN
+        ---------
+        `underway` encompasses starting, started, running, stopping, and canceling states
+        - abort if no jobs of the tag are found
+        - abort when jobs are in a terminal state
+
+        `terminal` encompasses done, stopped, cancelled, and error states
+
+        RETURN
+        ------
+        - `False` unsuccessful: no jobs of the tag are found or if secs_max is exceeded
+        - `True` success: all jobs satisfy stop_when target state
+        '''
+
+        if stop_when not in ('underway', 'terminal'):
+            raise ValueError('expecting stop_when underway or terminal')
+
+        if len(tag) == 0:
+            raise ValueError('tag cannot be zero length str')
+
+        # jobs found by tag
+        resp: Dict[str, Any] = self.wksp_jobs(wksp, tags=tag)
+        jobs: int = resp.get('count', 0)
+        if jobs == 0:
+            warn(f'Zero jobs with tag {tag} were submitted in the last seven days', stacklevel=2)
+            return False
+
+        # start polling
+        time_start: float = time.time()
+
+        while True:
+            delta: float = time.time() - time_start
+            min, max = self.__calculate_sleep_time(delta)
+            wait = randint(min, max)
+            time.sleep(wait)
+
+            resp = self.wksp_jobs(wksp, tags=tag)
+            if resp.get('crash'):
+                continue
+
+            # print(stop_when, tag, round(delta, 2), resp['statusCounts'])
+
+            all_jobs_terminal: bool = jobs == sum(
+                [resp['statusCounts'][state] for state in self.JOBSTATES_TERMINAL]
+            )
+
+            if all_jobs_terminal:
+                return True
+
+            if stop_when == 'underway' and resp['statusCounts']['submitted'] == 0:
+                # accommodates quick running jobs that can terminate early
+                return True
+
+            if delta >= secs_max:
+                return False
+
     def wksp_file_copy(
         self, wksp: str, file_path_src: str, file_path_dest: str, overwrite: bool = False
     ) -> dict:
         '''Copy a file within a workspace.
 
         :param wksp: workspace to download from
         :param file_path_src: file to copy from
```

## optilogic/pioneer/api/api_tests.py

```diff
@@ -31,15 +31,15 @@
 from io import StringIO
 from json import dumps, loads
 from numbers import Number
 from random import randint
 from re import fullmatch, match
 from sys import platform
 from threading import Thread
-from typing import Any, Dict, Final, List, Literal, Tuple, TypedDict, Optional, Union
+from typing import Any, Dict, Final, List, Literal, Tuple, TypedDict, Optional
 from uuid import uuid4
 
 
 class Templates(TypedDict):
     '''Typing database templates data structure.'''
 
     baseId: int
@@ -152,15 +152,15 @@
             cls.API.database_delete(d['name'])
 
         cache_store = cls.API._cache_store()
         keys_to_delete: list[str] = [key for key in cache_store.keys() if 'cache_' in key]
         for key in keys_to_delete:
             cls.API._cache_entry_delete(key)
 
-    def compare_words(self, str1: str, str2: str, echo: bool = False) -> List[str]:
+    def _compare_words(self, str1: str, str2: str, echo: bool = False) -> List[str]:
         '''Identify any word differences between two strings.
 
         :param str1: The first string for comparison.
         :param str2: The second string for comparison.
         :param echo: True to print diffs to stdout.
         '''
 
@@ -183,15 +183,15 @@
             if line.startswith(('+', '-')):
                 # Extract the plus and minus symbol from the word
                 symbol, word = line[:1], line[2:]
 
                 # Minus is unique to sequence 1, plus is unique to sequence 2
                 word_list = words1 if symbol == '-' else words2
 
-                # BUG OE-8786 index will return first match found
+                # TODO OE-9034 index will return first match found
                 word_index = word_list.index(word) if word in word_list else None
                 if word_index is None:
                     continue
 
                 # Get two words before and after the difference
                 start_idx = max(0, word_index - 2)
                 end_idx = min(len(word_list), word_index + 3)
@@ -200,72 +200,76 @@
                 char_position = ' '.join(word_list).find(context)
                 diffs.append(f'{symbol}{context} (position: {char_position})')
                 if echo:
                     print(f'{symbol}{context} (position: {char_position})')
 
         return diffs
 
-    def database_ensure_exist(self, name: str = 'pg_unittest') -> None:
+    def _database_ensure_exist(self, name: str = 'pg_unittest') -> None:
         '''Database must exist for db unit tests.'''
 
         # cache is for 10 seconds
         exists: bool = self.API.storagename_database_exists(name)
         if exists:
             resp: Dict[str, Any] = self.API.storage(name)
             assert resp.get('crash') is None
         else:
             self.API.database_create(name, desc='common db for unit tests', backups=False)
 
-    def databases_utilized_by_unittest(self, all: bool = False) -> List[Dict[str, Any]]:
+    def _databases_utilized_by_unittest(self, all: bool = False) -> List[Dict[str, Any]]:
         '''Which databases are used by unit tests.
 
         :param all: bool, False returns only temporary unittest databases
         '''
 
         dbs: List[Dict[str, Any]] = []
         if all is False:
             dbs = self.API._database_by_name(r'unittest.+\d{13}', wildcard=True)
         else:
             dbs = self.API._database_by_name('unittest', wildcard=True)
         return dbs
 
-    def date_isoformat(self, date_str: str) -> bool:
+    def _date_isoformat(self, date_str: str) -> bool:
         '''Verify date string isoformat.'''
 
         d: date
         try:
             d = date.fromisoformat(date_str)
             return isinstance(d, date)
         except ValueError:
             return False
 
-    def deserialize_pip_output(self, std_out: str) -> List[Dict[str, Any]]:
+    def _deserialize_pip_output(self, std_out: str) -> List[Dict[str, Any]]:
         '''Deserialization of `pip list --format json` from standard output.'''
 
+        if isinstance(std_out, str) is False:
+            raise TypeError(f'std_out must be a string, but got {type(std_out)}')
         if len(std_out) < 58:
             raise ValueError(f'len of std_out is {std_out}, too short')
+
         # remove cli earmarking from std_out
         m = match(r"(.|\n)+format\sjson'\}\n", std_out)
         if m is None:
-            raise ValueError(f'could not match std_out')
+            raise ValueError('could not match std_out')
+
         return loads(std_out.lstrip(m.group()))
 
-    def job_prereq(self) -> None:
+    def _job_prereq(self) -> None:
         '''For running test methods in isolation.'''
 
         resp = self.API.wksp_job_start(
             self.WKSP, self.py_quick, tags='unittest_prereq', resourceConfig='mini'
         )
         self.assertEqual(resp['result'], 'success')
         self.__jobkey_quick = resp['jobKey']
         # BUG ledger and metrics should be immediately available when job is running
         res: bool = self.API.util_job_monitor(self.WKSP, resp['jobKey'], stop_when='done')
         self.assertTrue(res)
 
-    def storage_common(self, d: Dict[str, Any]) -> None:
+    def _storage_common(self, d: Dict[str, Any]) -> None:
         '''Device attributes expected across afs, wksp, onedrive, and postgres storage devices.'''
 
         self.assertIsInstance(d['annotations'], dict)
         self.assertIsInstance(d['bytesUsed'], int)
         self.assertGreater(d['bytesUsed'], -1)
         self.assertIsInstance(d['created'], int)
         self.assertTrue(d['description'] is None or isinstance(d['description'], str))
@@ -283,70 +287,50 @@
                 self.assertIsInstance(s['name'], str)
                 self.assertIsInstance(s['path'], str)
                 self.assertIsInstance(s['type'], str)
         self.assertIsInstance(d['tags'], str)
         self.assertIsInstance(d['type'], str)
         self.assertIsInstance(d['updated'], int)
 
-        # DEBUG ONLY remove common keys
-        KEYS = (
-            'annotations',
-            'created',
-            'description',
-            'id',
-            'labels',
-            'lockoutReason',
-            'name',
-            'notes',
-            'secureFields',
-            'shortcuts',
-            'tags',
-            'updated',
-            'userId',
-        )
-
-        # for k in tuple(d.keys()):
-        #    if k in KEYS:
-        #        d.pop(k)
-
-    def storage_azure_afs(self, d: Dict[str, Any]) -> None:
+    def _storage_azure_afs(self, d: Dict[str, Any]) -> None:
         '''SSD device attributes for get device and devices.'''
 
-        self.storage_common(d)
+        self._storage_common(d)
         self.assertIsInstance(d['capacity'], int)
         self.assertEqual(d['capacity'], 100)
         self.assertIsInstance(d['internal'], bool)
         self.assertTrue(d['internal'])
         self.assertIsInstance(d['tier'], str)
         self.assertEqual(d['tier'], 'Premium')
 
-    def storage_azure_workspace(self, d: Dict[str, Any]) -> None:
+    def _storage_azure_workspace(self, d: Dict[str, Any]) -> None:
         '''Workspace device attributes for get device and devices.'''
 
-        self.storage_common(d)
+        self._storage_common(d)
         self.assertIsInstance(d['capacity'], int)
         self.assertTrue(4 <= d['capacity'] <= 512)  # default is 4 but some are custom
         self.assertIsInstance(d['internal'], bool)
         self.assertIsInstance(d['tier'], str)
         self.assertEqual(d['tier'], 'TransactionOptimized')
         self.assertIsInstance(d['workspaceKey'], str)
         self.assertEqual(len(d['workspaceKey']), 25)
         self.assertTrue(d['workspaceKey'].startswith('workspace'))
 
-    def storage_database(self, d: Dict[str, Any]) -> None:
+    def _storage_database(self, d: Dict[str, Any]) -> None:
         '''Database attributes for get device and devices.'''
 
-        self.storage_common(d)
+        self._storage_common(d)
         self.assertIsInstance(d['bytesUsedLastUpdated'], int)
         self.assertIsInstance(d['dbname'], str)
         self.assertIsInstance(d['defaultSchema'], str)
         self.assertIsInstance(d['host'], str)
         self.assertIsInstance(d['port'], int)
         self.assertIsInstance(d['schemaStatus'], str)
-        self.assertTrue(d['schemaStatus'] in ('error', 'invalid', 'valid'))  # BUG OE-8954
+        with self.subTest():
+            self.assertTrue(d['schemaStatus'] in ('error', 'invalid', 'valid'))  # BUG OE-8954
         self.assertIsInstance(d['schemaStatusLastUpdated'], Number)
         self.assertIsInstance(d['schemaVersion'], str)
         self.assertIsInstance(d['user'], str)
 
         # empty pg database vs anura schema
         if d['defaultSchema'].startswith('anura_2_'):
             self.assertRegex(d['schemaVersion'], r'2\.[4-9]\.\d+')
@@ -358,18 +342,18 @@
             )
             self.assertTrue(release_status)
             self.assertIsInstance(d['schemaStatusLastValidated'], Number)
         else:
             self.assertIn(d['defaultSchema'], ('"$user"', 'public'))
             self.assertTrue(len(d['schemaVersion']) == 0)
 
-    def storage_onedrive(self, d: dict) -> None:
+    def _storage_onedrive(self, d: dict) -> None:
         '''Onedrive storage attributes for get device and devices.'''
 
-        self.storage_common(d)
+        self._storage_common(d)
 
         self.assertIsInstance(d['authenticated'], int)
         dt: datetime = datetime.fromtimestamp(d['authenticated'] / 1000)
         self.assertGreaterEqual(dt.year, 2020)
 
         # connect is not in get devices call due to real time performance
         # self.assertIsInstance(d['connected'], bool)
@@ -396,15 +380,15 @@
         '''Get password uses a secret stream and input will not echo.'''
 
         if platform != 'linux':
             self.skipTest('only linux has timed inputs')
 
         with redirect_stdout(StringIO()) as out:
             try:
-                a = api.Api(auth_legacy=True, un=self.USERNAME, ut=True)
+                api.Api(auth_legacy=True, un=self.USERNAME, ut=True)
             except (EOFError, TimeoutError):
                 pass
             output: str = out.getvalue().strip()
 
         self.assertEqual(output.find('REQUIRED API User Password'), -1)
 
     def test_000_prereq(self) -> None:
@@ -581,24 +565,23 @@
                     self.assertIsNone(job['startDatetime'])
                     self.assertIsNone(job['endDatetime'])
                     # self.assertFalse(job['canHaveResult'])  # BUG 6710
 
     def test_account_jobs_active(self) -> None:
         '''Compare active account jobs count to all active wksp jobs.'''
 
+        # TODO OE-9033
         start_new_job: bool = bool(randint(0, 1))
         if start_new_job:
             self.API.wksp_job_start(
                 self.WKSP, self.py_sleep, tags='unittest_jobs_active', resourceConfig='mini'
             )
 
         active_account: int = 0
-        resp = self.API._account_jobs(
-            max_jobs=200
-        )  # BUG submitted counts as active therefore must account for excessive submitted jobs
+        resp = self.API._account_jobs(max_jobs=200)
         for job in resp['jobs']:
             if job['status'] in self.API.JOBSTATES_ACTIVE:
                 active_account += 1
 
         active_wksp: int = self.API._jobs_active
         self.assertEqual(active_account, active_wksp)
         if start_new_job:
@@ -614,24 +597,24 @@
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['storages'], list)
         with self.subTest():
             for device in resp['storages']:
                 d = OrderedDict(sorted(device.items()))
                 if d['type'] == 'azure_afs':
                     self.assertEqual(len(d), 16)
-                    self.storage_azure_afs(d)
+                    self._storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
                     self.assertEqual(len(d), 17)
-                    self.storage_azure_workspace(d)
+                    self._storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
                     self.assertEqual(len(d), 18)
-                    self.storage_onedrive(d)
+                    self._storage_onedrive(d)
                 elif d['type'] == 'postgres_db':
                     self.assertEqual(len(d), 24)
-                    self.storage_database(d)
+                    self._storage_database(d)
 
     def test_account_usage(self) -> None:
         '''Atlas and andromeda information.'''
 
         resp = self.API._account_usage()
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['andromeda'], dict)
@@ -652,15 +635,14 @@
         self.assertEqual(dt.month, now.month)
 
         self.assertIsInstance(resp['atlas'], dict)
         if (
             self.API._domain != 'https://api.optilogic.app'
             and resp['atlas'].get('lastLogin') is None
         ):
-            # BUG OE-7432
             self.assertEqual(len(resp['atlas']), 3)
         else:
             self.assertEqual(len(resp['atlas']), 4)
             self.assertIsInstance(resp['atlas']['lastLogin'], int)
             self.assertEqual(len(str(resp['atlas']['lastLogin'])), 13)
             dt: datetime = datetime.fromtimestamp(resp['atlas']['lastLogin'] / 1000)
             self.assertIsInstance(dt, datetime)
@@ -724,19 +706,19 @@
     def test_account_workspace_create_crash(self) -> None:
         '''Expected to not create the same workspace twice.'''
 
         resp = self.API.account_workspace_create('Studio')
         self.assertEqual(resp['crash'], True)
         self.assertEqual(resp['exception'].response.status_code, 400)
 
-    def test_account_workspace_delete(self):
+    def test_account_workspace_delete(self) -> None:
         '''Deleting a newly created workspace.'''
 
         with self.assertRaises(NotImplementedError):
-            resp = self.API.account_workspace_delete('delete_me')
+            self.API.account_workspace_delete('delete_me')
 
     def test_andromeda_configs(self) -> None:
         '''Memory and CPU configurations for Andromeda.'''
 
         NAMES = (
             'mini',
             '4XS',
@@ -851,15 +833,15 @@
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['message'], 'Job submitted')
         self.assertIsInstance(resp['jobKey'], str)
         self.assertIsInstance(resp['jobInfo'], dict)
         self.assertEqual(resp['jobInfo']['command'], cmd)
         self.assertTrue(tag in resp['jobInfo']['tags'])
         self.assertEqual(resp['jobInfo']['resourceConfig']['name'].lower(), size)
-        self.assertEqual(resp['jobInfo']['timeout'], secs)  # TODO OE-8733 not respected
+        self.assertEqual(resp['jobInfo']['timeout'], secs)
 
     def test_andromeda_utility_run_bad_utility_name(self) -> None:
         '''Attempt to run a CLI utility that does not exist in Andromeda.'''
 
         resp: Dict[str, Any] = self.API.wksp_job_start(
             wksp=self.WKSP,
             command='utility',
@@ -1017,29 +999,31 @@
         timestamp: float = self.API._cache_entry_get_updated(k)
         self.assertIsInstance(timestamp, float)
         self.assertEqual(timestamp, -1.0)
 
     def test_database_clone(self) -> None:
         '''Duplicate a postgres database.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         name_new: str = f'pg_unittest_{time.perf_counter_ns()}'
-        name: str = f'pg_unittest'
+        name: str = 'pg_unittest'
         resp: Dict[str, str] = self.API.database_clone(name, name_new)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp.keys()), 3)
         self.assertIsInstance(resp['jobKey'], str)
         self.assertIsInstance(resp['storageId'], str)
         self.assertTrue(bool(fullmatch(self.API.re_uuid4, resp['jobKey'], flags=2)))
         self.assertTrue(bool(fullmatch(self.API.re_uuid4, resp['storageId'], flags=2)))
 
-        # TODO verify database was actually created and time to complete system jobs
+        # TODO OE-9036 explicit database created verification via system jobs
         # 1) db_clone
         # 2) db_analyze
+
+        # 1-2) implicit wait for db_clone, and db_analyze to complete
         db_src: Dict[str, Any] = self.API.database(name)
         db_clone: Dict[str, Any] = {}
         ready: bool = False
         while ready is False:
             db_clone = self.API.database(name_new)
             if db_clone.get('lockoutReason') is None:
                 ready = True
@@ -1047,15 +1031,15 @@
             time.sleep(10)
 
         # 3) db now exists
         # self.assertEqual(db_clone['annotations']['sharedByUserName'], self.API.auth_username)
         self.assertEqual(db_clone['description'], 'Duplicated via OptiPy')
         self.assertIsNone(db_clone['annotations'].get('shared'))
         dbc = OrderedDict(sorted(db_clone.items()))
-        self.storage_database(dbc)
+        self._storage_database(dbc)
 
         bytes_diff: int = abs(db_src['bytesUsed'] - db_clone['bytesUsed'])
         bytes_diff_percent: float = bytes_diff / db_src['bytesUsed']
         self.assertLess(bytes_diff_percent, 1.0)
 
         self.assertEqual(db_src['defaultSchema'], db_clone['defaultSchema'])
         self.assertEqual(db_src['notes'], db_clone['notes'])
@@ -1162,15 +1146,15 @@
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['storageId'], str)
         self.assertEqual(len(resp['storageId']), 36)
 
         # verify database was created
         db: Dict[str, Any] = self.API.database(db_name)
-        self.storage_database(db)
+        self._storage_database(db)
         self.assertGreater(db['tags'].find('no-backup'), -1)
         self.assertDictEqual(db['labels'], {'no-backup': True})
 
         # delete database
         resp = self.API.storage_delete(db_name)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
@@ -1188,26 +1172,25 @@
 
         self.API._cache_entry_upsert(k, True)
 
         for template_name, tid in self.API.DATABASE_TEMPLATES_NAMEID.items():
             # create database
             db_name: str = f'pg_unittest_all_templates_{tid}_{time.perf_counter_ns()}'
             db_desc: str = f'unittest {db_name}'
-            # TODO make tid always be friendly str instead of guid
             resp: dict = self.API.database_create(
                 db_name, desc=db_desc, template=tid, backups=False, labels={'no-backup': True}
             )
             self.assertIsInstance(resp['result'], str)
             self.assertEqual(resp['result'], 'success')
             self.assertIsInstance(resp['storageId'], str)
             self.assertEqual(len(resp['storageId']), 36)
 
             # verify
             db: Dict[str, Any] = self.API.database(db_name)
-            self.storage_database(db)
+            self._storage_database(db)
             self.assertEqual(db['id'], resp['storageId'])
             self.assertEqual(db['name'], db_name)
             self.assertEqual(db['description'], db_desc)
             self.assertEqual(db['schemaStatus'], 'valid')
 
             print(f'\n\n{tid}, {template_name},')
             print(f"  tags: {len(db['tags'])},\n  {db['labels']}\n  {db['annotations']}")
@@ -1438,32 +1421,32 @@
         self.assertIsNone(resp.get('multipleFiles'))
         self.assertEqual(resp['sourceGroup'], G)
         self.assertEqual(resp['sourceList'], TBL)
         self.assertEqual(resp['sourceQuery'], Q)
         self.assertTrue(bool(fullmatch(self.API.re_uuid4, resp['storageId'])))
         self.assertEqual(resp['storageName'], DB)
 
-        # TODO
+        # TODO OE-8120
         # 1) poll job key status
         # 2) download zip
         # 3) unpack zip
         # 4) verify csv output folder: csv, source_query, metadata
         # 5) if xls format, verify xls output file and readme.txt
 
     def test_database_no_backups(self) -> None:
         '''Unittest databases should not be eligible for system backups.'''
 
-        dbs: List[Dict[str, Any]] = self.databases_utilized_by_unittest(all=True)
+        dbs: List[Dict[str, Any]] = self._databases_utilized_by_unittest(all=True)
         for db in dbs:
             self.assertGreater(db['tags'].find('no-backup'), -1)
 
     def test_database_objects(self) -> None:
         '''Tables and views with stats.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         resp: Dict[str, Any] = self.API.database_objects('pg_unittest')
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['schemas'], list)
         for schema in resp['schemas']:
             self.assertEqual(len(schema.keys()), 6)
             self.assertIsInstance(schema['isDefault'], bool)
             self.assertIsInstance(schema['name'], str)
@@ -1517,32 +1500,32 @@
                 self.assertIsInstance(v['notes'], str)
                 self.assertIsInstance(v['schemaName'], str)
                 self.assertRegex(v['schemaName'], r'anura_2_[4-9]')
                 self.assertIsInstance(v['schemaVersion'], str)
                 self.assertRegex(v['schemaVersion'], r'2\.[4-9]\.\d+')
                 self.assertIsInstance(v['status'], str)
                 self.assertIn(v['status'], ANURA_STATUS)
-                self.assertTrue(self.date_isoformat(v['generalAvailabilityDate']))
+                self.assertTrue(self._date_isoformat(v['generalAvailabilityDate']))
                 if v.get('defaultMigrationDate'):
                     self.assertIsInstance(v['defaultMigrationDate'], str)
-                    self.assertTrue(self.date_isoformat(v['defaultMigrationDate']))
+                    self.assertTrue(self._date_isoformat(v['defaultMigrationDate']))
                 if v.get('endOfLifeDate'):
                     self.assertIsInstance(v['endOfLifeDate'], str)
-                    self.assertTrue(self.date_isoformat(v['endOfLifeDate']))
+                    self.assertTrue(self._date_isoformat(v['endOfLifeDate']))
                 if v.get('limitedAvailabilityDate'):
                     self.assertIsInstance(v['limitedAvailabilityDate'], str)
-                    self.assertTrue(self.date_isoformat(v['limitedAvailabilityDate']))
+                    self.assertTrue(self._date_isoformat(v['limitedAvailabilityDate']))
                 if v.get('techPreviewDate'):
                     self.assertIsInstance(v['techPreviewDate'], str)
-                    self.assertTrue(self.date_isoformat(v['techPreviewDate']))
+                    self.assertTrue(self._date_isoformat(v['techPreviewDate']))
 
     def test_database_tables(self) -> None:
         '''List of schemas and tables.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         resp = self.API.database_tables(db['name'])
         self.assertIsInstance(resp['result'], str)
         for schema in resp['schemas']:
             self.assertIsInstance(schema['name'], str)
             self.assertIsInstance(schema['tables'], int)
             self.assertIsInstance(schema['is_default_schema'], bool)
@@ -1621,15 +1604,15 @@
         # 2023-06-02
         templates: Dict[float, Any] = {
             2.1: {
                 "id": 2.1,
                 "name": "Empty Database",
                 "templateType": "templateSet",
                 "pgTemplateName": "template1",
-                "schema": "anura",
+                "schema": "none",
                 "baselineSchemaVersion": "1.0",
                 "baseId": 2,
                 "templateVersion": "1.0",
                 "shortDescription": "Database with no schema objects.",
                 "longDescription": "Database with no schema objects.",
                 "isDefault": True,
                 "tags": [],
@@ -2368,15 +2351,15 @@
                 "isDefault": True,
                 "tags": ["User Contribution", "Simulation", "Optimization"],
                 "media": [{"url": "https://youtu.be/VR9ecjiDbEo", "type": "yt"}],
                 "schemaReleaseStatus": "stable",
             },
             40.2: {
                 "id": 40.2,
-                "name": "Multi-echelon Inventory Optimization",
+                "name": "Multi-echelon Inventory Optimization (2.7.14)",
                 "templateType": "templateSet",
                 "pgTemplateName": "db4ebece-7a97-11ee-a0df-0d9d492442f8",
                 "schema": "anura",
                 "baselineSchemaVersion": "2.7.14",
                 "baseId": 40,
                 "templateVersion": "2.7.14",
                 "shortDescription": "Optimize supply chain inventory across echelons with Optilogics' simulation-based strategy, enhancing accuracy and cost-effectiveness.",
@@ -2430,15 +2413,15 @@
             self.assertEqual(templates[t['id']]['baseId'], t['baseId'])
             self.assertEqual(
                 templates[t['id']]['baselineSchemaVersion'], t['baselineSchemaVersion']
             )
             self.assertEqual(templates[t['id']]['id'], t['id'])
             self.assertEqual(templates[t['id']]['isDefault'], t['isDefault'])
             if templates[t['id']]['longDescription'] != t['longDescription']:
-                diff: List[str] = self.compare_words(
+                diff: List[str] = self._compare_words(
                     t['longDescription'], templates[t['id']]['longDescription']
                 )
                 formatted_diff: str = '\n'.join(diff)
                 self.fail(f"{t['name']} longDescription mismatch: \n{formatted_diff}")
 
             self.assertEqual(templates[t['id']]['longDescription'], t['longDescription'])
             self.assertEqual(templates[t['id']]['media'], t['media'])
@@ -2468,22 +2451,23 @@
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertEqual(resp['count'], 38)
         self.assertIsInstance(resp['templates'], list)
         template_count: int = len(resp['templates'])
         self.assertEqual(template_count, 38)
         self.assertEqual(template_count, resp['count'])
-        # self.assertEqual(template_count, len(self.API.DATABASE_TEMPLATES))  # BUG OE-9007
+        self.assertEqual(template_count, len(self.API.DATABASE_TEMPLATES))
 
         TEMPLATE_KEYS: Tuple[str, ...] = ('id', 'is_default', 'name', 'role', 'schema')
         for t in resp['templates']:
             self.assertIsInstance(t, dict)
             for k in t.keys():
                 self.assertIsInstance(k, str)
-                self.assertIn(t['id'], self.API.DATABASE_TEMPLATES)  # BUG OE-9007
+                self.assertIn(t['id'], self.API.DATABASE_TEMPLATES)
+                self.assertIn(k, TEMPLATE_KEYS)
 
     def test_database_templates_legacy_by_name(self) -> None:
         '''Look up the database template id by case-insensitive template name.'''
 
         template_names: List[str] = [k for k in self.API.DATABASE_TEMPLATES_NAMEID.keys()]
 
         for name in template_names:
@@ -2524,70 +2508,69 @@
             print('\nStatic known ids not in legacy api call', static_not_in_legacy)
             for i in static_not_in_legacy:
                 for name, id in self.API.DATABASE_TEMPLATES_NAMEID.items():
                     if i == id:
                         print(name, id)
                         break
 
-            # BUG OE-9007
             old_not_in_static = old - static_ids
             print('\nLegacy api template ids not in static known ids', old_not_in_static)
             for i in old_not_in_static:
                 for t in legacy['templates']:
                     if i == t['id']:
                         print(t['name'], t['id'])
                         break
 
         self.assertEqual(len(diff), 0)
 
     def test_ip_address_allow(self) -> None:
         '''Whitelist ip address.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
         resp: Dict[str, str] = self.API.ip_address_allow(database_name=db['name'], ip='127.0.0.0')
 
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'accepted')
         self.assertIn('five-minute delay', resp['message'])
 
     def test_ip_address_allow_invalid(self) -> None:
         '''Unable to whitelist, ip address is invalid.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
         r: Dict[str, Any] = self.API.ip_address_allow(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
         self.assertEqual(resp['result'], 'error')
 
     def test_ip_address_allowed(self) -> None:
         '''Ip address is whitelisted.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
         resp: Dict[str, Any] = self.API.ip_address_allowed(database_name=db['name'], ip='127.0.0.0')
         self.assertIsInstance(resp['allowed'], bool)
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['allowed'], True)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'success')
         self.assertIn('is in the firewall', resp['message'])
 
     def test_ip_address_allowed_invalid(self) -> None:
         '''Ip address is invalid.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
         r: Dict[str, Any] = self.API.ip_address_allowed(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
 
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
@@ -2783,15 +2766,15 @@
             self.assertEqual(resp['result'], 'success')
             self.assertEqual(resp['id'], s['id'])
             self.assertEqual(resp['name'], s['name'])
 
     def test_sql_connect_info(self) -> None:
         '''Get the connection information for a sql storage item.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         pg = self.API.account_storage_device('postgres_db')
         resp = self.API.sql_connection_info(pg['name'])
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp['raw']), 6)
         self.assertIsInstance(resp['raw']['host'], str)
         self.assertTrue(
             resp['raw']['host'].endswith('postgres.database.azure.com')
@@ -2818,26 +2801,26 @@
         with self.subTest():
             for device in devices['storages']:
                 resp: Dict[str, Any] = self.API.storage(device['name'])
                 d = OrderedDict(sorted(resp.items()))
                 self.assertEqual(d['result'], 'success')
                 if d['type'] == 'azure_afs':
                     self.assertEqual(len(d), 17)
-                    self.storage_azure_afs(d)
+                    self._storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
                     self.assertEqual(len(d), 18)
-                    self.storage_azure_workspace(d)
+                    self._storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
                     self.assertEqual(len(d), 20)
-                    self.storage_onedrive(d)
+                    self._storage_onedrive(d)
                     self.assertIsInstance(d['connected'], bool)
                 elif d['type'] == 'postgres_db':
                     # storage item contains an additional result key
                     self.assertEqual(len(d), 25)
-                    self.storage_database(d)
+                    self._storage_database(d)
 
     def test_storage_attr(self) -> None:
         '''Device attributes: annotations, label, and tag.'''
 
         attrs: Tuple[Literal['annotations'], Literal['labels'], Literal['tags']] = (
             'annotations',
             'labels',
@@ -2868,15 +2851,15 @@
         '''Delete storage device.'''
 
         raise NotImplementedError
 
     def test_sql_query(self) -> None:
         '''Test sql statement execution.'''
 
-        self.database_ensure_exist()
+        self._database_ensure_exist()
         pg = self.API.account_storage_device(type='postgres_db')
         resp = self.API.sql_query(
             database_name=pg['name'], query='SELECT datname FROM pg_database;'
         )
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['rowCount'], int)
         self.assertGreaterEqual(resp['rowCount'], 1)
@@ -2913,14 +2896,43 @@
             self.API.util_job_monitor(self.WKSP, 'invalid')
         with self.assertRaises(ValueError):
             self.API.util_job_monitor(self.WKSP, '633e372-337a-454c-aae4-10084ea5bac6')
         # valid but job key does not exist
         resp: bool = self.API.util_job_monitor(self.WKSP, '00000000-0000-0000-0000-000000000000')
         self.assertFalse(resp)
 
+    def test_util_job_monitor_set(self) -> None:
+        '''Monitor jobs by tag.'''
+
+        resp: Dict[str, Any] = {}
+        jobs_max: int = 9
+        secs: int = 10
+
+        tag_time: float = int(time.time())
+        tag: str = f'monitor_set_{tag_time}'
+
+        for _ in range(jobs_max):
+            # spin up a few jobs to create load to evaluate
+            resp = self.API.wksp_job_start(
+                self.WKSP, self.py_sleep, tags=tag, timeout=secs, resourceConfig='mini'
+            )
+            if resp.get('crash'):
+                jobs_max -= 1
+                continue
+
+        success: bool = self.API.util_job_monitor_set(self.WKSP, tag)
+        self.assertTrue(success)
+
+    def test_util_job_monitor_set_quick(self) -> None:
+        '''Unittest_prereq tag to quickly check terminal state.'''
+
+        # prereq job runs before any unittest and will be in done state
+        success: bool = self.API.util_job_monitor_set(self.WKSP, 'unittest_prereq', 'terminal')
+        self.assertTrue(success)
+
     def test_wksp_file_copy(self) -> None:
         '''Make a copy of a file within a workspace.'''
 
         src: str = self.py_sleep
         dest: str = f'{self.dir_testdata_remote}/cp_test.txt'
         resp = self.API.wksp_file_copy(
             self.WKSP, file_path_src=src, file_path_dest=dest, overwrite=True
@@ -3249,15 +3261,14 @@
         type_strs: List[str] = sorted([t.__str__() for t in type_tup])
 
         job = self.API.wksp_job_start(
             self.WKSP,
             self.py_sidecar,
             resourceConfig='mini',
             tags='unittest',
-            preview_image=True,  # TODO drop preview_image
         )
         res: bool = self.API.util_job_monitor(self.WKSP, job['jobKey'], stop_when='done')
         self.assertTrue(res)
 
         resp = self.API.wksp_job_ledger(self.WKSP, job['jobKey'])
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
@@ -3289,15 +3300,15 @@
             if r['key'] == '96k':
                 self.assertEqual(len(r['message']), 32_000)  # OE-8083 truncate
 
     def test_wksp_job_metrics(self) -> None:
         '''Get one second cpu and memory sampling of a job.'''
 
         if len(self.__jobkey_quick) == 0:
-            self.job_prereq()
+            self._job_prereq()
         resp = self.API.wksp_job_metrics(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
@@ -3327,15 +3338,15 @@
         self.assertIsInstance(resp['records'][0]['memoryResident'], float)
         self.assertIsInstance(resp['records'][0]['processCount'], int)
 
     def test_wksp_job_metrics_max(self) -> None:
         '''Get peak cpu and memory stats of a job.'''
 
         if len(self.__jobkey_quick) == 0:
-            self.job_prereq()
+            self._job_prereq()
         resp = self.API.wksp_job_metrics_max(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
         self.assertIsInstance(resp['max']['memoryResident'], float)
         self.assertIsInstance(resp['max']['memoryAvailable'], int)
@@ -3431,86 +3442,94 @@
         )
         for key in resp['jobInfo'].keys():
             self.assertIn(key, job_info_keys)
 
     def test_wksp_job_start_preview(self) -> None:
         '''Create a job using andromeda preview image and compare to stable.'''
 
+        # 1) start preview job
         job_preview: Dict[str, Any] = self.API.wksp_job_start(
             self.WKSP,
             file_path=self.py_bash,
             commandArgs="'pip list -v --format json'",
             resourceConfig='mini',
             tags='unittest',
             preview_image=True,
         )
         self.assertEqual(job_preview['result'], 'success')
 
+        # 2) start stable job
         job_stable: Dict[str, Any] = self.API.wksp_job_start(
             self.WKSP,
             file_path=self.py_bash,
             commandArgs="'pip list -v --format json'",
             resourceConfig='mini',
             tags='unittest',
             preview_image=False,
         )
         self.assertEqual(job_stable['result'], 'success')
 
+        # 3) wait for jobs to finish
         done: bool = self.API.util_job_monitor(
             self.WKSP, job_key=job_preview['jobKey'], stop_when='done'
         )
         self.assertTrue(done)
 
-        # preview image
-        std_out: str = self.API.wksp_job_file_result(self.WKSP, job_preview['jobKey'])
-        self.assertIsInstance(std_out, str)
-        jsn: List[Dict[str, str]] = self.deserialize_pip_output(std_out)
-        pkgs_preview: Dict[str, Dict[str, str]] = {d['name']: d for d in jsn}
-        self.assertRegex(pkgs_preview['anura']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['costtoserve']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['dendro']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['frogspawn']['version'], r'2\.[6-7]\.\d+')
-        self.assertRegex(pkgs_preview['hopper']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['neo']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['optiengines']['version'], r'0\.\d\.\d+')
-        self.assertRegex(pkgs_preview['optilogic']['version'], r'2\.[8-9]\.\d')
-        self.assertRegex(pkgs_preview['riskrating']['version'], r'2\.7\.\d+')
-        self.assertRegex(pkgs_preview['scenarioexecution']['version'], r'1\.1\.([6-9]|1\d)')
-        self.assertRegex(pkgs_preview['throg']['version'], r'2\.7\.\d+')
-
-        # stable image
         done: bool = self.API.util_job_monitor(
             self.WKSP, job_key=job_stable['jobKey'], stop_when='done'
         )
         self.assertTrue(done)
+
+        # 4) Deserialize preview pip stdout into dict
+        std_out: str = self.API.wksp_job_file_result(self.WKSP, job_preview['jobKey'])
+        jsn: List[Dict[str, str]] = self._deserialize_pip_output(std_out)
+        pkgs_preview: Dict[str, Dict[str, str]] = {d['name']: d for d in jsn}
+
+        # 5) Deserialize stable pip stdout into dict
         std_out: str = self.API.wksp_job_file_result(self.WKSP, job_stable['jobKey'])
-        self.assertIsInstance(std_out, str)
-        jsn = self.deserialize_pip_output(std_out)
+        jsn = self._deserialize_pip_output(std_out)
         pkgs_stable: Dict[str, Dict[str, str]] = {d['name']: d for d in jsn}
-        self.assertRegex(pkgs_stable['anura']['version'], r'2\.6\.([3][6-9]|4\d)')
-        self.assertRegex(pkgs_stable['costtoserve']['version'], r'2\.6\.([2][4-9]|4\d)')
-        self.assertRegex(pkgs_stable['dendro']['version'], r'2\.6\.([6-9]|1\d)')
-        self.assertRegex(pkgs_stable['frogspawn']['version'], r'2\.6\.(\d|1\d)')
-        self.assertRegex(pkgs_stable['neo']['version'], r'2\.6\.(1[8-9]|2\d)')
-        self.assertRegex(pkgs_stable['optiengines']['version'], r'0\.\d\.\d+')
-        self.assertRegex(pkgs_stable['optilogic']['version'], r'2\.[8-9]\.\d')
-        self.assertRegex(pkgs_stable['riskrating']['version'], r'2\.6\.(9|1\d)')
-        self.assertRegex(pkgs_stable['scenarioexecution']['version'], r'1\.1\.([3-9]|1\d)')
-        self.assertRegex(pkgs_stable['throg']['version'], r'2\.6\.(9|1\d)')
 
+        # 6) Package differences
         diff: Dict[str, str] = {}
         for pkg in pkgs_stable:
             if pkgs_preview.get(pkg) is None:
                 diff[pkg] = 'not found in preview image'
             elif pkgs_stable[pkg]['version'] != pkgs_preview[pkg]['version']:
                 diff[pkg] = f"{pkgs_stable[pkg]['version']} != {pkgs_preview[pkg]['version']}"
         if diff:
             # diffs are expecting when preview image contains release candidates
             print(f'stable vs preview\n{dumps(diff, indent=2, sort_keys=True)}')
 
+        # preview image
+        self.assertRegex(pkgs_preview['anura']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['costtoserve']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['dendro']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['frogspawn']['version'], r'2\.[7-9]\.\d+')
+        self.assertRegex(pkgs_preview['hopper']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['neo']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['optiengines']['version'], r'0\.\d\.\d+')
+        self.assertRegex(pkgs_preview['optilogic']['version'], r'2\.[8-9]\.\d')
+        self.assertRegex(pkgs_preview['riskrating']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_preview['scenarioexecution']['version'], r'1\.1\.[6-9]')
+        self.assertRegex(pkgs_preview['throg']['version'], r'2\.7\.\d+')
+
+        # stable image
+        self.assertRegex(pkgs_stable['anura']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['costtoserve']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['dendro']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['frogspawn']['version'], r'2\.[7-9]\.\d+')
+        self.assertRegex(pkgs_stable['hopper']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['neo']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['optiengines']['version'], r'0\.\d\.\d+')
+        self.assertRegex(pkgs_stable['optilogic']['version'], r'2\.[8-9]\.\d')
+        self.assertRegex(pkgs_stable['riskrating']['version'], r'2\.7\.\d+')
+        self.assertRegex(pkgs_stable['scenarioexecution']['version'], r'1\.1\.[6-9]')
+        self.assertRegex(pkgs_stable['throg']['version'], r'2\.7\.\d+')
+
     def test_wksp_job_start_sample(self) -> None:
         '''Create job api call. The response time is the slow and fails often with 504s.'''
 
         max: int = int(self.API.account_info()['limits']['concurrentJobs'] * 0.5)
         min: int = 10
         job_count: int = max if max > min else min
         jobs: List[str] = []
@@ -3835,14 +3854,15 @@
             filenames.append(filename)
             filepaths.append(file_path)
 
         # verify uploaded files arrived
         up_arrived: bool = False
         up_count_verified: int = 0
         up_start: float = time.perf_counter()
+
         while up_arrived is False and time.perf_counter() - up_start < 30:
             resp = self.API.wksp_files('Studio', str(tag))
             if resp.get('crash'):
                 continue
 
             up_count_verified = resp.get('count', 0)
             print(
```

## Comparing `optilogic-2.8.4.dist-info/LICENSE` & `optilogic-2.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `optilogic-2.8.4.dist-info/METADATA` & `optilogic-2.9.0.dist-info/METADATA`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: optilogic
-Version: 2.8.4
+Version: 2.9.0
 Summary: Tools for interfacing with Optilogic Jobs and APIs
 Home-page: https://optilogic.com
 Author: Optilogic
 Author-email: support@optilogic.com
 License: MIT
 Project-URL: Documentation, https://api-docs.optilogic.app/documentation
 Keywords: mip,mixed integer programming,network optimization,optimization,risk,simulation,supply chain design
```

## Comparing `optilogic-2.8.4.dist-info/RECORD` & `optilogic-2.9.0.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 optilogic/__init__.py,sha256=t3hXtM_MyajzEyk-V1xxM1ZzFZCZtADNjJvvShxdl4A,22
 optilogic/pioneer/__init__.py,sha256=ELVJEdWM5Ia28-LjwVes2qvd7nGkORgVOw0g21vsGPU,48
 optilogic/pioneer/api/__init__.py,sha256=kaCK0y1h6dN5NhQnehgpJCEwyihVDPuuQ7xgtexVric,20
-optilogic/pioneer/api/api.py,sha256=iS4bpaPIi1yxjdcbl-Rd5Hx586JZBSBC1Z39JmeyyjU,88216
-optilogic/pioneer/api/api_tests.py,sha256=6L_9BUmYu8sx8fpYvYoSV0qNZF9Ve7mJAWj0_LxnEPk,197541
+optilogic/pioneer/api/api.py,sha256=WDx_DSdgvcCGsuG2cwHnnTHRMr61BGOr4DpU5XLlsuU,91380
+optilogic/pioneer/api/api_tests.py,sha256=J-prjnfnarT9Vz0LLdRVUbMTytmkx7H2CkqYbeNrGQs,198263
 optilogic/pioneer/api/quick_tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py,sha256=S6nxwIvWFqBo1tsFRi0D9VBD8ohBDhb42aBzXkKNgw4,5111
 optilogic/pioneer/api/quick_tests/bash.py,sha256=WPnbdoPVGxOwdMlsk-IyxVWeqXae5DbdN2bpMzNgVCQ,317
 optilogic/pioneer/api/quick_tests/quick.py,sha256=Yf_jhZbx5Nx5W3Dc_iJz8t70KvMDBn7xVTMmNvBn5gM,209
 optilogic/pioneer/api/quick_tests/sidecar.py,sha256=-qXaNpqtzGju9Lins93KoiFWKjEfr0Prba7ivPOsUaA,701
 optilogic/pioneer/api/quick_tests/sleep.py,sha256=CSb8T0inq6NAlDqW321vFCBmldPIiiGWfAU98OwUmV4,577
 optilogic/pioneer/job_utils/__init__.py,sha256=ewS513f69vgJB7L8QZIYtwl6umBFDtOqM52Ms0pAzy4,131
 optilogic/pioneer/job_utils/job_utils.py,sha256=JF1gXaxjBLmi6igMhHMLmk5XUpXBwa_top-CxAM1uL0,3707
-optilogic-2.8.4.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
-optilogic-2.8.4.dist-info/METADATA,sha256=TxDNySbkRmM93zc7pyGJEOFVx9sQmnjwB3Rb0dOi-mA,759
-optilogic-2.8.4.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
-optilogic-2.8.4.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
-optilogic-2.8.4.dist-info/RECORD,,
+optilogic-2.9.0.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
+optilogic-2.9.0.dist-info/METADATA,sha256=pBPAQ72n8z4MA0VxPnMyNxBhAGF7obozq-uk64byjY8,759
+optilogic-2.9.0.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
+optilogic-2.9.0.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
+optilogic-2.9.0.dist-info/RECORD,,
```

