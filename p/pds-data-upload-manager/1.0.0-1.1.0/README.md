# Comparing `tmp/pds_data_upload_manager-1.0.0-py3-none-any.whl.zip` & `tmp/pds_data_upload_manager-1.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 28384 bytes, number of entries: 23
--rw-r--r--  2.0 unx        6 b- defN 24-Mar-07 16:25 pds/ingress/VERSION.txt
--rw-r--r--  2.0 unx      320 b- defN 24-Mar-07 16:23 pds/ingress/__init__.py
--rw-r--r--  2.0 unx     1910 b- defN 24-Mar-07 16:23 pds/ingress/authorizer/README.md
--rw-r--r--  2.0 unx     3926 b- defN 24-Mar-07 16:23 pds/ingress/authorizer/index.js
--rw-r--r--  2.0 unx     8692 b- defN 24-Mar-07 16:23 pds/ingress/authorizer/package-lock.json
--rw-r--r--  2.0 unx      322 b- defN 24-Mar-07 16:23 pds/ingress/authorizer/package.json
--rw-r--r--  2.0 unx    10618 b- defN 24-Mar-07 16:23 pds/ingress/client/pds_ingress_client.py
--rw-r--r--  2.0 unx     5283 b- defN 24-Mar-07 16:23 pds/ingress/service/pds_ingress_app.py
--rw-r--r--  2.0 unx      454 b- defN 24-Mar-07 16:23 pds/ingress/service/config/bucket-map.yaml
--rw-r--r--  2.0 unx     2688 b- defN 24-Mar-07 16:23 pds/ingress/util/auth_util.py
--rw-r--r--  2.0 unx      555 b- defN 24-Mar-07 16:23 pds/ingress/util/conf.default.ini
--rw-r--r--  2.0 unx     1975 b- defN 24-Mar-07 16:23 pds/ingress/util/config_util.py
--rw-r--r--  2.0 unx    11133 b- defN 24-Mar-07 16:23 pds/ingress/util/log_util.py
--rw-r--r--  2.0 unx     1077 b- defN 24-Mar-07 16:23 pds/ingress/util/node_util.py
--rw-r--r--  2.0 unx     3815 b- defN 24-Mar-07 16:23 pds/ingress/util/path_util.py
--rw-r--r--  2.0 unx    10480 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     4811 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/METADATA
--rw-r--r--  2.0 unx     1632 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/NOTICE.txt
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       82 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        4 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-Mar-07 16:23 pds_data_upload_manager-1.0.0.dist-info/zip-safe
--rw-rw-r--  2.0 unx     2122 b- defN 24-Mar-07 16:25 pds_data_upload_manager-1.0.0.dist-info/RECORD
-23 files, 71998 bytes uncompressed, 24856 bytes compressed:  65.5%
+Zip file size: 29583 bytes, number of entries: 23
+-rw-r--r--  2.0 unx        6 b- defN 24-Apr-25 20:02 pds/ingress/VERSION.txt
+-rw-r--r--  2.0 unx      320 b- defN 24-Apr-25 20:01 pds/ingress/__init__.py
+-rw-r--r--  2.0 unx     1910 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/README.md
+-rw-r--r--  2.0 unx     3926 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/index.js
+-rw-r--r--  2.0 unx     8692 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/package-lock.json
+-rw-r--r--  2.0 unx      322 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/package.json
+-rw-r--r--  2.0 unx    12328 b- defN 24-Apr-25 20:01 pds/ingress/client/pds_ingress_client.py
+-rw-r--r--  2.0 unx     7714 b- defN 24-Apr-25 20:01 pds/ingress/service/pds_ingress_app.py
+-rw-r--r--  2.0 unx      454 b- defN 24-Apr-25 20:01 pds/ingress/service/config/bucket-map.yaml
+-rw-r--r--  2.0 unx     2688 b- defN 24-Apr-25 20:01 pds/ingress/util/auth_util.py
+-rw-r--r--  2.0 unx      555 b- defN 24-Apr-25 20:01 pds/ingress/util/conf.default.ini
+-rw-r--r--  2.0 unx     1975 b- defN 24-Apr-25 20:01 pds/ingress/util/config_util.py
+-rw-r--r--  2.0 unx    11032 b- defN 24-Apr-25 20:01 pds/ingress/util/log_util.py
+-rw-r--r--  2.0 unx     1077 b- defN 24-Apr-25 20:01 pds/ingress/util/node_util.py
+-rw-r--r--  2.0 unx     3815 b- defN 24-Apr-25 20:01 pds/ingress/util/path_util.py
+-rw-r--r--  2.0 unx    10480 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx     4842 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx     1632 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       82 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        4 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-25 20:01 pds_data_upload_manager-1.1.0.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     2122 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/RECORD
+23 files, 76069 bytes uncompressed, 26055 bytes compressed:  65.7%
```

## zipnote {}

```diff
@@ -39,32 +39,32 @@
 
 Filename: pds/ingress/util/node_util.py
 Comment: 
 
 Filename: pds/ingress/util/path_util.py
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/LICENSE.md
+Filename: pds_data_upload_manager-1.1.0.dist-info/LICENSE.md
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/METADATA
+Filename: pds_data_upload_manager-1.1.0.dist-info/METADATA
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/NOTICE.txt
+Filename: pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/WHEEL
+Filename: pds_data_upload_manager-1.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/entry_points.txt
+Filename: pds_data_upload_manager-1.1.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/top_level.txt
+Filename: pds_data_upload_manager-1.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/zip-safe
+Filename: pds_data_upload_manager-1.1.0.dist-info/zip-safe
 Comment: 
 
-Filename: pds_data_upload_manager-1.0.0.dist-info/RECORD
+Filename: pds_data_upload_manager-1.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pds/ingress/VERSION.txt

```diff
@@ -1 +1 @@
-1.0.0
+1.1.0
```

## pds/ingress/client/pds_ingress_client.py

```diff
@@ -3,30 +3,48 @@
 ==================
 pds_ingress_client
 ==================
 
 Client side script used to perform ingress request to the DUM service in AWS.
 """
 import argparse
+import hashlib
 import json
+import os
 
+import backoff
 import pds.ingress.util.log_util as log_util
 import requests
 from joblib import delayed
 from joblib import Parallel
 from pds.ingress.util.auth_util import AuthUtil
 from pds.ingress.util.config_util import ConfigUtil
 from pds.ingress.util.log_util import get_log_level
 from pds.ingress.util.log_util import get_logger
 from pds.ingress.util.node_util import NodeUtil
 from pds.ingress.util.path_util import PathUtil
 
 PARALLEL = Parallel(require="sharedmem")
 
 
+def fatal_code(err: requests.exceptions.RequestException) -> bool:
+    """Only retry for common transient errors"""
+    return 400 <= err.response.status_code < 500
+
+
+def backoff_logger(details):
+    """Log details about the current backoff/retry"""
+    logger = get_logger(__name__)
+    logger.warning(
+        f"Backing off {details['target']} function for {details['wait']:0.1f} "
+        f"seconds after {details['tries']} tries."
+    )
+    logger.warning(f"Total time elapsed: {details['elapsed']:0.1f} seconds.")
+
+
 def _perform_ingress(ingress_path, node_id, prefix, bearer_token, api_gateway_config):
     """
     Performs an ingress request and transfer to S3 using credentials obtained from
     Cognito. This helper function is intended for use with a Joblib parallelized
     loop.
 
     Parameters
@@ -44,35 +62,55 @@
     api_gateway_config : dict
         Dictionary containing configuration details for the API Gateway instance
         used to request ingress.
 
     """
     logger = get_logger(__name__)
 
+    # TODO: slurping entire file could be problematic for large files,
+    #       investigate alternative if/when necessary
+    with open(ingress_path, "rb") as object_file:
+        object_body = object_file.read()
+
     # Remove path prefix if one was configured
     trimmed_path = PathUtil.trim_ingress_path(ingress_path, prefix)
 
     try:
-        s3_ingress_url = request_file_for_ingress(trimmed_path, node_id, api_gateway_config, bearer_token)
+        s3_ingress_url = request_file_for_ingress(
+            object_body, ingress_path, trimmed_path, node_id, api_gateway_config, bearer_token
+        )
 
-        ingress_file_to_s3(ingress_path, trimmed_path, s3_ingress_url)
+        if s3_ingress_url:
+            ingress_file_to_s3(object_body, trimmed_path, s3_ingress_url)
     except Exception as err:
         # Only log the error as a warning, so we don't bring down the entire
         # transfer process
         logger.warning(f"{trimmed_path} : Ingress failed, reason: {str(err)}")
 
 
-def request_file_for_ingress(ingress_file_path, node_id, api_gateway_config, bearer_token):
+@backoff.on_exception(
+    backoff.constant,
+    requests.exceptions.RequestException,
+    max_time=300,
+    giveup=fatal_code,
+    on_backoff=backoff_logger,
+    interval=15,
+)
+def request_file_for_ingress(object_body, ingress_path, trimmed_path, node_id, api_gateway_config, bearer_token):
     """
     Submits a request for file ingress to the PDS Ingress App API.
 
     Parameters
     ----------
-    ingress_file_path : str
+    object_body : bytes
+        Contents of the file to be copied to S3.
+    ingress_path : str
         Local path to the file to request ingress for.
+    trimmed_path : str
+        Ingress path with any user-configured prefix removed
     node_id : str
         PDS node identifier.
     api_gateway_config : dict
         Dictionary or dictionary-like containing key/value pairs used to
         configure the API Gateway endpoint url.
     bearer_token : str
         The Bearer token authorizing the current user to access the Ingress
@@ -80,67 +118,92 @@
 
     Returns
     -------
     s3_ingress_url : str
         The presigned S3 URL returned from the Ingress service lambda, which
         identifies the location in S3 the client should upload the file to and
         includes temporary credentials to allow the client to upload to
-        S3 via an HTTP PUT.
+        S3 via an HTTP PUT. If this file already exists in S3 and should not
+        be overwritten, this function will return None instead.
 
     Raises
     ------
     RuntimeError
         If the request to the Ingress Service fails.
 
     """
     logger = get_logger(__name__)
 
-    logger.info(f"{ingress_file_path} : Requesting ingress for node ID {node_id}")
+    logger.info(f"{trimmed_path} : Requesting ingress for node ID {node_id}")
 
     # Extract the API Gateway configuration params
     api_gateway_template = api_gateway_config["url_template"]
     api_gateway_id = api_gateway_config["id"]
     api_gateway_region = api_gateway_config["region"]
     api_gateway_stage = api_gateway_config["stage"]
     api_gateway_resource = api_gateway_config["resource"]
 
     api_gateway_url = api_gateway_template.format(
         id=api_gateway_id, region=api_gateway_region, stage=api_gateway_stage, resource=api_gateway_resource
     )
 
+    # Calculate the MD5 checksum of the file payload
+    md5_digest = hashlib.md5(object_body).hexdigest()
+
+    # Get the size and last modified time of the file
+    file_size = os.stat(ingress_path).st_size
+    last_modified_time = os.path.getmtime(ingress_path)
+
     params = {"node": node_id, "node_name": NodeUtil.node_id_to_long_name[node_id]}
-    payload = {"url": ingress_file_path}
+    payload = {"url": trimmed_path}
     headers = {
         "Authorization": bearer_token,
         "UserGroup": NodeUtil.node_id_to_group_name(node_id),
+        "ContentMD5": md5_digest,
+        "ContentLength": str(file_size),
+        "LastModified": str(last_modified_time),
         "content-type": "application/json",
         "x-amz-docs-region": api_gateway_region,
     }
 
-    try:
-        response = requests.post(api_gateway_url, params=params, data=json.dumps(payload), headers=headers)
-        response.raise_for_status()
-    except requests.exceptions.HTTPError as err:
-        raise RuntimeError(f"Request to API gateway failed, reason: {str(err)}") from err
-
-    s3_ingress_url = json.loads(response.text)
-
-    logger.debug(f"{ingress_file_path} : Got URL for ingress path {s3_ingress_url.split('?')[0]}")
-
-    return s3_ingress_url
+    response = requests.post(api_gateway_url, params=params, data=json.dumps(payload), headers=headers)
+    response.raise_for_status()
 
-
-def ingress_file_to_s3(ingress_file_path, trimmed_path, s3_ingress_url):
+    # Ingress request successful
+    if response.status_code == 200:
+        s3_ingress_url = json.loads(response.text)
+
+        logger.debug(f"{trimmed_path} : Got URL for ingress path {s3_ingress_url.split('?')[0]}")
+
+        return s3_ingress_url
+    # Ingress service indiciates file already exists in S3 and should not be overwritten
+    elif response.status_code == 204:
+        logger.info(f"{trimmed_path} : File already exists unchanged on S3, skipping ingress")
+
+        return None
+    else:
+        raise RuntimeError(f"Unexpected status code ({response.status_code}) returned from ingress request")
+
+
+@backoff.on_exception(
+    backoff.constant,
+    requests.exceptions.RequestException,
+    max_time=300,
+    giveup=fatal_code,
+    on_backoff=backoff_logger,
+    interval=15,
+)
+def ingress_file_to_s3(object_body, trimmed_path, s3_ingress_url):
     """
     Copies the local file path to the S3 location returned from the Ingress App.
 
     Parameters
     ----------
-    ingress_file_path : str
-        Local path to the file to be copied to S3.
+    object_body : bytes
+        Contents of the file to be copied to S3.
     trimmed_path : str
         Trimmed version of the ingress file path. Used for logging purposes.
     s3_ingress_url : str
         The presigned S3 URL used for upload returned from the Ingress Service
         Lambda function.
 
     Raises
@@ -149,25 +212,16 @@
         If the S3 upload fails for any reason.
 
     """
     logger = get_logger(__name__)
 
     logger.info(f"{trimmed_path} : Ingesting to {s3_ingress_url.split('?')[0]}")
 
-    # TODO: slurping entire file could be problematic for large files,
-    #       investigate alternative if/when necessary
-    with open(ingress_file_path, "rb") as object_file:
-        object_body = object_file.read()
-
-    try:
-        response = requests.put(s3_ingress_url, data=object_body)
-        response.raise_for_status()
-    except requests.exceptions.HTTPError as err:
-        # TODO: add support for automatic retry in the case of a 500 errors
-        raise RuntimeError(f"S3 copy failed, reason: {str(err)}") from err
+    response = requests.put(s3_ingress_url, data=object_body)
+    response.raise_for_status()
 
     logger.info(f"{trimmed_path} : Ingest complete")
 
 
 def setup_argparser():
     """
     Helper function to perform setup of the ArgumentParser for the Ingress client
```

## pds/ingress/service/pds_ingress_app.py

```diff
@@ -5,17 +5,20 @@
 
 Lambda function which acts as the PDS Ingress Service, mapping local file paths
 to their destinations in S3.
 """
 import json
 import logging
 import os
+from datetime import datetime
+from datetime import timezone
 from os.path import join
 
 import boto3
+import botocore
 import yaml
 from botocore.exceptions import ClientError
 
 LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
 
 LEVEL_MAP = {
     "CRITICAL": logging.CRITICAL,
@@ -73,14 +76,71 @@
 
     logger.info(f"Bucket map {bucket_map_path} loaded")
     logger.debug(str(bucket_map))
 
     return bucket_map
 
 
+def should_overwrite_file(destination_bucket, object_key, headers):
+    """
+    Determines if the file requested for ingress already exists in the S3
+    location we plan to upload to, and whether it should be overwritten with a
+    new version based on file info provided in the request headers.
+
+    Parameters
+    ----------
+    destination_bucket : str
+        Name of the S3 bucket to be uploaded to.
+    object_key : str
+        Object key location within the S3 bucket to be uploaded to.
+    headers : dict
+        Contains the headers of the ingress HTTP request from the client.
+        This includes information about the file that will be used to
+        determine if an overwrite on S3 should occur
+
+    Returns
+    -------
+    True if overwrite (or write) should occur, False otherwise.
+
+    """
+    s3_client = boto3.client("s3")
+
+    try:
+        object_head = s3_client.head_object(Bucket=destination_bucket, Key=object_key)
+    except botocore.exceptions.ClientError as e:
+        if e.response["Error"]["Code"] == "404":
+            # File does not already exist, safe to write
+            return True
+        else:
+            # Some other kind of unexpected error
+            raise
+
+    object_length = int(object_head["ContentLength"])
+    object_last_modified = object_head["LastModified"]
+    object_md5 = object_head["ETag"][1:-1]  # strip embedded quotes
+
+    logger.debug(f"{object_length=}")
+    logger.debug(f"{object_last_modified=}")
+    logger.debug(f"{object_md5=}")
+
+    request_length = int(headers["ContentLength"])
+    request_last_modified = datetime.fromtimestamp(float(headers["LastModified"]), tz=timezone.utc)
+    request_md5 = headers["ContentMD5"]
+
+    logger.debug(f"{request_length=}")
+    logger.debug(f"{request_last_modified=}")
+    logger.debug(f"{request_md5=}")
+
+    # If the request object differs from current version in S3 (newer, different contents),
+    # then it should be overwritten
+    return not (
+        object_length == request_length and object_md5 == request_md5 and object_last_modified >= request_last_modified
+    )
+
+
 def generate_presigned_upload_url(bucket_name, object_key, expires_in=1000):
     """
     Generates a presigned URL suitable for uploading to the S3 location
     corresponding to the provided bucket name and object key.
 
     Parameters
     ----------
@@ -136,14 +196,15 @@
 
     """
     # Read the bucket map configured for the service
     bucket_map = initialize_bucket_map()
 
     # Parse request details from event object
     body = json.loads(event["body"])
+    headers = event["headers"]
     local_url = body.get("url")
     request_node = event["queryStringParameters"].get("node")
 
     if not local_url or not request_node:
         logger.exception("Both a local URL and request Node ID must be provided")
         raise RuntimeError
 
@@ -162,12 +223,16 @@
         logger.info(f"Resolved bucket location {destination_bucket} for prefix {prefix_key}")
     else:
         destination_bucket = node_bucket_map["default"]
         logger.warning(
             f"No bucket location configured for prefix {prefix_key}, using default bucket {destination_bucket}"
         )
 
-    object_key = join(request_node.upper(), local_url)
+    object_key = join(request_node.lower(), local_url)
 
-    s3_url = generate_presigned_upload_url(destination_bucket, object_key)
+    if should_overwrite_file(destination_bucket, object_key, headers):
+        s3_url = generate_presigned_upload_url(destination_bucket, object_key)
 
-    return {"statusCode": 200, "body": json.dumps(s3_url)}
+        return {"statusCode": 200, "body": json.dumps(s3_url)}
+    else:
+        logger.info(f"{object_key} already exists in bucket {destination_bucket} and should not be overwritten")
+        return {"statusCode": 204}
```

## pds/ingress/util/log_util.py

```diff
@@ -242,16 +242,15 @@
                 }
                 for record in self.buffer
             ]
 
             # CloudWatch Logs wants all records sorted by ascending timestamp
             log_events = list(sorted(log_events, key=lambda event: event["timestamp"]))
 
-            # deactivated until https://github.com/NASA-PDS/data-upload-manager/issues/75 is fixed
-            # self.send_log_events_to_cloud_watch(log_events)
+            self.send_log_events_to_cloud_watch(log_events)
 
             self.buffer.clear()
         except Exception as err:
             # Use the root logger since the console logger singleton may have been
             # closed by the time flush() is called
             logging.warning(f"Unable to submit to CloudWatch Logs, reason: {str(err)}")
         finally:
```

## Comparing `pds_data_upload_manager-1.0.0.dist-info/LICENSE.md` & `pds_data_upload_manager-1.1.0.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `pds_data_upload_manager-1.0.0.dist-info/METADATA` & `pds_data_upload_manager-1.1.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pds-data-upload-manager
-Version: 1.0.0
+Version: 1.1.0
 Summary: Planetary Data Service Data Delivery Manager
 Home-page: https://github.com/NASA-PDS/data-upload-manager
 Download-URL: https://github.com/NASA-PDS/data-upload-manager/releases/
 Author: PDS
 Author-email: pds_operator@jpl.nasa.gov
 License: apache-2.0
 Keywords: pds,planetary data,aws,s3,ingress,data upload
@@ -12,14 +12,15 @@
 Classifier: Programming Language :: Python :: 3.9
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE.md
 License-File: NOTICE.txt
+Requires-Dist: backoff ~=2.2.1
 Requires-Dist: boto3 ~=1.25
 Requires-Dist: boto3-stubs[apigateway,cognito,essential] ~=1.25
 Requires-Dist: joblib ~=1.3.1
 Requires-Dist: requests >=2.23
 Requires-Dist: types-requests >=2.23
 Requires-Dist: PyYAML ~=6.0
 Requires-Dist: types-PyYAML ~=6.0
```

## Comparing `pds_data_upload_manager-1.0.0.dist-info/NOTICE.txt` & `pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt`

 * *Files identical despite different names*

## Comparing `pds_data_upload_manager-1.0.0.dist-info/RECORD` & `pds_data_upload_manager-1.1.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-pds/ingress/VERSION.txt,sha256=WYVJhIUxBN9cNT4vaBoV_HkkdC-aLkaMKa8kjc5FzgM,6
+pds/ingress/VERSION.txt,sha256=FXXhr0qV8S9wtO5qatzoFglT2T6hfcJhG5CIPMw607g,6
 pds/ingress/__init__.py,sha256=agL6n-Xp3A2qjl4e1JnNWNeqxuWrwOOPa3uMO1KlFCQ,320
 pds/ingress/authorizer/README.md,sha256=0dtXHIm_7xFziuFtaoKMOwAiRB9Jo9tVnJPybkf_Cug,1910
 pds/ingress/authorizer/index.js,sha256=FvSrByKrvh8OG9Te-raJ60EslmXiZdJ1SHzhUiQ-qnU,3926
 pds/ingress/authorizer/package-lock.json,sha256=3N148t7ZjRnHiWv8twJWToIXMxvh6Tp8_pV8fa6JzwA,8692
 pds/ingress/authorizer/package.json,sha256=78mRokNuOqcEA6ZVo-8OnZxsDoY8b88mPrXp639xw5g,322
-pds/ingress/client/pds_ingress_client.py,sha256=rfA826xTvOwDBMWVIJt4rc6ZRLfF68JTcTL4Xwo3u7I,10618
-pds/ingress/service/pds_ingress_app.py,sha256=StXd9tPr1lTEe5mzi-vQrI49TmNoQ85kO1D4j6otHiQ,5283
+pds/ingress/client/pds_ingress_client.py,sha256=VnRar3QqCd98c7xxJE19L8C8mMpJdRXZR8FjQiUcEvw,12328
+pds/ingress/service/pds_ingress_app.py,sha256=axs2ei7GhbynxQ0XCpYfQ-oxqQUdrKuuN_ZhAV1ZMGc,7714
 pds/ingress/service/config/bucket-map.yaml,sha256=f1y6GK5JkBObGZFnmuP5xwWYQp66Lsl8s-JfqBI4Ml0,454
 pds/ingress/util/auth_util.py,sha256=w7wSlkIqzVoFqE6XO-7ZCSuT8dgJbCqMb-uwTr5kGFk,2688
 pds/ingress/util/conf.default.ini,sha256=tEpMh443CJBmqUW0vEhoBB9mwiAG5EzZEL2hmCk5dmE,555
 pds/ingress/util/config_util.py,sha256=QqCVqhtiiLgegHjrOMPCfe1CQzSWnJI41O9aYgPLjAw,1975
-pds/ingress/util/log_util.py,sha256=uOD0QEkNFScPtEhmDa_L5FHy_XqMKH6dSpaYDwx4Wzk,11133
+pds/ingress/util/log_util.py,sha256=420Me8QHQJQRtIWmlnGIb7Yvq0sJKEAXo2eRnDRAZzk,11032
 pds/ingress/util/node_util.py,sha256=tLiPvu4sg3zYNUig_phSeQBGuerZ8XmXJvQHrrfx1wM,1077
 pds/ingress/util/path_util.py,sha256=-W9TdVmz4WfayHn2LFKCCc4VYTxpCiPua1tjhHEHpHI,3815
-pds_data_upload_manager-1.0.0.dist-info/LICENSE.md,sha256=Lh-qBbuRV0-jiCIBhfV7NgdwFxQFOXH3BKOzK865hRs,10480
-pds_data_upload_manager-1.0.0.dist-info/METADATA,sha256=3ysTU2hz5wW11vB-b7TpvZ4vfJXpGCt3ZATRDchV77U,4811
-pds_data_upload_manager-1.0.0.dist-info/NOTICE.txt,sha256=DwSivJgxEEg-PP1U7fHOhwR2vKotAX4enw5v32OYE1c,1632
-pds_data_upload_manager-1.0.0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-pds_data_upload_manager-1.0.0.dist-info/entry_points.txt,sha256=4fRxirWyCD8J9t0zAW3NEeDY4IbUfNC1316OPeg3XWI,82
-pds_data_upload_manager-1.0.0.dist-info/top_level.txt,sha256=5SacHJznU3B9RSALhTt0yjidG0EmPyBMNQgoU-bxvo4,4
-pds_data_upload_manager-1.0.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-pds_data_upload_manager-1.0.0.dist-info/RECORD,,
+pds_data_upload_manager-1.1.0.dist-info/LICENSE.md,sha256=Lh-qBbuRV0-jiCIBhfV7NgdwFxQFOXH3BKOzK865hRs,10480
+pds_data_upload_manager-1.1.0.dist-info/METADATA,sha256=h1LT9z4h-j8vyQDlu6k51KlO2yYa8wvjLcNv8M_Gap4,4842
+pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt,sha256=DwSivJgxEEg-PP1U7fHOhwR2vKotAX4enw5v32OYE1c,1632
+pds_data_upload_manager-1.1.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+pds_data_upload_manager-1.1.0.dist-info/entry_points.txt,sha256=4fRxirWyCD8J9t0zAW3NEeDY4IbUfNC1316OPeg3XWI,82
+pds_data_upload_manager-1.1.0.dist-info/top_level.txt,sha256=5SacHJznU3B9RSALhTt0yjidG0EmPyBMNQgoU-bxvo4,4
+pds_data_upload_manager-1.1.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+pds_data_upload_manager-1.1.0.dist-info/RECORD,,
```

