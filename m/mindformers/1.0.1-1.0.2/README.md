# Comparing `tmp/mindformers-1.0.1-py3-none-any.whl.zip` & `tmp/mindformers-1.0.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,414 +1,415 @@
-Zip file size: 1150263 bytes, number of entries: 412
--rw-r--r--  2.0 unx    14676 b- defN 24-Mar-15 02:43 configs/README.md
--rw-r--r--  2.0 unx     4241 b- defN 24-Mar-15 02:43 configs/bert/run_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4257 b- defN 24-Mar-15 02:43 configs/bert/run_bert_tiny_uncased.yaml
--rw-r--r--  2.0 unx     6074 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
--rw-r--r--  2.0 unx     6139 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
--rw-r--r--  2.0 unx     4951 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     6214 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
--rw-r--r--  2.0 unx     4705 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
--rw-r--r--  2.0 unx     6272 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
--rw-r--r--  2.0 unx     6272 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
--rw-r--r--  2.0 unx     4761 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
--rw-r--r--  2.0 unx     4055 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_560m.yaml
--rw-r--r--  2.0 unx     4054 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b_910b.yaml
--rw-r--r--  2.0 unx     4156 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b_910b_fa.yaml
--rw-r--r--  2.0 unx     4137 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4148 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4295 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4139 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4286 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     6097 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b.yaml
--rw-r--r--  2.0 unx     5811 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_eval.yaml
--rw-r--r--  2.0 unx     5866 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_finetune.yaml
--rw-r--r--  2.0 unx     5870 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
--rw-r--r--  2.0 unx     4009 b- defN 24-Mar-15 02:43 configs/codellama/predict_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx     5330 b- defN 24-Mar-15 02:43 configs/codellama/run_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx     2637 b- defN 24-Mar-15 02:43 configs/general/run_general_task.yaml
--rw-r--r--  2.0 unx     5756 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_finetune.yaml
--rw-r--r--  2.0 unx     5677 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_infer.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_lora.yaml
--rw-r--r--  2.0 unx     5861 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_lora_infer.yaml
--rw-r--r--  2.0 unx     1564 b- defN 24-Mar-15 02:43 configs/glm2/export_glm2_6b.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b.yaml
--rw-r--r--  2.0 unx     5965 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune.yaml
--rw-r--r--  2.0 unx     5972 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_2k.yaml
--rw-r--r--  2.0 unx     5971 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_2k_910b.yaml
--rw-r--r--  2.0 unx     5919 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_910b.yaml
--rw-r--r--  2.0 unx     5865 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_eval.yaml
--rw-r--r--  2.0 unx     6246 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora.yaml
--rw-r--r--  2.0 unx     6254 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_2k.yaml
--rw-r--r--  2.0 unx     6252 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_2k_910b.yaml
--rw-r--r--  2.0 unx     6240 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_910b.yaml
--rw-r--r--  2.0 unx     6146 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_eval.yaml
--rw-r--r--  2.0 unx     6200 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_ptuning2.yaml
--rw-r--r--  2.0 unx     1640 b- defN 24-Mar-15 02:43 configs/glm3/export_glm3_6b.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm3/run_glm3_6b.yaml
--rw-r--r--  2.0 unx     5956 b- defN 24-Mar-15 02:43 configs/glm3/run_glm3_6b_finetune_2k_910b.yaml
--rw-r--r--  2.0 unx     4301 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2.yaml
--rw-r--r--  2.0 unx     4678 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_13b.yaml
--rw-r--r--  2.0 unx     4822 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_13b_910b.yaml
--rw-r--r--  2.0 unx     4628 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_52b.yaml
--rw-r--r--  2.0 unx     4331 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_lora.yaml
--rw-r--r--  2.0 unx     4261 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_txtcls.yaml
--rw-r--r--  2.0 unx     4631 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_xl.yaml
--rw-r--r--  2.0 unx     4824 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_xl_lora.yaml
--rwxr-xr-x  2.0 unx     5173 b- defN 24-Mar-15 02:43 configs/llama/run_llama_13b.yaml
--rw-r--r--  2.0 unx     5176 b- defN 24-Mar-15 02:43 configs/llama/run_llama_13b_910b.yaml
--rwxr-xr-x  2.0 unx     5169 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b.yaml
--rw-r--r--  2.0 unx     5167 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b_910b.yaml
--rw-r--r--  2.0 unx     5625 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b_lora.yaml
--rw-r--r--  2.0 unx     2890 b- defN 24-Mar-15 02:43 configs/llama2/export_llama2_13b.yaml
--rw-r--r--  2.0 unx     2721 b- defN 24-Mar-15 02:43 configs/llama2/export_llama2_7b.yaml
--rw-r--r--  2.0 unx     4057 b- defN 24-Mar-15 02:43 configs/llama2/predict_llama2_70b_910b.yaml
--rw-r--r--  2.0 unx     5424 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b.yaml
--rw-r--r--  2.0 unx     5087 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b.yaml
--rw-r--r--  2.0 unx     5655 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5137 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5420 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_lora_910b.yaml
--rw-r--r--  2.0 unx     5473 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b.yaml
--rw-r--r--  2.0 unx     5332 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b.yaml
--rw-r--r--  2.0 unx     5897 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5378 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5414 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b.yaml
--rw-r--r--  2.0 unx     5100 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b.yaml
--rw-r--r--  2.0 unx     5632 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5113 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5452 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_lora_910b.yaml
--rw-r--r--  2.0 unx     4785 b- defN 24-Mar-15 02:43 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
--rw-r--r--  2.0 unx     4942 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_13b.yaml
--rw-r--r--  2.0 unx     4791 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b.yaml
--rw-r--r--  2.0 unx     4229 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
--rw-r--r--  2.0 unx     4495 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
--rw-r--r--  2.0 unx     5531 b- defN 24-Mar-15 02:43 configs/qa/run_qa_bert_base_uncased.yaml
--rwxr-xr-x  2.0 unx     6818 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-b.yaml
--rwxr-xr-x  2.0 unx     6821 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-h.yaml
--rw-r--r--  2.0 unx     6821 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-l.yaml
--rw-r--r--  2.0 unx     6149 b- defN 24-Mar-15 02:43 configs/swin/run_swin_base_p4w7_224_100ep.yaml
--rw-r--r--  2.0 unx     4494 b- defN 24-Mar-15 02:43 configs/t5/run_t5_small_on_wmt16.yaml
--rw-r--r--  2.0 unx     4455 b- defN 24-Mar-15 02:43 configs/t5/run_t5_tiny_on_wmt16.yaml
--rw-r--r--  2.0 unx     5772 b- defN 24-Mar-15 02:43 configs/tokcls/run_tokcls_bert_base_chinese.yaml
--rw-r--r--  2.0 unx     5788 b- defN 24-Mar-15 02:43 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
--rw-r--r--  2.0 unx     4428 b- defN 24-Mar-15 02:43 configs/txtcls/run_txtcls_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4438 b- defN 24-Mar-15 02:43 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
--rw-r--r--  2.0 unx     6020 b- defN 24-Mar-15 02:43 configs/vit/run_vit_base_p16_224_100ep.yaml
--r--------  2.0 unx      277 b- defN 24-Mar-15 02:44 mindformers/.commit_id
--r--------  2.0 unx     1402 b- defN 24-Mar-15 02:43 mindformers/__init__.py
--r--------  2.0 unx    39712 b- defN 24-Mar-15 02:43 mindformers/auto_class.py
--r--------  2.0 unx    67667 b- defN 24-Mar-15 02:43 mindformers/mindformer_book.py
--r--------  2.0 unx    11096 b- defN 24-Mar-15 02:43 mindformers/version_control.py
--r--------  2.0 unx     1297 b- defN 24-Mar-15 02:43 mindformers/core/__init__.py
--r--------  2.0 unx     3956 b- defN 24-Mar-15 02:43 mindformers/core/clip_grad.py
--r--------  2.0 unx     2995 b- defN 24-Mar-15 02:43 mindformers/core/parallel_config.py
--r--------  2.0 unx      809 b- defN 24-Mar-15 02:43 mindformers/core/callback/__init__.py
--r--------  2.0 unx     3309 b- defN 24-Mar-15 02:43 mindformers/core/callback/build_callback.py
--r--------  2.0 unx    37078 b- defN 24-Mar-15 02:43 mindformers/core/callback/callback.py
--r--------  2.0 unx      833 b- defN 24-Mar-15 02:43 mindformers/core/context/__init__.py
--r--------  2.0 unx     8659 b- defN 24-Mar-15 02:43 mindformers/core/context/build_context.py
--r--------  2.0 unx      789 b- defN 24-Mar-15 02:43 mindformers/core/loss/__init__.py
--r--------  2.0 unx     2866 b- defN 24-Mar-15 02:43 mindformers/core/loss/build_loss.py
--r--------  2.0 unx    18164 b- defN 24-Mar-15 02:43 mindformers/core/loss/loss.py
--r--------  2.0 unx      802 b- defN 24-Mar-15 02:43 mindformers/core/lr/__init__.py
--r--------  2.0 unx     3844 b- defN 24-Mar-15 02:43 mindformers/core/lr/build_lr.py
--r--------  2.0 unx    14711 b- defN 24-Mar-15 02:43 mindformers/core/lr/lr_schedule.py
--r--------  2.0 unx      800 b- defN 24-Mar-15 02:43 mindformers/core/metric/__init__.py
--r--------  2.0 unx     2683 b- defN 24-Mar-15 02:43 mindformers/core/metric/build_metric.py
--r--------  2.0 unx    35009 b- defN 24-Mar-15 02:43 mindformers/core/metric/metric.py
--r--------  2.0 unx     1768 b- defN 24-Mar-15 02:43 mindformers/core/metric/utils.py
--r--------  2.0 unx      847 b- defN 24-Mar-15 02:43 mindformers/core/optim/__init__.py
--r--------  2.0 unx     4623 b- defN 24-Mar-15 02:43 mindformers/core/optim/build_optim.py
--r--------  2.0 unx    21063 b- defN 24-Mar-15 02:43 mindformers/core/optim/came.py
--r--------  2.0 unx    31399 b- defN 24-Mar-15 02:43 mindformers/core/optim/optim.py
--r--------  2.0 unx     2427 b- defN 24-Mar-15 02:43 mindformers/dataset/__init__.py
--r--------  2.0 unx     3379 b- defN 24-Mar-15 02:43 mindformers/dataset/base_dataset.py
--r--------  2.0 unx     2560 b- defN 24-Mar-15 02:43 mindformers/dataset/build_dataset.py
--r--------  2.0 unx    12908 b- defN 24-Mar-15 02:43 mindformers/dataset/causal_language_model_dataset.py
--r--------  2.0 unx     9758 b- defN 24-Mar-15 02:43 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
--r--------  2.0 unx     9994 b- defN 24-Mar-15 02:43 mindformers/dataset/img_cls_dataset.py
--r--------  2.0 unx    21930 b- defN 24-Mar-15 02:43 mindformers/dataset/keyword_gen_dataset.py
--r--------  2.0 unx    16192 b- defN 24-Mar-15 02:43 mindformers/dataset/labels.py
--r--------  2.0 unx     8478 b- defN 24-Mar-15 02:43 mindformers/dataset/mask_language_model_dataset.py
--r--------  2.0 unx     9100 b- defN 24-Mar-15 02:43 mindformers/dataset/mim_dataset.py
--r--------  2.0 unx     7973 b- defN 24-Mar-15 02:43 mindformers/dataset/question_answering_dataset.py
--r--------  2.0 unx    13957 b- defN 24-Mar-15 02:43 mindformers/dataset/reward_model_dataset.py
--r--------  2.0 unx     7515 b- defN 24-Mar-15 02:43 mindformers/dataset/text_classification_dataset.py
--r--------  2.0 unx     9984 b- defN 24-Mar-15 02:43 mindformers/dataset/token_classification_dataset.py
--r--------  2.0 unx    11591 b- defN 24-Mar-15 02:43 mindformers/dataset/translation_dataset.py
--r--------  2.0 unx     1949 b- defN 24-Mar-15 02:43 mindformers/dataset/utils.py
--r--------  2.0 unx     8502 b- defN 24-Mar-15 02:43 mindformers/dataset/zero_shot_image_classification_dataset.py
--r--------  2.0 unx     1489 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/__init__.py
--r--------  2.0 unx     6075 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/adgen_dataloader.py
--r--------  2.0 unx     2930 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/build_dataloader.py
--r--------  2.0 unx     7893 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/cifar100_dataloader.py
--r--------  2.0 unx     6961 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/cluener_dataloader.py
--r--------  2.0 unx     5344 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/datareaders.py
--r--------  2.0 unx     7099 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/flickr8k_dataloader.py
--r--------  2.0 unx     6483 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
--r--------  2.0 unx    11335 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/multi_source_dataloader.py
--r--------  2.0 unx    12659 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/sft_dataloader.py
--r--------  2.0 unx     7423 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/sft_map_functions.py
--r--------  2.0 unx    23879 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/squad_dataloader.py
--r--------  2.0 unx    20177 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/training_dataloader.py
--r--------  2.0 unx     4360 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/wmt16_dataloader.py
--r--------  2.0 unx      808 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/__init__.py
--r--------  2.0 unx     2196 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/build_mask.py
--r--------  2.0 unx     3760 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/vision_mask.py
--r--------  2.0 unx      754 b- defN 24-Mar-15 02:43 mindformers/dataset/sampler/__init__.py
--r--------  2.0 unx     2719 b- defN 24-Mar-15 02:43 mindformers/dataset/sampler/build_sampler.py
--r--------  2.0 unx     1192 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/__init__.py
--r--------  2.0 unx    33409 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/auto_augment.py
--r--------  2.0 unx     3364 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/build_transforms.py
--r--------  2.0 unx    11513 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/mixup.py
--r--------  2.0 unx     4846 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/random_erasing.py
--r--------  2.0 unx     6231 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/text_transforms.py
--r--------  2.0 unx    12281 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/vision_transforms.py
--r--------  2.0 unx      984 b- defN 24-Mar-15 02:43 mindformers/generation/__init__.py
--r--------  2.0 unx    18947 b- defN 24-Mar-15 02:43 mindformers/generation/beam_search.py
--r--------  2.0 unx     8660 b- defN 24-Mar-15 02:43 mindformers/generation/generation_config.py
--r--------  2.0 unx     9729 b- defN 24-Mar-15 02:43 mindformers/generation/logits_process.py
--r--------  2.0 unx    11746 b- defN 24-Mar-15 02:43 mindformers/generation/streamers.py
--r--------  2.0 unx    56513 b- defN 24-Mar-15 02:43 mindformers/generation/text_generator.py
--r--------  2.0 unx     2956 b- defN 24-Mar-15 02:43 mindformers/generation/utils.py
--r--------  2.0 unx      862 b- defN 24-Mar-15 02:43 mindformers/inference/__init__.py
--r--------  2.0 unx     1406 b- defN 24-Mar-15 02:43 mindformers/inference/context.py
--r--------  2.0 unx     2533 b- defN 24-Mar-15 02:43 mindformers/inference/infer_config.py
--r--------  2.0 unx     1930 b- defN 24-Mar-15 02:43 mindformers/inference/infer_task.py
--r--------  2.0 unx    10700 b- defN 24-Mar-15 02:43 mindformers/inference/pipeline.py
--r--------  2.0 unx     2561 b- defN 24-Mar-15 02:43 mindformers/inference/postprocess_sampler.py
--r--------  2.0 unx      704 b- defN 24-Mar-15 02:43 mindformers/inference/infers/__init__.py
--r--------  2.0 unx     8435 b- defN 24-Mar-15 02:43 mindformers/inference/infers/base_infer.py
--r--------  2.0 unx     3234 b- defN 24-Mar-15 02:43 mindformers/inference/infers/cache_engine.py
--r--------  2.0 unx    31700 b- defN 24-Mar-15 02:43 mindformers/inference/infers/text_generator_infer.py
--r--------  2.0 unx     1912 b- defN 24-Mar-15 02:43 mindformers/models/__init__.py
--r--------  2.0 unx    10530 b- defN 24-Mar-15 02:43 mindformers/models/base_config.py
--r--------  2.0 unx    38049 b- defN 24-Mar-15 02:43 mindformers/models/base_fast_tokenizer.py
--r--------  2.0 unx    17280 b- defN 24-Mar-15 02:43 mindformers/models/base_model.py
--r--------  2.0 unx    13391 b- defN 24-Mar-15 02:43 mindformers/models/base_processor.py
--r--------  2.0 unx   207274 b- defN 24-Mar-15 02:43 mindformers/models/base_tokenizer.py
--r--------  2.0 unx     2681 b- defN 24-Mar-15 02:43 mindformers/models/build_config.py
--r--------  2.0 unx     4428 b- defN 24-Mar-15 02:43 mindformers/models/build_model.py
--r--------  2.0 unx     2461 b- defN 24-Mar-15 02:43 mindformers/models/build_processor.py
--r--------  2.0 unx     3570 b- defN 24-Mar-15 02:43 mindformers/models/build_tokenizer.py
--r--------  2.0 unx    16629 b- defN 24-Mar-15 02:43 mindformers/models/convert_slow_tokenizer.py
--r--------  2.0 unx    50997 b- defN 24-Mar-15 02:43 mindformers/models/sentencepiece_model_pb2.py
--r--------  2.0 unx     6645 b- defN 24-Mar-15 02:43 mindformers/models/sentencepiece_model_pb2_new.py
--r--------  2.0 unx     1352 b- defN 24-Mar-15 02:43 mindformers/models/utils.py
--r--------  2.0 unx     1191 b- defN 24-Mar-15 02:43 mindformers/models/bert/__init__.py
--r--------  2.0 unx    29342 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert.py
--r--------  2.0 unx     8036 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_config.py
--r--------  2.0 unx     3608 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_processor.py
--r--------  2.0 unx    25708 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_tokenizer.py
--r--------  2.0 unx     8224 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_tokenizer_fast.py
--r--------  2.0 unx     8638 b- defN 24-Mar-15 02:43 mindformers/models/bert/convert_weight.py
--r--------  2.0 unx     1189 b- defN 24-Mar-15 02:43 mindformers/models/blip2/__init__.py
--r--------  2.0 unx     3727 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2.py
--r--------  2.0 unx     6665 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_config.py
--r--------  2.0 unx     9561 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_itm_evaluator.py
--r--------  2.0 unx     8252 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_llama.py
--r--------  2.0 unx    11459 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_llm.py
--r--------  2.0 unx     7077 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_processor.py
--r--------  2.0 unx    23696 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_qformer.py
--r--------  2.0 unx     1501 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_vit.py
--r--------  2.0 unx     5519 b- defN 24-Mar-15 02:43 mindformers/models/blip2/convert_weight.py
--r--------  2.0 unx     4032 b- defN 24-Mar-15 02:43 mindformers/models/blip2/layers.py
--r--------  2.0 unx    70194 b- defN 24-Mar-15 02:43 mindformers/models/blip2/qformer.py
--r--------  2.0 unx     5239 b- defN 24-Mar-15 02:43 mindformers/models/blip2/qformer_config.py
--r--------  2.0 unx     1137 b- defN 24-Mar-15 02:43 mindformers/models/bloom/__init__.py
--r--------  2.0 unx    16962 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom.py
--r--------  2.0 unx     9127 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_config.py
--r--------  2.0 unx     3474 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_processor.py
--r--------  2.0 unx     4595 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_reward.py
--r--------  2.0 unx    10501 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_tokenizer.py
--r--------  2.0 unx     6806 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_tokenizer_fast.py
--r--------  2.0 unx     6527 b- defN 24-Mar-15 02:43 mindformers/models/bloom/convert_weight.py
--r--------  2.0 unx    32878 b- defN 24-Mar-15 02:43 mindformers/models/bloom/layers.py
--r--------  2.0 unx     1030 b- defN 24-Mar-15 02:43 mindformers/models/clip/__init__.py
--r--------  2.0 unx    10123 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip.py
--r--------  2.0 unx     9082 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_config.py
--r--------  2.0 unx    10123 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_modules.py
--r--------  2.0 unx     4894 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_processor.py
--r--------  2.0 unx    11864 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_tokenizer.py
--r--------  2.0 unx     2989 b- defN 24-Mar-15 02:43 mindformers/models/clip/convert_weight.py
--r--------  2.0 unx     1036 b- defN 24-Mar-15 02:43 mindformers/models/glm/__init__.py
--r--------  2.0 unx    20251 b- defN 24-Mar-15 02:43 mindformers/models/glm/attention.py
--r--------  2.0 unx    16123 b- defN 24-Mar-15 02:43 mindformers/models/glm/chatglm_6b_tokenizer.py
--r--------  2.0 unx     2224 b- defN 24-Mar-15 02:43 mindformers/models/glm/convert_weight.py
--r--------  2.0 unx    22268 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm.py
--r--------  2.0 unx    11300 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm_config.py
--r--------  2.0 unx     4309 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm_processor.py
--r--------  2.0 unx    13117 b- defN 24-Mar-15 02:43 mindformers/models/glm/layers.py
--r--------  2.0 unx      901 b- defN 24-Mar-15 02:43 mindformers/models/glm2/__init__.py
--r--------  2.0 unx    13135 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2.py
--r--------  2.0 unx     4685 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_config.py
--r--------  2.0 unx     6386 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_modules.py
--r--------  2.0 unx    10928 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_tokenizer.py
--r--------  2.0 unx    32231 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_transformer.py
--r--------  2.0 unx      789 b- defN 24-Mar-15 02:43 mindformers/models/glm3/__init__.py
--r--------  2.0 unx    16842 b- defN 24-Mar-15 02:43 mindformers/models/glm3/glm3_tokenizer.py
--r--------  2.0 unx     1106 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/__init__.py
--r--------  2.0 unx     7212 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/convert_weight.py
--r--------  2.0 unx    28341 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2.py
--r--------  2.0 unx     8920 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_config.py
--r--------  2.0 unx     3430 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_processor.py
--r--------  2.0 unx    14380 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_tokenizer.py
--r--------  2.0 unx     9074 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_tokenizer_fast.py
--r--------  2.0 unx    11091 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt_modules.py
--r--------  2.0 unx     1061 b- defN 24-Mar-15 02:43 mindformers/models/llama/__init__.py
--r--------  2.0 unx     6999 b- defN 24-Mar-15 02:43 mindformers/models/llama/convert_weight.py
--r--------  2.0 unx    25605 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama.py
--r--------  2.0 unx    10886 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_config.py
--r--------  2.0 unx    34676 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_interleave.py
--r--------  2.0 unx    24857 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_layer.py
--r--------  2.0 unx     3440 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_processor.py
--r--------  2.0 unx    17102 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_tokenizer.py
--r--------  2.0 unx     9034 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_tokenizer_fast.py
--r--------  2.0 unx    30964 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_transformer.py
--r--------  2.0 unx      878 b- defN 24-Mar-15 02:43 mindformers/models/mae/__init__.py
--r--------  2.0 unx     3416 b- defN 24-Mar-15 02:43 mindformers/models/mae/convert_weight.py
--r--------  2.0 unx    18082 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae.py
--r--------  2.0 unx     7343 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_config.py
--r--------  2.0 unx    36274 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_modules.py
--r--------  2.0 unx     6194 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_processor.py
--r--------  2.0 unx     1022 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/__init__.py
--r--------  2.0 unx     5469 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/convert_weight.py
--r--------  2.0 unx    28383 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha.py
--r--------  2.0 unx     4372 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_config.py
--r--------  2.0 unx     2394 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_processor.py
--r--------  2.0 unx     8177 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_tokenizer.py
--r--------  2.0 unx     1180 b- defN 24-Mar-15 02:43 mindformers/models/sam/__init__.py
--r--------  2.0 unx     3489 b- defN 24-Mar-15 02:43 mindformers/models/sam/conver_weight.py
--r--------  2.0 unx     5967 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam.py
--r--------  2.0 unx     7758 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_config.py
--r--------  2.0 unx    17459 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_image_encoder.py
--r--------  2.0 unx     2944 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_layers.py
--r--------  2.0 unx    22703 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_mask_decoder.py
--r--------  2.0 unx     8556 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_processor.py
--r--------  2.0 unx    10575 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_prompt_encoder.py
--r--------  2.0 unx    22757 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_utils.py
--r--------  2.0 unx      885 b- defN 24-Mar-15 02:43 mindformers/models/swin/__init__.py
--r--------  2.0 unx     4949 b- defN 24-Mar-15 02:43 mindformers/models/swin/convert_weight.py
--r--------  2.0 unx    16303 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin.py
--r--------  2.0 unx     7783 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_config.py
--r--------  2.0 unx    31158 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_modules.py
--r--------  2.0 unx     5472 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_processor.py
--r--------  2.0 unx     1192 b- defN 24-Mar-15 02:43 mindformers/models/t5/__init__.py
--r--------  2.0 unx     8077 b- defN 24-Mar-15 02:43 mindformers/models/t5/convert_weight.py
--r--------  2.0 unx    10964 b- defN 24-Mar-15 02:43 mindformers/models/t5/mt5.py
--r--------  2.0 unx    90291 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5.py
--r--------  2.0 unx    11544 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_config.py
--r--------  2.0 unx     4164 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_processor.py
--r--------  2.0 unx    20276 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_tokenizer.py
--r--------  2.0 unx     9881 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_tokenizer_fast.py
--r--------  2.0 unx      948 b- defN 24-Mar-15 02:43 mindformers/models/vit/__init__.py
--r--------  2.0 unx     3364 b- defN 24-Mar-15 02:43 mindformers/models/vit/convert_weight.py
--r--------  2.0 unx    13411 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit.py
--r--------  2.0 unx     8231 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_config.py
--r--------  2.0 unx    37548 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_modules.py
--r--------  2.0 unx     4925 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_processor.py
--r--------  2.0 unx      992 b- defN 24-Mar-15 02:43 mindformers/modules/__init__.py
--r--------  2.0 unx    50360 b- defN 24-Mar-15 02:43 mindformers/modules/activation.py
--r--------  2.0 unx    13356 b- defN 24-Mar-15 02:43 mindformers/modules/kvcache_mgr.py
--r--------  2.0 unx    48060 b- defN 24-Mar-15 02:43 mindformers/modules/layers.py
--r--------  2.0 unx    14414 b- defN 24-Mar-15 02:43 mindformers/modules/local_block_sparse_attention.py
--r--------  2.0 unx     4545 b- defN 24-Mar-15 02:43 mindformers/modules/paged_attention_mgr.py
--r--------  2.0 unx     1335 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/__init__.py
--r--------  2.0 unx    39235 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/moe.py
--r--------  2.0 unx     8481 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/op_parallel_config.py
--r--------  2.0 unx   202706 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/transformer.py
--r--------  2.0 unx      897 b- defN 24-Mar-15 02:43 mindformers/pet/__init__.py
--r--------  2.0 unx     1176 b- defN 24-Mar-15 02:43 mindformers/pet/constants.py
--r--------  2.0 unx     4920 b- defN 24-Mar-15 02:43 mindformers/pet/pet_config.py
--r--------  2.0 unx     3928 b- defN 24-Mar-15 02:43 mindformers/pet/pet_model.py
--r--------  2.0 unx     1096 b- defN 24-Mar-15 02:43 mindformers/pet/utils.py
--r--------  2.0 unx      696 b- defN 24-Mar-15 02:43 mindformers/pet/models/__init__.py
--r--------  2.0 unx     3511 b- defN 24-Mar-15 02:43 mindformers/pet/models/lora.py
--r--------  2.0 unx      955 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/__init__.py
--r--------  2.0 unx     1176 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/ada_adapter.py
--r--------  2.0 unx     1194 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/adalora_adapter.py
--r--------  2.0 unx     5435 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/lora_adapter.py
--r--------  2.0 unx     1649 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/pet_adapter.py
--r--------  2.0 unx     1165 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/prefix_tuning_adapter.py
--r--------  2.0 unx     1350 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/ptuning2_adapter.py
--r--------  2.0 unx     2160 b- defN 24-Mar-15 02:43 mindformers/pipeline/__init__.py
--r--------  2.0 unx    10291 b- defN 24-Mar-15 02:43 mindformers/pipeline/base_pipeline.py
--r--------  2.0 unx     2393 b- defN 24-Mar-15 02:43 mindformers/pipeline/build_pipeline.py
--r--------  2.0 unx     6343 b- defN 24-Mar-15 02:43 mindformers/pipeline/fill_mask_pipeline.py
--r--------  2.0 unx     7544 b- defN 24-Mar-15 02:43 mindformers/pipeline/image_classification_pipeline.py
--r--------  2.0 unx     7168 b- defN 24-Mar-15 02:43 mindformers/pipeline/image_to_text_generation_pipeline.py
--r--------  2.0 unx     6591 b- defN 24-Mar-15 02:43 mindformers/pipeline/masked_image_modeling_pipeline.py
--r--------  2.0 unx     6226 b- defN 24-Mar-15 02:43 mindformers/pipeline/pipeline.py
--r--------  2.0 unx    18559 b- defN 24-Mar-15 02:43 mindformers/pipeline/question_answering_pipeline.py
--r--------  2.0 unx    27170 b- defN 24-Mar-15 02:43 mindformers/pipeline/segment_anything_pipeline.py
--r--------  2.0 unx    10648 b- defN 24-Mar-15 02:43 mindformers/pipeline/text_classification_pipeline.py
--r--------  2.0 unx    10971 b- defN 24-Mar-15 02:43 mindformers/pipeline/text_generation_pipeline.py
--r--------  2.0 unx    10121 b- defN 24-Mar-15 02:43 mindformers/pipeline/token_classification_pipeline.py
--r--------  2.0 unx     8167 b- defN 24-Mar-15 02:43 mindformers/pipeline/translation_pipeline.py
--r--------  2.0 unx     9276 b- defN 24-Mar-15 02:43 mindformers/pipeline/zero_shot_image_classification_pipeline.py
--r--------  2.0 unx     1127 b- defN 24-Mar-15 02:43 mindformers/tools/__init__.py
--r--------  2.0 unx    10202 b- defN 24-Mar-15 02:43 mindformers/tools/check_rules.py
--r--------  2.0 unx     4399 b- defN 24-Mar-15 02:43 mindformers/tools/download_tools.py
--r--------  2.0 unx     6243 b- defN 24-Mar-15 02:43 mindformers/tools/download_tools_multithread.py
--r--------  2.0 unx    12033 b- defN 24-Mar-15 02:43 mindformers/tools/export.py
--r--------  2.0 unx     6520 b- defN 24-Mar-15 02:43 mindformers/tools/hccl_tools.py
--r--------  2.0 unx     1925 b- defN 24-Mar-15 02:43 mindformers/tools/image_tools.py
--r--------  2.0 unx    22670 b- defN 24-Mar-15 02:43 mindformers/tools/logger.py
--r--------  2.0 unx     2316 b- defN 24-Mar-15 02:43 mindformers/tools/merge_hccl.py
--r--------  2.0 unx     5598 b- defN 24-Mar-15 02:43 mindformers/tools/moe_token_distribution_tools.py
--r--------  2.0 unx     3164 b- defN 24-Mar-15 02:43 mindformers/tools/transform_ckpt.py
--r--------  2.0 unx    13716 b- defN 24-Mar-15 02:43 mindformers/tools/utils.py
--r--------  2.0 unx      842 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/__init__.py
--r--------  2.0 unx     8522 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/cloud_adapter.py
--r--------  2.0 unx     3464 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/cloud_monitor.py
--r--------  2.0 unx      905 b- defN 24-Mar-15 02:43 mindformers/tools/register/__init__.py
--r--------  2.0 unx    11037 b- defN 24-Mar-15 02:43 mindformers/tools/register/config.py
--r--------  2.0 unx     7056 b- defN 24-Mar-15 02:43 mindformers/tools/register/register.py
--r--------  2.0 unx     1982 b- defN 24-Mar-15 02:43 mindformers/trainer/__init__.py
--r--------  2.0 unx    52479 b- defN 24-Mar-15 02:43 mindformers/trainer/base_trainer.py
--r--------  2.0 unx     4428 b- defN 24-Mar-15 02:43 mindformers/trainer/build_trainer.py
--r--------  2.0 unx    55851 b- defN 24-Mar-15 02:43 mindformers/trainer/config_args.py
--r--------  2.0 unx     5925 b- defN 24-Mar-15 02:43 mindformers/trainer/optimizer_grouped_parameters.py
--r--------  2.0 unx    51978 b- defN 24-Mar-15 02:43 mindformers/trainer/trainer.py
--r--------  2.0 unx    15070 b- defN 24-Mar-15 02:43 mindformers/trainer/training_args.py
--r--------  2.0 unx    35015 b- defN 24-Mar-15 02:43 mindformers/trainer/utils.py
--r--------  2.0 unx      821 b- defN 24-Mar-15 02:43 mindformers/trainer/causal_language_modeling/__init__.py
--r--------  2.0 unx    20146 b- defN 24-Mar-15 02:43 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
--r--------  2.0 unx      866 b- defN 24-Mar-15 02:43 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
--r--------  2.0 unx     4609 b- defN 24-Mar-15 02:43 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
--r--------  2.0 unx      795 b- defN 24-Mar-15 02:43 mindformers/trainer/general_task_trainer/__init__.py
--r--------  2.0 unx     9462 b- defN 24-Mar-15 02:43 mindformers/trainer/general_task_trainer/general_task_trainer.py
--r--------  2.0 unx      928 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/__init__.py
--r--------  2.0 unx     4622 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/group_ic_params.py
--r--------  2.0 unx     9476 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/image_classification.py
--r--------  2.0 unx     8209 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/zero_shot_image_classification.py
--r--------  2.0 unx      823 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_generation/__init__.py
--r--------  2.0 unx     6107 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
--r--------  2.0 unx      818 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/__init__.py
--r--------  2.0 unx    12076 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/eval_utils.py
--r--------  2.0 unx     7783 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
--r--------  2.0 unx      818 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/__init__.py
--r--------  2.0 unx     2197 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
--r--------  2.0 unx     7432 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
--r--------  2.0 unx      830 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_language_modeling/__init__.py
--r--------  2.0 unx     6859 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
--r--------  2.0 unx      799 b- defN 24-Mar-15 02:43 mindformers/trainer/question_answering/__init__.py
--r--------  2.0 unx     9218 b- defN 24-Mar-15 02:43 mindformers/trainer/question_answering/question_answering.py
--r--------  2.0 unx      803 b- defN 24-Mar-15 02:43 mindformers/trainer/text_classfication/__init__.py
--r--------  2.0 unx     9386 b- defN 24-Mar-15 02:43 mindformers/trainer/text_classfication/text_classification.py
--r--------  2.0 unx      807 b- defN 24-Mar-15 02:43 mindformers/trainer/token_classification/__init__.py
--r--------  2.0 unx     9500 b- defN 24-Mar-15 02:43 mindformers/trainer/token_classification/token_classification.py
--r--------  2.0 unx      795 b- defN 24-Mar-15 02:43 mindformers/trainer/translation/__init__.py
--r--------  2.0 unx     6738 b- defN 24-Mar-15 02:43 mindformers/trainer/translation/translation_finetune.py
--r--------  2.0 unx      913 b- defN 24-Mar-15 02:43 mindformers/wrapper/__init__.py
--r--------  2.0 unx    14329 b- defN 24-Mar-15 02:43 mindformers/wrapper/adaptive_loss_scale.py
--r--------  2.0 unx     4117 b- defN 24-Mar-15 02:43 mindformers/wrapper/build_wrapper.py
--r--------  2.0 unx    12204 b- defN 24-Mar-15 02:43 mindformers/wrapper/wrapper.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    19262 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/WHEEL
--r--------  2.0 unx       12 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    40604 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/RECORD
-412 files, 4212794 bytes uncompressed, 1084717 bytes compressed:  74.3%
+Zip file size: 1155258 bytes, number of entries: 413
+-rw-r--r--  2.0 unx    14807 b- defN 24-Apr-19 15:26 configs/README.md
+-rw-r--r--  2.0 unx     4241 b- defN 24-Apr-19 15:26 configs/bert/run_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4257 b- defN 24-Apr-19 15:26 configs/bert/run_bert_tiny_uncased.yaml
+-rw-r--r--  2.0 unx     6074 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
+-rw-r--r--  2.0 unx     6139 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
+-rw-r--r--  2.0 unx     4951 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6214 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
+-rw-r--r--  2.0 unx     4705 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     4761 b- defN 24-Apr-19 15:26 configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     4055 b- defN 24-Apr-19 15:26 configs/bloom/run_bloom_560m.yaml
+-rw-r--r--  2.0 unx     4054 b- defN 24-Apr-19 15:26 configs/bloom/run_bloom_7.1b.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Apr-19 15:26 configs/bloom/run_bloom_7.1b_910b.yaml
+-rw-r--r--  2.0 unx     4156 b- defN 24-Apr-19 15:26 configs/bloom/run_bloom_7.1b_910b_fa.yaml
+-rw-r--r--  2.0 unx     4137 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4148 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4295 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4139 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4286 b- defN 24-Apr-19 15:26 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6097 b- defN 24-Apr-19 15:26 configs/codegeex2/run_codegeex2_6b.yaml
+-rw-r--r--  2.0 unx     5811 b- defN 24-Apr-19 15:26 configs/codegeex2/run_codegeex2_6b_eval.yaml
+-rw-r--r--  2.0 unx     5866 b- defN 24-Apr-19 15:26 configs/codegeex2/run_codegeex2_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5870 b- defN 24-Apr-19 15:26 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
+-rw-r--r--  2.0 unx     4009 b- defN 24-Apr-19 15:26 configs/codellama/predict_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     5218 b- defN 24-Apr-19 15:26 configs/codellama/run_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     2637 b- defN 24-Apr-19 15:26 configs/general/run_general_task.yaml
+-rw-r--r--  2.0 unx     5756 b- defN 24-Apr-19 15:26 configs/glm/run_glm_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5677 b- defN 24-Apr-19 15:26 configs/glm/run_glm_6b_infer.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Apr-19 15:26 configs/glm/run_glm_6b_lora.yaml
+-rw-r--r--  2.0 unx     5861 b- defN 24-Apr-19 15:26 configs/glm/run_glm_6b_lora_infer.yaml
+-rw-r--r--  2.0 unx     1564 b- defN 24-Apr-19 15:26 configs/glm2/export_glm2_6b.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b.yaml
+-rw-r--r--  2.0 unx     5965 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5972 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_finetune_2k.yaml
+-rw-r--r--  2.0 unx     5971 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_finetune_2k_910b.yaml
+-rw-r--r--  2.0 unx     5919 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_finetune_910b.yaml
+-rw-r--r--  2.0 unx     5865 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_finetune_eval.yaml
+-rw-r--r--  2.0 unx     6246 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_lora.yaml
+-rw-r--r--  2.0 unx     6254 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_lora_2k.yaml
+-rw-r--r--  2.0 unx     6252 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_lora_2k_910b.yaml
+-rw-r--r--  2.0 unx     6240 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_lora_910b.yaml
+-rw-r--r--  2.0 unx     6146 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_lora_eval.yaml
+-rw-r--r--  2.0 unx     6200 b- defN 24-Apr-19 15:26 configs/glm2/run_glm2_6b_ptuning2.yaml
+-rw-r--r--  2.0 unx     1640 b- defN 24-Apr-19 15:26 configs/glm3/export_glm3_6b.yaml
+-rw-r--r--  2.0 unx     6268 b- defN 24-Apr-19 15:26 configs/glm3/export_glm3_6b_pa.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Apr-19 15:26 configs/glm3/run_glm3_6b.yaml
+-rw-r--r--  2.0 unx     5956 b- defN 24-Apr-19 15:26 configs/glm3/run_glm3_6b_finetune_2k_910b.yaml
+-rw-r--r--  2.0 unx     4301 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2.yaml
+-rw-r--r--  2.0 unx     4678 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_13b.yaml
+-rw-r--r--  2.0 unx     4822 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_13b_910b.yaml
+-rw-r--r--  2.0 unx     4628 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_52b.yaml
+-rw-r--r--  2.0 unx     4331 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_lora.yaml
+-rw-r--r--  2.0 unx     4261 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_txtcls.yaml
+-rw-r--r--  2.0 unx     4631 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_xl.yaml
+-rw-r--r--  2.0 unx     4824 b- defN 24-Apr-19 15:26 configs/gpt2/run_gpt2_xl_lora.yaml
+-rwxr-xr-x  2.0 unx     5024 b- defN 24-Apr-19 15:26 configs/llama/run_llama_13b.yaml
+-rw-r--r--  2.0 unx     5028 b- defN 24-Apr-19 15:26 configs/llama/run_llama_13b_910b.yaml
+-rwxr-xr-x  2.0 unx     5020 b- defN 24-Apr-19 15:26 configs/llama/run_llama_7b.yaml
+-rw-r--r--  2.0 unx     5019 b- defN 24-Apr-19 15:26 configs/llama/run_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     5473 b- defN 24-Apr-19 15:26 configs/llama/run_llama_7b_lora.yaml
+-rw-r--r--  2.0 unx     2766 b- defN 24-Apr-19 15:26 configs/llama2/export_llama2_13b.yaml
+-rw-r--r--  2.0 unx     2597 b- defN 24-Apr-19 15:26 configs/llama2/export_llama2_7b.yaml
+-rw-r--r--  2.0 unx     4056 b- defN 24-Apr-19 15:26 configs/llama2/predict_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5299 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_13b.yaml
+-rw-r--r--  2.0 unx     5087 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_13b_910b.yaml
+-rw-r--r--  2.0 unx     5531 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5137 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_13b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5296 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_13b_lora_910b.yaml
+-rw-r--r--  2.0 unx     5373 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_70b.yaml
+-rw-r--r--  2.0 unx     5332 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5773 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5352 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_70b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5289 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_7b.yaml
+-rw-r--r--  2.0 unx     5100 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_7b_910b.yaml
+-rw-r--r--  2.0 unx     5508 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5113 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_7b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5328 b- defN 24-Apr-19 15:26 configs/llama2/run_llama2_7b_lora_910b.yaml
+-rw-r--r--  2.0 unx     4785 b- defN 24-Apr-19 15:26 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
+-rw-r--r--  2.0 unx     4942 b- defN 24-Apr-19 15:26 configs/pangualpha/run_pangualpha_13b.yaml
+-rw-r--r--  2.0 unx     4791 b- defN 24-Apr-19 15:26 configs/pangualpha/run_pangualpha_2_6b.yaml
+-rw-r--r--  2.0 unx     4229 b- defN 24-Apr-19 15:26 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
+-rw-r--r--  2.0 unx     4495 b- defN 24-Apr-19 15:26 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
+-rw-r--r--  2.0 unx     5531 b- defN 24-Apr-19 15:26 configs/qa/run_qa_bert_base_uncased.yaml
+-rwxr-xr-x  2.0 unx     6818 b- defN 24-Apr-19 15:26 configs/sam/run_sam_vit-b.yaml
+-rwxr-xr-x  2.0 unx     6821 b- defN 24-Apr-19 15:26 configs/sam/run_sam_vit-h.yaml
+-rw-r--r--  2.0 unx     6821 b- defN 24-Apr-19 15:26 configs/sam/run_sam_vit-l.yaml
+-rw-r--r--  2.0 unx     6149 b- defN 24-Apr-19 15:26 configs/swin/run_swin_base_p4w7_224_100ep.yaml
+-rw-r--r--  2.0 unx     4494 b- defN 24-Apr-19 15:26 configs/t5/run_t5_small_on_wmt16.yaml
+-rw-r--r--  2.0 unx     4455 b- defN 24-Apr-19 15:26 configs/t5/run_t5_tiny_on_wmt16.yaml
+-rw-r--r--  2.0 unx     5772 b- defN 24-Apr-19 15:26 configs/tokcls/run_tokcls_bert_base_chinese.yaml
+-rw-r--r--  2.0 unx     5788 b- defN 24-Apr-19 15:26 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
+-rw-r--r--  2.0 unx     4428 b- defN 24-Apr-19 15:26 configs/txtcls/run_txtcls_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4438 b- defN 24-Apr-19 15:26 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
+-rw-r--r--  2.0 unx     6020 b- defN 24-Apr-19 15:26 configs/vit/run_vit_base_p16_224_100ep.yaml
+-r--------  2.0 unx      295 b- defN 24-Apr-19 15:26 mindformers/.commit_id
+-r--------  2.0 unx     1402 b- defN 24-Apr-19 15:26 mindformers/__init__.py
+-r--------  2.0 unx    39712 b- defN 24-Apr-19 15:26 mindformers/auto_class.py
+-r--------  2.0 unx    67667 b- defN 24-Apr-19 15:26 mindformers/mindformer_book.py
+-r--------  2.0 unx    11659 b- defN 24-Apr-19 15:26 mindformers/version_control.py
+-r--------  2.0 unx     1297 b- defN 24-Apr-19 15:26 mindformers/core/__init__.py
+-r--------  2.0 unx     3956 b- defN 24-Apr-19 15:26 mindformers/core/clip_grad.py
+-r--------  2.0 unx     2995 b- defN 24-Apr-19 15:26 mindformers/core/parallel_config.py
+-r--------  2.0 unx      809 b- defN 24-Apr-19 15:26 mindformers/core/callback/__init__.py
+-r--------  2.0 unx     3309 b- defN 24-Apr-19 15:26 mindformers/core/callback/build_callback.py
+-r--------  2.0 unx    37078 b- defN 24-Apr-19 15:26 mindformers/core/callback/callback.py
+-r--------  2.0 unx      833 b- defN 24-Apr-19 15:26 mindformers/core/context/__init__.py
+-r--------  2.0 unx     8659 b- defN 24-Apr-19 15:26 mindformers/core/context/build_context.py
+-r--------  2.0 unx      789 b- defN 24-Apr-19 15:26 mindformers/core/loss/__init__.py
+-r--------  2.0 unx     2866 b- defN 24-Apr-19 15:26 mindformers/core/loss/build_loss.py
+-r--------  2.0 unx    18164 b- defN 24-Apr-19 15:26 mindformers/core/loss/loss.py
+-r--------  2.0 unx      802 b- defN 24-Apr-19 15:26 mindformers/core/lr/__init__.py
+-r--------  2.0 unx     3844 b- defN 24-Apr-19 15:26 mindformers/core/lr/build_lr.py
+-r--------  2.0 unx    14711 b- defN 24-Apr-19 15:26 mindformers/core/lr/lr_schedule.py
+-r--------  2.0 unx      800 b- defN 24-Apr-19 15:26 mindformers/core/metric/__init__.py
+-r--------  2.0 unx     2683 b- defN 24-Apr-19 15:26 mindformers/core/metric/build_metric.py
+-r--------  2.0 unx    35009 b- defN 24-Apr-19 15:26 mindformers/core/metric/metric.py
+-r--------  2.0 unx     1768 b- defN 24-Apr-19 15:26 mindformers/core/metric/utils.py
+-r--------  2.0 unx      847 b- defN 24-Apr-19 15:26 mindformers/core/optim/__init__.py
+-r--------  2.0 unx     4623 b- defN 24-Apr-19 15:26 mindformers/core/optim/build_optim.py
+-r--------  2.0 unx    21063 b- defN 24-Apr-19 15:26 mindformers/core/optim/came.py
+-r--------  2.0 unx    31399 b- defN 24-Apr-19 15:26 mindformers/core/optim/optim.py
+-r--------  2.0 unx     2427 b- defN 24-Apr-19 15:26 mindformers/dataset/__init__.py
+-r--------  2.0 unx     3379 b- defN 24-Apr-19 15:26 mindformers/dataset/base_dataset.py
+-r--------  2.0 unx     2560 b- defN 24-Apr-19 15:26 mindformers/dataset/build_dataset.py
+-r--------  2.0 unx    12908 b- defN 24-Apr-19 15:26 mindformers/dataset/causal_language_model_dataset.py
+-r--------  2.0 unx     9758 b- defN 24-Apr-19 15:26 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
+-r--------  2.0 unx     9994 b- defN 24-Apr-19 15:26 mindformers/dataset/img_cls_dataset.py
+-r--------  2.0 unx    21930 b- defN 24-Apr-19 15:26 mindformers/dataset/keyword_gen_dataset.py
+-r--------  2.0 unx    16192 b- defN 24-Apr-19 15:26 mindformers/dataset/labels.py
+-r--------  2.0 unx     8478 b- defN 24-Apr-19 15:26 mindformers/dataset/mask_language_model_dataset.py
+-r--------  2.0 unx     9100 b- defN 24-Apr-19 15:26 mindformers/dataset/mim_dataset.py
+-r--------  2.0 unx     7973 b- defN 24-Apr-19 15:26 mindformers/dataset/question_answering_dataset.py
+-r--------  2.0 unx    13957 b- defN 24-Apr-19 15:26 mindformers/dataset/reward_model_dataset.py
+-r--------  2.0 unx     7515 b- defN 24-Apr-19 15:26 mindformers/dataset/text_classification_dataset.py
+-r--------  2.0 unx     9984 b- defN 24-Apr-19 15:26 mindformers/dataset/token_classification_dataset.py
+-r--------  2.0 unx    11591 b- defN 24-Apr-19 15:26 mindformers/dataset/translation_dataset.py
+-r--------  2.0 unx     1949 b- defN 24-Apr-19 15:26 mindformers/dataset/utils.py
+-r--------  2.0 unx     8502 b- defN 24-Apr-19 15:26 mindformers/dataset/zero_shot_image_classification_dataset.py
+-r--------  2.0 unx     1489 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/__init__.py
+-r--------  2.0 unx     6075 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/adgen_dataloader.py
+-r--------  2.0 unx     2930 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/build_dataloader.py
+-r--------  2.0 unx     7893 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/cifar100_dataloader.py
+-r--------  2.0 unx     6961 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/cluener_dataloader.py
+-r--------  2.0 unx     5344 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/datareaders.py
+-r--------  2.0 unx     7099 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/flickr8k_dataloader.py
+-r--------  2.0 unx     6483 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
+-r--------  2.0 unx    11335 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/multi_source_dataloader.py
+-r--------  2.0 unx    12659 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/sft_dataloader.py
+-r--------  2.0 unx     7423 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/sft_map_functions.py
+-r--------  2.0 unx    23879 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/squad_dataloader.py
+-r--------  2.0 unx    20177 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/training_dataloader.py
+-r--------  2.0 unx     4360 b- defN 24-Apr-19 15:26 mindformers/dataset/dataloader/wmt16_dataloader.py
+-r--------  2.0 unx      808 b- defN 24-Apr-19 15:26 mindformers/dataset/mask/__init__.py
+-r--------  2.0 unx     2196 b- defN 24-Apr-19 15:26 mindformers/dataset/mask/build_mask.py
+-r--------  2.0 unx     3760 b- defN 24-Apr-19 15:26 mindformers/dataset/mask/vision_mask.py
+-r--------  2.0 unx      754 b- defN 24-Apr-19 15:26 mindformers/dataset/sampler/__init__.py
+-r--------  2.0 unx     2719 b- defN 24-Apr-19 15:26 mindformers/dataset/sampler/build_sampler.py
+-r--------  2.0 unx     1192 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/__init__.py
+-r--------  2.0 unx    33409 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/auto_augment.py
+-r--------  2.0 unx     3364 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/build_transforms.py
+-r--------  2.0 unx    11513 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/mixup.py
+-r--------  2.0 unx     4846 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/random_erasing.py
+-r--------  2.0 unx     6231 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/text_transforms.py
+-r--------  2.0 unx    12281 b- defN 24-Apr-19 15:26 mindformers/dataset/transforms/vision_transforms.py
+-r--------  2.0 unx      984 b- defN 24-Apr-19 15:26 mindformers/generation/__init__.py
+-r--------  2.0 unx    18947 b- defN 24-Apr-19 15:26 mindformers/generation/beam_search.py
+-r--------  2.0 unx     8660 b- defN 24-Apr-19 15:26 mindformers/generation/generation_config.py
+-r--------  2.0 unx     9729 b- defN 24-Apr-19 15:26 mindformers/generation/logits_process.py
+-r--------  2.0 unx    11746 b- defN 24-Apr-19 15:26 mindformers/generation/streamers.py
+-r--------  2.0 unx    56513 b- defN 24-Apr-19 15:26 mindformers/generation/text_generator.py
+-r--------  2.0 unx     2956 b- defN 24-Apr-19 15:26 mindformers/generation/utils.py
+-r--------  2.0 unx      862 b- defN 24-Apr-19 15:26 mindformers/inference/__init__.py
+-r--------  2.0 unx     1406 b- defN 24-Apr-19 15:26 mindformers/inference/context.py
+-r--------  2.0 unx     2533 b- defN 24-Apr-19 15:26 mindformers/inference/infer_config.py
+-r--------  2.0 unx     1930 b- defN 24-Apr-19 15:26 mindformers/inference/infer_task.py
+-r--------  2.0 unx    10700 b- defN 24-Apr-19 15:26 mindformers/inference/pipeline.py
+-r--------  2.0 unx     2561 b- defN 24-Apr-19 15:26 mindformers/inference/postprocess_sampler.py
+-r--------  2.0 unx      704 b- defN 24-Apr-19 15:26 mindformers/inference/infers/__init__.py
+-r--------  2.0 unx     8464 b- defN 24-Apr-19 15:26 mindformers/inference/infers/base_infer.py
+-r--------  2.0 unx     3234 b- defN 24-Apr-19 15:26 mindformers/inference/infers/cache_engine.py
+-r--------  2.0 unx    31813 b- defN 24-Apr-19 15:26 mindformers/inference/infers/text_generator_infer.py
+-r--------  2.0 unx     1912 b- defN 24-Apr-19 15:26 mindformers/models/__init__.py
+-r--------  2.0 unx    10530 b- defN 24-Apr-19 15:26 mindformers/models/base_config.py
+-r--------  2.0 unx    38049 b- defN 24-Apr-19 15:26 mindformers/models/base_fast_tokenizer.py
+-r--------  2.0 unx    17280 b- defN 24-Apr-19 15:26 mindformers/models/base_model.py
+-r--------  2.0 unx    13391 b- defN 24-Apr-19 15:26 mindformers/models/base_processor.py
+-r--------  2.0 unx   207274 b- defN 24-Apr-19 15:26 mindformers/models/base_tokenizer.py
+-r--------  2.0 unx     2681 b- defN 24-Apr-19 15:26 mindformers/models/build_config.py
+-r--------  2.0 unx     4428 b- defN 24-Apr-19 15:26 mindformers/models/build_model.py
+-r--------  2.0 unx     2461 b- defN 24-Apr-19 15:26 mindformers/models/build_processor.py
+-r--------  2.0 unx     3570 b- defN 24-Apr-19 15:26 mindformers/models/build_tokenizer.py
+-r--------  2.0 unx    16629 b- defN 24-Apr-19 15:26 mindformers/models/convert_slow_tokenizer.py
+-r--------  2.0 unx    50997 b- defN 24-Apr-19 15:26 mindformers/models/sentencepiece_model_pb2.py
+-r--------  2.0 unx     6645 b- defN 24-Apr-19 15:26 mindformers/models/sentencepiece_model_pb2_new.py
+-r--------  2.0 unx     1352 b- defN 24-Apr-19 15:26 mindformers/models/utils.py
+-r--------  2.0 unx     1191 b- defN 24-Apr-19 15:26 mindformers/models/bert/__init__.py
+-r--------  2.0 unx    29342 b- defN 24-Apr-19 15:26 mindformers/models/bert/bert.py
+-r--------  2.0 unx     8036 b- defN 24-Apr-19 15:26 mindformers/models/bert/bert_config.py
+-r--------  2.0 unx     3608 b- defN 24-Apr-19 15:26 mindformers/models/bert/bert_processor.py
+-r--------  2.0 unx    25708 b- defN 24-Apr-19 15:26 mindformers/models/bert/bert_tokenizer.py
+-r--------  2.0 unx     8224 b- defN 24-Apr-19 15:26 mindformers/models/bert/bert_tokenizer_fast.py
+-r--------  2.0 unx     8638 b- defN 24-Apr-19 15:26 mindformers/models/bert/convert_weight.py
+-r--------  2.0 unx     1189 b- defN 24-Apr-19 15:26 mindformers/models/blip2/__init__.py
+-r--------  2.0 unx     3727 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2.py
+-r--------  2.0 unx     6665 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_config.py
+-r--------  2.0 unx     9561 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_itm_evaluator.py
+-r--------  2.0 unx     8252 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_llama.py
+-r--------  2.0 unx    11459 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_llm.py
+-r--------  2.0 unx     7077 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_processor.py
+-r--------  2.0 unx    23696 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_qformer.py
+-r--------  2.0 unx     1501 b- defN 24-Apr-19 15:26 mindformers/models/blip2/blip2_vit.py
+-r--------  2.0 unx     5519 b- defN 24-Apr-19 15:26 mindformers/models/blip2/convert_weight.py
+-r--------  2.0 unx     4032 b- defN 24-Apr-19 15:26 mindformers/models/blip2/layers.py
+-r--------  2.0 unx    70194 b- defN 24-Apr-19 15:26 mindformers/models/blip2/qformer.py
+-r--------  2.0 unx     5239 b- defN 24-Apr-19 15:26 mindformers/models/blip2/qformer_config.py
+-r--------  2.0 unx     1137 b- defN 24-Apr-19 15:26 mindformers/models/bloom/__init__.py
+-r--------  2.0 unx    16962 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom.py
+-r--------  2.0 unx     9127 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom_config.py
+-r--------  2.0 unx     3474 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom_processor.py
+-r--------  2.0 unx     4595 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom_reward.py
+-r--------  2.0 unx    10501 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom_tokenizer.py
+-r--------  2.0 unx     6806 b- defN 24-Apr-19 15:26 mindformers/models/bloom/bloom_tokenizer_fast.py
+-r--------  2.0 unx     6527 b- defN 24-Apr-19 15:26 mindformers/models/bloom/convert_weight.py
+-r--------  2.0 unx    32896 b- defN 24-Apr-19 15:26 mindformers/models/bloom/layers.py
+-r--------  2.0 unx     1030 b- defN 24-Apr-19 15:26 mindformers/models/clip/__init__.py
+-r--------  2.0 unx    10123 b- defN 24-Apr-19 15:26 mindformers/models/clip/clip.py
+-r--------  2.0 unx     9082 b- defN 24-Apr-19 15:26 mindformers/models/clip/clip_config.py
+-r--------  2.0 unx    10123 b- defN 24-Apr-19 15:26 mindformers/models/clip/clip_modules.py
+-r--------  2.0 unx     4894 b- defN 24-Apr-19 15:26 mindformers/models/clip/clip_processor.py
+-r--------  2.0 unx    11864 b- defN 24-Apr-19 15:26 mindformers/models/clip/clip_tokenizer.py
+-r--------  2.0 unx     2989 b- defN 24-Apr-19 15:26 mindformers/models/clip/convert_weight.py
+-r--------  2.0 unx     1036 b- defN 24-Apr-19 15:26 mindformers/models/glm/__init__.py
+-r--------  2.0 unx    20251 b- defN 24-Apr-19 15:26 mindformers/models/glm/attention.py
+-r--------  2.0 unx    16123 b- defN 24-Apr-19 15:26 mindformers/models/glm/chatglm_6b_tokenizer.py
+-r--------  2.0 unx     2224 b- defN 24-Apr-19 15:26 mindformers/models/glm/convert_weight.py
+-r--------  2.0 unx    22268 b- defN 24-Apr-19 15:26 mindformers/models/glm/glm.py
+-r--------  2.0 unx    11300 b- defN 24-Apr-19 15:26 mindformers/models/glm/glm_config.py
+-r--------  2.0 unx     4309 b- defN 24-Apr-19 15:26 mindformers/models/glm/glm_processor.py
+-r--------  2.0 unx    13117 b- defN 24-Apr-19 15:26 mindformers/models/glm/layers.py
+-r--------  2.0 unx      901 b- defN 24-Apr-19 15:26 mindformers/models/glm2/__init__.py
+-r--------  2.0 unx    20422 b- defN 24-Apr-19 15:26 mindformers/models/glm2/glm2.py
+-r--------  2.0 unx     7000 b- defN 24-Apr-19 15:26 mindformers/models/glm2/glm2_config.py
+-r--------  2.0 unx     8791 b- defN 24-Apr-19 15:26 mindformers/models/glm2/glm2_modules.py
+-r--------  2.0 unx    10928 b- defN 24-Apr-19 15:26 mindformers/models/glm2/glm2_tokenizer.py
+-r--------  2.0 unx    32609 b- defN 24-Apr-19 15:26 mindformers/models/glm2/glm2_transformer.py
+-r--------  2.0 unx      789 b- defN 24-Apr-19 15:26 mindformers/models/glm3/__init__.py
+-r--------  2.0 unx    16842 b- defN 24-Apr-19 15:26 mindformers/models/glm3/glm3_tokenizer.py
+-r--------  2.0 unx     1106 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/__init__.py
+-r--------  2.0 unx     7212 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/convert_weight.py
+-r--------  2.0 unx    28341 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt2.py
+-r--------  2.0 unx     8920 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt2_config.py
+-r--------  2.0 unx     3430 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt2_processor.py
+-r--------  2.0 unx    14380 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt2_tokenizer.py
+-r--------  2.0 unx     9074 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt2_tokenizer_fast.py
+-r--------  2.0 unx    11091 b- defN 24-Apr-19 15:26 mindformers/models/gpt2/gpt_modules.py
+-r--------  2.0 unx     1061 b- defN 24-Apr-19 15:26 mindformers/models/llama/__init__.py
+-r--------  2.0 unx     6999 b- defN 24-Apr-19 15:26 mindformers/models/llama/convert_weight.py
+-r--------  2.0 unx    25687 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama.py
+-r--------  2.0 unx    11014 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_config.py
+-r--------  2.0 unx    34023 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_interleave.py
+-r--------  2.0 unx    25605 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_layer.py
+-r--------  2.0 unx     3440 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_processor.py
+-r--------  2.0 unx    17102 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_tokenizer.py
+-r--------  2.0 unx     9034 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_tokenizer_fast.py
+-r--------  2.0 unx    32324 b- defN 24-Apr-19 15:26 mindformers/models/llama/llama_transformer.py
+-r--------  2.0 unx      878 b- defN 24-Apr-19 15:26 mindformers/models/mae/__init__.py
+-r--------  2.0 unx     3416 b- defN 24-Apr-19 15:26 mindformers/models/mae/convert_weight.py
+-r--------  2.0 unx    18082 b- defN 24-Apr-19 15:26 mindformers/models/mae/mae.py
+-r--------  2.0 unx     7343 b- defN 24-Apr-19 15:26 mindformers/models/mae/mae_config.py
+-r--------  2.0 unx    36274 b- defN 24-Apr-19 15:26 mindformers/models/mae/mae_modules.py
+-r--------  2.0 unx     6194 b- defN 24-Apr-19 15:26 mindformers/models/mae/mae_processor.py
+-r--------  2.0 unx     1022 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/__init__.py
+-r--------  2.0 unx     5469 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/convert_weight.py
+-r--------  2.0 unx    28383 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/pangualpha.py
+-r--------  2.0 unx     4372 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/pangualpha_config.py
+-r--------  2.0 unx     2394 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/pangualpha_processor.py
+-r--------  2.0 unx     8177 b- defN 24-Apr-19 15:26 mindformers/models/pangualpha/pangualpha_tokenizer.py
+-r--------  2.0 unx     1180 b- defN 24-Apr-19 15:26 mindformers/models/sam/__init__.py
+-r--------  2.0 unx     3489 b- defN 24-Apr-19 15:26 mindformers/models/sam/conver_weight.py
+-r--------  2.0 unx     5967 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam.py
+-r--------  2.0 unx     7758 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_config.py
+-r--------  2.0 unx    17459 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_image_encoder.py
+-r--------  2.0 unx     2944 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_layers.py
+-r--------  2.0 unx    22703 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_mask_decoder.py
+-r--------  2.0 unx     8556 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_processor.py
+-r--------  2.0 unx    10575 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_prompt_encoder.py
+-r--------  2.0 unx    22757 b- defN 24-Apr-19 15:26 mindformers/models/sam/sam_utils.py
+-r--------  2.0 unx      885 b- defN 24-Apr-19 15:26 mindformers/models/swin/__init__.py
+-r--------  2.0 unx     4949 b- defN 24-Apr-19 15:26 mindformers/models/swin/convert_weight.py
+-r--------  2.0 unx    16303 b- defN 24-Apr-19 15:26 mindformers/models/swin/swin.py
+-r--------  2.0 unx     7783 b- defN 24-Apr-19 15:26 mindformers/models/swin/swin_config.py
+-r--------  2.0 unx    31158 b- defN 24-Apr-19 15:26 mindformers/models/swin/swin_modules.py
+-r--------  2.0 unx     5472 b- defN 24-Apr-19 15:26 mindformers/models/swin/swin_processor.py
+-r--------  2.0 unx     1192 b- defN 24-Apr-19 15:26 mindformers/models/t5/__init__.py
+-r--------  2.0 unx     8077 b- defN 24-Apr-19 15:26 mindformers/models/t5/convert_weight.py
+-r--------  2.0 unx    10964 b- defN 24-Apr-19 15:26 mindformers/models/t5/mt5.py
+-r--------  2.0 unx    90291 b- defN 24-Apr-19 15:26 mindformers/models/t5/t5.py
+-r--------  2.0 unx    11544 b- defN 24-Apr-19 15:26 mindformers/models/t5/t5_config.py
+-r--------  2.0 unx     4164 b- defN 24-Apr-19 15:26 mindformers/models/t5/t5_processor.py
+-r--------  2.0 unx    20276 b- defN 24-Apr-19 15:26 mindformers/models/t5/t5_tokenizer.py
+-r--------  2.0 unx     9881 b- defN 24-Apr-19 15:26 mindformers/models/t5/t5_tokenizer_fast.py
+-r--------  2.0 unx      948 b- defN 24-Apr-19 15:26 mindformers/models/vit/__init__.py
+-r--------  2.0 unx     3364 b- defN 24-Apr-19 15:26 mindformers/models/vit/convert_weight.py
+-r--------  2.0 unx    13411 b- defN 24-Apr-19 15:26 mindformers/models/vit/vit.py
+-r--------  2.0 unx     8231 b- defN 24-Apr-19 15:26 mindformers/models/vit/vit_config.py
+-r--------  2.0 unx    37548 b- defN 24-Apr-19 15:26 mindformers/models/vit/vit_modules.py
+-r--------  2.0 unx     4925 b- defN 24-Apr-19 15:26 mindformers/models/vit/vit_processor.py
+-r--------  2.0 unx      992 b- defN 24-Apr-19 15:26 mindformers/modules/__init__.py
+-r--------  2.0 unx    50360 b- defN 24-Apr-19 15:26 mindformers/modules/activation.py
+-r--------  2.0 unx    13354 b- defN 24-Apr-19 15:26 mindformers/modules/kvcache_mgr.py
+-r--------  2.0 unx    48060 b- defN 24-Apr-19 15:26 mindformers/modules/layers.py
+-r--------  2.0 unx    14414 b- defN 24-Apr-19 15:26 mindformers/modules/local_block_sparse_attention.py
+-r--------  2.0 unx     4545 b- defN 24-Apr-19 15:26 mindformers/modules/paged_attention_mgr.py
+-r--------  2.0 unx     1335 b- defN 24-Apr-19 15:26 mindformers/modules/transformer/__init__.py
+-r--------  2.0 unx    39235 b- defN 24-Apr-19 15:26 mindformers/modules/transformer/moe.py
+-r--------  2.0 unx     8481 b- defN 24-Apr-19 15:26 mindformers/modules/transformer/op_parallel_config.py
+-r--------  2.0 unx   202745 b- defN 24-Apr-19 15:26 mindformers/modules/transformer/transformer.py
+-r--------  2.0 unx      897 b- defN 24-Apr-19 15:26 mindformers/pet/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Apr-19 15:26 mindformers/pet/constants.py
+-r--------  2.0 unx     4920 b- defN 24-Apr-19 15:26 mindformers/pet/pet_config.py
+-r--------  2.0 unx     3928 b- defN 24-Apr-19 15:26 mindformers/pet/pet_model.py
+-r--------  2.0 unx     1096 b- defN 24-Apr-19 15:26 mindformers/pet/utils.py
+-r--------  2.0 unx      696 b- defN 24-Apr-19 15:26 mindformers/pet/models/__init__.py
+-r--------  2.0 unx     3511 b- defN 24-Apr-19 15:26 mindformers/pet/models/lora.py
+-r--------  2.0 unx      955 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/ada_adapter.py
+-r--------  2.0 unx     1194 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/adalora_adapter.py
+-r--------  2.0 unx     5435 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/lora_adapter.py
+-r--------  2.0 unx     1649 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/pet_adapter.py
+-r--------  2.0 unx     1165 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/prefix_tuning_adapter.py
+-r--------  2.0 unx     1350 b- defN 24-Apr-19 15:26 mindformers/pet/tuners/ptuning2_adapter.py
+-r--------  2.0 unx     2160 b- defN 24-Apr-19 15:26 mindformers/pipeline/__init__.py
+-r--------  2.0 unx    10291 b- defN 24-Apr-19 15:26 mindformers/pipeline/base_pipeline.py
+-r--------  2.0 unx     2393 b- defN 24-Apr-19 15:26 mindformers/pipeline/build_pipeline.py
+-r--------  2.0 unx     6343 b- defN 24-Apr-19 15:26 mindformers/pipeline/fill_mask_pipeline.py
+-r--------  2.0 unx     7544 b- defN 24-Apr-19 15:26 mindformers/pipeline/image_classification_pipeline.py
+-r--------  2.0 unx     7168 b- defN 24-Apr-19 15:26 mindformers/pipeline/image_to_text_generation_pipeline.py
+-r--------  2.0 unx     6591 b- defN 24-Apr-19 15:26 mindformers/pipeline/masked_image_modeling_pipeline.py
+-r--------  2.0 unx     6226 b- defN 24-Apr-19 15:26 mindformers/pipeline/pipeline.py
+-r--------  2.0 unx    18559 b- defN 24-Apr-19 15:26 mindformers/pipeline/question_answering_pipeline.py
+-r--------  2.0 unx    27170 b- defN 24-Apr-19 15:26 mindformers/pipeline/segment_anything_pipeline.py
+-r--------  2.0 unx    10648 b- defN 24-Apr-19 15:26 mindformers/pipeline/text_classification_pipeline.py
+-r--------  2.0 unx    10971 b- defN 24-Apr-19 15:26 mindformers/pipeline/text_generation_pipeline.py
+-r--------  2.0 unx    10121 b- defN 24-Apr-19 15:26 mindformers/pipeline/token_classification_pipeline.py
+-r--------  2.0 unx     8167 b- defN 24-Apr-19 15:26 mindformers/pipeline/translation_pipeline.py
+-r--------  2.0 unx     9276 b- defN 24-Apr-19 15:26 mindformers/pipeline/zero_shot_image_classification_pipeline.py
+-r--------  2.0 unx     1127 b- defN 24-Apr-19 15:26 mindformers/tools/__init__.py
+-r--------  2.0 unx    10226 b- defN 24-Apr-19 15:26 mindformers/tools/check_rules.py
+-r--------  2.0 unx     4399 b- defN 24-Apr-19 15:26 mindformers/tools/download_tools.py
+-r--------  2.0 unx     6243 b- defN 24-Apr-19 15:26 mindformers/tools/download_tools_multithread.py
+-r--------  2.0 unx    12033 b- defN 24-Apr-19 15:26 mindformers/tools/export.py
+-r--------  2.0 unx     6520 b- defN 24-Apr-19 15:26 mindformers/tools/hccl_tools.py
+-r--------  2.0 unx     1925 b- defN 24-Apr-19 15:26 mindformers/tools/image_tools.py
+-r--------  2.0 unx    23336 b- defN 24-Apr-19 15:26 mindformers/tools/logger.py
+-r--------  2.0 unx     2316 b- defN 24-Apr-19 15:26 mindformers/tools/merge_hccl.py
+-r--------  2.0 unx     5598 b- defN 24-Apr-19 15:26 mindformers/tools/moe_token_distribution_tools.py
+-r--------  2.0 unx     3164 b- defN 24-Apr-19 15:26 mindformers/tools/transform_ckpt.py
+-r--------  2.0 unx    13716 b- defN 24-Apr-19 15:26 mindformers/tools/utils.py
+-r--------  2.0 unx      842 b- defN 24-Apr-19 15:26 mindformers/tools/cloud_adapter/__init__.py
+-r--------  2.0 unx     8522 b- defN 24-Apr-19 15:26 mindformers/tools/cloud_adapter/cloud_adapter.py
+-r--------  2.0 unx     3464 b- defN 24-Apr-19 15:26 mindformers/tools/cloud_adapter/cloud_monitor.py
+-r--------  2.0 unx      905 b- defN 24-Apr-19 15:26 mindformers/tools/register/__init__.py
+-r--------  2.0 unx    11037 b- defN 24-Apr-19 15:26 mindformers/tools/register/config.py
+-r--------  2.0 unx     7056 b- defN 24-Apr-19 15:26 mindformers/tools/register/register.py
+-r--------  2.0 unx     1982 b- defN 24-Apr-19 15:26 mindformers/trainer/__init__.py
+-r--------  2.0 unx    52670 b- defN 24-Apr-19 15:26 mindformers/trainer/base_trainer.py
+-r--------  2.0 unx     4428 b- defN 24-Apr-19 15:26 mindformers/trainer/build_trainer.py
+-r--------  2.0 unx    55851 b- defN 24-Apr-19 15:26 mindformers/trainer/config_args.py
+-r--------  2.0 unx     5925 b- defN 24-Apr-19 15:26 mindformers/trainer/optimizer_grouped_parameters.py
+-r--------  2.0 unx    51978 b- defN 24-Apr-19 15:26 mindformers/trainer/trainer.py
+-r--------  2.0 unx    15070 b- defN 24-Apr-19 15:26 mindformers/trainer/training_args.py
+-r--------  2.0 unx    35089 b- defN 24-Apr-19 15:26 mindformers/trainer/utils.py
+-r--------  2.0 unx      821 b- defN 24-Apr-19 15:26 mindformers/trainer/causal_language_modeling/__init__.py
+-r--------  2.0 unx    20146 b- defN 24-Apr-19 15:26 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
+-r--------  2.0 unx      866 b- defN 24-Apr-19 15:26 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
+-r--------  2.0 unx     4609 b- defN 24-Apr-19 15:26 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
+-r--------  2.0 unx      795 b- defN 24-Apr-19 15:26 mindformers/trainer/general_task_trainer/__init__.py
+-r--------  2.0 unx     9462 b- defN 24-Apr-19 15:26 mindformers/trainer/general_task_trainer/general_task_trainer.py
+-r--------  2.0 unx      928 b- defN 24-Apr-19 15:26 mindformers/trainer/image_classification/__init__.py
+-r--------  2.0 unx     4622 b- defN 24-Apr-19 15:26 mindformers/trainer/image_classification/group_ic_params.py
+-r--------  2.0 unx     9476 b- defN 24-Apr-19 15:26 mindformers/trainer/image_classification/image_classification.py
+-r--------  2.0 unx     8209 b- defN 24-Apr-19 15:26 mindformers/trainer/image_classification/zero_shot_image_classification.py
+-r--------  2.0 unx      823 b- defN 24-Apr-19 15:26 mindformers/trainer/image_to_text_generation/__init__.py
+-r--------  2.0 unx     6107 b- defN 24-Apr-19 15:26 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
+-r--------  2.0 unx      818 b- defN 24-Apr-19 15:26 mindformers/trainer/image_to_text_retrieval/__init__.py
+-r--------  2.0 unx    12076 b- defN 24-Apr-19 15:26 mindformers/trainer/image_to_text_retrieval/eval_utils.py
+-r--------  2.0 unx     7783 b- defN 24-Apr-19 15:26 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
+-r--------  2.0 unx      818 b- defN 24-Apr-19 15:26 mindformers/trainer/masked_image_modeling/__init__.py
+-r--------  2.0 unx     2197 b- defN 24-Apr-19 15:26 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
+-r--------  2.0 unx     7432 b- defN 24-Apr-19 15:26 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
+-r--------  2.0 unx      830 b- defN 24-Apr-19 15:26 mindformers/trainer/masked_language_modeling/__init__.py
+-r--------  2.0 unx     6859 b- defN 24-Apr-19 15:26 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
+-r--------  2.0 unx      799 b- defN 24-Apr-19 15:26 mindformers/trainer/question_answering/__init__.py
+-r--------  2.0 unx     9218 b- defN 24-Apr-19 15:26 mindformers/trainer/question_answering/question_answering.py
+-r--------  2.0 unx      803 b- defN 24-Apr-19 15:26 mindformers/trainer/text_classfication/__init__.py
+-r--------  2.0 unx     9386 b- defN 24-Apr-19 15:26 mindformers/trainer/text_classfication/text_classification.py
+-r--------  2.0 unx      807 b- defN 24-Apr-19 15:26 mindformers/trainer/token_classification/__init__.py
+-r--------  2.0 unx     9500 b- defN 24-Apr-19 15:26 mindformers/trainer/token_classification/token_classification.py
+-r--------  2.0 unx      795 b- defN 24-Apr-19 15:26 mindformers/trainer/translation/__init__.py
+-r--------  2.0 unx     6738 b- defN 24-Apr-19 15:26 mindformers/trainer/translation/translation_finetune.py
+-r--------  2.0 unx      913 b- defN 24-Apr-19 15:26 mindformers/wrapper/__init__.py
+-r--------  2.0 unx    14329 b- defN 24-Apr-19 15:26 mindformers/wrapper/adaptive_loss_scale.py
+-r--------  2.0 unx     4117 b- defN 24-Apr-19 15:26 mindformers/wrapper/build_wrapper.py
+-r--------  2.0 unx    12396 b- defN 24-Apr-19 15:26 mindformers/wrapper/wrapper.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-19 15:26 mindformers-1.0.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx    19381 b- defN 24-Apr-19 15:26 mindformers-1.0.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-19 15:26 mindformers-1.0.2.dist-info/WHEEL
+-r--------  2.0 unx       12 b- defN 24-Apr-19 15:26 mindformers-1.0.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    40696 b- defN 24-Apr-19 15:26 mindformers-1.0.2.dist-info/RECORD
+413 files, 4233276 bytes uncompressed, 1089566 bytes compressed:  74.3%
```

## zipnote {}

```diff
@@ -138,14 +138,17 @@
 
 Filename: configs/glm2/run_glm2_6b_ptuning2.yaml
 Comment: 
 
 Filename: configs/glm3/export_glm3_6b.yaml
 Comment: 
 
+Filename: configs/glm3/export_glm3_6b_pa.yaml
+Comment: 
+
 Filename: configs/glm3/run_glm3_6b.yaml
 Comment: 
 
 Filename: configs/glm3/run_glm3_6b_finetune_2k_910b.yaml
 Comment: 
 
 Filename: configs/gpt2/run_gpt2.yaml
@@ -1215,23 +1218,23 @@
 
 Filename: mindformers/wrapper/build_wrapper.py
 Comment: 
 
 Filename: mindformers/wrapper/wrapper.py
 Comment: 
 
-Filename: mindformers-1.0.1.dist-info/LICENSE
+Filename: mindformers-1.0.2.dist-info/LICENSE
 Comment: 
 
-Filename: mindformers-1.0.1.dist-info/METADATA
+Filename: mindformers-1.0.2.dist-info/METADATA
 Comment: 
 
-Filename: mindformers-1.0.1.dist-info/WHEEL
+Filename: mindformers-1.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: mindformers-1.0.1.dist-info/top_level.txt
+Filename: mindformers-1.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: mindformers-1.0.1.dist-info/RECORD
+Filename: mindformers-1.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## configs/README.md

```diff
@@ -133,14 +133,15 @@
         - do_sample: 使能top_k或top_p采样，为False时top_k和top_p均重置为1
         - use_past: 使能增量推理，为True时为增量推理，否则为自回归推理，使用时请参考[模型支持列表](https://gitee.com/mindspore/mindformers/tree/dev/docs#text-generator)
         - max_decode_length: 文本生成最大长度（输入长度统计在内）
         - repetition_penalty: 重复文本惩罚系数，该值不小于1，等于1时不惩罚
         - use_paged_attention: 是否开启Paged Attention推理，当前仅支持MS Lite推理时使用
         - pa_block_size: 使用Paged Attention推理时需设置，每块block的大小
         - pa_num_blocks: 使用Paged Attention推理时需设置，blocks的总数
+        - use_prompt_flash_attention: 是否开启Prompt Flash Attention推理，当前仅支持MS Lite推理全量阶段时使用
 - lr_schedule: 学习率配置
     - type: 学习率类
 - layer_scale: 是否开启层衰减
 - layer_decay: 层衰减系数
 - optimizer: 优化器配置
     - type: 优化器类
     - weight_decay: 权重衰减值
```

## configs/codellama/run_codellama_34b_910b.yaml

```diff
@@ -147,21 +147,18 @@
     theta: 1000000.0
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: True
     fine_grain_interleave: 2
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_13b.yaml

```diff
@@ -147,20 +147,17 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: False
     use_flash_attention: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_13b_910b.yaml

```diff
@@ -147,20 +147,17 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b.yaml

```diff
@@ -147,20 +147,17 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: False
     use_flash_attention: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b_910b.yaml

```diff
@@ -147,20 +147,17 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b_lora.yaml

```diff
@@ -149,20 +149,17 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_dtype: "float32"
     softmax_compute_dtype: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: False
     use_flash_attention: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama_7b_lora"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## configs/llama2/export_llama2_13b.yaml

```diff
@@ -71,26 +71,24 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False
     use_paged_attention: False  # PA only supported in inference
     block_size: 16
     num_blocks: 512
     is_dynamic: False
     use_kvcache_op: False
     is_flexible_shape: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "{path}/llama2_13b.ckpt" # 导出任务这里必填
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/export_llama2_7b.yaml

```diff
@@ -68,26 +68,24 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False
     use_paged_attention: False  # PA only supported in inference
     block_size: 16
     num_blocks: 512
     is_dynamic: False
     use_kvcache_op: False
     is_flexible_shape: False
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "{path}/llama2_7b.ckpt" # 导出任务这里必填
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/predict_llama2_70b_910b.yaml

```diff
@@ -28,15 +28,15 @@
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: False
+  enable_parallel_optimizer: True
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
 # default parallel of device num = 32 for Atlas 800T A2
 parallel_config:
   data_parallel: 1
```

## configs/llama2/run_llama2_13b.yaml

```diff
@@ -149,20 +149,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_13b_910b_auto_parallel.yaml

```diff
@@ -152,20 +152,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_13b_910b_finetune.yaml

```diff
@@ -119,15 +119,15 @@
 # mindspore context init config
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
   graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
   max_call_depth: 10000
-  max_device_memory: "58GB"
+  max_device_memory: "59GB"
   save_graphs: False
   save_graphs_path: "./graph"
   device_id: 0
   runtime_num_threads: 1
 
 # model config
 model:
```

## configs/llama2/run_llama2_13b_lora_910b.yaml

```diff
@@ -149,20 +149,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## configs/llama2/run_llama2_70b.yaml

```diff
@@ -150,20 +150,19 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_70b_910b_auto_parallel.yaml

```diff
@@ -155,20 +155,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_70b_910b_finetune.yaml

```diff
@@ -120,15 +120,15 @@
 # mindspore context init config
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
   graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
   max_call_depth: 10000
-  max_device_memory: "58GB"
+  max_device_memory: "55GB"
   save_graphs: False
   save_graphs_path: "./graph"
   device_id: 0
 
 # model config
 model:
   model_config:
@@ -155,15 +155,14 @@
     use_past: False
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     use_flash_attention: True  # FA can accelerate training or finetune
     fine_grain_interleave: 2
     qkv_concat: false
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b.yaml

```diff
@@ -149,20 +149,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b_910b_auto_parallel.yaml

```diff
@@ -152,20 +152,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: True  # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b_lora_910b.yaml

```diff
@@ -149,20 +149,18 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
-    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
-    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b_lora"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## mindformers/.commit_id

```diff
@@ -1,8 +1,8 @@
 r1.0
-commit c9b9436
-Merge: 8bb203e c5b84a8
+commit 45d1fa5
+Merge: aca5be9 6872843
 Author: i-robot <huawei_ci_bot@163.com>
-Date:   Thu Mar 14 11:29:13 2024 +0000
+Date:   Fri Apr 19 15:24:24 2024 +0000
 
-    !2428 【r1.0】【bugfix】边训练边评估流程中eval_network使用_virtual_dataset封装
-    Merge pull request !2428 from huanglei/virtual_ds
+    !2762 【r1.0】修复glm2_6b_ptuning2在增量推理时kvcache的序列维度没有扩充prefix的问题
+    Merge pull request !2762 from Xinrui Chen/r1.0-glm2config
```

## mindformers/version_control.py

```diff
@@ -169,29 +169,38 @@
 
 
 def fix_optim_global_step_sig():
     # when the version of mindspore bigger than 2.2.0, it should update global step explicitly.
     return is_version_ge(ms.__version__, "2.2.0")
 
 
-def check_valid_flash_attention(import_fa_valid=True):
-    """check mindspore version is valid for flash attention"""
-    version_valid = is_version_ge(ms.__version__, "2.2.0")
-    # below ms 2.2.0 is not support
+def check_valid_flash_attention(import_fa_valid=True, fa_type=None):
+    """check mindspore version is valid for input flash attention"""
+    version_map = {"PromptFlashAttention": "2.2.12",
+                   "FlashAttention": "2.2.0"}
+    valid_version = version_map.get(fa_type)
+    if not is_910b() and fa_type == "PromptFlashAttention":
+        logger.warning(f"Current device {get_ascend_soc_version()} do not support {fa_type}, "
+                       f"please use 910b device.")
+        return False
+    if valid_version is None:
+        raise ValueError(f"fa_type should be in {list(version_map.keys())}, but get {fa_type}")
+    version_valid = is_version_ge(ms.__version__, valid_version)
     if not version_valid:
-        logger.warning("Current MindSpore do not support FlashAttention, please upgrade to 2.2.0 or later version.")
+        logger.warning(f"Current MindSpore do not support {fa_type}, "
+                       f"please upgrade to {valid_version} or later version.")
         logger.warning("Now running on self-attention mode.")
         result = False
-    # ms 2.2.0 or latter version but import error is not support
     elif not import_fa_valid:
-        logger.warning("Import FlashAttention ERROR, please upgrade your MindSpore to 2.2.0 or later version. ")
+        logger.warning(f"Import {fa_type} ERROR, please upgrade your MindSpore to {valid_version} or later version. ")
         logger.warning("Now running on self-attention mode.")
         result = False
     # both pass should return True
     else:
+        logger.info(f"Enable {fa_type}.")
         result = True
     return result
 
 def choose_flash_attention_dtype():
     """
     attention_mask dtype should be float16 on ms 2.2.0, uint8 on 2.2.10
     ms version below 2.2.0 won't be in this func
@@ -240,9 +249,10 @@
     version_valid = is_version_ge(ms.__version__, "2.2.12")
     # below ms 2.2.12 is not support
     if not version_valid:
         logger.warning("Current MindSpore do not support PagedAttention, please upgrade to 2.2.12 or later version.")
         logger.warning("Now running on self-attention mode.")
         result = False
     else:
+        logger.info("Enable paged attention in inference.")
         result = True
     return result
```

## mindformers/inference/infers/base_infer.py

```diff
@@ -138,18 +138,18 @@
         if config is None:
             config = InferConfig()
         self.config = config
         self.model_type = config.model_type
         self.model_name = config.model_name
         self.seq_length = config.seq_length
         self.config_path = config.config_path
-        self.dynamic = config.dynamic
         self.paged_attention = config.paged_attention
         self.block_size = config.block_size
         self.num_blocks = config.num_blocks
+        self.dynamic = config.dynamic and not self.paged_attention
 
         self.context = build_context(config=self.config)
         self.tokenizer = tokenizer
         self.image_processor = image_processor
         self.full_model = None
         self.cache_model = None
         if config.prefill_model_path and config.increment_model_path:
```

## mindformers/inference/infers/text_generator_infer.py

```diff
@@ -191,16 +191,16 @@
     """
     Input of llm model.
     """
     MAPPING = {
         "bloom": CommonInputsOfInfer,
         "llama": LlamaInputsOfInfer,
         "codellama": LlamaInputsOfInfer,
-        "glm2": CommonInputsOfInfer,
-        "glm3": CommonInputsOfInfer,
+        "glm2": LlamaInputsOfInfer,
+        "glm3": LlamaInputsOfInfer,
         "gpt2": CommonInputsOfInfer,
         "codegeex2": CommonInputsOfInfer,
         "glm": GLMInputsOfInfer,
         "baichuan2": LlamaInputsOfInfer,
         "internlm": LlamaInputsOfInfer,
         "qwen": LlamaInputsOfInfer,
         "common": CommonInputsOfInfer
@@ -489,14 +489,15 @@
         # setup is_first_iteration flag for incremental infer
         is_first_iteration = True
         is_finished = [False] * batch_size
         use_past = self.full_model and self.cache_model
 
         if self.dynamic:
             batch_size_gear, act_len_gear = self.dynshape_gears.match(batch_size, max_length)
+            logger.info(f"[DYNAMIC] found matched gear: batch_size={batch_size_gear}, seq_length={act_len_gear}")
             bs_pad = batch_size_gear - batch_size
             pad_input_ids = np.pad(pad_input_ids, ((0, bs_pad), (0, 0)), 'constant', constant_values=pad_token_id)
             valid_length = np.pad(valid_length, (0, bs_pad), 'constant', constant_values=1)
             is_finished += [True] * bs_pad
             real_input_ids = np.array([np.pad(input_ids[i], (0, real_pad_length[i]), 'constant',
                                               constant_values=pad_token_id) for i in range(len(input_ids))], np.int32)
             real_input_ids = np.pad(real_input_ids, ((0, bs_pad), (0, 0)), 'constant', constant_values=pad_token_id)
```

## mindformers/models/bloom/layers.py

```diff
@@ -494,15 +494,15 @@
                                           offset,
                                           use_past,
                                           moe_config,
                                           parallel_config)
 
         config_to_layer = parallel_config.moe_parallel_config if self.use_moe else parallel_config.dp_mp_config
         if use_flash_attention:
-            use_flash_attention = check_valid_flash_attention(FLASHATTENTION_IMPORT_VALID)
+            use_flash_attention = check_valid_flash_attention(FLASHATTENTION_IMPORT_VALID, 'FlashAttention')
         if _get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation():
             self.num_layers = num_layers
             self.blocks = nn.CellList()
             for i in range(num_layers):
                 block = BloomBlock(hidden_size=hidden_size,
                                    batch_size=batch_size,
                                    ffn_hidden_size=ffn_hidden_size,
```

## mindformers/models/glm2/glm2.py

```diff
@@ -9,36 +9,43 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """ChatGLM2 model."""
+import mindspore as ms
 import mindspore.ops as ops
 import mindspore.common.dtype as mstype
 import mindspore.ops.operations as P
 from mindspore import Tensor, nn
+from mindspore.common.initializer import initializer
+from mindspore.context import ParallelMode
+from mindspore.parallel._utils import _get_parallel_mode
 from mindpet.delta.ptuning2 import PrefixEncoder
 
 import numpy as np
+from mindformers.tools import logger
 from mindformers.mindformer_book import MindFormerBook
-from mindformers.modules import VocabEmbedding, EmbeddingOpParallelConfig
 from mindformers.modules.layers import Linear
+from mindformers.modules import VocabEmbedding, EmbeddingOpParallelConfig
+from mindformers.modules.transformer.transformer import LowerTriangularMaskWithDynamic
 from mindformers.tools.register import MindFormerModuleType, MindFormerRegister
 from mindformers.core.loss import CrossEntropyLoss
 from mindformers.pet.tuners.pet_adapter import PetAdapter
-from mindformers.version_control import get_tril
+from mindformers.version_control import get_tril, check_valid_paged_attention
+from mindformers.modules import KVCachePreprocess
 
 from ..base_model import BaseModel
 from ..utils import cell_reuse
 from .glm2_config import ChatGLM2Config
-from .glm2_modules import precompute_rotary_emb_cache
+from .glm2_modules import RopeCache
 from .glm2_transformer import ChatGLM2Transformer
 
-__all__ = ['ChatGLM2ForConditionalGeneration', 'ChatGLM2Model', 'ChatGLM2WithPtuning2']
+__all__ = ['ChatGLM2ForConditionalGeneration', 'ChatGLM2WithPtuning2']
 
 
 class ChatGLM2Model(nn.Cell):
     r"""
     The backbone of ChatGLM2 network
 
     Args:
@@ -49,46 +56,84 @@
         self.num_layers = config.num_layers
         self.multi_query_group_num = config.multi_query_group_num
         self.kv_channels = config.kv_channels
         self.seq_length = config.seq_length
         self.compute_dtype = config.compute_dtype
         self.use_past = config.use_past
         self.is_first_iteration = True
-        # vocab embedding
-        embed_parallel_config = EmbeddingOpParallelConfig()
-        embed_parallel_config.data_parallel = config.parallel_config.data_parallel
-        embed_parallel_config.model_parallel = config.parallel_config.model_parallel
-        embed_parallel_config.vocab_emb_dp = False
-        self.embedding = VocabEmbedding(vocab_size=config.vocab_size, embedding_size=config.hidden_size,
-                                        parallel_config=embed_parallel_config)
-        self.embedding.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
+        self.pre_seq_len = config.pre_seq_len
+
+        self.is_dynamic = config.is_dynamic
+        self.use_kvcache_op = config.use_kvcache_op
+        self.is_flexible_shape = config.is_flexible_shape
+        self.use_paged_attention = config.use_paged_attention and check_valid_paged_attention()
+        if self.use_paged_attention:
+            logger.info("Enable paged attention.")
+
+        self.shape = P.Shape()
+        self.reshape = P.Reshape()
+        if config.is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
 
         # rotary embedding
         rotary_dim = (
             config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels
         )
-        self.rotary_pos_emb = precompute_rotary_emb_cache(
-            seq_len=self.seq_length,
-            dim=rotary_dim // 2
-        )
-        self.rotary_pos_emb = Tensor(self.rotary_pos_emb, config.compute_dtype)
+        self.rope_cache = RopeCache(config=config, dim=rotary_dim // 2, is_dynamic=config.is_dynamic)
+        use_flash_attention_flag = config.use_flash_attention or config.use_prompt_flash_attention
+        self.casual_mask = LowerTriangularMaskWithDynamic(seq_length=config.seq_length,
+                                                          compute_type=config.compute_dtype,
+                                                          mask_type=config.mask_type,
+                                                          is_dynamic=config.is_dynamic,
+                                                          pad_token_id=config.pad_token_id,
+                                                          use_flash_attention=use_flash_attention_flag)
+
+        max_seq_length = config.seq_length if not self.pre_seq_len else config.seq_length + self.pre_seq_len
+        self.kvcache_preprocess = KVCachePreprocess(max_batch_size=config.batch_size,
+                                                    max_seq_length=max_seq_length,
+                                                    is_dynamic=config.is_dynamic,
+                                                    use_kvcache_op=config.use_kvcache_op,
+                                                    is_flexible_shape=config.is_flexible_shape,
+                                                    use_paged_attention=self.use_paged_attention)
+
+        # vocab embedding
+        embed_parallel_config = EmbeddingOpParallelConfig()
+        embed_parallel_config.data_parallel = config.parallel_config.data_parallel
+        embed_parallel_config.model_parallel = config.parallel_config.model_parallel
+        embed_parallel_config.vocab_emb_dp = config.parallel_config.vocab_emb_dp
+        self.embedding = VocabEmbedding(vocab_size=config.vocab_size, embedding_size=config.hidden_size,
+                                        parallel_config=embed_parallel_config,
+                                        param_init=initializer('normal',
+                                                               [config.vocab_size, config.hidden_size],
+                                                               dtype=config.embedding_type),
+                                        )
+        self.embedding.embedding_table.parallel_optimizer = True
+        if config.parallel_config.pipeline_stage > 1:
+            self.embedding.pipeline_stage = 0
+            self.embedding.set_comm_fusion(2)
+        else:
+            self.embedding.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
         self.encoder = ChatGLM2Transformer(config)
 
         self.output_layer = Linear(config.hidden_size,
                                    config.vocab_size,
                                    has_bias=False,
                                    param_init_type=config.param_init_type,
-                                   compute_dtype=config.compute_dtype)
+                                   compute_dtype=config.compute_dtype,
+                                   skip_redistribution=config.is_dynamic)
+
         self.output_layer.shard(strategy_matmul=((config.parallel_config.data_parallel, 1),
                                                  (config.parallel_config.model_parallel, 1)))
+
         if config.parallel_config.pipeline_stage > 1:
             self.output_layer.pipeline_stage = config.parallel_config.pipeline_stage - 1
-        self.output_layer.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
-
+            self.output_layer.set_comm_fusion(2)
+        else:
+            self.output_layer.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
         self.tril = get_tril()
         self.ones = P.Ones()
         self.less = P.Less()
         self.gather = P.Gather()
         self.expand_dims = P.ExpandDims()
         self.reshape = P.Reshape()
@@ -109,38 +154,51 @@
         if self.use_past and not self.is_first_iteration:
             # [bs, 1, seq_len] for incremental infer
             attention_mask = self.gather(attention_mask.view(-1, self.seq_length), input_position, 0)
         # [bs, 1, seq_len, seq_len] for normal, [bs, 1, 1, seq_len] for incremental infer
         attention_mask = self.reshape(attention_mask, (batch_size, 1, -1, self.seq_length))
         return attention_mask
 
-    def construct(self, input_ids, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, full_attention_mask=None,
-                  prefix_key_values=None):
+    def construct(self, input_ids, batch_valid_length=None, batch_index=None, zactivate_len=None,
+                  block_tables=None, slot_mapping=None, prefix_key_values=None):
         """ChatGLM2 model."""
-        _ = position_ids
-        batch_size, _ = input_ids.shape
-        if input_embeds is None:
-            input_embeds, _ = self.embedding(input_ids)  # (bs, seq_len, hs)
-
-        if full_attention_mask is None:
-            # (bs, 1, seq_len, seq_len)
-            full_attention_mask = self.get_masks(batch_size, attention_mask, input_position)
-
-        # (sen length, kv_channels // 4, 2)
-        rotary_pos_emb = self.rotary_pos_emb
-        if self.use_past and not self.is_first_iteration and batch_valid_length is not None:
-            # only take [bs, 1, kv_channels // 4, 2]
-            batch_gather_position = batch_valid_length.view(-1, 1) - 1  # [bs, seq_len=1]
-            rotary_pos_emb = self.gather(rotary_pos_emb, batch_gather_position, 0)
+        batch_size, seq_len = self.shape(input_ids)
+        input_embeds, _ = self.embedding(input_ids)
+
+        if not self.use_past:
+            rotary_pos_emb = self.rope_cache()
+            full_attention_mask = self.casual_mask(input_ids)  # full_attention_mask: [bs, seq, seq]
+            full_attention_mask = self.casual_mask.post_process(full_attention_mask)
+            kvcache_inputs = None
+        else:
+            if self.is_first_iteration:
+                rotary_pos_emb = self.rope_cache(seq_len)  # 2048, 32, 2
+                full_attention_mask = self.casual_mask(input_ids)  # full_attention_mask: [bs, seq, seq]
+            else:
+                rotary_pos_emb = self.rope_cache.increment(batch_valid_length, batch_size)
+                if self.is_dynamic and self.is_flexible_shape and not self.use_kvcache_op:
+                    full_attention_mask = self.casual_mask.increment_slice(
+                        self.kvcache_preprocess.range,
+                        self.kvcache_preprocess.max_cache_length // batch_size, batch_valid_length,
+                        zactivate_len)
+                else:
+                    full_attention_mask = self.casual_mask.increment(self.kvcache_preprocess.range,
+                                                                     batch_valid_length, zactivate_len)
+
+            full_attention_mask = self.casual_mask.post_process(full_attention_mask)
+
+            if batch_valid_length is not None and isinstance(self.pre_seq_len, int):
+                batch_valid_length = batch_valid_length + self.pre_seq_len
+
+            kvcache_inputs = self.kvcache_preprocess(batch_size, batch_valid_length, batch_index, zactivate_len,
+                                                     block_tables, slot_mapping)
 
         # Run encoder.
         hidden_states = self.encoder(
-            input_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,
-            init_reset=init_reset, batch_valid_length=batch_valid_length,
+            input_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb, kvcache_inputs=kvcache_inputs,
             prefix_key_values=prefix_key_values)
 
         return hidden_states
 
 
 @MindFormerRegister.register(MindFormerModuleType.MODELS)
 class ChatGLM2ForConditionalGeneration(BaseModel):
@@ -156,93 +214,154 @@
     _support_list = MindFormerBook.get_model_support_list()['glm2']
     _support_list.extend(MindFormerBook.get_model_support_list()['glm3'])
     _support_list.extend(MindFormerBook.get_model_support_list()['codegeex2'])
 
     @cell_reuse
     def __init__(self, config: ChatGLM2Config, **kwargs):
         super(ChatGLM2ForConditionalGeneration, self).__init__(config, **kwargs)
+        self.config = config
+        self.shape = P.Shape()
+        self.reshape = P.Reshape()
+        if config.is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
+        self.sub_batch_valid_len = P.Sub()
+
         self.max_seq_len = config.max_length
         self.transformer = ChatGLM2Model(config=config)
         self.cast = P.Cast()
         self.gather = P.Gather()
         self.is_first_iteration = True
         self.loss = CrossEntropyLoss(parallel_config=config.parallel_config)
         self.gmask = config.gmask_token_id
         self.bos_token_id = config.bos_token_id
         self.use_past = config.use_past
         self.is_first_iteration = True
         self.not_equal = P.NotEqual()
         self.add = P.Add()
         self.load_checkpoint(config)
         self.vocab_size = config.padded_vocab_size
+        dp = config.parallel_config.data_parallel
+        if _get_parallel_mode() not in (ParallelMode.AUTO_PARALLEL,):
+            self.gather.shard(((dp, 1), (dp,)))
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         """prepare inputs for generation."""
         input_position = kwargs.get("current_index", None)
         if input_position is not None:
             input_position = Tensor(input_position, mstype.int32)
         return {
             "input_ids": Tensor(input_ids, mstype.int32),
             "input_position": input_position
         }
 
+    def prepare_inputs_for_export(self, full_model=True):
+        dyn = self.config.is_dynamic
+        use_paged_attention = self.config.use_paged_attention and check_valid_paged_attention()
+        use_past = self.config.use_past
+        if dyn:
+            logger.info(f"Exporting dynamic MindIR...")
+        if use_paged_attention:
+            logger.info(f"Exporting model with paged attention...")
+        seq_length = self.config.seq_length
+        bs = None if dyn else self.config.batch_size
+        seq_len = None if dyn else self.config.seq_length
+
+        max_num_blocks_pre_batch = None if dyn else seq_len // self.config.block_size
+        logger.info(f"max num blocks pre batch: {max_num_blocks_pre_batch}")
+
+        def dummy_tensor(shape, dtype):
+            if None in shape:
+                return Tensor(shape=shape, dtype=dtype)
+            return Tensor(np.ones(shape=tuple(shape)), dtype=dtype)
+
+        batch_valid_length = dummy_tensor(shape=[bs], dtype=ms.int32) if use_past else None
+        batch_index = None if use_paged_attention else dummy_tensor(shape=[bs], dtype=ms.int64)
+        zactivate_len = None if use_paged_attention else dummy_tensor(shape=[seq_len], dtype=ms.int64)
+        prefill_mapping_len = None if dyn else bs * seq_len
+        inc_mapping_len = None if dyn else bs * 1
+
+        if full_model:
+            logger.info('\nexporting with batch_size = %s, seq = %s ...', self.config.batch_size, seq_length)
+            input_ids = dummy_tensor(shape=[bs, seq_len], dtype=ms.int32)
+            slot_mapping = dummy_tensor(shape=[prefill_mapping_len], dtype=ms.int32) if use_paged_attention else None
+            block_tables = None
+        else:
+            logger.info('\nexporting with batch_size = %s, seq = 1 ...', self.config.batch_size)
+            input_ids = dummy_tensor(shape=[bs, 1], dtype=ms.int32)
+            slot_mapping = dummy_tensor(shape=[inc_mapping_len], dtype=ms.int32) if use_paged_attention else None
+            block_tables = dummy_tensor(shape=[inc_mapping_len, max_num_blocks_pre_batch],
+                                        dtype=ms.int32) if use_paged_attention else None
+        return input_ids, None, None, None, None, None, None, batch_valid_length, None, batch_index, zactivate_len, \
+               block_tables, slot_mapping
+
+    # pylint: disable=W0613
     def construct(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None):
-        """ChatGLM2 for conditional generation model."""
-        # input_ids: (bs, seq_len)
-        # position_ids: (bs, seq_len)
-        # attention_mask: (bs, seq_len)
-        bs, seq_len = input_ids.shape
+                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None,
+                  batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None):
+        """ChatGLM2k for conditional generation model."""
+        bsz, seqlen = self.shape(input_ids)
+        tokens = input_ids
+        if self.use_past:
+            if not isinstance(batch_valid_length, Tensor):
+                batch_valid_length = self.ones((bsz,), mstype.int32)
+        if batch_valid_length is not None:
+            batch_valid_length = self.reshape(batch_valid_length, (-1,))
+        if not self.is_first_iteration:
+            batch_valid_length = self.sub_batch_valid_len(batch_valid_length, 1)
+
         hidden_states = self.transformer(
-            input_ids=input_ids,
-            input_position=input_position,
-            position_ids=position_ids,
-            attention_mask=attention_mask,
-            input_embeds=input_embeds,
-            init_reset=init_reset,
+            input_ids=tokens,
             batch_valid_length=batch_valid_length,
+            batch_index=batch_index,
+            zactivate_len=zactivate_len,
+            block_tables=block_tables,
+            slot_mapping=slot_mapping,
             prefix_key_values=prefix_key_values
         )
+        # gather前移
+        pre_gather = (not self.use_past or self.is_first_iteration) and batch_valid_length is not None
+        if pre_gather:
+            hidden_states = self.reshape(hidden_states, (-1, hidden_states.shape[-1]))
+            hidden_states = self.gather(hidden_states, self.sub_batch_valid_len(batch_valid_length, 1), 0)
+
         lm_logits = self.transformer.output_layer(hidden_states)
+        if not self.training and not pre_gather:
+            lm_logits = self.reshape(lm_logits, (bsz, seqlen, -1))
+
         outputs = (lm_logits,)
 
         # train
         if labels is not None:
-            logits = lm_logits.to(mstype.float32)
-            labels = labels.reshape((-1,))
-            logits = logits.reshape((-1, logits.shape[-1]))
+            logits = self.cast(lm_logits, mstype.float32)
+            logits_shape = logits.shape
+            labels = self.reshape(labels, (-1,))
+            logits = self.reshape(logits, (-1, logits_shape[-1]))
             input_mask = self.not_equal(labels, -100).to(mstype.float32)
-            input_mask = input_mask.reshape((-1,))
+            input_mask = self.reshape(input_mask, (-1,))
 
             if self.training:
                 # if training, return loss directly
                 outputs = self.loss(logits, labels, input_mask)
             else:
                 # eval in train ppl
                 # pre-shift to fit mindformers/core/metric/utils.py:PerplexityCell
-                zeros = ops.zeros((bs, 1, self.vocab_size), dtype=logits.dtype)
-                logits = logits.reshape((bs, seq_len, self.vocab_size))
+                zeros = ops.zeros((bsz, 1, self.vocab_size), dtype=logits.dtype)
+                logits = logits.reshape((bsz, seqlen, self.vocab_size))
                 logits = ops.cat((logits, zeros), axis=1)
 
-                zeros = ops.zeros((bs, 1), dtype=labels.dtype)
-                labels = labels.reshape((bs, seq_len))
+                zeros = ops.zeros((bsz, 1), dtype=labels.dtype)
+                labels = labels.reshape((bsz, seqlen))
                 labels = ops.cat((zeros, labels), axis=1)
 
                 zeros = zeros.to(input_mask.dtype)
-                input_mask = input_mask.reshape((bs, seq_len))
+                input_mask = input_mask.reshape((bsz, seqlen))
                 input_mask = ops.cat((zeros, input_mask), axis=1)
 
                 outputs = logits, labels, input_mask
 
-        # generation process
-        if (not self.use_past or self.is_first_iteration) and input_position is not None:
-            lm_logits = lm_logits.reshape((-1, lm_logits.shape[-1]))
-            lm_logits = self.gather(lm_logits, input_position, 0)
-            outputs = (lm_logits,)
-
         return outputs
 
 
 @MindFormerRegister.register(MindFormerModuleType.MODELS)
 class ChatGLM2WithPtuning2(ChatGLM2ForConditionalGeneration):
     """
     ChatGLM2 Model for pretraining with p-tuning-v2
@@ -281,24 +400,29 @@
             config.checkpoint_name_or_path = ckpt_cfg
             self.load_checkpoint(config)
 
         # freeze pretrained model
         PetAdapter.freeze_pretrained_model(self, config.pet_config.pet_type)
 
     def construct(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None):
+                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None,
+                  batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None):
 
         if not self.use_past or self.is_first_iteration:
             batch_size = input_ids.shape[0]
             prefix_key_values = self.prefix_encoder(batch_size)
 
         return super().construct(
             input_ids=input_ids,
             labels=labels,
             input_position=input_position,
             position_ids=position_ids,
             attention_mask=attention_mask,
             input_embeds=input_embeds,
             init_reset=init_reset,
             batch_valid_length=batch_valid_length,
-            prefix_key_values=prefix_key_values
+            prefix_key_values=prefix_key_values,
+            batch_index=batch_index,
+            zactivate_len=zactivate_len,
+            block_tables=block_tables,
+            slot_mapping=slot_mapping
         )
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## mindformers/models/glm2/glm2_config.py

```diff
@@ -9,44 +9,54 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """ChatGLM2 config"""
+
+from typing import Union
+
+from mindspore._checkparam import args_type_check
+
 from mindformers.tools.register import MindFormerRegister, MindFormerModuleType
-from mindformers.modules.transformer.transformer import default_transformer_config
-from ..base_config import BaseConfig
-from ..utils import convert_mstype
-from ...mindformer_book import MindFormerBook
+from mindformers.modules.transformer.transformer import default_transformer_config, TransformerOpParallelConfig
+from mindformers.models.base_config import BaseConfig
+from mindformers.models.utils import convert_mstype
+from mindformers.tools.logger import logger
+from mindformers.mindformer_book import MindFormerBook
 
 __all__ = ['ChatGLM2Config']
 
 
 @MindFormerRegister.register(MindFormerModuleType.CONFIG)
 class ChatGLM2Config(BaseConfig):
     """
     ChatGLM2 model config class.
     """
+
+    model_type = "glm2"
     _support_list = MindFormerBook.get_config_support_list()['glm2']
     _support_list.extend(MindFormerBook.get_config_support_list()['glm3'])
     _support_list.extend(MindFormerBook.get_config_support_list()['codegeex2'])
 
+    @args_type_check(parallel_config=(dict, TransformerOpParallelConfig))
     def __init__(self,
-                 batch_size=1,   # only for incremental infer
+                 batch_size=1,  # only for incremental infer
                  num_layers=28,
                  padded_vocab_size=65024,
                  hidden_size=4096,
                  ffn_hidden_size=13696,
                  kv_channels=128,
                  num_attention_heads=32,
                  seq_length=2048,
                  hidden_dropout=0.0,
                  attention_dropout=0.0,
                  layernorm_epsilon=1e-5,
+                 rope_ratio=1,
                  rmsnorm=True,
                  apply_residual_connection_post_layernorm=False,
                  post_layer_norm=True,
                  add_bias_linear=False,
                  add_qkv_bias=True,
                  bias_dropout_fusion=True,
                  multi_query_attention=True,
@@ -56,34 +66,53 @@
                  fp32_residual_connection=False,
                  quantization_bit=0,
                  pre_seq_len=None,
                  prefix_projection=False,
                  param_init_type: str = "float16",
                  compute_dtype: str = "float16",
                  layernorm_compute_type: str = "float32",
+                 embedding_type: str = "float32",
+                 mask_type: str = "float32",
+                 prefix_name: str = "glm2",
                  use_past=False,
                  use_flash_attention=False,
+                 use_prompt_flash_attention=False,
+                 use_incre_flash_attention=False,
+                 use_paged_attention: bool = False,
+                 is_dynamic: bool = False,
+                 use_kvcache_op: bool = False,
+                 is_flexible_shape: bool = False,
+                 block_size: int = 16,
+                 num_blocks: int = 512,
+                 no_recompute_layers=None,
                  eos_token_id=2,
                  pad_token_id=0,
+                 gmask_token_id=None,
+                 bos_token_id=None,
                  repetition_penalty=1.0,
-                 parallel_config=default_transformer_config,
+                 checkpoint_name_or_path=None,
+                 max_length=None,
+                 parallel_config: Union[dict, TransformerOpParallelConfig] = default_transformer_config,
                  **kwargs):
         super().__init__(**kwargs)
+        if isinstance(parallel_config, dict):
+            parallel_config = TransformerOpParallelConfig(**parallel_config)
         self.batch_size = batch_size
         self.num_layers = num_layers
         self.vocab_size = padded_vocab_size
         self.padded_vocab_size = padded_vocab_size
         self.hidden_size = hidden_size
         self.ffn_hidden_size = ffn_hidden_size
         self.kv_channels = kv_channels
         self.num_attention_heads = num_attention_heads
         self.seq_length = seq_length
         self.hidden_dropout = hidden_dropout
         self.attention_dropout = attention_dropout
         self.layernorm_epsilon = layernorm_epsilon
+        self.rope_ratio = rope_ratio
         self.rmsnorm = rmsnorm
         self.apply_residual_connection_post_layernorm = apply_residual_connection_post_layernorm
         self.post_layer_norm = post_layer_norm
         self.add_bias_linear = add_bias_linear
         self.add_qkv_bias = add_qkv_bias
         self.bias_dropout_fusion = bias_dropout_fusion
         self.multi_query_attention = multi_query_attention
@@ -93,13 +122,33 @@
         self.fp32_residual_connection = fp32_residual_connection
         self.quantization_bit = quantization_bit
         self.pre_seq_len = pre_seq_len
         self.prefix_projection = prefix_projection
         self.param_init_type = convert_mstype(param_init_type)
         self.compute_dtype = convert_mstype(compute_dtype)
         self.layernorm_compute_type = convert_mstype(layernorm_compute_type)
+        self.embedding_type = convert_mstype(embedding_type)
+        self.mask_type = convert_mstype(mask_type)
+        self.prefix_name = prefix_name
         self.use_past = use_past
         self.use_flash_attention = use_flash_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
+        self.use_incre_flash_attention = use_incre_flash_attention
+        self.use_paged_attention = use_paged_attention
+        self.block_size = block_size
+        self.num_blocks = num_blocks
+        self.is_dynamic = is_dynamic
+        self.use_kvcache_op = use_kvcache_op
+        self.is_flexible_shape = is_flexible_shape
+        self.no_recompute_layers = no_recompute_layers
         self.eos_token_id = eos_token_id
         self.pad_token_id = pad_token_id
         self.repetition_penalty = repetition_penalty
         self.parallel_config = parallel_config
+        self.checkpoint_name_or_path = checkpoint_name_or_path
+        self.max_length = max_length if max_length else seq_length
+        self.gmask_token_id = gmask_token_id
+        self.bos_token_id = bos_token_id
+        if batch_size * seq_length // self.block_size >= self.num_blocks:
+            logger.warning(
+                f"Argument `num blocks` is less than the maximum possible block numbers. "
+                f"May cause `block pool is out of memory` error")
```

## mindformers/models/glm2/glm2_modules.py

```diff
@@ -11,53 +11,91 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """ChatGLM2 Modules."""
 import numpy as np
 
+import mindspore as ms
 import mindspore.common.dtype as mstype
 import mindspore.ops.operations as P
 from mindspore import nn, Parameter, Tensor
 from mindspore.common.initializer import initializer
 
 from mindformers.modules.layers import Linear
+from mindformers.tools.utils import is_version_ge
 
 from .glm2_config import ChatGLM2Config
 
 
-def precompute_rotary_emb_cache(seq_len: int, dim: int, dtype=np.float32, base: int = 10000):
+def precompute_rotary_emb_cache(seq_len: int, dim: int, dtype=np.float32, rope_ratio: int = 1, base: int = 10000):
     """pre compute rotary emb cache."""
     # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
+    base = base * rope_ratio
     theta = 1.0 / (base ** (np.arange(0, dim, 2, dtype=dtype) / dim))
 
     # Create position indexes `[0, 1, ..., seq_len - 1]`
     seq_idx = np.arange(seq_len, dtype=dtype)
 
     # Calculate the product of position index and $\theta_i$
     idx_theta = np.outer(seq_idx, theta).astype(np.float32)
 
     cache = np.stack((np.cos(idx_theta), np.sin(idx_theta)), axis=-1).astype(dtype)
     return cache
 
 
+class RopeCache(nn.Cell):
+    r"""A self-defined RopeCache operation"""
+    def __init__(self, config, dim=None, dtype=np.float32, base=10000, is_dynamic=False):
+        super().__init__()
+        base = base * config.rope_ratio
+        theta = 1.0 / (base ** (np.arange(0, dim, 2, dtype=dtype) / dim))
+        seq_idx = np.arange(config.seq_length, dtype=dtype)
+        idx_theta = np.outer(seq_idx, theta).astype(np.float32)
+        self.is_dynamic = is_dynamic
+        self.use_past = config.use_past
+        self.seq_length = config.seq_length
+        self.dim = dim
+        self.rotary_pos_emb = np.stack((np.cos(idx_theta), np.sin(idx_theta)), axis=-1).astype(dtype)
+        self.rotary_pos_emb = Tensor(self.rotary_pos_emb, config.compute_dtype)
+
+        self.reshape = P.Reshape()
+        self.half_dim = dim // 2
+        if is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
+        self.slice = P.StridedSlice().shard(((1, 1),))
+        self.gather = P.Gather().shard(((1, 1), (1,)))
+
+    def construct(self, seq_len=None):
+        rotary_pos_emb = self.rotary_pos_emb
+        if self.is_dynamic and self.use_past:
+            rotary_pos_emb = self.slice(rotary_pos_emb, (0, 0, 0), (seq_len, self.half_dim, 2), (1, 1, 1))
+        return rotary_pos_emb
+
+    def increment(self, batch_valid_length, batch_size):
+        rotary_pos_emb = self.gather(self.rotary_pos_emb, batch_valid_length, 0)
+        rotary_pos_emb = self.reshape(rotary_pos_emb, (batch_size, 1, self.half_dim, 2))
+        return rotary_pos_emb
+
+
 class ChatGLM2RMSNorm(nn.Cell):
     r"""
     A self-defined RMSNorm operation using reduce mean.
 
         Args:
             dim (tuple): The shape of the input tensor
             eps (float): The epsilon value of the denominator. Default 1e-5.
             param_init_type: The param init type.
         Inputs:
             - **x** (Tensor) - Tensor of shape :math:`(batch, seq\_length, hidden\_size)`.
 
         Outputs:
             Tensor of shape :math:`(batch, seq_length, hidden_size)`.
     """
+
     def __init__(self, dim, eps=1e-6, param_init_type=mstype.float32):
         super(ChatGLM2RMSNorm, self).__init__()
         self.eps = Tensor(float(eps), dtype=param_init_type)
         self.weight = Parameter(initializer('ones', (dim,), dtype=param_init_type))
         self.square = P.Square()
         self.mean = P.ReduceMean(keep_dims=True)
         self.add = P.Add()
@@ -84,14 +122,21 @@
         self.square.shard(strategy)
         self.mean.shard(strategy)
         self.rsqrt.shard(strategy)
         self.add.shard((strategy[0], ()))
         self.mul.shard((strategy[0], strategy[0]))
         self.mul2.shard((strategy[0], (1,)))
 
+    def recompute(self, mode=True):
+        self.square.recompute(mode)
+        self.mean.recompute(mode)
+        self.rsqrt.recompute(mode)
+        self.add.recompute(mode)
+        self.mul.recompute(mode)
+        self.mul2.recompute(mode)
 
 class ChatGLM2SiLU(nn.Cell):
     r"""
     A self-defined SwiGlu.
 
         Inputs:
             - **x** (Tensor) - Tensor.
@@ -136,43 +181,63 @@
     MLP will take the input with h hidden state, project it to 4*h
     hidden dimension, perform nonlinear transformation, and project the
     state back into h hidden dimension.
     """
     def __init__(self, config: ChatGLM2Config):
         super(ChatGLM2MLP, self).__init__()
         self.add_bias = config.add_bias_linear
+        self.prefix_name = config.prefix_name
         self.dense_h_to_4h = Linear(
             config.hidden_size,
             config.ffn_hidden_size * 2,
             has_bias=self.add_bias,
             param_init_type=config.param_init_type,
             compute_dtype=config.compute_dtype,
+            skip_redistribution=config.is_dynamic
         )
-        self.dense_h_to_4h.shard(
-            strategy_matmul=((config.parallel_config.data_parallel, 1), (config.parallel_config.model_parallel, 1)),
-            strategy_bias=((config.parallel_config.data_parallel, config.parallel_config.model_parallel),
-                           (config.parallel_config.model_parallel,)))
 
         self.activation_func = ChatGLM2SwiGLU()
-        # shard need to be checked.
-        self.activation_func.shard(((config.parallel_config.data_parallel, 1, 1),))
 
         # Project back to h.
         self.dense_4h_to_h = Linear(
             config.ffn_hidden_size,
             config.hidden_size,
             has_bias=self.add_bias,
             param_init_type=config.param_init_type,
             compute_dtype=config.compute_dtype,
+            skip_redistribution=config.is_dynamic
         )
-        self.dense_4h_to_h.shard(
-            strategy_matmul=((config.parallel_config.data_parallel, config.parallel_config.model_parallel),
-                             (1, config.parallel_config.model_parallel)),
-            strategy_bias=((config.parallel_config.data_parallel, 1), (1,)))
 
     def construct(self, hidden_states):
         # [bs, seq_len, 4 * hidden_size]
         intermediate_parallel = self.dense_h_to_4h(hidden_states)
+        origin_dtype = intermediate_parallel.dtype
+        intermediate_parallel = self.cast(intermediate_parallel, mstype.float32)
         intermediate_parallel = self.activation_func(intermediate_parallel)
+        intermediate_parallel = self.cast(intermediate_parallel, origin_dtype)
         # [bs, seq_len, hidden_size]
         output = self.dense_4h_to_h(intermediate_parallel)
         return output
+
+    def shard(self, parallel_config):
+        """sharding for feedforward"""
+        dp, mp = parallel_config.data_parallel, parallel_config.model_parallel
+        if self.prefix_name.startswith("glm32k"):
+            mp = 1
+        self.dense_h_to_4h.shard(strategy_matmul=((dp, 1), (mp, 1)),
+                                 strategy_bias=((dp, mp), (mp,)))
+        self.activation_func.shard(((dp, 1, 1),))
+        self.dense_4h_to_h.shard(strategy_matmul=((dp, mp), (1, mp)),
+                                 strategy_bias=((dp, 1), (1,)))
+
+
+# pylint: disable=R1703
+def check_promt_flash_attention_version():
+    """
+    the outputs of prompt flash attention are different when using 2.3 and ms version below 2.3
+    """
+    cur_ver = ms.__version__
+    if is_version_ge(cur_ver, "2.3"):
+        pfa_flag = True
+    else:
+        pfa_flag = False
+    return pfa_flag
```

## mindformers/models/glm2/glm2_transformer.py

```diff
@@ -10,37 +10,49 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """ChatGLM2 Transformer."""
 import math
-import numpy as np
 
 import mindspore.ops.functional as F
 import mindspore.ops.operations as P
-from mindspore import Parameter, Tensor, nn, ops
+from mindspore import Tensor, nn, ops
 from mindspore import dtype as mstype
+
 try:
     from mindspore.nn.layer.flash_attention import FlashAttention
+
     FLASHATTENTION_IMPORT_VALID = True
 except ImportError:
     FLASHATTENTION_IMPORT_VALID = False
+try:
+    from mindspore.ops.operations.nn_ops import PromptFlashAttention
+
+    PROMPTFLASHATTENTION_VALID = True
+except ImportError:
+    PROMPTFLASHATTENTION_VALID = False
 
-from mindformers.modules import LayerNorm
+from mindspore.context import ParallelMode
+from mindspore.parallel._utils import _get_parallel_mode
+
+from mindformers.modules import LayerNorm, KVCacheMgr
 from mindformers.modules.layers import Linear
 from mindformers.pet.tuners.ptuning2_adapter import Ptuning2Adapter
-from mindformers.version_control import get_dropout, check_valid_flash_attention, choose_flash_attention_dtype
+from mindformers.version_control import get_dropout, check_valid_flash_attention, choose_flash_attention_dtype, \
+    check_valid_paged_attention
 
 from .glm2_config import ChatGLM2Config
 from .glm2_modules import ChatGLM2MLP, ChatGLM2RMSNorm
 
 
 class CoreAttention(nn.Cell):
     """ChatGLM2 core attention."""
+
     def __init__(self, config: ChatGLM2Config, layer_number):
         super(CoreAttention, self).__init__()
         self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
         self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32
         if self.apply_query_key_layer_scaling:
             self.attention_softmax_in_fp32 = True
         self.layer_number = max(1, layer_number)
@@ -67,15 +79,22 @@
         self.batch_matmul = P.BatchMatMul().shard(
             ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),
              (parallel_config.data_parallel, parallel_config.model_parallel, 1, 1)))
         self.softmax = nn.Softmax(axis=-1)
 
         self.merger_head_transpose = P.Transpose().shard(
             ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),))
+
+        self.shape = P.Shape()
         self.reshape = P.Reshape()
+        if config.is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
+        self.expand_dim = P.ExpandDims()
+        self.use_prompt_flash_attention = config.use_prompt_flash_attention
+        self.use_past = config.use_past
 
         self.compute_dtype = config.compute_dtype
 
     def construct(self, query_layer, key_layer, value_layer, attention_mask):
         """
         calculate attention function
         """
@@ -95,23 +114,19 @@
         matmul_result = self.batch_matmul_q_k(query_layer, key_layer)
 
         # record original score dtype
         attention_scores_dtype = matmul_result.dtype
         # [b, heads, seq, seq]
         attention_scores = matmul_result
 
-        if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:
-            attention_mask = ops.ones((attention_scores.shape[0],
-                                       1,
-                                       attention_scores.shape[2],
-                                       attention_scores.shape[3]), dtype=mstype.bool_)
-            attention_mask.tril()
-            attention_mask = ~attention_mask
         if attention_mask is not None:
-            attention_mask = self.mul_mask(attention_mask, -10000)
+            if self.use_prompt_flash_attention and self.use_past:
+                attention_mask = F.cast(attention_mask, mstype.float16)
+                attention_mask = self.expand_dim(attention_mask, 1)
+                attention_mask = self.mul_mask(attention_mask, -10000)
             attention_scores = self.add(attention_scores, attention_mask)
 
         if self.attention_softmax_in_fp32:
             attention_scores = F.cast(attention_scores, mstype.float32)
 
         attention_probs = self.softmax(attention_scores)
         attention_probs = F.cast(attention_probs, attention_scores_dtype)
@@ -133,142 +148,191 @@
         Inputs:
             x: input tensor
 
         Output:
             x_merge: the 2d output
         """
         x = self.merger_head_transpose(x, (0, 2, 1, 3))  # bs, seq_length, head, size_per_head
-        x_shape = x.shape
-        new_shape = (x_shape[0], x_shape[1], -1)
+        bs, seq_len, n_head, head_dim = self.shape(x)
+        new_shape = (bs, seq_len, n_head * head_dim)
         x_merge = self.reshape(x, new_shape)
         return x_merge
 
 
 class ChatGLM2SelfAttention(nn.Cell):
     """ChatGLM2 self-attention."""
+
     def __init__(self, config: ChatGLM2Config, layer_number):
         super(ChatGLM2SelfAttention, self).__init__()
         self.layer_number = max(1, layer_number)
-
         self.projection_size = config.kv_channels * config.num_attention_heads
         # Per attention head and per partition values.
         self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads
+        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
+        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
         self.num_attention_heads_per_partition = config.num_attention_heads
         self.params_dtype = config.param_init_type
         self.compute_dtype = config.compute_dtype
         self.batch_size = config.batch_size
         self.seq_length = config.seq_length
         self.pre_seq_len = config.pre_seq_len
 
         self.multi_query_attention = config.multi_query_attention
         self.qkv_hidden_size = 3 * self.projection_size
 
         if self.multi_query_attention:
             self.num_multi_query_groups_per_partition = config.multi_query_group_num
-            self.qkv_hidden_size = (
-                self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num)
+            self.qkv_hidden_size = \
+                (self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num)
+            self.tile_kv = P.Tile()
+            self.n_rep = self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition
 
         parallel_config = config.parallel_config
-        self.query_key_value = Linear(config.hidden_size,
-                                      self.qkv_hidden_size,
+        self.query_key_value = Linear(config.hidden_size, self.qkv_hidden_size,
                                       has_bias=config.add_bias_linear or config.add_qkv_bias,
-                                      param_init_type=self.params_dtype,
-                                      compute_dtype=self.compute_dtype)
-        self.query_key_value.shard(strategy_matmul=((parallel_config.data_parallel, 1),
-                                                    (parallel_config.model_parallel, 1)),
-                                   strategy_bias=((parallel_config.data_parallel, parallel_config.model_parallel),
-                                                  (parallel_config.model_parallel,))
-                                   )
+                                      param_init_type=self.params_dtype, compute_dtype=self.compute_dtype,
+                                      skip_redistribution=config.is_dynamic)
 
         self.core_attention = CoreAttention(config, self.layer_number)
 
-        self.dense = Linear(self.projection_size,
-                            config.hidden_size,
-                            has_bias=config.add_bias_linear,
-                            param_init_type=self.params_dtype,
-                            compute_dtype=self.compute_dtype)
-        self.dense.shard(strategy_matmul=((parallel_config.data_parallel, 1),
-                                          (parallel_config.model_parallel, 1)),
-                         strategy_bias=((parallel_config.data_parallel, 1), (1,)))
+        self.dense = Linear(self.projection_size, config.hidden_size, has_bias=config.add_bias_linear,
+                            param_init_type=self.params_dtype, compute_dtype=self.compute_dtype,
+                            skip_redistribution=config.is_dynamic)
 
+        self.shape = P.Shape()
         self.reshape = P.Reshape()
+        if config.is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
+        self.head_dim = config.hidden_size // config.num_attention_heads
+        self.use_paged_attention = config.use_paged_attention and check_valid_paged_attention()
+        self.slice = P.StridedSlice()
+        self.print = P.Print()
+        self.kv_channels = config.kv_channels
+
         self.stack = P.Stack(axis=-1)
         self.gather = P.Gather()
         self.index_0 = Tensor(0)
         self.index_1 = Tensor(1)
         self.mul = P.Mul()
         self.sub = P.Sub()
         self.add = P.Add()
         self.concat = P.Concat(axis=-1)
         self.split_3 = P.Split(axis=-1, output_num=3)
         self.transpose = P.Transpose()
+        self.merger_head_transpose = P.Transpose()
+        self.cast = P.Cast()
 
         self.use_past = config.use_past
         if self.use_past:
             self.is_first_iteration = True
-            total_seq_length = self.seq_length
-            if isinstance(config.pre_seq_len, int):
-                total_seq_length = total_seq_length + config.pre_seq_len
-            seq_range = np.arange(total_seq_length).reshape(1, 1, -1)
-            self.range = Tensor(
-                np.tile(seq_range, (self.batch_size, 1, 1)), mstype.int32)
-            self.less = P.Less()
-            self.mul1 = P.Mul().shard(((1, 1, 1, 1), (1, 1, 1, 1)))
-            self.expand_dims = P.ExpandDims().shard(((1, 1, 1),))
-            self.equal = P.Equal().shard(((1, 1, 1), (1, 1, 1)))
-            self.tile = P.Tile().shard(((1, 1, 1, 1),))
+            kv_num_partition = config.num_attention_heads
+            if config.multi_query_attention:
+                kv_num_partition = config.multi_query_group_num
+
+            if self.use_paged_attention:
+                from mindformers.modules import PagedAttentionMgr
+                self.paged_attention_mgr = PagedAttentionMgr(n_heads=config.num_attention_heads,
+                                                             head_dim=self.head_dim, hidden_size=config.hidden_size,
+                                                             n_kv_heads=kv_num_partition, block_size=config.block_size,
+                                                             num_blocks=config.num_blocks,
+                                                             compute_dtype=config.compute_dtype)
+                self.paged_attention_mgr.shard(parallel_config)
+            else:
+                max_seq_length = config.seq_length if not self.pre_seq_len else config.seq_length + self.pre_seq_len
+                self.kvcache_mgr = KVCacheMgr(kv_num_partition, self.head_dim,
+                                              max_batch_size=config.batch_size, max_seq_length=max_seq_length,
+                                              compute_dtype=config.compute_dtype, is_dynamic=config.is_dynamic,
+                                              use_kvcache_op=config.use_kvcache_op,
+                                              is_flexible_shape=config.is_flexible_shape)
+                self.kvcache_mgr.shard(parallel_config)
+
         self.use_flash_attention = config.use_flash_attention
+        self.use_prompt_flash_attention = config.use_prompt_flash_attention
+        self.flash_attention, self.prompt_flash_attention = self.init_flash_attention_func(config)
+
+        dp, mp = config.parallel_config.data_parallel, config.parallel_config.model_parallel
+        if _get_parallel_mode() not in (ParallelMode.AUTO_PARALLEL,):
+            if config.prefix_name.startswith("glm32k"):
+                mp = 1
+            self.merger_head_transpose.shard(((dp, mp, 1, 1),))
+            self.query_key_value.shard(strategy_matmul=((dp, 1), (mp, 1)), strategy_bias=((dp, mp), (mp,)))
+            self.dense.shard(strategy_matmul=((dp, 1), (mp, 1)), strategy_bias=((dp, 1), (1,)))
+
+
+    def init_flash_attention_func(self, config):
+        """init the flash attention operator"""
+        dp, mp = config.parallel_config.data_parallel, config.parallel_config.model_parallel
+        fa_op, pfa_op = None, None
+
         if self.use_flash_attention:
             self.attention_mask_dtype = choose_flash_attention_dtype()
-            self.sub_attention = P.Sub().shard(((), (parallel_config.data_parallel, 1, 1)))
-            self.flash_attention = FlashAttention(head_dim=config.hidden_size // config.num_attention_heads,
-                                                  head_num=config.num_attention_heads,
-                                                  dropout_rate=config.attention_dropout, prev_block_num=65536,
-                                                  next_block_num=0, dp=parallel_config.data_parallel,
-                                                  mp=parallel_config.model_parallel,
-                                                  high_precision=True)
-            self.merger_head_transpose = P.Transpose().shard(((parallel_config.data_parallel,
-                                                               parallel_config.model_parallel, 1, 1),))
+            fa_op = FlashAttention(head_dim=config.hidden_size // config.num_attention_heads,
+                                   head_num=config.num_attention_heads,
+                                   dropout_rate=config.attention_dropout,
+                                   prev_block_num=65536, next_block_num=0,
+                                   dp=dp, mp=mp, high_precision=True)
+
+        if self.use_prompt_flash_attention:
+            self.attention_mask_dtype = choose_flash_attention_dtype()
+            pfa_op = PromptFlashAttention(num_heads=config.num_attention_heads,
+                                          scale_value=1 / self.norm_factor,
+                                          num_key_value_heads=0, input_layout='BNSD',
+                                          pre_tokens=65536, next_tokens=0)
+
+        return fa_op, pfa_op
+
+
     def _merge_heads(self, x):
         """
         convert a 4d input to a 2d output
 
         Inputs:
             x: input tensor
 
         Output:
             x_merge: the 2d output
         """
-        x = self.merger_head_transpose(x, (0, 2, 1, 3))  # bs, seq_length, head, size_per_head
-        x_shape = x.shape
-        new_shape = (x_shape[0], x_shape[1], -1)
+        x = self.merger_head_transpose(x, (0, 2, 1, 3))
+        bs, seq_len, n_head, head_dim = self.shape(x)
+        new_shape = (bs, seq_len, n_head * head_dim)
         x_merge = self.reshape(x, new_shape)
         return x_merge
 
+    def _repeat_kv(self, x, num_repeat):
+        if num_repeat == 1:
+            return x
+        bs, n_kv_head, seqlen, head_dim = self.shape(x)
+        x = self.reshape(x, (bs, n_kv_head, 1, seqlen * head_dim))
+        x = self.tile_kv(x, (1, 1, num_repeat, 1))
+        x = self.reshape(x, (bs, n_kv_head * num_repeat, seqlen, head_dim))
+        return x
+
     def apply_rotary_pos_emb(self, x: Tensor, rope_cache: Tensor) -> Tensor:
         """apply rotary position embedding to q,k."""
         # x: [b, heads, seq, hidden_size_per_head]
-        bs, num_heads, seq_len, _ = x.shape  # 1, 32，4, 128
+        bs, num_heads, seq_len, head_dim = self.shape(x)
         # rope_cache: first (seq_len, kv_channels//4, 2), other (1, kv_channels//4, 2)
-        rot_dim = rope_cache.shape[-2] * 2  # kv_channels // 2
-        x, x_pass = x[..., :rot_dim], x[..., rot_dim:]
+        rot_dim = self.kv_channels // 2
+        # rot_dim = rope_cache.shape[-2] * 2
+        x1 = self.slice(x, (0, 0, 0, 0), (bs, num_heads, seq_len, rot_dim), (1, 1, 1, 1))
+        x_pass = self.slice(x, (0, 0, 0, rot_dim), (bs, num_heads, seq_len, head_dim), (1, 1, 1, 1))
         # ms not support variable sizes
         # truncate to support variable sizes
-        # rope_cache = rope_cache[:sq]
         # [bs, nh, sq, kv_channels//4, 2]
-        xshaped = self.reshape(x, (bs, num_heads, seq_len, rot_dim // 2, 2))
+        xshaped = self.reshape(x1, (bs, num_heads, seq_len, rot_dim // 2, 2))
+        _, _, _, kv_shape, _ = self.shape(xshaped)
         # [bs, 1, sq, kv_channels//4, 2]
-        rope_cache = self.reshape(rope_cache, (-1, 1, seq_len, xshaped.shape[3], 2))
-
+        rope_cache = self.reshape(rope_cache, (-1, 1, seq_len, kv_shape, 2))
         xshaped_0, xshaped_1 = ops.split(xshaped, 1, -1)
         rope_cache_0, rope_cache_1 = ops.split(rope_cache, 1, -1)
         x_out1 = self.sub(self.mul(xshaped_0, rope_cache_0), self.mul(xshaped_1, rope_cache_1))
         x_out2 = self.add(self.mul(xshaped_1, rope_cache_0), self.mul(xshaped_0, rope_cache_1))
         x_out = self.stack((x_out1, x_out2))
-        x_out = self.reshape(x_out, (x_out.shape[0], x_out.shape[1], x_out.shape[2], -1))
+        bs_x, num_heads_x, seq_len_x, _, _, _ = self.shape(x_out)
+        x_out = self.reshape(x_out, (bs_x, num_heads_x, seq_len_x, -1))
         # [bs, sq, nh, hidden_size_per_head]
         return self.concat((x_out, x_pass))
 
     def add_prefix_if_need(self, prefix_key_value, key_layer, value_layer, attention_mask):
         """
         add p-tuning v2 prefix if need
         """
@@ -279,25 +343,24 @@
 
         key_layer, value_layer = Ptuning2Adapter.add_prefix(
             prefix_key_value,
             key_layer,
             value_layer
         )
 
-        if attention_mask is not None:
+        if attention_mask is not None and getattr(self, "is_first_iteration", True):
             batch_size = attention_mask.shape[0]
             prefix_mask = attention_mask.new_zeros((batch_size, 1, seq_len, self.pre_seq_len))
             m_cat = P.Concat(3)
             # [bs, 1, seq_len, pre_seq_len + seq_len]
             attention_mask = m_cat((prefix_mask, attention_mask))
 
         return key_layer, value_layer, attention_mask
 
-    def construct(self, hidden_states, attention_mask, rotary_pos_emb, key_past=None, value_past=None,
-                  batch_valid_length=None, prefix_key_value=None):
+    def construct(self, hidden_states, attention_mask, rotary_pos_emb, kvcache_inputs=None, prefix_key_value=None):
         """Forward process of self-attention."""
         # hidden_states: [bs, seq_len, hidden_size]
         # attention_mask (bs, 1, seq_len, seq_len)
         # rotary_pos_emb: first: (sen length, kv_channels//4, 2)， after:(1, kv_channels//4, 2]
 
         # [bs, seq_len, qkv_hidden_size]
         mixed_raw_layer = self.query_key_value(hidden_states)
@@ -307,38 +370,41 @@
             (query_layer, key_layer, value_layer) = mixed_raw_layer.split(
                 [self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,
                  self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
                  self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
                  ],
                 axis=-1,
             )
-            # [bs, seq_len, nh, hidden_size_per_attention_head] -> [bs, nh, seq_len, hidden_size_per_attention_head]
-            query_layer = query_layer.view(
-                query_layer.shape[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)
-            )
-            query_layer = self.transpose(query_layer, (0, 2, 1, 3))
-            # [bs, seq_len, multi_query_groups, hidden_size_per_attention_head]
-            # -> [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
-            key_layer = key_layer.view(
-                key_layer.shape[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
-            )
-            key_layer = self.transpose(key_layer, (0, 2, 1, 3))
-            # [bs, seq_len, multi_query_groups, hidden_size_per_attention_head]
-            # -> [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
-            value_layer = value_layer.view(
-                value_layer.shape[:-1]
-                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
-            )
-            value_layer = self.transpose(value_layer, (0, 2, 1, 3))
+            # [bs,seq_len,nh*hidden_size_per_attention_head]
+            bs, seq_len, _ = self.shape(query_layer)
+            if self.use_past and not self.is_first_iteration:
+                query_layer = self.reshape(query_layer, (
+                    bs, self.num_attention_heads_per_partition, 1, self.hidden_size_per_attention_head))
+                key_layer = self.reshape(key_layer, (
+                    bs, self.num_multi_query_groups_per_partition, 1, self.hidden_size_per_attention_head))
+                value_layer = self.reshape(value_layer, (
+                    bs, self.num_multi_query_groups_per_partition, 1, self.hidden_size_per_attention_head))
+            else:
+                query_layer = self.reshape(query_layer, (bs, seq_len, self.num_attention_heads_per_partition,
+                                                         self.hidden_size_per_attention_head))
+                key_layer = self.reshape(key_layer, (bs, seq_len, self.num_multi_query_groups_per_partition,
+                                                     self.hidden_size_per_attention_head))
+                value_layer = self.reshape(value_layer, (bs, seq_len, self.num_multi_query_groups_per_partition,
+                                                         self.hidden_size_per_attention_head))
+                # [bs, nh, seq_len, hidden_size_per_attention_head]
+                query_layer = self.transpose(query_layer, (0, 2, 1, 3))
+                # [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
+                key_layer = self.transpose(key_layer, (0, 2, 1, 3))
+                # [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
+                value_layer = self.transpose(value_layer, (0, 2, 1, 3))
         else:
             # [b, seq, (heads * 3 * hidden_size_per_head)] --> [b, seq, heads, 3 * hidden_size_per_head]
-            new_tensor_shape = mixed_raw_layer.shape[:-1] + (
-                self.num_attention_heads_per_partition, 3 * self.hidden_size_per_attention_head,
-            )
-            mixed_raw_layer = mixed_raw_layer.view(*new_tensor_shape)
+            bs, seq_len, _ = self.shape(mixed_raw_layer)
+            mixed_raw_layer = self.reshape(mixed_raw_layer, (bs, seq_len, self.num_attention_heads_per_partition,
+                                                             3 * self.hidden_size_per_attention_head))
             # [b, seq, heads, hidden_size_per_head]
             (query_layer, key_layer, value_layer) = self.split_3(mixed_raw_layer)
             # [b, seq, heads, hidden_size_per_head] -> [bs, num_heads, seq_len, hidden_size_per_head]
             query_layer = self.transpose(query_layer, (0, 2, 1, 3))
             key_layer = self.transpose(key_layer, (0, 2, 1, 3))
             value_layer = self.transpose(value_layer, (0, 2, 1, 3))
 
@@ -354,82 +420,58 @@
             key_layer,
             value_layer,
             attention_mask
         )
 
         # key and value for current token(s)
         # [bs, heads, seq_len, hidden_size_per_head]
-        key_present = key_layer
-        value_present = value_layer
         if self.use_past:
-            # The first graph with the input size of (bs, seq_length)
-            if self.is_first_iteration:
-                # Get the valid input length without padding
-                valid_length_vector = F.cast(self.less(self.range, batch_valid_length.view(-1, 1, 1)),
-                                             self.params_dtype)  # [bs, 1, seq_len]
-                # Cover the key and value numbers corresponding to the padding position
-                key_present = self.mul1(key_present, self.expand_dims(valid_length_vector, 3))
-                value_present = self.mul1(value_present, self.expand_dims(valid_length_vector, 3))
-            # The second graph with the inpus size of (bs, 1)
-            # the shape of query is (bs, num_heads, 1, size_per_head)
-            # the shape of key is   (bs, multi_query_groups, 1, size_per_head)
-            # the shape of value is (bs, multi_query_groups, 1, size_per_head)
+            if self.use_paged_attention:
+                _, _, slot_mapping = kvcache_inputs
+                key_out = self.paged_attention_mgr(key_layer, value_layer, slot_mapping)
+                query_layer = ops.depend(query_layer, key_out)
             else:
-                # Get the current token position index
-                # key_past: [batch_size, multi_query_groups, seq_length, size_per_head]
-                valid_length = batch_valid_length - 1
-                valid_length = self.reshape(valid_length, (-1, 1, 1))  # [bs, 1, 1]
-                # self.range: [bs, 1, config.seq_len]
-                valid_length_vector = F.cast(self.equal(valid_length, self.range), self.params_dtype)
-                # Pad the key and value to seq_length with only the position index not zero
-                current_key = self.mul1(key_present, self.expand_dims(valid_length_vector, 3))
-                current_value = self.mul1(value_present, self.expand_dims(valid_length_vector, 3))
-                # Concat the previous saved state and current state
-                # [batch_size, multi_query_groups, seq_length, size_per_head]
-                key_present = self.add(key_past, current_key)
-                value_present = self.add(value_past, current_value)
-            # update k v for attention
-            # [batch_size, multi_query_groups, seq_length, size_per_head]
-            key_layer = key_present
-            # [batch_size, multi_query_groups, seq_length, size_per_head]
-            value_layer = value_present
-
-        layer_present = (key_present, value_present)
+                key_layer, value_layer = self.kvcache_mgr(key_layer, value_layer, kvcache_inputs)
 
         # tile k,v to num_heads
         if self.multi_query_attention:
-            bs, heads, _, hs_ph = key_layer.shape
-            key_layer = key_layer.view((bs, heads, -1))
+            key_layer = self._repeat_kv(key_layer, self.n_rep)
+            value_layer = self._repeat_kv(value_layer, self.n_rep)
 
-            key_layer = key_layer.unsqueeze(2)
-            key_layer = key_layer.tile(
-                (1, 1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, 1))
-            # [b, heads, seq, hidden_size_per_head]
-            key_layer = key_layer.view((bs, self.num_attention_heads_per_partition, -1, hs_ph))
+        context_layer = \
+            self.compute_flash_attention_func(query_layer, key_layer, value_layer, attention_mask, kvcache_inputs)
 
-            value_layer = value_layer.view((bs, heads, -1))
-            value_layer = value_layer.unsqueeze(2)
-            value_layer = value_layer.tile(
-                (1, 1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, 1))
-            # [b, heads, seq, hidden_size_per_head]
-            value_layer = value_layer.view((bs, self.num_attention_heads_per_partition, -1, hs_ph))
+        # Output. [bs, seq_len, hidden_size]
+        output = self.dense(context_layer)
 
-        if self.use_flash_attention:
-            attention_mask = attention_mask.squeeze(1).to(self.attention_mask_dtype)
-            context_layer = self.flash_attention(query_layer, key_layer, value_layer, attention_mask)
-            context_layer = self._merge_heads(context_layer)
-        else:
-            context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
-        # # =================
-        # # Output. [bs, seq_len, hidden_size]
-        # # =================
+        return output
 
-        output = self.dense(context_layer)
+    def compute_flash_attention_func(self, query_layer, key_layer, value_layer, attention_mask, kvcache_inputs):
+        """compute context_layer_score with or without flash attention"""
+        if not self.training:
+            if self.use_prompt_flash_attention and \
+                    ((self.use_past and self.is_first_iteration) or (not self.use_past)):
+                attention_mask = attention_mask.to(self.attention_mask_dtype)
+                context_layer = self.prompt_flash_attention(query_layer, key_layer, value_layer, attention_mask,
+                                                            None, None, None, None, None, None, None, None)[0]
+                context_layer = self._merge_heads(context_layer)
+            elif self.use_paged_attention and (self.use_past and not self.is_first_iteration):
+                batch_valid_length, block_tables, _ = kvcache_inputs
+                context_layer = self.paged_attention_mgr.paged_attn(query_layer, batch_valid_length, block_tables)
+            else:
+                context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
 
-        return output, layer_present
+        else:
+            if self.use_flash_attention:
+                attention_mask = attention_mask.to(self.attention_mask_dtype)
+                context_layer = self.flash_attention(query_layer, key_layer, value_layer, attention_mask)
+                context_layer = self._merge_heads(context_layer)
+            else:
+                context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
+        return context_layer
 
 
 class ChatGLM2Block(nn.Cell):
     """A single transformer layer.
 
     Transformer layer takes input with size [s, b, h] and returns an
     output of the same size.
@@ -441,14 +483,16 @@
         self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm
         self.fp32_residual_connection = config.fp32_residual_connection
         self.use_past = config.use_past
         self.params_dtype = config.param_init_type
         self.layernorm_dtype = config.layernorm_compute_type
         self.compute_dtype = config.compute_dtype
         self.seq_length = config.seq_length
+        self.use_seq_parallel = config.parallel_config.use_seq_parallel
+        self.add = P.Add()
 
         layer_norm_func = ChatGLM2RMSNorm if config.rmsnorm else LayerNorm
         # Layernorm on the input data.
         self.input_layernorm = layer_norm_func(config.hidden_size, eps=config.layernorm_epsilon,
                                                param_init_type=self.layernorm_dtype)
 
         self.input_layernorm.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
@@ -456,141 +500,100 @@
         # Self attention.
         self.self_attention = ChatGLM2SelfAttention(config, layer_number)
         self.hidden_dropout = config.hidden_dropout
 
         # Layernorm on the attention output
         self.post_attention_layernorm = layer_norm_func(config.hidden_size, eps=config.layernorm_epsilon,
                                                         param_init_type=self.layernorm_dtype)
-        # self.post_attention_layernorm.shard()
 
         # MLP
         self.mlp = ChatGLM2MLP(config)
 
         self.dropout = get_dropout(self.hidden_dropout)
-        self.dropout.dropout.shard(((config.parallel_config.data_parallel, 1, 1),))
 
         self.cast = P.Cast()
 
-        self.key_past = None
-        self.value_past = None
-        if self.use_past:
-            size_per_head = config.hidden_size // config.num_attention_heads
-            kv_num_partition = config.num_attention_heads
-            if config.multi_query_attention:
-                kv_num_partition = config.multi_query_group_num
-
-            total_seq_length = self.seq_length
-            if isinstance(config.pre_seq_len, int):
-                total_seq_length = total_seq_length + config.pre_seq_len
-
-            kv_shape = (config.batch_size, kv_num_partition, total_seq_length, size_per_head)
-            # parameters saving key and value states
-            self.key_past = Parameter(
-                Tensor(np.zeros(shape=kv_shape), self.params_dtype), name="key_past")
-            self.value_past = Parameter(
-                Tensor(np.zeros(shape=kv_shape), self.params_dtype), name="value_past")
-            self.mul = P.Mul().shard(((1, 1, 1, 1), (1,)))
-            self.assign = P.Assign().shard(((1, 1, 1, 1), (1, 1, 1, 1)))
+        dp = config.parallel_config.data_parallel
+        if _get_parallel_mode() not in (ParallelMode.AUTO_PARALLEL,):
+            self.mlp.shard(config.parallel_config)
+            self.input_layernorm.shard(((dp, 1, 1),))
+            self.post_attention_layernorm.shard(((dp, 1, 1),))
+            self.add.shard(((dp, 1, 1), (dp, 1, 1)))
+            self.dropout.dropout.shard(((dp, 1, 1),))
+
+    def set_select_recompute(self):
+        self.input_layernorm.recompute(False)
+        self.post_attention_layernorm.recompute(False)
+        self.self_attention.recompute()
+        self.mlp.recompute()
+        self.dropout.dropout.recompute(False)
+        self.cast.recompute(False)
 
-    def construct(self, hidden_states, attention_mask, rotary_pos_emb,
-                  init_reset=True, batch_valid_length=None, prefix_key_value=None):
+    def construct(self, hidden_states, attention_mask, rotary_pos_emb, kvcache_inputs=None, prefix_key_value=None):
         """Forward process of the transformer layer."""
         # hidden_states: [bs, seq_len, hidden_size]
         # attention_mask first: (bs, 1, seq_len, seq_len), after: (bs, 1, 1, seq_len)
         # rotary_pos_emb: first: (seq_len, kv_channels//4, 2)， after: (1, kv_channels//4, 2)
 
         # Layer norm at the beginning of the transformer layer.
         hidden_states = self.cast(hidden_states, self.layernorm_dtype)
         layernorm_output = self.input_layernorm(hidden_states)
         # fp32 -> fp16
         layernorm_output = self.cast(layernorm_output, self.compute_dtype)
 
-        key_reset = None
-        value_reset = None
-        if self.use_past:
-            # reset states, init_reset True for reuse and False for reset
-            key_reset = self.assign(self.key_past, self.mul(
-                self.key_past, F.cast(init_reset, self.params_dtype)))
-            value_reset = self.assign(self.value_past, self.mul(
-                self.value_past, F.cast(init_reset, self.params_dtype)))
-            # add dependency for desired execution order
-            layernorm_output = F.depend(layernorm_output, key_reset)
-            layernorm_output = F.depend(layernorm_output, value_reset)
-
         # Self attention.
-        attention_output, layer_present = self.self_attention(
+        attention_output = self.self_attention(
             layernorm_output,
             attention_mask,
             rotary_pos_emb,
-            self.key_past,
-            self.value_past,
-            batch_valid_length,
+            kvcache_inputs,
             prefix_key_value
         )
 
         # Residual connection.
         # False on default.
         if self.apply_residual_connection_post_layernorm:
             residual = layernorm_output
         else:
             residual = hidden_states
 
         layernorm_input = self.dropout(attention_output)
-        layernorm_input = residual + layernorm_input
+        layernorm_input = self.add(residual, layernorm_input)
 
         # Layer norm post the self attention.
         layernorm_output = self.post_attention_layernorm(layernorm_input)
         layernorm_output = self.cast(layernorm_output, self.compute_dtype)
 
         # MLP.
         mlp_output = self.mlp(layernorm_output)
 
-        value_update = None
-        key_update = None
-        if self.use_past:
-            # current key and value
-            key_present, value_present = layer_present
-            # update key and value calculated this step
-            self.assign(self.key_past, key_present)
-            key_update = self.key_past
-            self.assign(self.value_past, value_present)
-            value_update = self.value_past
-            # add dependency for desired execution order
-            key_update = F.depend(key_update, key_reset)
-            value_update = F.depend(value_update, value_reset)
-
-        # add dependency for desired execution order
-        mlp_output = F.depend(mlp_output, value_update)
-        mlp_output = F.depend(mlp_output, key_update)
-
         # Second residual connection.
         # False on default.
         if self.apply_residual_connection_post_layernorm:
             residual = layernorm_output
         else:
             residual = layernorm_input
 
         output = self.dropout(mlp_output)
-        output = residual + output
+        output = self.add(residual, output)
 
         return output
 
 
-def set_parallel_configure_for_layer(layer, layer_id, offset, parallel_config, n_layers, select_recompute=False):
+def set_parallel_configure_for_layer(layer, layer_id, offset, parallel_config, n_layers, no_recompute_layers=None):
     r"""
         Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.
 
         Args:
             layer(Cell) - Represents the transformer block
             layer_id(int) - Means the layer index for the current module, counts from zero.
             offset(int) - Means the layer_index needs a offset, if there are other modules in the net.
             parallel_config(dict) - Parallel Config
             n_layers(int) - The total layers used for the model.
     """
-    _ = select_recompute
     pp_dis = max(int((n_layers + 1) / parallel_config.pipeline_stage), 1)
     if isinstance(offset, list):
         if len(offset) != parallel_config.pipeline_stage:
             raise ValueError(f"The length of `offset` {len(offset)} do not match "
                              f"`pipeline stage` {parallel_config.pipeline_stage}.")
         i = min(layer_id // pp_dis, parallel_config.pipeline_stage - 1)
         offset_layer = offset[i]
@@ -598,28 +601,29 @@
         offset_layer = offset
     else:
         raise TypeError(f"`offset` must be `int` of list of `int`, but got {type(offset)}.")
 
     pp_id = min((layer_id + offset_layer) // pp_dis, parallel_config.pipeline_stage - 1)
     layer.pipeline_stage = pp_id
 
-    # Used for optimizer's fusion tag
-    dis = max(int((n_layers + 1) / parallel_config.gradient_aggregation_group), 1)
-    if parallel_config.pipeline_stage > 1:
-        # we give the fusion in pipeline mode a fixed value, otherwise the performance may become worse.
-        layer.set_comm_fusion(2)
-    else:
-        layer.set_comm_fusion(int((layer_id + offset) / dis) + 1)
-    # Used for enabling recomputation of the block
-    if isinstance(parallel_config.recompute, bool):
-        if parallel_config.recompute:
-            layer.recompute()
+    if not parallel_config.recompute.select_recompute:
+        if isinstance(parallel_config.recompute, bool):
+            if parallel_config.recompute:
+                layer.recompute()
+        else:
+            if parallel_config.recompute.recompute:
+                layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
     else:
-        if parallel_config.recompute.recompute:
-            layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
+        if not no_recompute_layers:
+            layer.set_select_recompute()
+        elif layer_id not in no_recompute_layers:
+            if parallel_config.recompute.recompute:
+                layer.recompute()
+            else:
+                layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
 
 
 class ChatGLM2Transformer(nn.Cell):
     """Transformer class."""
 
     def __init__(self, config: ChatGLM2Config):
         super(ChatGLM2Transformer, self).__init__()
@@ -627,64 +631,71 @@
         self.post_layer_norm = config.post_layer_norm
         self.compute_dtype = config.compute_dtype
 
         # Number of layers.
         self.num_layers = config.num_layers
 
         self.pre_seq_len = config.pre_seq_len
+
         if config.use_flash_attention:
-            config.use_flash_attention = check_valid_flash_attention(FLASHATTENTION_IMPORT_VALID)
+            config.use_flash_attention = check_valid_flash_attention(FLASHATTENTION_IMPORT_VALID, 'FlashAttention')
+
+        if config.use_prompt_flash_attention:
+            config.use_prompt_flash_attention = check_valid_flash_attention(PROMPTFLASHATTENTION_VALID,
+                                                                            "PromptFlashAttention")
 
         # Transformer layers.
         def build_layer(layer_number):
             return ChatGLM2Block(config, layer_number)
 
         self.layers = nn.CellList()
         for i in range(self.num_layers):
             layer = build_layer(i + 1)
+
             set_parallel_configure_for_layer(layer, layer_id=i, offset=0, n_layers=self.num_layers,
                                              parallel_config=config.parallel_config,
-                                             select_recompute=config.parallel_config.recompute.select_recompute)
+                                             no_recompute_layers=config.no_recompute_layers)
+
             self.layers.append(layer)
 
         if self.post_layer_norm:
             layer_norm_func = ChatGLM2RMSNorm if config.rmsnorm else LayerNorm
             # Final layer norm before output.
             self.final_layernorm = layer_norm_func(config.hidden_size, eps=config.layernorm_epsilon,
                                                    param_init_type=config.layernorm_compute_type)
-            # self.final_layernorm.shard()
-            self.final_layernorm.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
+            if config.parallel_config.pipeline_stage > 1:
+                self.final_layernorm.pipeline_stage = config.parallel_config.pipeline_stage - 1
+                self.final_layernorm.set_comm_fusion(2)
+            else:
+                self.final_layernorm.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
+
+            self.final_layernorm.shard(((config.parallel_config.data_parallel, 1, 1),))
 
     def construct(self,
                   hidden_states,
                   attention_mask,
                   rotary_pos_emb,
-                  init_reset=True,
-                  batch_valid_length=None,
+                  kvcache_inputs=None,
                   prefix_key_values=None):
         """Forward process of the transformer."""
         # hidden_states (bs, seq_len, hs)
         # attention_mask (bs, 1, seq_len, seq_len)
         # rotary_pos_emb: first: (sen length, kv_channels//2, 2)， after:[1, kv_channels // 2, 2]
 
-        if batch_valid_length is not None and isinstance(self.pre_seq_len, int):
-            batch_valid_length = batch_valid_length + self.pre_seq_len
-
         for i in range(self.num_layers):
             prefix_key_value = None
             if prefix_key_values is not None:
                 prefix_key_value = prefix_key_values[i]
             layer = self.layers[i]
 
             hidden_states = layer(
                 hidden_states,
                 attention_mask,
                 rotary_pos_emb,
-                init_reset=init_reset,
-                batch_valid_length=batch_valid_length,
+                kvcache_inputs,
                 prefix_key_value=prefix_key_value
             )
 
         # Final layer norm.
         if self.post_layer_norm:
             hidden_states = self.final_layernorm(hidden_states)
             hidden_states = self.cast(hidden_states, self.compute_dtype)
```

## mindformers/models/llama/llama.py

```diff
@@ -158,15 +158,15 @@
                                                           pad_token_id=config.pad_token_id,
                                                           use_flash_attention=config.use_flash_attention)
         self.tok_embeddings = LlamaEmbedding(vocab_table_size=config.vocab_size,
                                              embedding_size=config.hidden_size,
                                              param_init_type=config.param_init_type)
         self.layers = nn.CellList()
         for layer_id in range(config.num_layers):
-            if config.fine_grain_interleave > 1:
+            if config.fine_grain_interleave > 1 and config.parallel_config.model_parallel > 1:
                 layer = LLamaDecodeLayerInterleave(config.batch_size,
                                                    config.seq_length,
                                                    layer_id,
                                                    dim=config.hidden_size,
                                                    n_heads=config.num_heads,
                                                    num_layers=config.num_layers,
                                                    multiple_of=config.multiple_of,
@@ -391,15 +391,16 @@
             logger.info(f"Exporting dynamic MindIR...")
         if use_paged_attention:
             logger.info(f"Exporting model with paged attention...")
         seq_length = self.seq_length
         bs = None if dyn else self.config.batch_size
         seq_len = None if dyn else self.seq_length
         max_num_blocks_pre_batch = None if dyn else seq_len // self.config.block_size
-        logger.info(f"max num blocks pre batch: {max_num_blocks_pre_batch}")
+        if use_paged_attention:
+            logger.info(f"max num blocks pre batch: {max_num_blocks_pre_batch}")
 
         def dummy_tensor(shape, dtype):
             if None in shape:
                 return Tensor(shape=shape, dtype=dtype)
             return Tensor(np.ones(shape=tuple(shape)), dtype=dtype)
 
         batch_valid_length = dummy_tensor(shape=[bs], dtype=ms.int32)
```

## mindformers/models/llama/llama_config.py

```diff
@@ -133,14 +133,15 @@
                  extend_method: str = "None",
                  scaling_factor: float = 1.0,
                  is_dynamic: bool = False,
                  use_kvcache_op: bool = False,
                  is_flexible_shape: bool = False,
                  use_rope_slice: bool = False,
                  use_flash_attention: bool = False,
+                 use_prompt_flash_attention: bool = False,
                  use_paged_attention: bool = False,
                  fine_grain_interleave: int = 1,
                  offset: int = 0,
                  checkpoint_name_or_path: str = "",
                  repetition_penalty: float = 1.0,
                  max_decode_length: int = 1024,
                  block_size: int = 16,
@@ -188,14 +189,15 @@
         self.extend_method = extend_method
         self.scaling_factor = scaling_factor
         self.is_dynamic = is_dynamic
         self.use_kvcache_op = use_kvcache_op
         self.is_flexible_shape = is_flexible_shape
         self.use_rope_slice = use_rope_slice
         self.use_flash_attention = use_flash_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
         self.fine_grain_interleave = fine_grain_interleave
         self.offset = offset
         self.repetition_penalty = repetition_penalty
         self.max_decode_length = max_decode_length
         self.top_k = top_k
         self.top_p = top_p
         self.do_sample = do_sample
```

## mindformers/models/llama/llama_interleave.py

```diff
@@ -10,25 +10,27 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """LLaMA fine grain interleave transformer Layer's APIs."""
 
-from typing import Optional
 import math
+from typing import Optional
 
-from mindspore import nn
 import mindspore.common.dtype as mstype
+from mindspore import nn
 from mindspore.common.tensor import Tensor
 from mindspore.context import ParallelMode
 from mindspore.ops import operations as P
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
+
 try:
     from mindspore.nn.layer.flash_attention import FlashAttention
+
     FLASHATTENTION_VALID = True
 except ImportError:
     FLASHATTENTION_VALID = False
 
 from mindformers.models.llama.llama_layer import LlamaFeedForward, LlamaRMSNorm, LlamaRotaryEmbedding
 from mindformers.modules.layers import _check_input_dtype, Linear
 from mindformers.modules.transformer import TransformerOpParallelConfig
@@ -66,16 +68,16 @@
                 if j == self.axis_list[k]:
                     strided_slice_begin += (micro_batch_begin,)
                     strided_slice_end += (micro_batch_end,)
                 else:
                     strided_slice_begin += (0,)
                     strided_slice_end += (input_shape[j],)
 
-            micro_input = self.strided_slice_list[k](each_input, strided_slice_begin,\
-                strided_slice_end, strided_slice_strides)
+            micro_input = self.strided_slice_list[k](each_input, strided_slice_begin, \
+                                                     strided_slice_end, strided_slice_strides)
             micro_inputs += (micro_input,)
             k += 1
         return micro_inputs
 
 
 class LLamaAttentionInterleave(nn.Cell):
     r"""
@@ -134,14 +136,15 @@
                 shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),
                 if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).
 
             - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with
                 ((batch_size, num_heads, head_dim, tgt_seq_length),
                 (batch_size, num_heads, tgt_seq_length, head_dim)).
     """
+
     def __init__(self,
                  batch_size,
                  seq_length,
                  dim: int = 512,
                  n_heads: int = 8,
                  n_kv_heads: Optional[int] = None,
                  qkv_concat=False,
@@ -258,48 +261,48 @@
                 self.mul.recompute()
                 self.add.recompute()
                 self.cast_attn.recompute()
                 self.softmax.softmax.recompute()
                 self.batch_matmul.recompute()
 
         if self.use_flash_attention:
-            self.flash_attention = FlashAttention(self.head_dim, n_heads, dp=dp, mp=mp,\
-                prev_block_num=65536, next_block_num=0, high_precision=True)
+            self.flash_attention = FlashAttention(self.head_dim, n_heads, dp=dp, mp=mp, \
+                                                  prev_block_num=65536, next_block_num=0, high_precision=True)
 
-    def compute_qkv(self, x, freqs_cis, interleave_num):
+    def compute_qkv(self, x):
         """compute the qkv with interleave number"""
         x = self.reshape(x, (-1, x.shape[-1]))
         if self.qkv_concat:
             bs_seq = x.shape[0]
             qkv = self.cast(self.w(x), self.dtype)
             query = self.slice_qkv(qkv, (0, 0), (bs_seq, self.hidden_size), (1, 1))
             key = self.slice_qkv(qkv, (0, self.hidden_size),
                                  (bs_seq, self.hidden_size + self.kv_dim), (1, 1))
             value = self.slice_qkv(qkv, (0, self.hidden_size + self.kv_dim),
                                    (bs_seq, self.hidden_size + self.kv_dim * 2), (1, 1))
         else:
             query = self.cast(self.wq(x), self.dtype)  # dp, 1 -> dp, mp
-            key = self.cast(self.wk(x), self.dtype)    # dp, 1 -> dp, mp
+            key = self.cast(self.wk(x), self.dtype)  # dp, 1 -> dp, mp
             value = self.cast(self.wv(x), self.dtype)  # dp, 1 -> dp, mp
+        return query, key, value
 
-        query = self.reshape(query, (-1, self.seq_length // interleave_num, self.n_head, self.head_dim))
-        key = self.reshape(key, (-1, self.seq_length // interleave_num, self.n_kv_head, self.head_dim))
-        value = self.reshape(value, (-1, self.seq_length // interleave_num, self.n_kv_head, self.head_dim))
+    def cal_attn(self, query, key, value, mask, freqs_cis):
+        """cal_attn"""
+        query = self.reshape(query, (-1, self.seq_length, self.n_head, self.head_dim))
+        key = self.reshape(key, (-1, self.seq_length, self.n_kv_head, self.head_dim))
+        value = self.reshape(value, (-1, self.seq_length, self.n_kv_head, self.head_dim))
 
         # [bs, seq/1, n_head/n_kv_head, head_dim]
         query = self.transpose(query, (0, 2, 1, 3))
         key = self.transpose(key, (0, 2, 1, 3))
         value = self.transpose(value, (0, 2, 1, 3))
 
         # [bs, n_head/n_kv_head, seq/1, head_dim]
-        query, key = self.apply_rotary_emb(query, key, freqs_cis) # dp, mp, 1, 1
-        return query, key, value
+        query, key = self.apply_rotary_emb(query, key, freqs_cis)  # dp, mp, 1, 1
 
-    def cal_attn(self, query, key, value, mask):
-        """cal_attn"""
         # kv share: [bs, n_kv_head, seq, head_dim] -> [bs, n_head, seq, head_dim]
         bs, n_head, seq, head_dim = query.shape
         n_kv_head = key.shape[1]
         query = self.reshape(query, (bs, n_head, seq, head_dim))
         key = self.reshape(key, (bs, n_kv_head, seq, head_dim))
         value = self.reshape(value, (bs, n_kv_head, seq, head_dim))
 
@@ -311,15 +314,15 @@
             key = self._repeat_kv(key, self.n_rep)
             value = self._repeat_kv(value, self.n_rep)
             attention = self._attn(query, key, value, mask)
         return attention
 
     def cal_output_proj(self, attention):
         """cal_output_proj"""
-        output = self.wo(attention) # dp, mp -> dp, 1 / dp * mp, 1
+        output = self.wo(attention)  # dp, mp -> dp, 1 / dp * mp, 1
         return output
 
     def _repeat_kv(self, x, rep):
         """repeat_kv"""
         if rep == 1:
             return x
         bs, n_kv_head, seqlen, head_dim = x.shape
@@ -335,15 +338,15 @@
         Inputs:
             x: input tensor
 
         Output:
             x_merge: the 2d output
         """
         # [bs, n_head, seq/1, head_dim]
-        x = self.merger_head_transpose(x, (0, 2, 1, 3)) # dp,mp,1,1 -> dp,1,mp,1
+        x = self.merger_head_transpose(x, (0, 2, 1, 3))  # dp,mp,1,1 -> dp,1,mp,1
         # [bs, seq/1, n_head, head_dim]
         x_shape = x.shape
         # [bs * seq/1, hidden_dim]
         new_shape = (-1, x_shape[-2] * x_shape[-1])
         x_merge = self.reshape(x, new_shape)
         return x_merge
 
@@ -431,14 +434,15 @@
               False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size)
 
             - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with
               ((batch_size, num_heads, head_dim, seq_length),
               (batch_size, num_heads, seq_length, head_dim)).
 
     """
+
     def __init__(self,
                  batch_size,
                  seq_length,
                  layer_id,
                  dim: int = 512,
                  n_heads: int = 8,
                  num_layers: int = 32,
@@ -510,55 +514,56 @@
             self.add.shard(((dp * mp, 1), (dp * mp, 1)))
             self.attention_norm.shard((dp * mp, 1))
             self.ffn_norm.shard((dp * mp, 1))
             self.feed_forward.w2.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
 
         if parallel_config.recompute.select_recompute or (
                 isinstance(parallel_config.recompute, bool) and not parallel_config.recompute
-            ) or not parallel_config.recompute.recompute and self.layer_id < (num_layers // 2):
+        ) or not parallel_config.recompute.recompute and self.layer_id < (num_layers // 2):
             self.feed_forward.mul.recompute()
             self.feed_forward.w1.activation.silu.recompute()
 
         concat_stra1 = []
         concat_stra2 = []
         self.interleave1_inputs = nn.CellList()
         self.interleave1_inputs_ = nn.CellList()
         self.interleave2_inputs = nn.CellList()
-        self.interleaved_concat1 = P.Concat(axis=2)
+        self.interleaved_concat1 = P.Concat(axis=0)
         self.interleaved_concat1.add_prim_attr("fine_grained_interleaved_index", self.layer_id)
-        self.interleaved_concat_1 = P.Concat(axis=2)
-        self.interleaved_concat2 = P.Concat(axis=1)
+        self.interleaved_concat_1 = P.Concat(axis=0)
+        self.interleaved_concat2 = P.Concat(axis=0)
         if self.layer_id != self.num_layers - 2:
             self.interleaved_concat2.add_prim_attr("fine_grained_interleaved_index", 1000)
 
         for _ in range(self.interleave_num):
-            concat_stra1.append((dp, mp, 1, 1))
-            interleave_data1 = _MicroBatch(self.interleave_num, 1, [1])
+            concat_stra1.append((dp, mp))
+            interleave_data1 = _MicroBatch(self.interleave_num, 1, [0])
             interleave_data1.strided_slice_list[0].add_prim_attr("skip_redistribution", True)
             interleave_data1_ = _MicroBatch(self.interleave_num, 1, [0])
             interleave_data1_.strided_slice_list[0].add_prim_attr("skip_redistribution", True)
-            interleave_data2 = _MicroBatch(self.interleave_num, 2, [1, 0])
+            interleave_data2 = _MicroBatch(self.interleave_num, 2, [0, 0])
             if parallel_config.use_seq_parallel:
                 if self.layer_id == self.num_layers - 2:
-                    concat_stra2.append((dp, 1, 1))
+                    concat_stra2.append((dp, 1))
                 else:
-                    concat_stra2.append((dp, mp, 1))
+                    concat_stra2.append((dp * mp, 1))
                 if self.layer_id == self.num_layers - 1:
-                    interleave_data1.strided_slice_list[0].shard(((dp, 1, 1),))
+                    interleave_data1.strided_slice_list[0].shard(((dp, 1),))
                 else:
-                    interleave_data1.strided_slice_list[0].shard(((dp, mp, 1),))
+                    interleave_data1.strided_slice_list[0].shard(((dp * mp, 1),))
                 interleave_data1_.strided_slice_list[0].shard(((1, 1),))
-                interleave_data2.strided_slice_list[0].shard(((dp, mp, 1),))
+                interleave_data2.strided_slice_list[0].shard(((dp * mp, 1),))
             else:
-                concat_stra2.append((dp, 1, 1))
-                interleave_data1.strided_slice_list[0].shard(((dp, 1, 1),))
+                concat_stra2.append((dp, 1))
+                interleave_data1.strided_slice_list[0].shard(((dp, 1),))
                 interleave_data1_.strided_slice_list[0].shard(((1, 1),))
-                interleave_data2.strided_slice_list[0].shard(((dp, 1, 1),))
+                interleave_data2.strided_slice_list[0].shard(((dp, 1),))
             if self.layer_id == 0 and parallel_config.use_seq_parallel:
-                interleave_data2.strided_slice_list[0].shard(((dp, 1, 1),))
+                interleave_data2.strided_slice_list[0].shard(((dp, 1),))
+                interleave_data2.strided_slice_list[0].add_prim_attr("skip_redistribution", True)
             else:
                 interleave_data2.strided_slice_list[0].add_prim_attr("skip_redistribution", True)
 
             interleave_data2.strided_slice_list[0].add_prim_attr("fine_grained_interleaved_index", self.layer_id)
             interleave_data2.strided_slice_list[1].shard(((dp, mp),))
             interleave_data2.strided_slice_list[1].add_prim_attr("fine_grained_interleaved_index", self.layer_id)
             interleave_data2.strided_slice_list[1].add_prim_attr("skip_redistribution", True)
@@ -570,18 +575,18 @@
         self.interleaved_concat1.shard(concat_stra1)
         self.interleaved_concat1.add_prim_attr("skip_redistribution", True)
         self.interleaved_concat_1.shard(concat_stra1)
         self.interleaved_concat_1.add_prim_attr("skip_redistribution", True)
         self.interleaved_concat2.shard(concat_stra2)
         self.interleaved_concat2.add_prim_attr("skip_redistribution", True)
 
-    def linear_layer1(self, x, freqs_cis, interleave_num):
+    def linear_layer1(self, x):
         """layer part 1"""
         input_x = self.attention_norm(x)
-        query, key, value = self.attention.compute_qkv(input_x, freqs_cis, interleave_num)
+        query, key, value = self.attention.compute_qkv(input_x)
         return query, key, value
 
     def linear_layer2(self, x, attention):
         """layer part 2"""
         attention_output = self.attention.cal_output_proj(attention)
         # For post-layernorm the inputs for residual path are output of self-attention and output of layernorm
         x = self.add(x, attention_output)
@@ -593,49 +598,41 @@
     # pylint: disable=W0613
     def construct(self, x, freqs_cis, mask=None, kvcache_inputs=None):
         """ Forward of transformer block. """
         self._check_input(x, freqs_cis, mask)
         x = self.reshape(x, (-1, x.shape[-1]))
         # ============linear-layer1================
         if self.layer_id == 0:
-            query, key, value = self.linear_layer1(x, freqs_cis, 1)
+            query, key, value = self.linear_layer1(x)
         else:
             query_tuple = ()
             key_tuple = ()
             value_tuple = ()
             for i in range(self.interleave_num):
-                x_part, = self.interleave1_inputs[i](i, self.reshape(x, (-1, self.seq_length, self.hidden_size)))
-                x_part = self.reshape(x_part, (-1, self.hidden_size))
-                freqs_cos, freqs_sin, swap_mask = freqs_cis
-                freqs_cos_, = self.interleave1_inputs_[i](i, freqs_cos)
-                freqs_sin_, = self.interleave1_inputs_[i](i, freqs_sin)
-                freqs_part = (freqs_cos_, freqs_sin_, swap_mask)
-                query_part, key_part, value_part = self.linear_layer1(x_part, freqs_part, self.interleave_num)
+                x_part, = self.interleave1_inputs[i](i, x)
+                query_part, key_part, value_part = self.linear_layer1(x_part)
                 query_tuple += (query_part,)
                 key_tuple += (key_part,)
                 value_tuple += (value_part,)
             query = self.interleaved_concat1(query_tuple)
             key = self.interleaved_concat_1(key_tuple)
             value = self.interleaved_concat_1(value_tuple)
         # ===========linear-layer1 end=============
-        attention = self.attention.cal_attn(query, key, value, mask)
+        attention = self.attention.cal_attn(query, key, value, mask, freqs_cis)
         # ============linear-layer2================
         if self.layer_id == self.num_layers - 1:
             output = self.linear_layer2(x, attention)
         else:
             output_tuple = ()
             for i in range(self.interleave_num):
-                x_part, attention_part = self.interleave2_inputs[i](i, self.reshape(
-                    x, (-1, self.seq_length, self.hidden_size)), attention)
-                x_part = self.reshape(x_part, (-1, self.hidden_size))
+                x_part, attention_part = self.interleave2_inputs[i](i, x, attention)
                 output_part = self.linear_layer2(x_part, attention_part)
-                output_part = self.reshape(output_part, (-1, self.inter_seq_length, self.hidden_size))
                 output_tuple += (output_part,)
             output = self.interleaved_concat2(output_tuple)
-         # ============linear-layer2 end===========
+        # ============linear-layer2 end===========
         return output
 
     def _check_input(self, x, freqs_cis, mask):
         r"""Check inputs"""
         _check_input_dtype(
             x.dtype, "x", [mstype.float32, mstype.float16, mstype.bfloat16], self.cls_name)
         freqs_cos, freqs_sin, swap_mask = freqs_cis
```

## mindformers/models/llama/llama_layer.py

```diff
@@ -344,14 +344,17 @@
         self.compute_type = compute_type
         self.weight = Parameter(initializer('ones', (dim,), dtype=mstype.float32), parallel_optimizer=False)
 
         if check_valid_big_kernel() and not is_910a() and not is_dynamic:
             self.norm = P.RmsNorm(eps)
             self.rms_norm = self._rms_norm
             self.self_define = False
+            self.cast = P.Cast()
+            self.rcast = P.Cast()
+            self.cast.recompute()
         else:
             self.cast = P.Cast()
             self.mul = P.Mul()
             self.mul2 = P.Mul()
             self.square = P.Square()
             self.mean = P.ReduceMean(keep_dims=True)
             self.add = P.Add()
@@ -367,15 +370,16 @@
         norm_factor = self.rsqrt(norm_factor)
         output = self.mul(x, self.cast(norm_factor, original_type))
         output = self.mul2(output, self.cast(self.weight, original_type))
         return output
 
     def _rms_norm(self, x):
         original_type = x.dtype
-        return self.norm(x, self.cast(self.weight, original_type))[0]
+        output = self.norm(self.cast(x, self.compute_type), self.weight)[0]
+        return self.rcast(output, original_type)
 
     def construct(self, x):
         """Forward of RMSNorm."""
         return self.rms_norm(x)
 
     def shard(self, strategy_in):
         """Parallel strategy configuratiuon interface."""
@@ -503,23 +507,26 @@
 
 
 class CausalMask(nn.Cell):
     r""" Get the Lower triangular matrix from the input_ids. """
     @_LogActionOnce(m_logger=logger, key='AttentionMask',
                     no_warning=_get_parallel_mode() in (ParallelMode.STAND_ALONE,))
     def __init__(self, seq_length, compute_type=mstype.float16,
-                 is_dynamic=False, pad_token_id=0, use_flash_attention=False):
+                 is_dynamic=False, pad_token_id=0, use_flash_attention=False,
+                 use_prompt_flash_attention=False):
         super().__init__()
         self.dtype = compute_type
         self.is_dynamic = is_dynamic
         self.pad_token_id = pad_token_id
         self.use_flash_attention = use_flash_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
+        self.is_first_iteration = True
         self.multiply_data = Tensor([-10000.0], dtype=compute_type)
         self.one = Tensor([1.0], dtype=compute_type)
-        self.lower_triangle_mask = Tensor(np.tril(np.ones(shape=(seq_length, seq_length))), mstype.float32)
+        self.lower_triangle_mask = Tensor(np.tril(np.ones(shape=(seq_length, seq_length))), dtype=compute_type)
 
         self.shape = P.Shape()
         self.cast = P.Cast()
         self.reshape = P.Reshape()
         self.not_equal = P.NotEqual()
         self.less_equal = P.LessEqual()
         self.bmm = P.BatchMatMul()
@@ -538,47 +545,53 @@
             input_mask = self.cast(self.not_equal(tokens, self.pad_token_id), self.dtype)
         else:
             bs = self.shape(masks)[0]
             seq_len = self.shape(masks)[1]
             input_mask = self.cast(masks, self.dtype)
 
         shape_right = (bs, 1, seq_len)
-        shape_left = (bs, seq_len, 1)
         # Mask the padded inputs
-        mask_left = self.reshape(input_mask, shape_left)
-        mask_right = self.reshape(input_mask, shape_right)
-        attention_mask = self.bmm(mask_left, mask_right)
+        attention_mask = self.reshape(input_mask, shape_right)
         if not self.is_dynamic:
             lower_traiangle = self.expand_dim(self.lower_triangle_mask, 0)
         else:
             lower_triangle_mask = self.slice(self.lower_triangle_mask, (0, 0), (seq_len, seq_len), (1, 1))
             lower_traiangle = self.expand_dim(lower_triangle_mask, 0)
         # the returned shape is [bs, seq_length, seq_length]
         attention_mask = self.mul(attention_mask, lower_traiangle)
+        attention_mask = self.sub(self.one, attention_mask)
+        attention_mask = self.expand_dim_post(attention_mask, 1)
+        if not self.use_flash_attention and not self.use_prompt_flash_attention:
+            attention_mask = self.mul_post(attention_mask, self.multiply_data)
+        elif self.use_flash_attention:
+            attention_mask = self.cast(attention_mask, mstype.uint8)
         return attention_mask
 
     def increment(self, seq_range, batch_valid_length, zactivate_len=None):
+        "Get mask for incremental inference."
         if zactivate_len is not None:
             seq_range = self.slice(seq_range, (0, 0, 0), (1, 1, self.shape(zactivate_len)[0]), (1, 1, 1))
         mask = self.less_equal(self.reshape(seq_range, (1, 1, -1)), self.reshape(batch_valid_length, (-1, 1, 1)))
-        return self.cast(mask, self.dtype)
+        mask = self.cast(mask, self.dtype)
+        mask = self.sub(self.one, mask)
+        mask = self.expand_dim_post(mask, 1)
+        mask = self.mul_post(mask, self.multiply_data)
+        return mask
 
     def increment_slice(self, seq_range, seq_length, batch_valid_length, zactivate_len=None):
+        "Get mask for incremental inference and apply slice."
         if zactivate_len is not None:
             seq_range_mask = self.slice(seq_range, (0, 0, 0), (1, 1, self.shape(zactivate_len)[0]), (1, 1, 1))
         else:
             seq_range_mask = self.slice(seq_range, (0, 0, 0), (1, 1, seq_length), (1, 1, 1))
         mask = self.less_equal(self.reshape(seq_range_mask, (1, 1, -1)), self.reshape(batch_valid_length, (-1, 1, 1)))
-        return self.cast(mask, self.dtype)
-
-    def post_process(self, mask):
+        mask = self.cast(mask, self.dtype)
         mask = self.sub(self.one, mask)
-        if not self.use_flash_attention:
-            mask = self.expand_dim_post(mask, 1)
-            mask = self.mul_post(mask, self.multiply_data)
+        mask = self.expand_dim_post(mask, 1)
+        mask = self.mul_post(mask, self.multiply_data)
         return mask
 
     def shard(self, parallel_config):
         dp = parallel_config.data_parallel
         self.not_equal.shard(((dp, 1), ()))
         self.bmm.shard(((dp, 1, 1), (dp, 1, 1)))
         self.expand_dim.shard(((1, 1),))
```

## mindformers/models/llama/llama_transformer.py

```diff
@@ -116,35 +116,37 @@
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
                  use_rope_slice=False,
                  use_flash_attention=False,
+                 use_prompt_flash_attention=False,
                  use_paged_attention=False,
                  block_size: Optional[int] = None,
                  num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
         self.seq_length = seq_length
         self.hidden_size = dim
         self.n_head = n_heads
         self.head_dim = dim // n_heads
         self.n_kv_head = n_heads if n_kv_heads is None else n_kv_heads
         self.n_rep = self.n_head // self.n_kv_head
         self.kv_dim = self.n_kv_head * self.head_dim
-        self.block_size = block_size
-        self.num_blocks = num_blocks
 
         self.dtype = compute_dtype
         self.softmax_dtype = softmax_compute_dtype
         self.is_first_iteration = True
         self.use_past = use_past
-        self.use_flash_attention = use_flash_attention and FLASHATTENTION_VALID
+        self.use_flash_attention = use_flash_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
         self.use_paged_attention = use_paged_attention
+        self.block_size = block_size
+        self.num_blocks = num_blocks
         self.qkv_concat = qkv_concat
 
         if self.hidden_size % self.n_head != 0:
             raise ValueError("For 'MultiHeadAttention', the class variable 'hidden_size' must be a multiple "
                              "of 'n_head', but got the hidden_size is {} and the n_head is {}."
                              .format(self.hidden_size, self.n_head))
         if self.n_kv_head % parallel_config.model_parallel != 0:
@@ -199,14 +201,22 @@
         self.wo = Linear(in_channels=self.hidden_size,
                          out_channels=self.hidden_size,
                          has_bias=False,
                          compute_dtype=compute_dtype,
                          param_init_type=param_init_type,
                          skip_redistribution=is_dynamic)
 
+        if self.use_prompt_flash_attention:
+            self.prompt_flash_attention = P.nn_ops.PromptFlashAttention(num_heads=self.n_head,
+                                                                        num_key_value_heads=self.n_kv_head,
+                                                                        pre_tokens=65536,
+                                                                        next_tokens=0,
+                                                                        scale_value=1. / math.sqrt(self.head_dim),
+                                                                        input_layout='BNSD')
+
         dp = parallel_config.data_parallel
         mp = parallel_config.model_parallel
         if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
             self.transpose.shard(((dp, 1, mp, 1),))
             self.merger_head_transpose.shard(((dp, mp, 1, 1),))
             self.batch_matmul_q_k.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
             self.batch_matmul.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
@@ -227,14 +237,16 @@
             else:
                 self.wq.shard(((dp, 1), (mp, 1)))
                 self.wk.shard(((dp, 1), (mp, 1)))
                 self.wv.shard(((dp, 1), (mp, 1)))
             self.wo.shard(((dp, mp), (1, mp)))
             if parallel_config.use_seq_parallel and self.is_first_iteration:
                 self.wo.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
+            if self.use_prompt_flash_attention:
+                self.prompt_flash_attention.shard(((dp, mp, 1, 1), (dp, mp, 1, 1), (dp, mp, 1, 1), (dp, 1, 1, 1)))
             if parallel_config.recompute.select_recompute:
                 self.apply_rotary_emb.recompute()
                 self.tile_kv.recompute()
                 self.batch_matmul_q_k.recompute()
                 self.mul.recompute()
                 self.add.recompute()
                 self.cast_attn.recompute()
@@ -316,14 +328,18 @@
         if self.use_flash_attention:
             attention = self.flash_attention(query, key, value, mask)
             attention = self._merge_heads(attention)
         else:
             if not self.is_first_iteration and self.use_paged_attention:
                 batch_valid_length, block_tables, _ = kvcache_inputs
                 attention = self.kvcache_mgr.paged_attn(query, batch_valid_length, block_tables)
+            elif self.is_first_iteration and self.use_prompt_flash_attention:
+                attention = self.prompt_flash_attention(query, key, value, mask,
+                                                        None, None, None, None, None, None, None, None)[0]
+                attention = self._merge_heads(attention)
             else:
                 attention = self._attn(query, key, value, mask)
         # [bs, seq/1, hidden_dim] or [bs * seq/1, hidden_dim]
         output = self.wo(attention) # dp, mp -> dp, 1 / dp * mp, 1
         output = self.cast(output, ori_dtype)
 
         return output
@@ -464,14 +480,15 @@
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
                  use_rope_slice=False,
                  use_flash_attention=False,
+                 use_prompt_flash_attention=False,
                  use_paged_attention=False,
                  block_size: Optional[int] = None,
                  num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
         if batch_size or use_past:
             Validator.check_positive_int(batch_size)
@@ -507,14 +524,15 @@
                                         qkv_has_bias=qkv_has_bias,
                                         use_past=use_past,
                                         is_dynamic=is_dynamic,
                                         use_kvcache_op=use_kvcache_op,
                                         is_flexible_shape=is_flexible_shape,
                                         use_rope_slice=use_rope_slice,
                                         use_flash_attention=use_flash_attention,
+                                        use_prompt_flash_attention=use_prompt_flash_attention,
                                         use_paged_attention=use_paged_attention,
                                         block_size=block_size,
                                         num_blocks=num_blocks,
                                         parallel_config=parallel_config)
         self.feed_forward = LlamaFeedForward(dim=self.hidden_size,
                                              intermediate_size=intermediate_size,
                                              hidden_dim=4 * self.hidden_size,
```

## mindformers/modules/kvcache_mgr.py

```diff
@@ -19,15 +19,14 @@
 from mindspore import nn, Parameter, ops
 import mindspore.common.dtype as mstype
 from mindspore.ops import operations as P
 
 
 class KVCacheMgr(nn.Cell):
     """KVCache Manager."""
-
     def __init__(self,
                  n_head,
                  head_dim,
                  max_batch_size=8,
                  max_seq_length=4096,
                  compute_dtype=mstype.float16,
                  is_dynamic=False,
@@ -190,15 +189,14 @@
             key, value = self.trimming(key, value, zactivate_len, batch_size=batch_size)
 
         return key, value
 
 
 class KVCachePreprocess(nn.Cell):
     """KVCache Manager."""
-
     def __init__(self,
                  max_batch_size=8,
                  max_seq_length=4096,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
                  use_paged_attention=False
```

## mindformers/modules/transformer/transformer.py

```diff
@@ -2350,10321 +2350,10323 @@
 000092d0: 7761 726e 696e 673d 5f67 6574 5f70 6172  warning=_get_par
 000092e0: 616c 6c65 6c5f 6d6f 6465 2829 2069 6e20  allel_mode() in 
 000092f0: 2850 6172 616c 6c65 6c4d 6f64 652e 5354  (ParallelMode.ST
 00009300: 414e 445f 414c 4f4e 452c 2929 0a20 2020  AND_ALONE,)).   
 00009310: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
 00009320: 6c66 2c20 7365 715f 6c65 6e67 7468 2c20  lf, seq_length, 
 00009330: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
-00009340: 7970 652e 666c 6f61 7431 362c 0a20 2020  ype.float16,.   
-00009350: 2020 2020 2020 2020 2020 2020 2020 6973                is
-00009360: 5f64 796e 616d 6963 3d46 616c 7365 2c20  _dynamic=False, 
-00009370: 7061 645f 746f 6b65 6e5f 6964 3d32 2c20  pad_token_id=2, 
-00009380: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
-00009390: 696f 6e3d 4661 6c73 6529 3a0a 2020 2020  ion=False):.    
-000093a0: 2020 2020 7375 7065 7228 292e 5f5f 696e      super().__in
-000093b0: 6974 5f5f 2829 0a20 2020 2020 2020 2073  it__().        s
-000093c0: 656c 662e 6474 7970 6520 3d20 636f 6d70  elf.dtype = comp
-000093d0: 7574 655f 7479 7065 0a20 2020 2020 2020  ute_type.       
-000093e0: 2073 656c 662e 6973 5f64 796e 616d 6963   self.is_dynamic
-000093f0: 203d 2069 735f 6479 6e61 6d69 630a 2020   = is_dynamic.  
-00009400: 2020 2020 2020 7365 6c66 2e70 6164 5f74        self.pad_t
-00009410: 6f6b 656e 5f69 6420 3d20 7061 645f 746f  oken_id = pad_to
-00009420: 6b65 6e5f 6964 0a20 2020 2020 2020 2073  ken_id.        s
-00009430: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
-00009440: 7465 6e74 696f 6e20 3d20 7573 655f 666c  tention = use_fl
-00009450: 6173 685f 6174 7465 6e74 696f 6e0a 2020  ash_attention.  
-00009460: 2020 2020 2020 7365 6c66 2e6d 756c 7469        self.multi
-00009470: 706c 795f 6461 7461 203d 2054 656e 736f  ply_data = Tenso
-00009480: 7228 5b2d 3130 3030 302e 305d 2c20 6474  r([-10000.0], dt
-00009490: 7970 653d 636f 6d70 7574 655f 7479 7065  ype=compute_type
-000094a0: 290a 2020 2020 2020 2020 7365 6c66 2e6f  ).        self.o
-000094b0: 6e65 203d 2054 656e 736f 7228 5b31 2e30  ne = Tensor([1.0
-000094c0: 5d2c 2064 7479 7065 3d63 6f6d 7075 7465  ], dtype=compute
-000094d0: 5f74 7970 6529 0a20 2020 2020 2020 2073  _type).        s
-000094e0: 656c 662e 6c6f 7765 725f 7472 6961 6e67  elf.lower_triang
-000094f0: 6c65 5f6d 6173 6b20 3d20 5465 6e73 6f72  le_mask = Tensor
-00009500: 286e 702e 7472 696c 286e 702e 6f6e 6573  (np.tril(np.ones
-00009510: 2873 6861 7065 3d28 7365 715f 6c65 6e67  (shape=(seq_leng
-00009520: 7468 2c20 7365 715f 6c65 6e67 7468 2929  th, seq_length))
-00009530: 292c 206d 7374 7970 652e 666c 6f61 7433  ), mstype.float3
-00009540: 3229 0a0a 2020 2020 2020 2020 7365 6c66  2)..        self
-00009550: 2e73 6861 7065 203d 2050 2e53 6861 7065  .shape = P.Shape
-00009560: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
-00009570: 6361 7374 203d 2050 2e43 6173 7428 290a  cast = P.Cast().
-00009580: 2020 2020 2020 2020 7365 6c66 2e72 6573          self.res
-00009590: 6861 7065 203d 2050 2e52 6573 6861 7065  hape = P.Reshape
-000095a0: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
-000095b0: 6e6f 745f 6571 7561 6c20 3d20 502e 4e6f  not_equal = P.No
-000095c0: 7445 7175 616c 2829 0a20 2020 2020 2020  tEqual().       
-000095d0: 2073 656c 662e 6c65 7373 5f65 7175 616c   self.less_equal
-000095e0: 203d 2050 2e4c 6573 7345 7175 616c 2829   = P.LessEqual()
-000095f0: 0a20 2020 2020 2020 2073 656c 662e 6578  .        self.ex
-00009600: 7061 6e64 5f64 696d 203d 2050 2e45 7870  pand_dim = P.Exp
-00009610: 616e 6444 696d 7328 290a 2020 2020 2020  andDims().      
-00009620: 2020 7365 6c66 2e73 6c69 6365 203d 2050    self.slice = P
-00009630: 2e53 7472 6964 6564 536c 6963 6528 290a  .StridedSlice().
-00009640: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-00009650: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-00009660: 2020 2073 656c 662e 7375 6220 3d20 502e     self.sub = P.
-00009670: 5375 6228 290a 2020 2020 2020 2020 7365  Sub().        se
-00009680: 6c66 2e6d 756c 5f70 6f73 7420 3d20 502e  lf.mul_post = P.
-00009690: 4d75 6c28 290a 2020 2020 2020 2020 7365  Mul().        se
-000096a0: 6c66 2e65 7870 616e 645f 6469 6d5f 706f  lf.expand_dim_po
-000096b0: 7374 203d 2050 2e45 7870 616e 6444 696d  st = P.ExpandDim
-000096c0: 7328 290a 0a20 2020 2064 6566 2063 6f6e  s()..    def con
-000096d0: 7374 7275 6374 2873 656c 662c 2074 6f6b  struct(self, tok
-000096e0: 656e 733d 4e6f 6e65 2c20 6d61 736b 733d  ens=None, masks=
-000096f0: 4e6f 6e65 293a 0a20 2020 2020 2020 2022  None):.        "
-00009700: 2222 466f 7277 6172 6420 7072 6f63 6573  ""Forward proces
-00009710: 7320 6f66 2074 6865 2043 6175 7361 6c4d  s of the CausalM
-00009720: 6173 6b22 2222 0a20 2020 2020 2020 2069  ask""".        i
-00009730: 6620 746f 6b65 6e73 2069 7320 6e6f 7420  f tokens is not 
-00009740: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
-00009750: 2020 6273 203d 2073 656c 662e 7368 6170    bs = self.shap
-00009760: 6528 746f 6b65 6e73 295b 305d 0a20 2020  e(tokens)[0].   
-00009770: 2020 2020 2020 2020 2073 6571 5f6c 656e           seq_len
-00009780: 203d 2073 656c 662e 7368 6170 6528 746f   = self.shape(to
-00009790: 6b65 6e73 295b 315d 0a20 2020 2020 2020  kens)[1].       
-000097a0: 2020 2020 2069 6e70 7574 5f6d 6173 6b20       input_mask 
-000097b0: 3d20 7365 6c66 2e63 6173 7428 7365 6c66  = self.cast(self
-000097c0: 2e6e 6f74 5f65 7175 616c 2874 6f6b 656e  .not_equal(token
-000097d0: 732c 2073 656c 662e 7061 645f 746f 6b65  s, self.pad_toke
-000097e0: 6e5f 6964 292c 2073 656c 662e 6474 7970  n_id), self.dtyp
-000097f0: 6529 0a20 2020 2020 2020 2065 6c73 653a  e).        else:
-00009800: 0a20 2020 2020 2020 2020 2020 2062 7320  .            bs 
-00009810: 3d20 7365 6c66 2e73 6861 7065 286d 6173  = self.shape(mas
-00009820: 6b73 295b 305d 0a20 2020 2020 2020 2020  ks)[0].         
-00009830: 2020 2073 6571 5f6c 656e 203d 2073 656c     seq_len = sel
-00009840: 662e 7368 6170 6528 6d61 736b 7329 5b31  f.shape(masks)[1
-00009850: 5d0a 2020 2020 2020 2020 2020 2020 696e  ].            in
-00009860: 7075 745f 6d61 736b 203d 2073 656c 662e  put_mask = self.
-00009870: 6361 7374 286d 6173 6b73 2c20 7365 6c66  cast(masks, self
-00009880: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-00009890: 7368 6170 655f 7269 6768 7420 3d20 2862  shape_right = (b
-000098a0: 732c 2031 2c20 7365 715f 6c65 6e29 0a20  s, 1, seq_len). 
-000098b0: 2020 2020 2020 2023 204d 6173 6b20 7468         # Mask th
-000098c0: 6520 7061 6464 6564 2069 6e70 7574 730a  e padded inputs.
-000098d0: 2020 2020 2020 2020 6d61 736b 5f72 6967          mask_rig
-000098e0: 6874 203d 2073 656c 662e 7265 7368 6170  ht = self.reshap
-000098f0: 6528 696e 7075 745f 6d61 736b 2c20 7368  e(input_mask, sh
-00009900: 6170 655f 7269 6768 7429 0a20 2020 2020  ape_right).     
-00009910: 2020 2061 7474 656e 7469 6f6e 5f6d 6173     attention_mas
-00009920: 6b20 3d20 6d61 736b 5f72 6967 6874 0a20  k = mask_right. 
-00009930: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
-00009940: 6c66 2e69 735f 6479 6e61 6d69 633a 0a20  lf.is_dynamic:. 
-00009950: 2020 2020 2020 2020 2020 206c 6f77 6572             lower
-00009960: 5f74 7269 616e 676c 6520 3d20 7365 6c66  _triangle = self
-00009970: 2e65 7870 616e 645f 6469 6d28 7365 6c66  .expand_dim(self
-00009980: 2e6c 6f77 6572 5f74 7269 616e 676c 655f  .lower_triangle_
-00009990: 6d61 736b 2c20 3029 0a20 2020 2020 2020  mask, 0).       
-000099a0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-000099b0: 2020 206c 6f77 6572 5f74 7269 616e 676c     lower_triangl
-000099c0: 655f 6d61 736b 203d 2073 656c 662e 736c  e_mask = self.sl
-000099d0: 6963 6528 7365 6c66 2e6c 6f77 6572 5f74  ice(self.lower_t
-000099e0: 7269 616e 676c 655f 6d61 736b 2c20 2830  riangle_mask, (0
-000099f0: 2c20 3029 2c20 2873 6571 5f6c 656e 2c20  , 0), (seq_len, 
-00009a00: 7365 715f 6c65 6e29 2c20 2831 2c20 3129  seq_len), (1, 1)
-00009a10: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
-00009a20: 7765 725f 7472 6961 6e67 6c65 203d 2073  wer_triangle = s
-00009a30: 656c 662e 6578 7061 6e64 5f64 696d 286c  elf.expand_dim(l
-00009a40: 6f77 6572 5f74 7269 616e 676c 655f 6d61  ower_triangle_ma
-00009a50: 736b 2c20 3029 0a20 2020 2020 2020 2023  sk, 0).        #
-00009a60: 2074 6865 2072 6574 7572 6e65 6420 7368   the returned sh
-00009a70: 6170 6520 6973 205b 6273 2c20 7365 715f  ape is [bs, seq_
-00009a80: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
-00009a90: 7468 5d0a 2020 2020 2020 2020 6174 7465  th].        atte
-00009aa0: 6e74 696f 6e5f 6d61 736b 203d 2073 656c  ntion_mask = sel
-00009ab0: 662e 6d75 6c28 6174 7465 6e74 696f 6e5f  f.mul(attention_
-00009ac0: 6d61 736b 2c20 6c6f 7765 725f 7472 6961  mask, lower_tria
-00009ad0: 6e67 6c65 290a 2020 2020 2020 2020 7265  ngle).        re
-00009ae0: 7475 726e 2061 7474 656e 7469 6f6e 5f6d  turn attention_m
-00009af0: 6173 6b0a 0a20 2020 2064 6566 2069 6e63  ask..    def inc
-00009b00: 7265 6d65 6e74 2873 656c 662c 2073 6571  rement(self, seq
-00009b10: 5f72 616e 6765 2c20 6261 7463 685f 7661  _range, batch_va
-00009b20: 6c69 645f 6c65 6e67 7468 2c20 7a61 6374  lid_length, zact
-00009b30: 6976 6174 655f 6c65 6e3d 4e6f 6e65 293a  ivate_len=None):
-00009b40: 0a20 2020 2020 2020 2069 6620 7a61 6374  .        if zact
-00009b50: 6976 6174 655f 6c65 6e20 6973 206e 6f74  ivate_len is not
-00009b60: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
-00009b70: 2020 2073 6571 5f72 616e 6765 203d 2073     seq_range = s
-00009b80: 656c 662e 736c 6963 6528 7365 715f 7261  elf.slice(seq_ra
-00009b90: 6e67 652c 2028 302c 2030 2c20 3029 2c20  nge, (0, 0, 0), 
-00009ba0: 2831 2c20 312c 2073 656c 662e 7368 6170  (1, 1, self.shap
-00009bb0: 6528 7a61 6374 6976 6174 655f 6c65 6e29  e(zactivate_len)
-00009bc0: 5b30 5d29 2c20 2831 2c20 312c 2031 2929  [0]), (1, 1, 1))
-00009bd0: 0a20 2020 2020 2020 206d 6173 6b20 3d20  .        mask = 
-00009be0: 7365 6c66 2e6c 6573 735f 6571 7561 6c28  self.less_equal(
-00009bf0: 7365 6c66 2e72 6573 6861 7065 2873 6571  self.reshape(seq
-00009c00: 5f72 616e 6765 2c20 2831 2c20 312c 202d  _range, (1, 1, -
-00009c10: 3129 292c 2073 656c 662e 7265 7368 6170  1)), self.reshap
-00009c20: 6528 6261 7463 685f 7661 6c69 645f 6c65  e(batch_valid_le
-00009c30: 6e67 7468 2c20 282d 312c 2031 2c20 3129  ngth, (-1, 1, 1)
-00009c40: 2929 0a20 2020 2020 2020 2072 6574 7572  )).        retur
-00009c50: 6e20 6d61 736b 0a0a 2020 2020 6465 6620  n mask..    def 
-00009c60: 696e 6372 656d 656e 745f 736c 6963 6528  increment_slice(
-00009c70: 7365 6c66 2c20 7365 715f 7261 6e67 652c  self, seq_range,
-00009c80: 2073 6571 5f6c 656e 6774 682c 2062 6174   seq_length, bat
-00009c90: 6368 5f76 616c 6964 5f6c 656e 6774 682c  ch_valid_length,
-00009ca0: 207a 6163 7469 7661 7465 5f6c 656e 3d4e   zactivate_len=N
-00009cb0: 6f6e 6529 3a0a 2020 2020 2020 2020 6966  one):.        if
-00009cc0: 207a 6163 7469 7661 7465 5f6c 656e 2069   zactivate_len i
-00009cd0: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
-00009ce0: 2020 2020 2020 2020 7365 715f 7261 6e67          seq_rang
-00009cf0: 655f 6d61 736b 203d 2073 656c 662e 736c  e_mask = self.sl
-00009d00: 6963 6528 7365 715f 7261 6e67 652c 2028  ice(seq_range, (
-00009d10: 302c 2030 2c20 3029 2c20 2831 2c20 312c  0, 0, 0), (1, 1,
-00009d20: 2073 656c 662e 7368 6170 6528 7a61 6374   self.shape(zact
-00009d30: 6976 6174 655f 6c65 6e29 5b30 5d29 2c20  ivate_len)[0]), 
-00009d40: 2831 2c20 312c 2031 2929 0a20 2020 2020  (1, 1, 1)).     
-00009d50: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00009d60: 2020 2020 2073 6571 5f72 616e 6765 5f6d       seq_range_m
-00009d70: 6173 6b20 3d20 7365 6c66 2e73 6c69 6365  ask = self.slice
-00009d80: 2873 6571 5f72 616e 6765 2c20 2830 2c20  (seq_range, (0, 
-00009d90: 302c 2030 292c 2028 312c 2031 2c20 7365  0, 0), (1, 1, se
-00009da0: 715f 6c65 6e67 7468 292c 2028 312c 2031  q_length), (1, 1
-00009db0: 2c20 3129 290a 2020 2020 2020 2020 6d61  , 1)).        ma
-00009dc0: 736b 203d 2073 656c 662e 6c65 7373 5f65  sk = self.less_e
-00009dd0: 7175 616c 2873 656c 662e 7265 7368 6170  qual(self.reshap
-00009de0: 6528 7365 715f 7261 6e67 655f 6d61 736b  e(seq_range_mask
-00009df0: 2c20 2831 2c20 312c 202d 3129 292c 2073  , (1, 1, -1)), s
-00009e00: 656c 662e 7265 7368 6170 6528 6261 7463  elf.reshape(batc
-00009e10: 685f 7661 6c69 645f 6c65 6e67 7468 2c20  h_valid_length, 
-00009e20: 282d 312c 2031 2c20 3129 2929 0a20 2020  (-1, 1, 1))).   
-00009e30: 2020 2020 2072 6574 7572 6e20 6d61 736b       return mask
-00009e40: 0a0a 2020 2020 6465 6620 706f 7374 5f70  ..    def post_p
-00009e50: 726f 6365 7373 2873 656c 662c 206d 6173  rocess(self, mas
-00009e60: 6b29 3a0a 2020 2020 2020 2020 6d61 736b  k):.        mask
-00009e70: 203d 2073 656c 662e 7375 6228 7365 6c66   = self.sub(self
-00009e80: 2e6f 6e65 2c20 7365 6c66 2e63 6173 7428  .one, self.cast(
-00009e90: 6d61 736b 2c20 7365 6c66 2e64 7479 7065  mask, self.dtype
-00009ea0: 2929 0a20 2020 2020 2020 2069 6620 6e6f  )).        if no
-00009eb0: 7420 7365 6c66 2e75 7365 5f66 6c61 7368  t self.use_flash
-00009ec0: 5f61 7474 656e 7469 6f6e 3a0a 2020 2020  _attention:.    
-00009ed0: 2020 2020 2020 2020 6d61 736b 203d 2073          mask = s
-00009ee0: 656c 662e 6578 7061 6e64 5f64 696d 5f70  elf.expand_dim_p
-00009ef0: 6f73 7428 6d61 736b 2c20 3129 0a20 2020  ost(mask, 1).   
-00009f00: 2020 2020 2020 2020 206d 6173 6b20 3d20           mask = 
-00009f10: 7365 6c66 2e6d 756c 5f70 6f73 7428 6d61  self.mul_post(ma
-00009f20: 736b 2c20 7365 6c66 2e6d 756c 7469 706c  sk, self.multipl
-00009f30: 795f 6461 7461 290a 2020 2020 2020 2020  y_data).        
-00009f40: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00009f50: 2020 6d61 736b 203d 2073 656c 662e 6361    mask = self.ca
-00009f60: 7374 286d 6173 6b2c 206d 7374 7970 652e  st(mask, mstype.
-00009f70: 7569 6e74 3829 0a20 2020 2020 2020 2072  uint8).        r
-00009f80: 6574 7572 6e20 6d61 736b 0a0a 2020 2020  eturn mask..    
-00009f90: 6465 6620 7368 6172 6428 7365 6c66 2c20  def shard(self, 
-00009fa0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-00009fb0: 3a0a 2020 2020 2020 2020 6470 203d 2070  :.        dp = p
-00009fc0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00009fd0: 6174 615f 7061 7261 6c6c 656c 0a20 2020  ata_parallel.   
-00009fe0: 2020 2020 2073 656c 662e 6e6f 745f 6571       self.not_eq
-00009ff0: 7561 6c2e 7368 6172 6428 2828 6470 2c20  ual.shard(((dp, 
-0000a000: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
-0000a010: 2073 656c 662e 6578 7061 6e64 5f64 696d   self.expand_dim
-0000a020: 2e73 6861 7264 2828 2831 2c20 3129 2c29  .shard(((1, 1),)
-0000a030: 290a 2020 2020 2020 2020 7365 6c66 2e6d  ).        self.m
-0000a040: 756c 2e73 6861 7264 2828 2864 702c 2031  ul.shard(((dp, 1
-0000a050: 2c20 3129 2c20 2831 2c20 312c 2031 2929  , 1), (1, 1, 1))
-0000a060: 290a 2020 2020 2020 2020 7365 6c66 2e6c  ).        self.l
-0000a070: 6573 735f 6571 7561 6c2e 7368 6172 6428  ess_equal.shard(
-0000a080: 2828 312c 2031 2c20 3129 2c20 2831 2c20  ((1, 1, 1), (1, 
-0000a090: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-0000a0a0: 7365 6c66 2e73 7562 2e73 6861 7264 2828  self.sub.shard((
-0000a0b0: 2831 2c29 2c20 2864 702c 2031 2c20 3129  (1,), (dp, 1, 1)
-0000a0c0: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
-0000a0d0: 6d75 6c5f 706f 7374 2e73 6861 7264 2828  mul_post.shard((
-0000a0e0: 2864 702c 2031 2c20 312c 2031 292c 2028  (dp, 1, 1, 1), (
-0000a0f0: 312c 2929 290a 2020 2020 2020 2020 7365  1,))).        se
-0000a100: 6c66 2e65 7870 616e 645f 6469 6d5f 706f  lf.expand_dim_po
-0000a110: 7374 2e73 6861 7264 2828 2864 702c 2031  st.shard(((dp, 1
-0000a120: 2c20 3129 2c29 290a 0a0a 0a63 6c61 7373  , 1),))....class
-0000a130: 2056 6f63 6162 456d 6265 6464 696e 6728   VocabEmbedding(
-0000a140: 4365 6c6c 293a 0a20 2020 2022 2222 0a20  Cell):.    """. 
-0000a150: 2020 2020 2020 2054 6865 2065 6d62 6564         The embed
-0000a160: 6469 6e67 206c 6f6f 6b75 7020 7461 626c  ding lookup tabl
-0000a170: 6520 6672 6f6d 2074 6865 2030 2d74 6820  e from the 0-th 
-0000a180: 6469 6d20 6f66 2074 6865 2070 6172 616d  dim of the param
-0000a190: 6574 6572 2074 6162 6c65 2e20 5768 656e  eter table. When
-0000a1a0: 2074 6865 2070 6172 616c 6c65 6c5f 636f   the parallel_co
-0000a1b0: 6e66 6967 2e76 6f63 6162 5f65 6d62 5f64  nfig.vocab_emb_d
-0000a1c0: 7020 6973 0a20 2020 2020 2020 2054 7275  p is.        Tru
-0000a1d0: 6520 616e 6420 696e 2074 6865 2060 4155  e and in the `AU
-0000a1e0: 544f 5f50 4152 414c 4c45 4c60 206d 6f64  TO_PARALLEL` mod
-0000a1f0: 652c 2074 6865 2065 6d62 6564 6469 6e67  e, the embedding
-0000a200: 206c 6f6f 6b75 7020 7769 6c6c 2062 6520   lookup will be 
-0000a210: 7472 6169 6e65 6420 6279 2074 6865 2064  trained by the d
-0000a220: 6174 6120 7061 7261 6c6c 656c 2077 6179  ata parallel way
-0000a230: 2c20 6173 2074 6865 0a20 2020 2020 2020  , as the.       
-0000a240: 2070 6172 616d 6574 6572 7320 7769 6c6c   parameters will
-0000a250: 2062 6520 7265 7065 6174 6564 206f 6e20   be repeated on 
-0000a260: 6561 6368 2064 6576 6963 652e 2049 6620  each device. If 
-0000a270: 6661 6c73 652c 2074 6865 2065 6d62 6564  false, the embed
-0000a280: 6469 6e67 2074 6162 6c65 2077 696c 6c20  ding table will 
-0000a290: 6265 2073 6861 7264 6564 2069 6e74 6f20  be sharded into 
-0000a2a0: 6e20 7061 7274 7320 6174 0a20 2020 2020  n parts at.     
-0000a2b0: 2020 2074 6865 2030 2d74 6820 6469 6d65     the 0-th dime
-0000a2c0: 6e73 696f 6e20 6f66 2074 6865 2065 6d62  nsion of the emb
-0000a2d0: 6564 6469 6e67 2074 6162 6c65 2c20 7768  edding table, wh
-0000a2e0: 6572 6520 7468 6520 6e20 6973 2074 6865  ere the n is the
-0000a2f0: 206d 6f64 656c 2070 6172 616c 6c65 6c20   model parallel 
-0000a300: 7761 7920 6465 7465 726d 696e 6564 2062  way determined b
-0000a310: 790a 2020 2020 2020 2020 6070 6172 616c  y.        `paral
-0000a320: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0000a330: 5f70 6172 616c 6c65 6c60 2028 456d 6265  _parallel` (Embe
-0000a340: 6464 696e 674f 7050 6172 616c 6c65 6c43  ddingOpParallelC
-0000a350: 6f6e 6669 6729 2e0a 0a20 2020 2020 2020  onfig)...       
-0000a360: 204e 6f74 653a 0a20 2020 2020 2020 2020   Note:.         
-0000a370: 2020 2057 6865 6e20 6041 5554 4f5f 5041     When `AUTO_PA
-0000a380: 5241 4c4c 454c 6020 6f72 2060 5345 4d49  RALLEL` or `SEMI
-0000a390: 5f41 5554 4f5f 5041 5241 4c4c 454c 6020  _AUTO_PARALLEL` 
-0000a3a0: 6d6f 6465 2069 7320 656e 6162 6c65 642c  mode is enabled,
-0000a3b0: 2074 6869 7320 6c61 7965 7220 7375 7070   this layer supp
-0000a3c0: 6f72 7420 6f6e 6c79 2032 2d64 2064 696d  ort only 2-d dim
-0000a3d0: 656e 7369 6f6e 2069 6e70 7574 732c 0a20  ension inputs,. 
-0000a3e0: 2020 2020 2020 2020 2020 2061 7320 7468             as th
-0000a3f0: 6520 7368 6172 6420 6973 2064 6573 6967  e shard is desig
-0000a400: 6e65 6420 666f 7220 3264 2069 6e70 7574  ned for 2d input
-0000a410: 732e 0a0a 2020 2020 2020 2020 4172 6773  s...        Args
-0000a420: 3a0a 2020 2020 2020 2020 2020 2020 766f  :.            vo
-0000a430: 6361 625f 7369 7a65 2028 696e 7429 3a20  cab_size (int): 
-0000a440: 5369 7a65 206f 6620 7468 6520 6469 6374  Size of the dict
-0000a450: 696f 6e61 7279 206f 6620 656d 6265 6464  ionary of embedd
-0000a460: 696e 6773 2e0a 2020 2020 2020 2020 2020  ings..          
-0000a470: 2020 656d 6265 6464 696e 675f 7369 7a65    embedding_size
-0000a480: 2028 696e 7429 3a20 5468 6520 7369 7a65   (int): The size
-0000a490: 206f 6620 6561 6368 2065 6d62 6564 6469   of each embeddi
-0000a4a0: 6e67 2076 6563 746f 722e 0a20 2020 2020  ng vector..     
-0000a4b0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-0000a4c0: 636f 6e66 6967 2028 456d 6265 6464 696e  config (Embeddin
-0000a4d0: 674f 7050 6172 616c 6c65 6c43 6f6e 6669  gOpParallelConfi
-0000a4e0: 6729 3a20 5468 6520 7061 7261 6c6c 656c  g): The parallel
-0000a4f0: 2063 6f6e 6669 6720 6f66 206e 6574 776f   config of netwo
-0000a500: 726b 2e20 4465 6661 756c 740a 2020 2020  rk. Default.    
-0000a510: 2020 2020 2020 2020 2020 2020 6064 6566              `def
-0000a520: 6175 6c74 5f65 6d62 6564 6469 6e67 5f70  ault_embedding_p
-0000a530: 6172 616c 6c65 6c5f 636f 6e66 6967 602c  arallel_config`,
-0000a540: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
-0000a550: 6045 6d62 6564 6469 6e67 4f70 5061 7261  `EmbeddingOpPara
-0000a560: 6c6c 656c 436f 6e66 6967 6020 7769 7468  llelConfig` with
-0000a570: 2064 6566 6175 6c74 2061 7267 732e 0a20   default args.. 
-0000a580: 2020 2020 2020 2020 2020 2070 6172 616d             param
-0000a590: 5f69 6e69 7420 2855 6e69 6f6e 5b54 656e  _init (Union[Ten
-0000a5a0: 736f 722c 2073 7472 2c20 496e 6974 6961  sor, str, Initia
-0000a5b0: 6c69 7a65 722c 206e 756d 6265 7273 2e4e  lizer, numbers.N
-0000a5c0: 756d 6265 725d 293a 2049 6e69 7469 616c  umber]): Initial
-0000a5d0: 697a 6572 2066 6f72 2074 6865 2065 6d62  izer for the emb
-0000a5e0: 6564 6469 6e67 5f74 6162 6c65 2e0a 2020  edding_table..  
-0000a5f0: 2020 2020 2020 2020 2020 2020 2020 5265                Re
-0000a600: 6665 7220 746f 2063 6c61 7373 2060 696e  fer to class `in
-0000a610: 6974 6961 6c69 7a65 7260 2066 6f72 2074  itializer` for t
-0000a620: 6865 2076 616c 7565 7320 6f66 2073 7472  he values of str
-0000a630: 696e 6720 7768 656e 2061 2073 7472 696e  ing when a strin
-0000a640: 670a 2020 2020 2020 2020 2020 2020 2020  g.              
-0000a650: 2020 6973 2073 7065 6369 6669 6564 2e20    is specified. 
-0000a660: 4465 6661 756c 743a 2027 6e6f 726d 616c  Default: 'normal
-0000a670: 272e 0a0a 2020 2020 2020 2020 496e 7075  '...        Inpu
-0000a680: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
-0000a690: 2d20 2a2a 696e 7075 745f 6964 732a 2a20  - **input_ids** 
-0000a6a0: 2854 656e 736f 7229 202d 2054 6865 2074  (Tensor) - The t
-0000a6b0: 6f6b 656e 697a 6564 2069 6e70 7574 7320  okenized inputs 
-0000a6c0: 7769 7468 2064 6174 6174 7970 6520 696e  with datatype in
-0000a6d0: 7433 3220 7769 7468 2073 6861 7065 2028  t32 with shape (
-0000a6e0: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
-0000a6f0: 6c65 6e67 7468 290a 0a20 2020 2020 2020  length)..       
-0000a700: 204f 7574 7075 7473 3a0a 2020 2020 2020   Outputs:.      
-0000a710: 2020 2020 2020 5475 706c 652c 2061 2074        Tuple, a t
-0000a720: 7570 6c65 2063 6f6e 7461 696e 7320 2860  uple contains (`
-0000a730: 6f75 7470 7574 602c 2060 656d 6265 6464  output`, `embedd
-0000a740: 696e 675f 7461 626c 6560 290a 0a20 2020  ing_table`)..   
-0000a750: 2020 2020 2020 2020 202d 202a 2a6f 7574           - **out
-0000a760: 7075 742a 2a20 2854 656e 736f 7229 202d  put** (Tensor) -
-0000a770: 2054 6865 2065 6d62 6564 6469 6e67 2076   The embedding v
-0000a780: 6563 746f 7220 666f 7220 7468 6520 696e  ector for the in
-0000a790: 7075 7420 7769 7468 2073 6861 7065 2028  put with shape (
-0000a7a0: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
-0000a7b0: 2020 2020 2020 2020 2020 7365 715f 6c65            seq_le
-0000a7c0: 6e67 7468 2c20 656d 6265 6464 696e 675f  ngth, embedding_
-0000a7d0: 7369 7a65 292e 0a20 2020 2020 2020 2020  size)..         
-0000a7e0: 2020 202d 202a 2a65 6d62 6564 6469 6e67     - **embedding
-0000a7f0: 5f74 6162 6c65 2a2a 2028 5465 6e73 6f72  _table** (Tensor
-0000a800: 2920 2d20 5468 6520 656d 6265 6464 696e  ) - The embeddin
-0000a810: 6720 7461 626c 6520 7769 7468 2073 6861  g table with sha
-0000a820: 7065 2028 766f 6361 625f 7369 7a65 2c20  pe (vocab_size, 
-0000a830: 656d 6265 6464 696e 675f 7369 7a65 292e  embedding_size).
-0000a840: 0a0a 2020 2020 2020 2020 5261 6973 6573  ..        Raises
-0000a850: 3a0a 2020 2020 2020 2020 2020 2020 5661  :.            Va
-0000a860: 6c75 6545 7272 6f72 3a20 4966 2074 6865  lueError: If the
-0000a870: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0000a880: 2e76 6f63 6162 5f65 6d62 5f64 7020 6973  .vocab_emb_dp is
-0000a890: 2054 7275 652c 2074 6865 2076 6f63 6162   True, the vocab
-0000a8a0: 2073 697a 6520 6973 206e 6f74 2061 206d   size is not a m
-0000a8b0: 756c 7469 706c 6520 6f66 0a20 2020 2020  ultiple of.     
-0000a8c0: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0000a8d0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0000a8e0: 5f70 6172 616c 6c65 6c0a 2020 2020 2020  _parallel.      
-0000a8f0: 2020 2020 2020 5661 6c75 6545 7272 6f72        ValueError
-0000a900: 3a20 6076 6f63 6162 5f73 697a 6560 2069  : `vocab_size` i
-0000a910: 7320 6e6f 7420 6120 706f 7369 7469 7665  s not a positive
-0000a920: 2076 616c 7565 2e0a 2020 2020 2020 2020   value..        
-0000a930: 2020 2020 5661 6c75 6545 7272 6f72 3a20      ValueError: 
-0000a940: 6065 6d62 6564 6469 6e67 5f73 697a 6560  `embedding_size`
-0000a950: 2069 7320 6e6f 7420 6120 706f 7369 7469   is not a positi
-0000a960: 7665 2076 616c 7565 2e0a 2020 2020 2020  ve value..      
-0000a970: 2020 2020 2020 5479 7065 4572 726f 723a        TypeError:
-0000a980: 2060 7061 7261 6c6c 656c 5f63 6f6e 6669   `parallel_confi
-0000a990: 6760 2069 7320 6e6f 7420 6120 7375 6263  g` is not a subc
-0000a9a0: 6c61 7373 206f 6620 4f70 5061 7261 6c6c  lass of OpParall
-0000a9b0: 656c 436f 6e66 6967 2e0a 0a20 2020 2020  elConfig...     
-0000a9c0: 2020 2053 7570 706f 7274 6564 2050 6c61     Supported Pla
-0000a9d0: 7466 6f72 6d73 3a0a 2020 2020 2020 2020  tforms:.        
-0000a9e0: 2020 2020 6060 4173 6365 6e64 6060 2060      ``Ascend`` `
-0000a9f0: 6047 5055 6060 0a0a 2020 2020 2020 2020  `GPU``..        
-0000aa00: 4578 616d 706c 6573 3a0a 2020 2020 2020  Examples:.      
-0000aa10: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-0000aa20: 206e 756d 7079 2061 7320 6e70 0a20 2020   numpy as np.   
-0000aa30: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
-0000aa40: 6d20 6d69 6e64 666f 726d 6572 732e 6d6f  m mindformers.mo
-0000aa50: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-0000aa60: 7220 696d 706f 7274 2056 6f63 6162 456d  r import VocabEm
-0000aa70: 6265 6464 696e 670a 2020 2020 2020 2020  bedding.        
-0000aa80: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
-0000aa90: 6473 706f 7265 2069 6d70 6f72 7420 5465  dspore import Te
-0000aaa0: 6e73 6f72 0a20 2020 2020 2020 2020 2020  nsor.           
-0000aab0: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
-0000aac0: 6f72 6520 696d 706f 7274 2064 7479 7065  ore import dtype
-0000aad0: 2061 7320 6d73 7479 7065 0a20 2020 2020   as mstype.     
-0000aae0: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
-0000aaf0: 203d 2056 6f63 6162 456d 6265 6464 696e   = VocabEmbeddin
-0000ab00: 6728 766f 6361 625f 7369 7a65 3d33 302c  g(vocab_size=30,
-0000ab10: 2065 6d62 6564 6469 6e67 5f73 697a 653d   embedding_size=
-0000ab20: 3330 290a 2020 2020 2020 2020 2020 2020  30).            
-0000ab30: 3e3e 3e20 7465 6e73 6f72 203d 2054 656e  >>> tensor = Ten
-0000ab40: 736f 7228 6e70 2e6f 6e65 7328 2832 302c  sor(np.ones((20,
-0000ab50: 2031 3529 292c 206d 7374 7970 652e 696e   15)), mstype.in
-0000ab60: 7433 3229 0a20 2020 2020 2020 2020 2020  t32).           
-0000ab70: 203e 3e3e 206f 7574 7075 742c 2074 6162   >>> output, tab
-0000ab80: 6c65 203d 206d 6f64 656c 2874 656e 736f  le = model(tenso
-0000ab90: 7229 0a20 2020 2020 2020 2020 2020 203e  r).            >
-0000aba0: 3e3e 2070 7269 6e74 286f 7574 7075 742e  >> print(output.
-0000abb0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-0000abc0: 2020 2028 3230 2c20 3135 2c20 3330 290a     (20, 15, 30).
-0000abd0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-0000abe0: 7072 696e 7428 7461 626c 652e 7368 6170  print(table.shap
-0000abf0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-0000ac00: 3330 2c20 3330 290a 2020 2020 2222 220a  30, 30).    """.
-0000ac10: 0a20 2020 2040 5f4c 6f67 4163 7469 6f6e  .    @_LogAction
-0000ac20: 4f6e 6365 286d 5f6c 6f67 6765 723d 6c6f  Once(m_logger=lo
-0000ac30: 6767 6572 2c20 6b65 793d 2756 6f63 6162  gger, key='Vocab
-0000ac40: 456d 6265 6464 696e 6727 2c0a 2020 2020  Embedding',.    
-0000ac50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ac60: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
-0000ac70: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
-0000ac80: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-0000ac90: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
-0000aca0: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
-0000acb0: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
-0000acc0: 766f 6361 625f 7369 7a65 3d56 616c 6964  vocab_size=Valid
-0000acd0: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-0000ace0: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-0000acf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ad00: 2020 2020 2020 2020 2065 6d62 6564 6469           embeddi
-0000ad10: 6e67 5f73 697a 653d 5661 6c69 6461 746f  ng_size=Validato
-0000ad20: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
-0000ad30: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
-0000ad40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ad50: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-0000ad60: 6f6e 6669 673d 5f76 616c 6964 5f74 7970  onfig=_valid_typ
-0000ad70: 655f 6368 6563 6b73 285b 456d 6265 6464  e_checks([Embedd
-0000ad80: 696e 674f 7050 6172 616c 6c65 6c43 6f6e  ingOpParallelCon
-0000ad90: 6669 675d 2c20 2256 6f63 6162 456d 6265  fig], "VocabEmbe
-0000ada0: 6464 696e 6722 2929 0a20 2020 2064 6566  dding")).    def
-0000adb0: 205f 5f69 6e69 745f 5f28 7365 6c66 2c20   __init__(self, 
-0000adc0: 766f 6361 625f 7369 7a65 2c20 656d 6265  vocab_size, embe
-0000add0: 6464 696e 675f 7369 7a65 2c20 7061 7261  dding_size, para
-0000ade0: 6c6c 656c 5f63 6f6e 6669 673d 6465 6661  llel_config=defa
-0000adf0: 756c 745f 656d 6265 6464 696e 675f 7061  ult_embedding_pa
-0000ae00: 7261 6c6c 656c 5f63 6f6e 6669 672c 0a20  rallel_config,. 
-0000ae10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ae20: 7061 7261 6d5f 696e 6974 3d27 6e6f 726d  param_init='norm
-0000ae30: 616c 2729 3a0a 2020 2020 2020 2020 7375  al'):.        su
-0000ae40: 7065 7228 566f 6361 6245 6d62 6564 6469  per(VocabEmbeddi
-0000ae50: 6e67 2c20 7365 6c66 292e 5f5f 696e 6974  ng, self).__init
-0000ae60: 5f5f 2829 0a20 2020 2020 2020 205f 6368  __().        _ch
-0000ae70: 6563 6b5f 636f 6e66 6967 2870 6172 616c  eck_config(paral
-0000ae80: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
-0000ae90: 2020 2020 7365 6c66 2e76 6f63 6162 5f73      self.vocab_s
-0000aea0: 697a 6520 3d20 766f 6361 625f 7369 7a65  ize = vocab_size
-0000aeb0: 0a20 2020 2020 2020 2073 656c 662e 656d  .        self.em
-0000aec0: 6265 6464 696e 675f 7369 7a65 203d 2065  bedding_size = e
-0000aed0: 6d62 6564 6469 6e67 5f73 697a 650a 2020  mbedding_size.  
-0000aee0: 2020 2020 2020 7365 6c66 2e65 6d62 6564        self.embed
-0000aef0: 6469 6e67 5f74 6162 6c65 203d 2050 6172  ding_table = Par
-0000af00: 616d 6574 6572 2869 6e69 7469 616c 697a  ameter(initializ
-0000af10: 6572 2870 6172 616d 5f69 6e69 742c 205b  er(param_init, [
-0000af20: 7365 6c66 2e76 6f63 6162 5f73 697a 652c  self.vocab_size,
-0000af30: 2073 656c 662e 656d 6265 6464 696e 675f   self.embedding_
-0000af40: 7369 7a65 5d29 2c0a 2020 2020 2020 2020  size]),.        
-0000af50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009340: 7970 652e 666c 6f61 7431 362c 206d 6173  ype.float16, mas
+00009350: 6b5f 7479 7065 3d6d 7374 7970 652e 666c  k_type=mstype.fl
+00009360: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
+00009370: 2020 2020 2020 2020 6973 5f64 796e 616d          is_dynam
+00009380: 6963 3d46 616c 7365 2c20 7061 645f 746f  ic=False, pad_to
+00009390: 6b65 6e5f 6964 3d32 2c20 7573 655f 666c  ken_id=2, use_fl
+000093a0: 6173 685f 6174 7465 6e74 696f 6e3d 4661  ash_attention=Fa
+000093b0: 6c73 6529 3a0a 2020 2020 2020 2020 7375  lse):.        su
+000093c0: 7065 7228 292e 5f5f 696e 6974 5f5f 2829  per().__init__()
+000093d0: 0a20 2020 2020 2020 2073 656c 662e 6474  .        self.dt
+000093e0: 7970 6520 3d20 636f 6d70 7574 655f 7479  ype = compute_ty
+000093f0: 7065 0a20 2020 2020 2020 2073 656c 662e  pe.        self.
+00009400: 6973 5f64 796e 616d 6963 203d 2069 735f  is_dynamic = is_
+00009410: 6479 6e61 6d69 630a 2020 2020 2020 2020  dynamic.        
+00009420: 7365 6c66 2e70 6164 5f74 6f6b 656e 5f69  self.pad_token_i
+00009430: 6420 3d20 7061 645f 746f 6b65 6e5f 6964  d = pad_token_id
+00009440: 0a20 2020 2020 2020 2073 656c 662e 7573  .        self.us
+00009450: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+00009460: 6e20 3d20 7573 655f 666c 6173 685f 6174  n = use_flash_at
+00009470: 7465 6e74 696f 6e0a 2020 2020 2020 2020  tention.        
+00009480: 7365 6c66 2e6d 756c 7469 706c 795f 6461  self.multiply_da
+00009490: 7461 203d 2054 656e 736f 7228 5b2d 3130  ta = Tensor([-10
+000094a0: 3030 302e 305d 2c20 6474 7970 653d 636f  000.0], dtype=co
+000094b0: 6d70 7574 655f 7479 7065 290a 2020 2020  mpute_type).    
+000094c0: 2020 2020 7365 6c66 2e6f 6e65 203d 2054      self.one = T
+000094d0: 656e 736f 7228 5b31 2e30 5d2c 2064 7479  ensor([1.0], dty
+000094e0: 7065 3d63 6f6d 7075 7465 5f74 7970 6529  pe=compute_type)
+000094f0: 0a20 2020 2020 2020 2073 656c 662e 6c6f  .        self.lo
+00009500: 7765 725f 7472 6961 6e67 6c65 5f6d 6173  wer_triangle_mas
+00009510: 6b20 3d20 5465 6e73 6f72 286e 702e 7472  k = Tensor(np.tr
+00009520: 696c 286e 702e 6f6e 6573 2873 6861 7065  il(np.ones(shape
+00009530: 3d28 7365 715f 6c65 6e67 7468 2c20 7365  =(seq_length, se
+00009540: 715f 6c65 6e67 7468 2929 292c 206d 6173  q_length))), mas
+00009550: 6b5f 7479 7065 290a 0a20 2020 2020 2020  k_type)..       
+00009560: 2073 656c 662e 7368 6170 6520 3d20 502e   self.shape = P.
+00009570: 5368 6170 6528 290a 2020 2020 2020 2020  Shape().        
+00009580: 7365 6c66 2e63 6173 7420 3d20 502e 4361  self.cast = P.Ca
+00009590: 7374 2829 0a20 2020 2020 2020 2073 656c  st().        sel
+000095a0: 662e 7265 7368 6170 6520 3d20 502e 5265  f.reshape = P.Re
+000095b0: 7368 6170 6528 290a 2020 2020 2020 2020  shape().        
+000095c0: 7365 6c66 2e6e 6f74 5f65 7175 616c 203d  self.not_equal =
+000095d0: 2050 2e4e 6f74 4571 7561 6c28 290a 2020   P.NotEqual().  
+000095e0: 2020 2020 2020 7365 6c66 2e6c 6573 735f        self.less_
+000095f0: 6571 7561 6c20 3d20 502e 4c65 7373 4571  equal = P.LessEq
+00009600: 7561 6c28 290a 2020 2020 2020 2020 7365  ual().        se
+00009610: 6c66 2e65 7870 616e 645f 6469 6d20 3d20  lf.expand_dim = 
+00009620: 502e 4578 7061 6e64 4469 6d73 2829 0a20  P.ExpandDims(). 
+00009630: 2020 2020 2020 2073 656c 662e 736c 6963         self.slic
+00009640: 6520 3d20 502e 5374 7269 6465 6453 6c69  e = P.StridedSli
+00009650: 6365 2829 0a20 2020 2020 2020 2073 656c  ce().        sel
+00009660: 662e 6d75 6c20 3d20 502e 4d75 6c28 290a  f.mul = P.Mul().
+00009670: 2020 2020 2020 2020 7365 6c66 2e73 7562          self.sub
+00009680: 203d 2050 2e53 7562 2829 0a20 2020 2020   = P.Sub().     
+00009690: 2020 2073 656c 662e 6d75 6c5f 706f 7374     self.mul_post
+000096a0: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
+000096b0: 2020 2073 656c 662e 6578 7061 6e64 5f64     self.expand_d
+000096c0: 696d 5f70 6f73 7420 3d20 502e 4578 7061  im_post = P.Expa
+000096d0: 6e64 4469 6d73 2829 0a0a 2020 2020 6465  ndDims()..    de
+000096e0: 6620 636f 6e73 7472 7563 7428 7365 6c66  f construct(self
+000096f0: 2c20 746f 6b65 6e73 3d4e 6f6e 652c 206d  , tokens=None, m
+00009700: 6173 6b73 3d4e 6f6e 6529 3a0a 2020 2020  asks=None):.    
+00009710: 2020 2020 2222 2246 6f72 7761 7264 2070      """Forward p
+00009720: 726f 6365 7373 206f 6620 7468 6520 4361  rocess of the Ca
+00009730: 7573 616c 4d61 736b 2222 220a 2020 2020  usalMask""".    
+00009740: 2020 2020 6966 2074 6f6b 656e 7320 6973      if tokens is
+00009750: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
+00009760: 2020 2020 2020 2062 7320 3d20 7365 6c66         bs = self
+00009770: 2e73 6861 7065 2874 6f6b 656e 7329 5b30  .shape(tokens)[0
+00009780: 5d0a 2020 2020 2020 2020 2020 2020 7365  ].            se
+00009790: 715f 6c65 6e20 3d20 7365 6c66 2e73 6861  q_len = self.sha
+000097a0: 7065 2874 6f6b 656e 7329 5b31 5d0a 2020  pe(tokens)[1].  
+000097b0: 2020 2020 2020 2020 2020 696e 7075 745f            input_
+000097c0: 6d61 736b 203d 2073 656c 662e 6361 7374  mask = self.cast
+000097d0: 2873 656c 662e 6e6f 745f 6571 7561 6c28  (self.not_equal(
+000097e0: 746f 6b65 6e73 2c20 7365 6c66 2e70 6164  tokens, self.pad
+000097f0: 5f74 6f6b 656e 5f69 6429 2c20 7365 6c66  _token_id), self
+00009800: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
+00009810: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00009820: 2020 6273 203d 2073 656c 662e 7368 6170    bs = self.shap
+00009830: 6528 6d61 736b 7329 5b30 5d0a 2020 2020  e(masks)[0].    
+00009840: 2020 2020 2020 2020 7365 715f 6c65 6e20          seq_len 
+00009850: 3d20 7365 6c66 2e73 6861 7065 286d 6173  = self.shape(mas
+00009860: 6b73 295b 315d 0a20 2020 2020 2020 2020  ks)[1].         
+00009870: 2020 2069 6e70 7574 5f6d 6173 6b20 3d20     input_mask = 
+00009880: 7365 6c66 2e63 6173 7428 6d61 736b 732c  self.cast(masks,
+00009890: 2073 656c 662e 6474 7970 6529 0a20 2020   self.dtype).   
+000098a0: 2020 2020 2073 6861 7065 5f72 6967 6874       shape_right
+000098b0: 203d 2028 6273 2c20 312c 2073 6571 5f6c   = (bs, 1, seq_l
+000098c0: 656e 290a 2020 2020 2020 2020 2320 4d61  en).        # Ma
+000098d0: 736b 2074 6865 2070 6164 6465 6420 696e  sk the padded in
+000098e0: 7075 7473 0a20 2020 2020 2020 206d 6173  puts.        mas
+000098f0: 6b5f 7269 6768 7420 3d20 7365 6c66 2e72  k_right = self.r
+00009900: 6573 6861 7065 2869 6e70 7574 5f6d 6173  eshape(input_mas
+00009910: 6b2c 2073 6861 7065 5f72 6967 6874 290a  k, shape_right).
+00009920: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+00009930: 6e5f 6d61 736b 203d 206d 6173 6b5f 7269  n_mask = mask_ri
+00009940: 6768 740a 2020 2020 2020 2020 6966 206e  ght.        if n
+00009950: 6f74 2073 656c 662e 6973 5f64 796e 616d  ot self.is_dynam
+00009960: 6963 3a0a 2020 2020 2020 2020 2020 2020  ic:.            
+00009970: 6c6f 7765 725f 7472 6961 6e67 6c65 203d  lower_triangle =
+00009980: 2073 656c 662e 6578 7061 6e64 5f64 696d   self.expand_dim
+00009990: 2873 656c 662e 6c6f 7765 725f 7472 6961  (self.lower_tria
+000099a0: 6e67 6c65 5f6d 6173 6b2c 2030 290a 2020  ngle_mask, 0).  
+000099b0: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+000099c0: 2020 2020 2020 2020 6c6f 7765 725f 7472          lower_tr
+000099d0: 6961 6e67 6c65 5f6d 6173 6b20 3d20 7365  iangle_mask = se
+000099e0: 6c66 2e73 6c69 6365 2873 656c 662e 6c6f  lf.slice(self.lo
+000099f0: 7765 725f 7472 6961 6e67 6c65 5f6d 6173  wer_triangle_mas
+00009a00: 6b2c 2028 302c 2030 292c 2028 7365 715f  k, (0, 0), (seq_
+00009a10: 6c65 6e2c 2073 6571 5f6c 656e 292c 2028  len, seq_len), (
+00009a20: 312c 2031 2929 0a20 2020 2020 2020 2020  1, 1)).         
+00009a30: 2020 206c 6f77 6572 5f74 7269 616e 676c     lower_triangl
+00009a40: 6520 3d20 7365 6c66 2e65 7870 616e 645f  e = self.expand_
+00009a50: 6469 6d28 6c6f 7765 725f 7472 6961 6e67  dim(lower_triang
+00009a60: 6c65 5f6d 6173 6b2c 2030 290a 2020 2020  le_mask, 0).    
+00009a70: 2020 2020 2320 7468 6520 7265 7475 726e      # the return
+00009a80: 6564 2073 6861 7065 2069 7320 5b62 732c  ed shape is [bs,
+00009a90: 2073 6571 5f6c 656e 6774 682c 2073 6571   seq_length, seq
+00009aa0: 5f6c 656e 6774 685d 0a20 2020 2020 2020  _length].       
+00009ab0: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
+00009ac0: 3d20 7365 6c66 2e6d 756c 2861 7474 656e  = self.mul(atten
+00009ad0: 7469 6f6e 5f6d 6173 6b2c 206c 6f77 6572  tion_mask, lower
+00009ae0: 5f74 7269 616e 676c 6529 0a20 2020 2020  _triangle).     
+00009af0: 2020 2072 6574 7572 6e20 6174 7465 6e74     return attent
+00009b00: 696f 6e5f 6d61 736b 0a0a 2020 2020 6465  ion_mask..    de
+00009b10: 6620 696e 6372 656d 656e 7428 7365 6c66  f increment(self
+00009b20: 2c20 7365 715f 7261 6e67 652c 2062 6174  , seq_range, bat
+00009b30: 6368 5f76 616c 6964 5f6c 656e 6774 682c  ch_valid_length,
+00009b40: 207a 6163 7469 7661 7465 5f6c 656e 3d4e   zactivate_len=N
+00009b50: 6f6e 6529 3a0a 2020 2020 2020 2020 6966  one):.        if
+00009b60: 207a 6163 7469 7661 7465 5f6c 656e 2069   zactivate_len i
+00009b70: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
+00009b80: 2020 2020 2020 2020 7365 715f 7261 6e67          seq_rang
+00009b90: 6520 3d20 7365 6c66 2e73 6c69 6365 2873  e = self.slice(s
+00009ba0: 6571 5f72 616e 6765 2c20 2830 2c20 302c  eq_range, (0, 0,
+00009bb0: 2030 292c 2028 312c 2031 2c20 7365 6c66   0), (1, 1, self
+00009bc0: 2e73 6861 7065 287a 6163 7469 7661 7465  .shape(zactivate
+00009bd0: 5f6c 656e 295b 305d 292c 2028 312c 2031  _len)[0]), (1, 1
+00009be0: 2c20 3129 290a 2020 2020 2020 2020 6d61  , 1)).        ma
+00009bf0: 736b 203d 2073 656c 662e 6c65 7373 5f65  sk = self.less_e
+00009c00: 7175 616c 2873 656c 662e 7265 7368 6170  qual(self.reshap
+00009c10: 6528 7365 715f 7261 6e67 652c 2028 312c  e(seq_range, (1,
+00009c20: 2031 2c20 2d31 2929 2c20 7365 6c66 2e72   1, -1)), self.r
+00009c30: 6573 6861 7065 2862 6174 6368 5f76 616c  eshape(batch_val
+00009c40: 6964 5f6c 656e 6774 682c 2028 2d31 2c20  id_length, (-1, 
+00009c50: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+00009c60: 7265 7475 726e 206d 6173 6b0a 0a20 2020  return mask..   
+00009c70: 2064 6566 2069 6e63 7265 6d65 6e74 5f73   def increment_s
+00009c80: 6c69 6365 2873 656c 662c 2073 6571 5f72  lice(self, seq_r
+00009c90: 616e 6765 2c20 7365 715f 6c65 6e67 7468  ange, seq_length
+00009ca0: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+00009cb0: 6e67 7468 2c20 7a61 6374 6976 6174 655f  ngth, zactivate_
+00009cc0: 6c65 6e3d 4e6f 6e65 293a 0a20 2020 2020  len=None):.     
+00009cd0: 2020 2069 6620 7a61 6374 6976 6174 655f     if zactivate_
+00009ce0: 6c65 6e20 6973 206e 6f74 204e 6f6e 653a  len is not None:
+00009cf0: 0a20 2020 2020 2020 2020 2020 2073 6571  .            seq
+00009d00: 5f72 616e 6765 5f6d 6173 6b20 3d20 7365  _range_mask = se
+00009d10: 6c66 2e73 6c69 6365 2873 6571 5f72 616e  lf.slice(seq_ran
+00009d20: 6765 2c20 2830 2c20 302c 2030 292c 2028  ge, (0, 0, 0), (
+00009d30: 312c 2031 2c20 7365 6c66 2e73 6861 7065  1, 1, self.shape
+00009d40: 287a 6163 7469 7661 7465 5f6c 656e 295b  (zactivate_len)[
+00009d50: 305d 292c 2028 312c 2031 2c20 3129 290a  0]), (1, 1, 1)).
+00009d60: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00009d70: 2020 2020 2020 2020 2020 7365 715f 7261            seq_ra
+00009d80: 6e67 655f 6d61 736b 203d 2073 656c 662e  nge_mask = self.
+00009d90: 736c 6963 6528 7365 715f 7261 6e67 652c  slice(seq_range,
+00009da0: 2028 302c 2030 2c20 3029 2c20 2831 2c20   (0, 0, 0), (1, 
+00009db0: 312c 2073 6571 5f6c 656e 6774 6829 2c20  1, seq_length), 
+00009dc0: 2831 2c20 312c 2031 2929 0a20 2020 2020  (1, 1, 1)).     
+00009dd0: 2020 206d 6173 6b20 3d20 7365 6c66 2e6c     mask = self.l
+00009de0: 6573 735f 6571 7561 6c28 7365 6c66 2e72  ess_equal(self.r
+00009df0: 6573 6861 7065 2873 6571 5f72 616e 6765  eshape(seq_range
+00009e00: 5f6d 6173 6b2c 2028 312c 2031 2c20 2d31  _mask, (1, 1, -1
+00009e10: 2929 2c20 7365 6c66 2e72 6573 6861 7065  )), self.reshape
+00009e20: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
+00009e30: 6774 682c 2028 2d31 2c20 312c 2031 2929  gth, (-1, 1, 1))
+00009e40: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+00009e50: 206d 6173 6b0a 0a20 2020 2064 6566 2070   mask..    def p
+00009e60: 6f73 745f 7072 6f63 6573 7328 7365 6c66  ost_process(self
+00009e70: 2c20 6d61 736b 293a 0a20 2020 2020 2020  , mask):.       
+00009e80: 206d 6173 6b20 3d20 7365 6c66 2e73 7562   mask = self.sub
+00009e90: 2873 656c 662e 6f6e 652c 2073 656c 662e  (self.one, self.
+00009ea0: 6361 7374 286d 6173 6b2c 2073 656c 662e  cast(mask, self.
+00009eb0: 6474 7970 6529 290a 2020 2020 2020 2020  dtype)).        
+00009ec0: 6966 206e 6f74 2073 656c 662e 7573 655f  if not self.use_
+00009ed0: 666c 6173 685f 6174 7465 6e74 696f 6e3a  flash_attention:
+00009ee0: 0a20 2020 2020 2020 2020 2020 206d 6173  .            mas
+00009ef0: 6b20 3d20 7365 6c66 2e65 7870 616e 645f  k = self.expand_
+00009f00: 6469 6d5f 706f 7374 286d 6173 6b2c 2031  dim_post(mask, 1
+00009f10: 290a 2020 2020 2020 2020 2020 2020 6d61  ).            ma
+00009f20: 736b 203d 2073 656c 662e 6d75 6c5f 706f  sk = self.mul_po
+00009f30: 7374 286d 6173 6b2c 2073 656c 662e 6d75  st(mask, self.mu
+00009f40: 6c74 6970 6c79 5f64 6174 6129 0a20 2020  ltiply_data).   
+00009f50: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00009f60: 2020 2020 2020 206d 6173 6b20 3d20 7365         mask = se
+00009f70: 6c66 2e63 6173 7428 6d61 736b 2c20 6d73  lf.cast(mask, ms
+00009f80: 7479 7065 2e75 696e 7438 290a 2020 2020  type.uint8).    
+00009f90: 2020 2020 7265 7475 726e 206d 6173 6b0a      return mask.
+00009fa0: 0a20 2020 2064 6566 2073 6861 7264 2873  .    def shard(s
+00009fb0: 656c 662c 2070 6172 616c 6c65 6c5f 636f  elf, parallel_co
+00009fc0: 6e66 6967 293a 0a20 2020 2020 2020 2064  nfig):.        d
+00009fd0: 7020 3d20 7061 7261 6c6c 656c 5f63 6f6e  p = parallel_con
+00009fe0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00009ff0: 6c0a 2020 2020 2020 2020 7365 6c66 2e6e  l.        self.n
+0000a000: 6f74 5f65 7175 616c 2e73 6861 7264 2828  ot_equal.shard((
+0000a010: 2864 702c 2031 292c 2028 2929 290a 2020  (dp, 1), ())).  
+0000a020: 2020 2020 2020 7365 6c66 2e65 7870 616e        self.expan
+0000a030: 645f 6469 6d2e 7368 6172 6428 2828 312c  d_dim.shard(((1,
+0000a040: 2031 292c 2929 0a20 2020 2020 2020 2073   1),)).        s
+0000a050: 656c 662e 6d75 6c2e 7368 6172 6428 2828  elf.mul.shard(((
+0000a060: 6470 2c20 312c 2031 292c 2028 312c 2031  dp, 1, 1), (1, 1
+0000a070: 2c20 3129 2929 0a20 2020 2020 2020 2073  , 1))).        s
+0000a080: 656c 662e 6c65 7373 5f65 7175 616c 2e73  elf.less_equal.s
+0000a090: 6861 7264 2828 2831 2c20 312c 2031 292c  hard(((1, 1, 1),
+0000a0a0: 2028 312c 2031 2c20 3129 2929 0a20 2020   (1, 1, 1))).   
+0000a0b0: 2020 2020 2073 656c 662e 7375 622e 7368       self.sub.sh
+0000a0c0: 6172 6428 2828 312c 292c 2028 6470 2c20  ard(((1,), (dp, 
+0000a0d0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+0000a0e0: 7365 6c66 2e6d 756c 5f70 6f73 742e 7368  self.mul_post.sh
+0000a0f0: 6172 6428 2828 6470 2c20 312c 2031 2c20  ard(((dp, 1, 1, 
+0000a100: 3129 2c20 2831 2c29 2929 0a20 2020 2020  1), (1,))).     
+0000a110: 2020 2073 656c 662e 6578 7061 6e64 5f64     self.expand_d
+0000a120: 696d 5f70 6f73 742e 7368 6172 6428 2828  im_post.shard(((
+0000a130: 6470 2c20 312c 2031 292c 2929 0a0a 0a0a  dp, 1, 1),))....
+0000a140: 636c 6173 7320 566f 6361 6245 6d62 6564  class VocabEmbed
+0000a150: 6469 6e67 2843 656c 6c29 3a0a 2020 2020  ding(Cell):.    
+0000a160: 2222 220a 2020 2020 2020 2020 5468 6520  """.        The 
+0000a170: 656d 6265 6464 696e 6720 6c6f 6f6b 7570  embedding lookup
+0000a180: 2074 6162 6c65 2066 726f 6d20 7468 6520   table from the 
+0000a190: 302d 7468 2064 696d 206f 6620 7468 6520  0-th dim of the 
+0000a1a0: 7061 7261 6d65 7465 7220 7461 626c 652e  parameter table.
+0000a1b0: 2057 6865 6e20 7468 6520 7061 7261 6c6c   When the parall
+0000a1c0: 656c 5f63 6f6e 6669 672e 766f 6361 625f  el_config.vocab_
+0000a1d0: 656d 625f 6470 2069 730a 2020 2020 2020  emb_dp is.      
+0000a1e0: 2020 5472 7565 2061 6e64 2069 6e20 7468    True and in th
+0000a1f0: 6520 6041 5554 4f5f 5041 5241 4c4c 454c  e `AUTO_PARALLEL
+0000a200: 6020 6d6f 6465 2c20 7468 6520 656d 6265  ` mode, the embe
+0000a210: 6464 696e 6720 6c6f 6f6b 7570 2077 696c  dding lookup wil
+0000a220: 6c20 6265 2074 7261 696e 6564 2062 7920  l be trained by 
+0000a230: 7468 6520 6461 7461 2070 6172 616c 6c65  the data paralle
+0000a240: 6c20 7761 792c 2061 7320 7468 650a 2020  l way, as the.  
+0000a250: 2020 2020 2020 7061 7261 6d65 7465 7273        parameters
+0000a260: 2077 696c 6c20 6265 2072 6570 6561 7465   will be repeate
+0000a270: 6420 6f6e 2065 6163 6820 6465 7669 6365  d on each device
+0000a280: 2e20 4966 2066 616c 7365 2c20 7468 6520  . If false, the 
+0000a290: 656d 6265 6464 696e 6720 7461 626c 6520  embedding table 
+0000a2a0: 7769 6c6c 2062 6520 7368 6172 6465 6420  will be sharded 
+0000a2b0: 696e 746f 206e 2070 6172 7473 2061 740a  into n parts at.
+0000a2c0: 2020 2020 2020 2020 7468 6520 302d 7468          the 0-th
+0000a2d0: 2064 696d 656e 7369 6f6e 206f 6620 7468   dimension of th
+0000a2e0: 6520 656d 6265 6464 696e 6720 7461 626c  e embedding tabl
+0000a2f0: 652c 2077 6865 7265 2074 6865 206e 2069  e, where the n i
+0000a300: 7320 7468 6520 6d6f 6465 6c20 7061 7261  s the model para
+0000a310: 6c6c 656c 2077 6179 2064 6574 6572 6d69  llel way determi
+0000a320: 6e65 6420 6279 0a20 2020 2020 2020 2060  ned by.        `
+0000a330: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000a340: 6d6f 6465 6c5f 7061 7261 6c6c 656c 6020  model_parallel` 
+0000a350: 2845 6d62 6564 6469 6e67 4f70 5061 7261  (EmbeddingOpPara
+0000a360: 6c6c 656c 436f 6e66 6967 292e 0a0a 2020  llelConfig)...  
+0000a370: 2020 2020 2020 4e6f 7465 3a0a 2020 2020        Note:.    
+0000a380: 2020 2020 2020 2020 5768 656e 2060 4155          When `AU
+0000a390: 544f 5f50 4152 414c 4c45 4c60 206f 7220  TO_PARALLEL` or 
+0000a3a0: 6053 454d 495f 4155 544f 5f50 4152 414c  `SEMI_AUTO_PARAL
+0000a3b0: 4c45 4c60 206d 6f64 6520 6973 2065 6e61  LEL` mode is ena
+0000a3c0: 626c 6564 2c20 7468 6973 206c 6179 6572  bled, this layer
+0000a3d0: 2073 7570 706f 7274 206f 6e6c 7920 322d   support only 2-
+0000a3e0: 6420 6469 6d65 6e73 696f 6e20 696e 7075  d dimension inpu
+0000a3f0: 7473 2c0a 2020 2020 2020 2020 2020 2020  ts,.            
+0000a400: 6173 2074 6865 2073 6861 7264 2069 7320  as the shard is 
+0000a410: 6465 7369 676e 6564 2066 6f72 2032 6420  designed for 2d 
+0000a420: 696e 7075 7473 2e0a 0a20 2020 2020 2020  inputs...       
+0000a430: 2041 7267 733a 0a20 2020 2020 2020 2020   Args:.         
+0000a440: 2020 2076 6f63 6162 5f73 697a 6520 2869     vocab_size (i
+0000a450: 6e74 293a 2053 697a 6520 6f66 2074 6865  nt): Size of the
+0000a460: 2064 6963 7469 6f6e 6172 7920 6f66 2065   dictionary of e
+0000a470: 6d62 6564 6469 6e67 732e 0a20 2020 2020  mbeddings..     
+0000a480: 2020 2020 2020 2065 6d62 6564 6469 6e67         embedding
+0000a490: 5f73 697a 6520 2869 6e74 293a 2054 6865  _size (int): The
+0000a4a0: 2073 697a 6520 6f66 2065 6163 6820 656d   size of each em
+0000a4b0: 6265 6464 696e 6720 7665 6374 6f72 2e0a  bedding vector..
+0000a4c0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0000a4d0: 6c6c 656c 5f63 6f6e 6669 6720 2845 6d62  llel_config (Emb
+0000a4e0: 6564 6469 6e67 4f70 5061 7261 6c6c 656c  eddingOpParallel
+0000a4f0: 436f 6e66 6967 293a 2054 6865 2070 6172  Config): The par
+0000a500: 616c 6c65 6c20 636f 6e66 6967 206f 6620  allel config of 
+0000a510: 6e65 7477 6f72 6b2e 2044 6566 6175 6c74  network. Default
+0000a520: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000a530: 2060 6465 6661 756c 745f 656d 6265 6464   `default_embedd
+0000a540: 696e 675f 7061 7261 6c6c 656c 5f63 6f6e  ing_parallel_con
+0000a550: 6669 6760 2c20 616e 2069 6e73 7461 6e63  fig`, an instanc
+0000a560: 6520 6f66 2060 456d 6265 6464 696e 674f  e of `EmbeddingO
+0000a570: 7050 6172 616c 6c65 6c43 6f6e 6669 6760  pParallelConfig`
+0000a580: 2077 6974 6820 6465 6661 756c 7420 6172   with default ar
+0000a590: 6773 2e0a 2020 2020 2020 2020 2020 2020  gs..            
+0000a5a0: 7061 7261 6d5f 696e 6974 2028 556e 696f  param_init (Unio
+0000a5b0: 6e5b 5465 6e73 6f72 2c20 7374 722c 2049  n[Tensor, str, I
+0000a5c0: 6e69 7469 616c 697a 6572 2c20 6e75 6d62  nitializer, numb
+0000a5d0: 6572 732e 4e75 6d62 6572 5d29 3a20 496e  ers.Number]): In
+0000a5e0: 6974 6961 6c69 7a65 7220 666f 7220 7468  itializer for th
+0000a5f0: 6520 656d 6265 6464 696e 675f 7461 626c  e embedding_tabl
+0000a600: 652e 0a20 2020 2020 2020 2020 2020 2020  e..             
+0000a610: 2020 2052 6566 6572 2074 6f20 636c 6173     Refer to clas
+0000a620: 7320 6069 6e69 7469 616c 697a 6572 6020  s `initializer` 
+0000a630: 666f 7220 7468 6520 7661 6c75 6573 206f  for the values o
+0000a640: 6620 7374 7269 6e67 2077 6865 6e20 6120  f string when a 
+0000a650: 7374 7269 6e67 0a20 2020 2020 2020 2020  string.         
+0000a660: 2020 2020 2020 2069 7320 7370 6563 6966         is specif
+0000a670: 6965 642e 2044 6566 6175 6c74 3a20 276e  ied. Default: 'n
+0000a680: 6f72 6d61 6c27 2e0a 0a20 2020 2020 2020  ormal'...       
+0000a690: 2049 6e70 7574 733a 0a20 2020 2020 2020   Inputs:.       
+0000a6a0: 2020 2020 202d 202a 2a69 6e70 7574 5f69       - **input_i
+0000a6b0: 6473 2a2a 2028 5465 6e73 6f72 2920 2d20  ds** (Tensor) - 
+0000a6c0: 5468 6520 746f 6b65 6e69 7a65 6420 696e  The tokenized in
+0000a6d0: 7075 7473 2077 6974 6820 6461 7461 7479  puts with dataty
+0000a6e0: 7065 2069 6e74 3332 2077 6974 6820 7368  pe int32 with sh
+0000a6f0: 6170 6520 2862 6174 6368 5f73 697a 652c  ape (batch_size,
+0000a700: 2073 6571 5f6c 656e 6774 6829 0a0a 2020   seq_length)..  
+0000a710: 2020 2020 2020 4f75 7470 7574 733a 0a20        Outputs:. 
+0000a720: 2020 2020 2020 2020 2020 2054 7570 6c65             Tuple
+0000a730: 2c20 6120 7475 706c 6520 636f 6e74 6169  , a tuple contai
+0000a740: 6e73 2028 606f 7574 7075 7460 2c20 6065  ns (`output`, `e
+0000a750: 6d62 6564 6469 6e67 5f74 6162 6c65 6029  mbedding_table`)
+0000a760: 0a0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
+0000a770: 2a2a 6f75 7470 7574 2a2a 2028 5465 6e73  **output** (Tens
+0000a780: 6f72 2920 2d20 5468 6520 656d 6265 6464  or) - The embedd
+0000a790: 696e 6720 7665 6374 6f72 2066 6f72 2074  ing vector for t
+0000a7a0: 6865 2069 6e70 7574 2077 6974 6820 7368  he input with sh
+0000a7b0: 6170 6520 2862 6174 6368 5f73 697a 652c  ape (batch_size,
+0000a7c0: 0a20 2020 2020 2020 2020 2020 2020 2073  .              s
+0000a7d0: 6571 5f6c 656e 6774 682c 2065 6d62 6564  eq_length, embed
+0000a7e0: 6469 6e67 5f73 697a 6529 2e0a 2020 2020  ding_size)..    
+0000a7f0: 2020 2020 2020 2020 2d20 2a2a 656d 6265          - **embe
+0000a800: 6464 696e 675f 7461 626c 652a 2a20 2854  dding_table** (T
+0000a810: 656e 736f 7229 202d 2054 6865 2065 6d62  ensor) - The emb
+0000a820: 6564 6469 6e67 2074 6162 6c65 2077 6974  edding table wit
+0000a830: 6820 7368 6170 6520 2876 6f63 6162 5f73  h shape (vocab_s
+0000a840: 697a 652c 2065 6d62 6564 6469 6e67 5f73  ize, embedding_s
+0000a850: 697a 6529 2e0a 0a20 2020 2020 2020 2052  ize)...        R
+0000a860: 6169 7365 733a 0a20 2020 2020 2020 2020  aises:.         
+0000a870: 2020 2056 616c 7565 4572 726f 723a 2049     ValueError: I
+0000a880: 6620 7468 6520 7061 7261 6c6c 656c 5f63  f the parallel_c
+0000a890: 6f6e 6669 672e 766f 6361 625f 656d 625f  onfig.vocab_emb_
+0000a8a0: 6470 2069 7320 5472 7565 2c20 7468 6520  dp is True, the 
+0000a8b0: 766f 6361 6220 7369 7a65 2069 7320 6e6f  vocab size is no
+0000a8c0: 7420 6120 6d75 6c74 6970 6c65 206f 660a  t a multiple of.
+0000a8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000a8e0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000a8f0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 0a20  model_parallel. 
+0000a900: 2020 2020 2020 2020 2020 2056 616c 7565             Value
+0000a910: 4572 726f 723a 2060 766f 6361 625f 7369  Error: `vocab_si
+0000a920: 7a65 6020 6973 206e 6f74 2061 2070 6f73  ze` is not a pos
+0000a930: 6974 6976 6520 7661 6c75 652e 0a20 2020  itive value..   
+0000a940: 2020 2020 2020 2020 2056 616c 7565 4572           ValueEr
+0000a950: 726f 723a 2060 656d 6265 6464 696e 675f  ror: `embedding_
+0000a960: 7369 7a65 6020 6973 206e 6f74 2061 2070  size` is not a p
+0000a970: 6f73 6974 6976 6520 7661 6c75 652e 0a20  ositive value.. 
+0000a980: 2020 2020 2020 2020 2020 2054 7970 6545             TypeE
+0000a990: 7272 6f72 3a20 6070 6172 616c 6c65 6c5f  rror: `parallel_
+0000a9a0: 636f 6e66 6967 6020 6973 206e 6f74 2061  config` is not a
+0000a9b0: 2073 7562 636c 6173 7320 6f66 204f 7050   subclass of OpP
+0000a9c0: 6172 616c 6c65 6c43 6f6e 6669 672e 0a0a  arallelConfig...
+0000a9d0: 2020 2020 2020 2020 5375 7070 6f72 7465          Supporte
+0000a9e0: 6420 506c 6174 666f 726d 733a 0a20 2020  d Platforms:.   
+0000a9f0: 2020 2020 2020 2020 2060 6041 7363 656e           ``Ascen
+0000aa00: 6460 6020 6060 4750 5560 600a 0a20 2020  d`` ``GPU``..   
+0000aa10: 2020 2020 2045 7861 6d70 6c65 733a 0a20       Examples:. 
+0000aa20: 2020 2020 2020 2020 2020 203e 3e3e 2069             >>> i
+0000aa30: 6d70 6f72 7420 6e75 6d70 7920 6173 206e  mport numpy as n
+0000aa40: 700a 2020 2020 2020 2020 2020 2020 3e3e  p.            >>
+0000aa50: 3e20 6672 6f6d 206d 696e 6466 6f72 6d65  > from mindforme
+0000aa60: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+0000aa70: 666f 726d 6572 2069 6d70 6f72 7420 566f  former import Vo
+0000aa80: 6361 6245 6d62 6564 6469 6e67 0a20 2020  cabEmbedding.   
+0000aa90: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
+0000aaa0: 6d20 6d69 6e64 7370 6f72 6520 696d 706f  m mindspore impo
+0000aab0: 7274 2054 656e 736f 720a 2020 2020 2020  rt Tensor.      
+0000aac0: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
+0000aad0: 696e 6473 706f 7265 2069 6d70 6f72 7420  indspore import 
+0000aae0: 6474 7970 6520 6173 206d 7374 7970 650a  dtype as mstype.
+0000aaf0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000ab00: 6d6f 6465 6c20 3d20 566f 6361 6245 6d62  model = VocabEmb
+0000ab10: 6564 6469 6e67 2876 6f63 6162 5f73 697a  edding(vocab_siz
+0000ab20: 653d 3330 2c20 656d 6265 6464 696e 675f  e=30, embedding_
+0000ab30: 7369 7a65 3d33 3029 0a20 2020 2020 2020  size=30).       
+0000ab40: 2020 2020 203e 3e3e 2074 656e 736f 7220       >>> tensor 
+0000ab50: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
+0000ab60: 2828 3230 2c20 3135 2929 2c20 6d73 7479  ((20, 15)), msty
+0000ab70: 7065 2e69 6e74 3332 290a 2020 2020 2020  pe.int32).      
+0000ab80: 2020 2020 2020 3e3e 3e20 6f75 7470 7574        >>> output
+0000ab90: 2c20 7461 626c 6520 3d20 6d6f 6465 6c28  , table = model(
+0000aba0: 7465 6e73 6f72 290a 2020 2020 2020 2020  tensor).        
+0000abb0: 2020 2020 3e3e 3e20 7072 696e 7428 6f75      >>> print(ou
+0000abc0: 7470 7574 2e73 6861 7065 290a 2020 2020  tput.shape).    
+0000abd0: 2020 2020 2020 2020 2832 302c 2031 352c          (20, 15,
+0000abe0: 2033 3029 0a20 2020 2020 2020 2020 2020   30).           
+0000abf0: 203e 3e3e 2070 7269 6e74 2874 6162 6c65   >>> print(table
+0000ac00: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+0000ac10: 2020 2020 2833 302c 2033 3029 0a20 2020      (30, 30).   
+0000ac20: 2022 2222 0a0a 2020 2020 405f 4c6f 6741   """..    @_LogA
+0000ac30: 6374 696f 6e4f 6e63 6528 6d5f 6c6f 6767  ctionOnce(m_logg
+0000ac40: 6572 3d6c 6f67 6765 722c 206b 6579 3d27  er=logger, key='
+0000ac50: 566f 6361 6245 6d62 6564 6469 6e67 272c  VocabEmbedding',
+0000ac60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000ac70: 2020 2020 206e 6f5f 7761 726e 696e 673d       no_warning=
+0000ac80: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
+0000ac90: 6465 2829 2069 6e20 2850 6172 616c 6c65  de() in (Paralle
+0000aca0: 6c4d 6f64 652e 5354 414e 445f 414c 4f4e  lMode.STAND_ALON
+0000acb0: 452c 2929 0a20 2020 2040 5f61 7267 735f  E,)).    @_args_
+0000acc0: 7479 7065 5f76 616c 6964 6174 6f72 5f63  type_validator_c
+0000acd0: 6865 636b 2876 6f63 6162 5f73 697a 653d  heck(vocab_size=
+0000ace0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+0000acf0: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
+0000ad00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ad10: 2020 2020 2020 2020 2020 2020 2020 656d                em
+0000ad20: 6265 6464 696e 675f 7369 7a65 3d56 616c  bedding_size=Val
+0000ad30: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
+0000ad40: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
+0000ad50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ad60: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+0000ad70: 6c65 6c5f 636f 6e66 6967 3d5f 7661 6c69  lel_config=_vali
+0000ad80: 645f 7479 7065 5f63 6865 636b 7328 5b45  d_type_checks([E
+0000ad90: 6d62 6564 6469 6e67 4f70 5061 7261 6c6c  mbeddingOpParall
+0000ada0: 656c 436f 6e66 6967 5d2c 2022 566f 6361  elConfig], "Voca
+0000adb0: 6245 6d62 6564 6469 6e67 2229 290a 2020  bEmbedding")).  
+0000adc0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+0000add0: 656c 662c 2076 6f63 6162 5f73 697a 652c  elf, vocab_size,
+0000ade0: 2065 6d62 6564 6469 6e67 5f73 697a 652c   embedding_size,
+0000adf0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0000ae00: 3d64 6566 6175 6c74 5f65 6d62 6564 6469  =default_embeddi
+0000ae10: 6e67 5f70 6172 616c 6c65 6c5f 636f 6e66  ng_parallel_conf
+0000ae20: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
+0000ae30: 2020 2020 2070 6172 616d 5f69 6e69 743d       param_init=
+0000ae40: 276e 6f72 6d61 6c27 293a 0a20 2020 2020  'normal'):.     
+0000ae50: 2020 2073 7570 6572 2856 6f63 6162 456d     super(VocabEm
+0000ae60: 6265 6464 696e 672c 2073 656c 6629 2e5f  bedding, self)._
+0000ae70: 5f69 6e69 745f 5f28 290a 2020 2020 2020  _init__().      
+0000ae80: 2020 5f63 6865 636b 5f63 6f6e 6669 6728    _check_config(
+0000ae90: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+0000aea0: 0a20 2020 2020 2020 2073 656c 662e 766f  .        self.vo
+0000aeb0: 6361 625f 7369 7a65 203d 2076 6f63 6162  cab_size = vocab
+0000aec0: 5f73 697a 650a 2020 2020 2020 2020 7365  _size.        se
+0000aed0: 6c66 2e65 6d62 6564 6469 6e67 5f73 697a  lf.embedding_siz
+0000aee0: 6520 3d20 656d 6265 6464 696e 675f 7369  e = embedding_si
+0000aef0: 7a65 0a20 2020 2020 2020 2073 656c 662e  ze.        self.
+0000af00: 656d 6265 6464 696e 675f 7461 626c 6520  embedding_table 
+0000af10: 3d20 5061 7261 6d65 7465 7228 696e 6974  = Parameter(init
+0000af20: 6961 6c69 7a65 7228 7061 7261 6d5f 696e  ializer(param_in
+0000af30: 6974 2c20 5b73 656c 662e 766f 6361 625f  it, [self.vocab_
+0000af40: 7369 7a65 2c20 7365 6c66 2e65 6d62 6564  size, self.embed
+0000af50: 6469 6e67 5f73 697a 655d 292c 0a20 2020  ding_size]),.   
 0000af60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000af70: 206e 616d 653d 2765 6d62 6564 6469 6e67   name='embedding
-0000af80: 5f74 6162 6c65 272c 2070 6172 616c 6c65  _table', paralle
-0000af90: 6c5f 6f70 7469 6d69 7a65 723d 4661 6c73  l_optimizer=Fals
-0000afa0: 6529 0a0a 2020 2020 2020 2020 6966 2070  e)..        if p
-0000afb0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e76  arallel_config.v
-0000afc0: 6f63 6162 5f65 6d62 5f64 703a 0a20 2020  ocab_emb_dp:.   
-0000afd0: 2020 2020 2020 2020 2073 656c 662e 6761           self.ga
-0000afe0: 7468 6572 203d 2050 2e47 6174 6865 7228  ther = P.Gather(
-0000aff0: 292e 7368 6172 6428 2828 312c 2031 292c  ).shard(((1, 1),
-0000b000: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
-0000b010: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-0000b020: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-0000b030: 2020 6c6f 6767 6572 2e69 6e66 6f28 6622    logger.info(f"
-0000b040: 5573 696e 6720 7b70 6172 616c 6c65 6c5f  Using {parallel_
-0000b050: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-0000b060: 6c6c 656c 7d20 6461 7461 2070 6172 616c  llel} data paral
-0000b070: 6c65 6c20 666f 7220 7468 6520 656d 6265  lel for the embe
-0000b080: 6464 696e 6720 6c6f 6f6b 7570 2e22 290a  dding lookup.").
-0000b090: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-0000b0a0: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-0000b0b0: 662e 766f 6361 625f 7369 7a65 2025 2070  f.vocab_size % p
-0000b0c0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0000b0d0: 6f64 656c 5f70 6172 616c 6c65 6c20 213d  odel_parallel !=
-0000b0e0: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-0000b0f0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-0000b100: 7272 6f72 2866 2254 6865 2076 6f63 6162  rror(f"The vocab
-0000b110: 2073 697a 6520 6f66 2074 6865 2065 6d62   size of the emb
-0000b120: 6564 6469 6e67 207b 7365 6c66 2e76 6f63  edding {self.voc
-0000b130: 6162 5f73 697a 657d 206d 7573 7420 6265  ab_size} must be
-0000b140: 2061 2022 0a20 2020 2020 2020 2020 2020   a ".           
-0000b150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000b160: 2020 2020 2020 6622 6d75 6c74 6970 6c65        f"multiple
-0000b170: 206f 6620 7061 7261 6c6c 656c 5f63 6f6e   of parallel_con
-0000b180: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000b190: 656c 207b 7061 7261 6c6c 656c 5f63 6f6e  el {parallel_con
-0000b1a0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000b1b0: 656c 7d2e 2229 0a20 2020 2020 2020 2020  el}.").         
-0000b1c0: 2020 2073 656c 662e 6761 7468 6572 203d     self.gather =
-0000b1d0: 2050 2e47 6174 6865 7228 292e 7368 6172   P.Gather().shar
-0000b1e0: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
-0000b1f0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000b200: 656c 2c20 3129 2c20 2870 6172 616c 6c65  el, 1), (paralle
-0000b210: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-0000b220: 7261 6c6c 656c 2c20 3129 2929 0a20 2020  rallel, 1))).   
-0000b230: 2020 2020 2020 2020 206c 6f67 6765 722e           logger.
-0000b240: 696e 666f 2866 2255 7369 6e67 207b 7061  info(f"Using {pa
-0000b250: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-0000b260: 7461 5f70 6172 616c 6c65 6c7d 2064 6174  ta_parallel} dat
-0000b270: 6120 7061 7261 6c6c 656c 2061 6e64 207b  a parallel and {
-0000b280: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0000b290: 6d6f 6465 6c5f 7061 7261 6c6c 656c 7d20  model_parallel} 
-0000b2a0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-0000b2b0: 2020 2020 2020 2020 2020 6622 6d6f 6465            f"mode
-0000b2c0: 6c20 7061 7261 6c6c 656c 2066 6f72 2074  l parallel for t
-0000b2d0: 6865 2065 6d62 6564 6469 6e67 206c 6f6f  he embedding loo
-0000b2e0: 6b75 702e 2229 0a0a 2020 2020 6465 6620  kup.")..    def 
-0000b2f0: 636f 6e73 7472 7563 7428 7365 6c66 2c20  construct(self, 
-0000b300: 696e 7075 745f 6964 7329 3a0a 2020 2020  input_ids):.    
-0000b310: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
-0000b320: 5f64 7479 7065 2846 2e64 7479 7065 2869  _dtype(F.dtype(i
-0000b330: 6e70 7574 5f69 6473 292c 2022 696e 7075  nput_ids), "inpu
-0000b340: 745f 6964 7322 2c20 5b6d 7374 7970 652e  t_ids", [mstype.
-0000b350: 696e 7433 325d 2c20 7365 6c66 2e63 6c73  int32], self.cls
-0000b360: 5f6e 616d 6529 0a20 2020 2020 2020 206f  _name).        o
-0000b370: 7574 7075 7420 3d20 7365 6c66 2e67 6174  utput = self.gat
-0000b380: 6865 7228 7365 6c66 2e65 6d62 6564 6469  her(self.embeddi
-0000b390: 6e67 5f74 6162 6c65 2c20 696e 7075 745f  ng_table, input_
-0000b3a0: 6964 732c 2030 290a 2020 2020 2020 2020  ids, 0).        
-0000b3b0: 7265 7475 726e 206f 7574 7075 742c 2073  return output, s
-0000b3c0: 656c 662e 656d 6265 6464 696e 675f 7461  elf.embedding_ta
-0000b3d0: 626c 652e 7661 6c75 6528 290a 0a0a 636c  ble.value()...cl
-0000b3e0: 6173 7320 4d75 6c74 6948 6561 6441 7474  ass MultiHeadAtt
-0000b3f0: 656e 7469 6f6e 2843 656c 6c29 3a0a 2020  ention(Cell):.  
-0000b400: 2020 7222 2222 0a20 2020 2020 2020 2054    r""".        T
-0000b410: 6869 7320 6973 2061 6e20 696d 706c 656d  his is an implem
-0000b420: 656e 7461 7469 6f6e 206f 6620 6d75 6c74  entation of mult
-0000b430: 6968 6561 6420 6174 7465 6e74 696f 6e20  ihead attention 
-0000b440: 696e 2074 6865 2070 6170 6572 2060 4174  in the paper `At
-0000b450: 7465 6e74 696f 6e20 6973 2061 6c6c 2079  tention is all y
-0000b460: 6f75 206e 6565 640a 2020 2020 2020 2020  ou need.        
-0000b470: 3c68 7474 7073 3a2f 2f61 7278 6976 2e6f  <https://arxiv.o
-0000b480: 7267 2f70 6466 2f31 3730 362e 3033 3736  rg/pdf/1706.0376
-0000b490: 3276 352e 7064 663e 605f 2e20 4769 7665  2v5.pdf>`_. Give
-0000b4a0: 6e20 7468 6520 7175 6572 7920 7665 6374  n the query vect
-0000b4b0: 6f72 2077 6974 6820 736f 7572 6365 206c  or with source l
-0000b4c0: 656e 6774 682c 2061 6e64 2074 6865 0a20  ength, and the. 
-0000b4d0: 2020 2020 2020 206b 6579 2061 6e64 2076         key and v
-0000b4e0: 616c 7565 2076 6563 746f 7220 7769 7468  alue vector with
-0000b4f0: 2074 6172 6765 7420 6c65 6e67 7468 2c20   target length, 
-0000b500: 7468 6520 6174 7465 6e74 696f 6e20 7769  the attention wi
-0000b510: 6c6c 2062 6520 7065 7266 6f72 6d65 6420  ll be performed 
-0000b520: 6173 2074 6865 2066 6f6c 6c6f 7769 6e67  as the following
-0000b530: 0a0a 2020 2020 2020 2020 2e2e 206d 6174  ..        .. mat
-0000b540: 683a 3a0a 2020 2020 2020 2020 2020 2020  h::.            
-0000b550: 2020 204d 756c 7469 4865 6164 4174 7465     MultiHeadAtte
-0000b560: 6e74 696f 6e28 7175 6572 792c 206b 6579  ntion(query, key
-0000b570: 2c20 7665 6374 6f72 2920 3d20 436f 6e63  , vector) = Conc
-0000b580: 6174 2868 6561 645f 312c 205c 646f 7473  at(head_1, \dots
-0000b590: 2c20 6865 6164 5f68 2957 5e4f 0a0a 2020  , head_h)W^O..  
-0000b5a0: 2020 2020 2020 7768 6572 6520 3a6d 6174        where :mat
-0000b5b0: 683a 6068 6561 645f 6920 3d20 4174 7465  h:`head_i = Atte
-0000b5c0: 6e74 696f 6e28 5157 5f69 5e51 2c20 4b57  ntion(QW_i^Q, KW
-0000b5d0: 5f69 5e4b 2c20 5657 5f69 5e56 2960 2e20  _i^K, VW_i^V)`. 
-0000b5e0: 5468 6520 6465 6661 756c 7420 6973 2077  The default is w
-0000b5f0: 6974 6820 6120 6269 6173 2e0a 0a20 2020  ith a bias...   
-0000b600: 2020 2020 2069 6620 7175 6572 792c 206b       if query, k
-0000b610: 6579 2061 6e64 2076 616c 7565 2074 656e  ey and value ten
-0000b620: 736f 7220 6973 2073 616d 652c 2074 6865  sor is same, the
-0000b630: 6e20 6974 2077 696c 6c20 6265 2073 656c  n it will be sel
-0000b640: 6620 6174 7465 6e74 696f 6e2e 0a0a 2020  f attention...  
-0000b650: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
-0000b660: 2020 2020 2020 2020 6261 7463 685f 7369          batch_si
-0000b670: 7a65 2869 6e74 293a 2054 6865 2062 6174  ze(int): The bat
-0000b680: 6368 2073 697a 6520 6f66 2074 6865 2069  ch size of the i
-0000b690: 6e70 7574 2074 656e 736f 7220 7768 656e  nput tensor when
-0000b6a0: 2064 6f20 696e 6372 656e 6d65 6e74 616c   do increnmental
-0000b6b0: 2070 7265 6469 6374 696f 6e2e 2053 686f   prediction. Sho
-0000b6c0: 756c 6420 6265 2061 2070 6f73 6974 6976  uld be a positiv
-0000b6d0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-0000b6e0: 2020 7661 6c75 652e 2057 6865 6e20 646f    value. When do
-0000b6f0: 2074 7261 696e 696e 6720 6f72 2070 7265   training or pre
-0000b700: 6469 6374 696f 6e2c 2074 6865 2061 7267  diction, the arg
-0000b710: 756d 656e 7420 7769 6c6c 206e 6f74 2077  ument will not w
-0000b720: 6f72 6b20 616e 6420 7468 6520 7573 6572  ork and the user
-0000b730: 2063 616e 206a 7573 7420 7061 7373 204e   can just pass N
-0000b740: 6f6e 6520 746f 0a20 2020 2020 2020 2020  one to.         
-0000b750: 2020 2020 2020 2074 6865 2061 7267 756d         the argum
-0000b760: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
-0000b770: 2073 7263 5f73 6571 5f6c 656e 6774 6828   src_seq_length(
-0000b780: 696e 7429 3a20 5468 6520 7365 7175 656e  int): The sequen
-0000b790: 6365 206c 656e 6774 6820 6f66 2074 6865  ce length of the
-0000b7a0: 2071 7565 7279 2076 6563 746f 722e 0a20   query vector.. 
-0000b7b0: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
-0000b7c0: 6571 5f6c 656e 6774 6828 696e 7429 3a20  eq_length(int): 
-0000b7d0: 5468 6520 7365 7175 656e 6365 206c 656e  The sequence len
-0000b7e0: 6774 6820 6f66 2074 6865 206b 6579 2061  gth of the key a
-0000b7f0: 6e64 2076 616c 7565 2076 6563 746f 722e  nd value vector.
-0000b800: 0a20 2020 2020 2020 2020 2020 2068 6964  .            hid
-0000b810: 6465 6e5f 7369 7a65 2869 6e74 293a 2054  den_size(int): T
-0000b820: 6865 2068 6964 6465 6e20 7369 7a65 206f  he hidden size o
-0000b830: 6620 7468 6520 696e 7075 742e 0a20 2020  f the input..   
-0000b840: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
-0000b850: 6473 2869 6e74 293a 2054 6865 206e 756d  ds(int): The num
-0000b860: 6265 7220 6f66 2074 6865 2068 6561 6473  ber of the heads
-0000b870: 2e0a 2020 2020 2020 2020 2020 2020 6869  ..            hi
-0000b880: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-0000b890: 6528 666c 6f61 7429 3a20 5468 6520 6472  e(float): The dr
-0000b8a0: 6f70 6f75 7420 7261 7465 206f 6620 7468  opout rate of th
-0000b8b0: 6520 6669 6e61 6c20 6f75 7470 7574 206f  e final output o
-0000b8c0: 6620 7468 6520 6c61 7965 722e 2044 6566  f the layer. Def
-0000b8d0: 6175 6c74 3a30 2e31 2e0a 2020 2020 2020  ault:0.1..      
-0000b8e0: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
-0000b8f0: 6472 6f70 6f75 745f 7261 7465 2866 6c6f  dropout_rate(flo
-0000b900: 6174 293a 2054 6865 2064 726f 706f 7574  at): The dropout
-0000b910: 2072 6174 6520 6f66 2074 6865 2061 7474   rate of the att
-0000b920: 656e 7469 6f6e 2073 636f 7265 732e 2044  ention scores. D
-0000b930: 6566 6175 6c74 3a30 2e31 2e0a 2020 2020  efault:0.1..    
-0000b940: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
-0000b950: 6474 7970 6528 6474 7970 652e 4e75 6d62  dtype(dtype.Numb
-0000b960: 6572 293a 2054 6865 2063 6f6d 7075 7461  er): The computa
-0000b970: 7469 6f6e 2074 7970 6520 6f66 2064 656e  tion type of den
-0000b980: 7365 2e20 4465 6661 756c 7420 6d73 7479  se. Default msty
-0000b990: 7065 2e66 6c6f 6174 3136 2e0a 2020 2020  pe.float16..    
-0000b9a0: 2020 2020 2020 2020 2020 2020 5368 6f75              Shou
-0000b9b0: 6c64 2062 6520 6d73 7479 7065 2e66 6c6f  ld be mstype.flo
-0000b9c0: 6174 3332 206f 7220 6d73 7479 7065 2e66  at32 or mstype.f
-0000b9d0: 6c6f 6174 3136 2e0a 2020 2020 2020 2020  loat16..        
-0000b9e0: 2020 2020 736f 6674 6d61 785f 636f 6d70      softmax_comp
-0000b9f0: 7574 655f 7479 7065 2864 7479 7065 2e4e  ute_type(dtype.N
-0000ba00: 756d 6265 7229 3a20 5468 6520 7479 7065  umber): The type
-0000ba10: 206f 6620 736f 6674 6d61 7820 636f 6d70   of softmax comp
-0000ba20: 7574 6174 696f 6e20 6d6f 6475 6c65 2e20  utation module. 
-0000ba30: 4465 6661 756c 7420 6d73 7479 7065 2e66  Default mstype.f
-0000ba40: 6c6f 6174 3332 2e0a 2020 2020 2020 2020  loat32..        
-0000ba50: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
-0000ba60: 6520 6d73 7479 7065 2e66 6c6f 6174 3332  e mstype.float32
-0000ba70: 206f 7220 6d73 7479 7065 2e66 6c6f 6174   or mstype.float
-0000ba80: 3136 2e0a 2020 2020 2020 2020 2020 2020  16..            
-0000ba90: 7061 7261 6d5f 696e 6974 5f74 7970 6528  param_init_type(
-0000baa0: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-0000bab0: 6865 2070 6172 616d 6574 6572 2069 6e69  he parameter ini
-0000bac0: 7469 616c 697a 6174 696f 6e20 7479 7065  tialization type
-0000bad0: 206f 6620 7468 6520 6d6f 6475 6c65 2e20   of the module. 
-0000bae0: 4465 6661 756c 7420 6d73 7479 7065 2e66  Default mstype.f
-0000baf0: 6c6f 6174 3332 2e0a 2020 2020 2020 2020  loat32..        
-0000bb00: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
-0000bb10: 6520 6d73 7479 7065 2e66 6c6f 6174 3332  e mstype.float32
-0000bb20: 206f 7220 6d73 7479 7065 2e66 6c6f 6174   or mstype.float
-0000bb30: 3136 2e0a 2020 2020 2020 2020 2020 2020  16..            
-0000bb40: 7573 655f 7061 7374 2862 6f6f 6c29 3a20  use_past(bool): 
-0000bb50: 5573 6520 7468 6520 7061 7374 2073 7461  Use the past sta
-0000bb60: 7465 2074 6f20 636f 6d70 7574 652c 2075  te to compute, u
-0000bb70: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
-0000bb80: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
-0000bb90: 466f 7220 6578 616d 706c 652c 2069 6620  For example, if 
-0000bba0: 7765 2068 6176 6520 7477 6f0a 2020 2020  we have two.    
-0000bbb0: 2020 2020 2020 2020 2020 2020 776f 7264              word
-0000bbc0: 7320 616e 6420 7761 6e74 2074 6f20 6765  s and want to ge
-0000bbd0: 6e65 7261 7465 2074 6865 2074 656e 206d  nerate the ten m
-0000bbe0: 6f72 6520 776f 7264 732e 2057 6520 6a75  ore words. We ju
-0000bbf0: 7374 206e 6565 6420 746f 2063 6f6d 7075  st need to compu
-0000bc00: 7465 2074 6865 2074 776f 2077 6f72 6473  te the two words
-0000bc10: 2720 7374 6174 6520 6f6e 6c79 206f 6e63  ' state only onc
-0000bc20: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0000bc30: 2020 2061 6e64 2067 656e 6572 6174 6520     and generate 
-0000bc40: 7468 6520 6e65 7874 2077 6f72 6420 6f6e  the next word on
-0000bc50: 6520 6279 206f 6e65 2e20 5768 656e 2075  e by one. When u
-0000bc60: 7365 5f70 6173 7420 6973 2054 7275 652c  se_past is True,
-0000bc70: 2074 6865 7265 2061 7265 2074 776f 2073   there are two s
-0000bc80: 7465 7073 2074 6f20 7275 6e20 7468 6520  teps to run the 
-0000bc90: 7072 6564 6963 7469 6f6e 2e0a 2020 2020  prediction..    
-0000bca0: 2020 2020 2020 2020 2020 2020 496e 2074              In t
-0000bcb0: 6865 2066 6972 7374 2073 7465 702c 2073  he first step, s
-0000bcc0: 6574 2074 6865 2069 735f 6669 7273 745f  et the is_first_
-0000bcd0: 6974 6572 6174 696f 6e20 746f 2062 6520  iteration to be 
-0000bce0: 5472 7565 2062 790a 2020 2020 2020 2020  True by.        
-0000bcf0: 2020 2020 2020 2020 606d 6f64 656c 2e61          `model.a
-0000bd00: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
-0000bd10: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
-0000bd20: 6174 696f 6e3d 5472 7565 2960 2c20 616e  ation=True)`, an
-0000bd30: 6420 7061 7373 2074 6865 2066 756c 6c20  d pass the full 
-0000bd40: 696e 7075 7473 2e20 5468 656e 2c20 7365  inputs. Then, se
-0000bd50: 7420 7468 650a 2020 2020 2020 2020 2020  t the.          
-0000bd60: 2020 2020 2020 6973 5f66 6972 7374 5f69        is_first_i
-0000bd70: 7465 7261 7469 6f6e 2074 6f20 6265 2046  teration to be F
-0000bd80: 616c 7365 2062 7920 606d 6f64 656c 2e61  alse by `model.a
-0000bd90: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
-0000bda0: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
-0000bdb0: 6174 696f 6e3d 4661 6c73 6529 602e 2041  ation=False)`. A
-0000bdc0: 7420 7468 6973 206d 6f6d 656e 742c 0a20  t this moment,. 
-0000bdd0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0000bde0: 6173 7320 7468 6520 7369 6e67 6c65 2073  ass the single s
-0000bdf0: 7465 7027 7320 696e 7075 7420 7465 6e73  tep's input tens
-0000be00: 6f72 2c20 616e 6420 6c6f 6f70 2069 742e  or, and loop it.
-0000be10: 2044 6566 6175 6c74 2046 616c 7365 2e0a   Default False..
-0000be20: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0000be30: 6c6c 656c 5f63 6f6e 6669 6728 4f70 5061  llel_config(OpPa
-0000be40: 7261 6c6c 656c 436f 6e66 6967 293a 2054  rallelConfig): T
-0000be50: 6865 2070 6172 616c 6c65 6c20 636f 6e66  he parallel conf
-0000be60: 6967 7572 652e 2044 6566 6175 6c74 2060  igure. Default `
-0000be70: 6465 6661 756c 745f 6470 6d70 5f63 6f6e  default_dpmp_con
-0000be80: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
-0000be90: 2020 2020 2020 616e 2069 6e73 7461 6e63        an instanc
-0000bea0: 6520 6f66 2060 4f70 5061 7261 6c6c 656c  e of `OpParallel
-0000beb0: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
-0000bec0: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
-0000bed0: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
-0000bee0: 2020 2020 2020 2020 2d20 2a2a 7175 6572          - **quer
-0000bef0: 795f 7465 6e73 6f72 2a2a 2028 5465 6e73  y_tensor** (Tens
-0000bf00: 6f72 2920 2d20 5468 6520 7175 6572 7920  or) - The query 
-0000bf10: 7665 6374 6f72 2077 6974 6820 7368 6170  vector with shap
-0000bf20: 6520 2862 6174 6368 5f73 697a 652c 2073  e (batch_size, s
-0000bf30: 7263 5f73 6571 5f6c 656e 6774 682c 2068  rc_seq_length, h
-0000bf40: 6964 6465 6e5f 7369 7a65 2920 6f72 0a20  idden_size) or. 
-0000bf50: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
-0000bf60: 7463 685f 7369 7a65 202a 2073 7263 5f73  tch_size * src_s
-0000bf70: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0000bf80: 6e5f 7369 7a65 292c 2069 6620 7468 6520  n_size), if the 
-0000bf90: 7573 655f 7061 7374 2069 7320 4661 6c73  use_past is Fals
-0000bfa0: 6520 6f72 2069 735f 6669 7273 745f 6974  e or is_first_it
-0000bfb0: 6572 6174 696f 6e3d 5472 7565 2e0a 2020  eration=True..  
-0000bfc0: 2020 2020 2020 2020 2020 2020 4f74 6865              Othe
-0000bfd0: 7277 6973 652c 206d 7573 7420 6265 2028  rwise, must be (
-0000bfe0: 6261 7463 685f 7369 7a65 2c20 312c 2068  batch_size, 1, h
-0000bff0: 6964 6465 6e5f 7369 7a65 290a 2020 2020  idden_size).    
-0000c000: 2020 2020 2020 2020 2d20 2a2a 6b65 795f          - **key_
-0000c010: 7465 6e73 6f72 2a2a 2028 5465 6e73 6f72  tensor** (Tensor
-0000c020: 2920 2d20 5468 6520 6b65 7920 7665 6374  ) - The key vect
-0000c030: 6f72 2077 6974 6820 7368 6170 6520 2862  or with shape (b
-0000c040: 6174 6368 5f73 697a 652c 2074 6774 5f73  atch_size, tgt_s
-0000c050: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0000c060: 6e5f 7369 7a65 2920 6f72 0a20 2020 2020  n_size) or.     
-0000c070: 2020 2020 2020 2020 2028 6261 7463 685f           (batch_
-0000c080: 7369 7a65 202a 2074 6774 5f73 6571 5f6c  size * tgt_seq_l
-0000c090: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-0000c0a0: 7a65 292c 2069 6620 7468 6520 7573 655f  ze), if the use_
-0000c0b0: 7061 7374 2069 7320 4661 6c73 6520 6f72  past is False or
-0000c0c0: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
-0000c0d0: 696f 6e3d 5472 7565 2e0a 2020 2020 2020  ion=True..      
-0000c0e0: 2020 2020 2020 2020 4f74 6865 7277 6973          Otherwis
-0000c0f0: 652c 206d 7573 7420 6265 2028 6261 7463  e, must be (batc
-0000c100: 685f 7369 7a65 2c20 312c 2068 6964 6465  h_size, 1, hidde
-0000c110: 6e5f 7369 7a65 290a 2020 2020 2020 2020  n_size).        
-0000c120: 2020 2020 2d20 2a2a 7661 6c75 655f 7465      - **value_te
-0000c130: 6e73 6f72 2a2a 2028 5465 6e73 6f72 2920  nsor** (Tensor) 
-0000c140: 2d20 5468 6520 7661 6c75 6520 7665 6374  - The value vect
-0000c150: 6f72 2077 6974 6820 7368 6170 6520 2862  or with shape (b
-0000c160: 6174 6368 5f73 697a 652c 2074 6774 5f73  atch_size, tgt_s
-0000c170: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0000c180: 6e5f 7369 7a65 2920 6f72 0a20 2020 2020  n_size) or.     
-0000c190: 2020 2020 2020 2020 2028 6261 7463 685f           (batch_
-0000c1a0: 7369 7a65 202a 2074 6774 5f73 6571 5f6c  size * tgt_seq_l
-0000c1b0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-0000c1c0: 7a65 292c 2069 6620 7468 6520 7573 655f  ze), if the use_
-0000c1d0: 7061 7374 2069 7320 4661 6c73 6520 6f72  past is False or
-0000c1e0: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
-0000c1f0: 696f 6e3d 5472 7565 2e0a 2020 2020 2020  ion=True..      
-0000c200: 2020 2020 2020 2020 4f74 6865 7277 6973          Otherwis
-0000c210: 652c 206d 7573 7420 6265 2028 6261 7463  e, must be (batc
-0000c220: 685f 7369 7a65 2c20 312c 2068 6964 6465  h_size, 1, hidde
-0000c230: 6e5f 7369 7a65 290a 2020 2020 2020 2020  n_size).        
-0000c240: 2020 2020 2d20 2a2a 6174 7465 6e74 696f      - **attentio
-0000c250: 6e5f 6d61 736b 2a2a 2028 5465 6e73 6f72  n_mask** (Tensor
-0000c260: 2920 2d20 4966 2074 6865 2075 7365 5f70  ) - If the use_p
-0000c270: 6173 7420 6973 2046 616c 7365 206f 7220  ast is False or 
-0000c280: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-0000c290: 6f6e 3d54 7275 652c 2074 6865 2061 7474  on=True, the att
-0000c2a0: 656e 7469 6f6e 206d 6173 6b0a 2020 2020  ention mask.    
-0000c2b0: 2020 2020 2020 2020 2020 6d61 7472 6978            matrix
-0000c2c0: 2073 686f 756c 6420 6261 2028 6261 7463   should ba (batc
-0000c2d0: 685f 7369 7a65 2c20 7372 635f 7365 715f  h_size, src_seq_
-0000c2e0: 6c65 6e67 7468 2c20 7467 745f 7365 715f  length, tgt_seq_
-0000c2f0: 6c65 6e67 7468 292c 206f 7220 4e6f 6e65  length), or None
-0000c300: 2e20 4e6f 6e65 206d 6561 6e73 2074 6865  . None means the
-0000c310: 7265 2077 696c 6c20 6265 206e 6f20 6d61  re will be no ma
-0000c320: 736b 0a20 2020 2020 2020 2020 2020 2020  sk.             
-0000c330: 2069 6e20 736f 6674 6d61 7820 636f 6d70   in softmax comp
-0000c340: 7574 6174 696f 6e2e 204f 7468 6572 7769  utation. Otherwi
-0000c350: 7365 2c20 7468 6520 6d61 736b 206d 7573  se, the mask mus
-0000c360: 7420 6265 2028 6261 7463 685f 7369 7a65  t be (batch_size
-0000c370: 2c20 312c 2074 6774 5f73 6571 5f6c 656e  , 1, tgt_seq_len
-0000c380: 6774 6829 0a20 2020 2020 2020 2020 2020  gth).           
-0000c390: 202d 202a 2a6b 6579 5f70 6173 742a 2a20   - **key_past** 
-0000c3a0: 2854 656e 736f 7229 202d 2046 6c6f 6174  (Tensor) - Float
-0000c3b0: 3136 2074 656e 736f 7220 7769 7468 2073  16 tensor with s
-0000c3c0: 6861 7065 2028 6261 7463 685f 7369 7a65  hape (batch_size
-0000c3d0: 2c20 6e75 6d5f 6865 6164 732c 2073 697a  , num_heads, siz
-0000c3e0: 655f 7065 725f 6865 6164 2c20 7467 745f  e_per_head, tgt_
-0000c3f0: 7365 715f 6c65 6e67 7468 292e 0a20 2020  seq_length)..   
-0000c400: 2020 2020 2020 2020 2020 2054 6865 2070             The p
-0000c410: 6173 7420 6361 6c63 756c 6174 6564 206b  ast calculated k
-0000c420: 6579 2076 6563 746f 722e 2055 7365 6420  ey vector. Used 
-0000c430: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
-0000c440: 7072 6564 6963 7469 6f6e 2077 6865 6e20  prediction when 
-0000c450: 7468 6520 7573 655f 7061 7374 2069 7320  the use_past is 
-0000c460: 5472 7565 2e0a 2020 2020 2020 2020 2020  True..          
-0000c470: 2020 2020 4465 6661 756c 7420 4e6f 6e65      Default None
-0000c480: 2e0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
-0000c490: 2a2a 7661 6c75 655f 7061 7374 2a2a 2028  **value_past** (
-0000c4a0: 5465 6e73 6f72 2920 2d20 466c 6f61 7431  Tensor) - Float1
-0000c4b0: 3620 7465 6e73 6f72 2077 6974 6820 7368  6 tensor with sh
-0000c4c0: 6170 650a 2020 2020 2020 2020 2020 2020  ape.            
-0000c4d0: 2020 2862 6174 6368 5f73 697a 652c 206e    (batch_size, n
-0000c4e0: 756d 5f68 6561 6473 2c20 7467 745f 7365  um_heads, tgt_se
-0000c4f0: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-0000c500: 6572 5f68 6561 6429 2e0a 2020 2020 2020  er_head)..      
-0000c510: 2020 2020 2020 2020 5468 6520 7061 7374          The past
-0000c520: 2063 616c 6375 6c61 7465 6420 7661 6c75   calculated valu
-0000c530: 6520 7665 6374 6f72 2e20 5573 6564 2066  e vector. Used f
-0000c540: 6f72 2069 6e63 7265 6d65 6e74 616c 2070  or incremental p
-0000c550: 7265 6469 6374 696f 6e20 7768 656e 2074  rediction when t
-0000c560: 6865 2075 7365 5f70 6173 7420 6973 2054  he use_past is T
-0000c570: 7275 652e 0a20 2020 2020 2020 2020 2020  rue..           
-0000c580: 2020 2044 6566 6175 6c74 204e 6f6e 652e     Default None.
-0000c590: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-0000c5a0: 2a62 6174 6368 5f76 616c 6964 5f6c 656e  *batch_valid_len
-0000c5b0: 6774 682a 2a20 2854 656e 736f 7229 202d  gth** (Tensor) -
-0000c5c0: 2049 6e74 3332 2074 656e 736f 7220 7769   Int32 tensor wi
-0000c5d0: 7468 2073 6861 7065 2028 6261 7463 685f  th shape (batch_
-0000c5e0: 7369 7a65 2c29 2074 6865 2070 6173 7420  size,) the past 
-0000c5f0: 6361 6c63 756c 6174 6564 2074 6865 2069  calculated the i
-0000c600: 6e64 6578 2e0a 2020 2020 2020 2020 2020  ndex..          
-0000c610: 2020 2020 5573 6564 2066 6f72 2069 6e63      Used for inc
-0000c620: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
-0000c630: 696f 6e20 7768 656e 2074 6865 2075 7365  ion when the use
-0000c640: 5f70 6173 7420 6973 2054 7275 652e 2044  _past is True. D
-0000c650: 6566 6175 6c74 204e 6f6e 652e 0a0a 2020  efault None...  
-0000c660: 2020 2020 2020 4f75 7470 7574 733a 0a20        Outputs:. 
-0000c670: 2020 2020 2020 2020 2020 2054 7570 6c65             Tuple
-0000c680: 2c20 6120 7475 706c 6520 636f 6e74 6169  , a tuple contai
-0000c690: 6e73 2860 6f75 7470 7574 602c 2060 6c61  ns(`output`, `la
-0000c6a0: 7965 725f 7072 6573 656e 7460 290a 0a20  yer_present`).. 
-0000c6b0: 2020 2020 2020 2020 2020 202d 202a 2a6f             - **o
-0000c6c0: 7574 7075 742a 2a20 2854 656e 736f 7229  utput** (Tensor)
-0000c6d0: 202d 2054 656e 736f 722c 2074 6865 2066   - Tensor, the f
-0000c6e0: 6c6f 6174 2074 656e 736f 7220 6f66 2074  loat tensor of t
-0000c6f0: 6865 206f 7574 7075 7420 6f66 2074 6865  he output of the
-0000c700: 206c 6179 6572 2077 6974 680a 2020 2020   layer with.    
-0000c710: 2020 2020 2020 2020 2020 7368 6170 6520            shape 
-0000c720: 2862 6174 6368 5f73 697a 652c 2073 7263  (batch_size, src
-0000c730: 5f73 6571 5f6c 656e 6774 682c 2068 6964  _seq_length, hid
-0000c740: 6465 6e5f 7369 7a65 2920 6f72 2028 6261  den_size) or (ba
-0000c750: 7463 685f 7369 7a65 202a 2073 7263 5f73  tch_size * src_s
-0000c760: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0000c770: 6e5f 7369 7a65 292c 0a20 2020 2020 2020  n_size),.       
-0000c780: 2020 2020 2020 2069 6620 7468 6520 7573         if the us
-0000c790: 655f 7061 7374 2069 7320 4661 6c73 6520  e_past is False 
-0000c7a0: 6f72 2069 735f 6669 7273 745f 6974 6572  or is_first_iter
-0000c7b0: 6174 696f 6e3d 5472 7565 2e20 4f74 6865  ation=True. Othe
-0000c7c0: 7277 6973 652c 2069 7420 7769 6c6c 2062  rwise, it will b
-0000c7d0: 6520 2862 6174 6368 5f73 697a 652c 2031  e (batch_size, 1
-0000c7e0: 2c20 6869 6464 656e 5f73 697a 6529 2e0a  , hidden_size)..
-0000c7f0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-0000c800: 2a6c 6179 6572 5f70 7265 7365 6e74 2a2a  *layer_present**
-0000c810: 2028 5475 706c 6529 202d 2041 2074 7570   (Tuple) - A tup
-0000c820: 6c65 206f 6620 7468 6520 5465 6e73 6f72  le of the Tensor
-0000c830: 206f 6620 7468 6520 7072 6f6a 6563 7465   of the projecte
-0000c840: 6420 6b65 7920 616e 6420 7661 6c75 6520  d key and value 
-0000c850: 7665 6374 6f72 2077 6974 680a 2020 2020  vector with.    
-0000c860: 2020 2020 2020 2020 2020 2828 6261 7463            ((batc
-0000c870: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-0000c880: 732c 2073 697a 655f 7065 725f 6865 6164  s, size_per_head
-0000c890: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
-0000c8a0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0000c8b0: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
-0000c8c0: 6d5f 6865 6164 732c 2074 6774 5f73 6571  m_heads, tgt_seq
-0000c8d0: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
-0000c8e0: 725f 6865 6164 2929 2e0a 0a20 2020 2020  r_head))...     
-0000c8f0: 2020 2053 7570 706f 7274 6564 2050 6c61     Supported Pla
-0000c900: 7466 6f72 6d73 3a0a 2020 2020 2020 2020  tforms:.        
-0000c910: 2020 2020 6060 4173 6365 6e64 6060 2060      ``Ascend`` `
-0000c920: 6047 5055 6060 0a0a 2020 2020 2020 2020  `GPU``..        
-0000c930: 4578 616d 706c 6573 3a0a 2020 2020 2020  Examples:.      
-0000c940: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-0000c950: 206e 756d 7079 2061 7320 6e70 0a20 2020   numpy as np.   
-0000c960: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
-0000c970: 6d20 6d69 6e64 666f 726d 6572 732e 6d6f  m mindformers.mo
-0000c980: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-0000c990: 7220 696d 706f 7274 204d 756c 7469 4865  r import MultiHe
-0000c9a0: 6164 4174 7465 6e74 696f 6e0a 2020 2020  adAttention.    
-0000c9b0: 2020 2020 2020 2020 3e3e 3e20 6672 6f6d          >>> from
-0000c9c0: 206d 696e 6473 706f 7265 2069 6d70 6f72   mindspore impor
-0000c9d0: 7420 6474 7970 6520 6173 206d 7374 7970  t dtype as mstyp
-0000c9e0: 650a 2020 2020 2020 2020 2020 2020 3e3e  e.            >>
-0000c9f0: 3e20 6672 6f6d 206d 696e 6473 706f 7265  > from mindspore
-0000ca00: 2069 6d70 6f72 7420 5465 6e73 6f72 0a20   import Tensor. 
-0000ca10: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
-0000ca20: 6f64 656c 203d 204d 756c 7469 4865 6164  odel = MultiHead
-0000ca30: 4174 7465 6e74 696f 6e28 6261 7463 685f  Attention(batch_
-0000ca40: 7369 7a65 3d4e 6f6e 652c 2068 6964 6465  size=None, hidde
-0000ca50: 6e5f 7369 7a65 3d31 352c 2073 7263 5f73  n_size=15, src_s
-0000ca60: 6571 5f6c 656e 6774 683d 3230 2c20 7467  eq_length=20, tg
-0000ca70: 745f 7365 715f 6c65 6e67 7468 3d32 302c  t_seq_length=20,
-0000ca80: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
-0000ca90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000caa0: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
-0000cab0: 6865 6164 733d 3329 0a20 2020 2020 2020  heads=3).       
-0000cac0: 2020 2020 203e 3e3e 2066 726f 6d5f 7465       >>> from_te
-0000cad0: 6e73 6f72 203d 2054 656e 736f 7228 6e70  nsor = Tensor(np
-0000cae0: 2e6f 6e65 7328 2832 2c20 3230 2c20 3135  .ones((2, 20, 15
-0000caf0: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
-0000cb00: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
-0000cb10: 3e3e 3e20 746f 5f74 656e 736f 7220 3d20  >>> to_tensor = 
-0000cb20: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
-0000cb30: 322c 2032 302c 2031 3529 292c 206d 7374  2, 20, 15)), mst
-0000cb40: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
-0000cb50: 2020 2020 2020 2020 203e 3e3e 2061 7474           >>> att
-0000cb60: 656e 7469 6f6e 5f6d 6173 6b20 3d20 5465  ention_mask = Te
-0000cb70: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-0000cb80: 2032 302c 2032 3029 292c 206d 7374 7970   20, 20)), mstyp
-0000cb90: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-0000cba0: 2020 2020 2020 203e 3e3e 2061 7474 6e5f         >>> attn_
-0000cbb0: 6f75 742c 2070 6173 7420 3d20 6d6f 6465  out, past = mode
-0000cbc0: 6c28 6672 6f6d 5f74 656e 736f 722c 2074  l(from_tensor, t
-0000cbd0: 6f5f 7465 6e73 6f72 2c20 746f 5f74 656e  o_tensor, to_ten
-0000cbe0: 736f 722c 2061 7474 656e 7469 6f6e 5f6d  sor, attention_m
-0000cbf0: 6173 6b29 0a20 2020 2020 2020 2020 2020  ask).           
-0000cc00: 203e 3e3e 2070 7269 6e74 2861 7474 6e5f   >>> print(attn_
-0000cc10: 6f75 742e 7368 6170 6529 0a20 2020 2020  out.shape).     
-0000cc20: 2020 2020 2020 2028 322c 2032 302c 2031         (2, 20, 1
-0000cc30: 3529 0a20 2020 2020 2020 2020 2020 203e  5).            >
-0000cc40: 3e3e 2070 7269 6e74 2870 6173 745b 305d  >> print(past[0]
-0000cc50: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
-0000cc60: 2020 2020 2832 2c20 332c 2035 2c20 3230      (2, 3, 5, 20
-0000cc70: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-0000cc80: 3e20 7072 696e 7428 7061 7374 5b31 5d2e  > print(past[1].
-0000cc90: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-0000cca0: 2020 2028 322c 2033 2c20 3230 2c20 3529     (2, 3, 20, 5)
-0000ccb0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000ccc0: 2023 2057 6865 6e20 7573 6520 7573 655f   # When use use_
-0000ccd0: 7061 7374 3d54 7275 652c 2069 7420 696e  past=True, it in
-0000cce0: 636c 7564 6573 2074 776f 2073 7465 7073  cludes two steps
-0000ccf0: 2074 6f20 696d 706c 656d 656e 7420 7468   to implement th
-0000cd00: 6520 696e 6372 656d 656e 7461 6c20 7072  e incremental pr
-0000cd10: 6564 6963 7469 6f6e 2e0a 2020 2020 2020  ediction..      
-0000cd20: 2020 2020 2020 3e3e 3e20 2320 5374 6570        >>> # Step
-0000cd30: 2031 3a20 7365 7420 6973 5f66 6972 7374   1: set is_first
-0000cd40: 5f69 7465 7261 7469 6f6e 3d54 7275 652c  _iteration=True,
-0000cd50: 2061 6e64 2069 6e70 7574 2074 6865 2066   and input the f
-0000cd60: 756c 6c20 7365 7175 656e 6365 206c 656e  ull sequence len
-0000cd70: 6774 6827 7320 7374 6174 652e 0a20 2020  gth's state..   
-0000cd80: 2020 2020 2020 2020 203e 3e3e 2023 2057           >>> # W
-0000cd90: 6520 6e65 6564 2074 6f20 7072 6570 6172  e need to prepar
-0000cda0: 6520 7468 6520 6d65 6d6f 7279 2070 6172  e the memory par
-0000cdb0: 616d 6574 6572 7320 666f 7220 7361 7669  ameters for savi
-0000cdc0: 6e67 206b 6579 2061 6e64 2076 616c 7565  ng key and value
-0000cdd0: 2073 7461 7465 7320 6669 7273 746c 792e   states firstly.
-0000cde0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000cdf0: 206d 6f64 656c 203d 204d 756c 7469 4865   model = MultiHe
-0000ce00: 6164 4174 7465 6e74 696f 6e28 6261 7463  adAttention(batc
-0000ce10: 685f 7369 7a65 3d32 2c20 6869 6464 656e  h_size=2, hidden
-0000ce20: 5f73 697a 653d 3135 2c20 7372 635f 7365  _size=15, src_se
-0000ce30: 715f 6c65 6e67 7468 3d32 302c 2074 6774  q_length=20, tgt
-0000ce40: 5f73 6571 5f6c 656e 6774 683d 3230 2c0a  _seq_length=20,.
-0000ce50: 2020 2020 2020 2020 2020 2020 2e2e 2e20              ... 
-0000ce60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ce70: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
-0000ce80: 6561 6473 3d33 2c20 7573 655f 7061 7374  eads=3, use_past
-0000ce90: 3d54 7275 6529 0a20 2020 2020 2020 2020  =True).         
-0000cea0: 2020 203e 3e3e 206b 6579 5f70 6173 7420     >>> key_past 
-0000ceb0: 3d20 5465 6e73 6f72 286e 702e 7a65 726f  = Tensor(np.zero
-0000cec0: 7328 7368 6170 653d 2832 2c20 332c 2035  s(shape=(2, 3, 5
-0000ced0: 2c20 3230 2929 2c20 6d73 7479 7065 2e66  , 20)), mstype.f
-0000cee0: 6c6f 6174 3136 290a 2020 2020 2020 2020  loat16).        
-0000cef0: 2020 2020 3e3e 3e20 7661 6c75 655f 7061      >>> value_pa
-0000cf00: 7374 203d 2054 656e 736f 7228 6e70 2e7a  st = Tensor(np.z
-0000cf10: 6572 6f73 2873 6861 7065 3d28 322c 2033  eros(shape=(2, 3
-0000cf20: 2c20 3230 2c20 3529 292c 206d 7374 7970  , 20, 5)), mstyp
-0000cf30: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-0000cf40: 2020 2020 2020 203e 3e3e 2062 6174 6368         >>> batch
-0000cf50: 5f76 616c 6964 5f6c 656e 6774 6820 3d20  _valid_length = 
-0000cf60: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
-0000cf70: 322c 2929 2c20 6d73 7479 7065 2e69 6e74  2,)), mstype.int
-0000cf80: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
-0000cf90: 3e3e 3e20 2320 5365 7420 6973 5f66 6972  >>> # Set is_fir
-0000cfa0: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
-0000cfb0: 6520 746f 2067 656e 6572 6174 6520 7468  e to generate th
-0000cfc0: 6520 6675 6c6c 206d 656d 6f72 7920 7374  e full memory st
-0000cfd0: 6174 6573 0a20 2020 2020 2020 2020 2020  ates.           
-0000cfe0: 203e 3e3e 206d 6f64 656c 2e61 6464 5f66   >>> model.add_f
-0000cff0: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
-0000d000: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-0000d010: 6e3d 5472 7565 290a 2020 2020 2020 2020  n=True).        
-0000d020: 2020 2020 3e3e 3e20 6174 746e 5f6f 7574      >>> attn_out
-0000d030: 2c20 7061 7374 203d 206d 6f64 656c 2866  , past = model(f
-0000d040: 726f 6d5f 7465 6e73 6f72 2c20 746f 5f74  rom_tensor, to_t
-0000d050: 656e 736f 722c 2074 6f5f 7465 6e73 6f72  ensor, to_tensor
-0000d060: 2c20 6174 7465 6e74 696f 6e5f 6d61 736b  , attention_mask
-0000d070: 2c20 6b65 795f 7061 7374 2c20 7661 6c75  , key_past, valu
-0000d080: 655f 7061 7374 2c0a 2020 2020 2020 2020  e_past,.        
-0000d090: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
-0000d0a0: 2020 2020 2020 2020 2020 2020 2020 2062                 b
-0000d0b0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-0000d0c0: 6829 0a20 2020 2020 2020 2020 2020 203e  h).            >
-0000d0d0: 3e3e 2070 7269 6e74 2861 7474 6e5f 6f75  >> print(attn_ou
-0000d0e0: 742e 7368 6170 6529 0a20 2020 2020 2020  t.shape).       
-0000d0f0: 2020 2020 2028 322c 2032 302c 2031 3529       (2, 20, 15)
-0000d100: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000d110: 2070 7269 6e74 2870 6173 745b 305d 2e73   print(past[0].s
-0000d120: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
-0000d130: 2020 2832 2c20 332c 2035 2c20 3230 290a    (2, 3, 5, 20).
-0000d140: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-0000d150: 7072 696e 7428 7061 7374 5b31 5d2e 7368  print(past[1].sh
-0000d160: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-0000d170: 2028 322c 2033 2c20 3230 2c20 3529 0a20   (2, 3, 20, 5). 
-0000d180: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
-0000d190: 726f 6d5f 7465 6e73 6f72 203d 2054 656e  rom_tensor = Ten
-0000d1a0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-0000d1b0: 312c 2031 3529 292c 206d 7374 7970 652e  1, 15)), mstype.
-0000d1c0: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-0000d1d0: 2020 2020 203e 3e3e 2074 6f5f 7465 6e73       >>> to_tens
-0000d1e0: 6f72 203d 2054 656e 736f 7228 6e70 2e6f  or = Tensor(np.o
-0000d1f0: 6e65 7328 2832 2c20 312c 2031 3529 292c  nes((2, 1, 15)),
-0000d200: 206d 7374 7970 652e 666c 6f61 7431 3629   mstype.float16)
-0000d210: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000d220: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
-0000d230: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-0000d240: 2828 322c 2031 2c20 3230 2929 2c20 6d73  ((2, 1, 20)), ms
-0000d250: 7479 7065 2e66 6c6f 6174 3136 290a 2020  type.float16).  
-0000d260: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
-0000d270: 5374 6570 2032 3a20 7365 7420 6973 5f66  Step 2: set is_f
-0000d280: 6972 7374 5f69 7465 7261 7469 6f6e 3d46  irst_iteration=F
-0000d290: 616c 7365 2c20 616e 6420 7061 7373 2074  alse, and pass t
-0000d2a0: 6865 2073 696e 676c 6520 776f 7264 2074  he single word t
-0000d2b0: 6f20 7275 6e20 7468 6520 7072 6564 6963  o run the predic
-0000d2c0: 7469 6f6e 2072 6174 6865 7220 7468 616e  tion rather than
-0000d2d0: 2074 6865 0a20 2020 2020 2020 2020 2020   the.           
-0000d2e0: 203e 3e3e 2023 2066 756c 6c20 7365 7175   >>> # full sequ
-0000d2f0: 656e 6365 2e0a 2020 2020 2020 2020 2020  ence..          
-0000d300: 2020 3e3e 3e20 6d6f 6465 6c2e 6164 645f    >>> model.add_
-0000d310: 666c 6167 735f 7265 6375 7273 6976 6528  flags_recursive(
-0000d320: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-0000d330: 6f6e 3d46 616c 7365 290a 2020 2020 2020  on=False).      
-0000d340: 2020 2020 2020 3e3e 3e20 6174 746e 5f6f        >>> attn_o
-0000d350: 7574 2c20 7061 7374 203d 206d 6f64 656c  ut, past = model
-0000d360: 2866 726f 6d5f 7465 6e73 6f72 2c20 746f  (from_tensor, to
-0000d370: 5f74 656e 736f 722c 2074 6f5f 7465 6e73  _tensor, to_tens
-0000d380: 6f72 2c20 6174 7465 6e74 696f 6e5f 6d61  or, attention_ma
-0000d390: 736b 2c20 6b65 795f 7061 7374 2c20 7661  sk, key_past, va
-0000d3a0: 6c75 655f 7061 7374 2c0a 2020 2020 2020  lue_past,.      
-0000d3b0: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
-0000d3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d3d0: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-0000d3e0: 6774 6829 0a20 2020 2020 2020 2020 2020  gth).           
-0000d3f0: 203e 3e3e 2070 7269 6e74 2861 7474 6e5f   >>> print(attn_
-0000d400: 6f75 742e 7368 6170 6529 0a20 2020 2020  out.shape).     
-0000d410: 2020 2020 2020 2028 322c 2031 2c20 3135         (2, 1, 15
-0000d420: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-0000d430: 3e20 7072 696e 7428 7061 7374 5b30 5d2e  > print(past[0].
-0000d440: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-0000d450: 2020 2028 322c 2033 2c20 352c 2032 3029     (2, 3, 5, 20)
-0000d460: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000d470: 2070 7269 6e74 2870 6173 745b 315d 2e73   print(past[1].s
-0000d480: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
-0000d490: 2020 2832 2c20 332c 2032 302c 2035 290a    (2, 3, 20, 5).
-0000d4a0: 2020 2020 2222 220a 0a20 2020 2040 5f4c      """..    @_L
-0000d4b0: 6f67 4163 7469 6f6e 4f6e 6365 286d 5f6c  ogActionOnce(m_l
-0000d4c0: 6f67 6765 723d 6c6f 6767 6572 2c20 6b65  ogger=logger, ke
-0000d4d0: 793d 274d 756c 7469 4865 6164 4174 7465  y='MultiHeadAtte
-0000d4e0: 6e74 696f 6e27 2c0a 2020 2020 2020 2020  ntion',.        
-0000d4f0: 2020 2020 2020 2020 2020 2020 6e6f 5f77              no_w
-0000d500: 6172 6e69 6e67 3d5f 6765 745f 7061 7261  arning=_get_para
-0000d510: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
-0000d520: 5061 7261 6c6c 656c 4d6f 6465 2e53 5441  ParallelMode.STA
-0000d530: 4e44 5f41 4c4f 4e45 2c29 290a 2020 2020  ND_ALONE,)).    
-0000d540: 405f 6172 6773 5f74 7970 655f 7661 6c69  @_args_type_vali
-0000d550: 6461 746f 725f 6368 6563 6b28 6869 6464  dator_check(hidd
-0000d560: 656e 5f73 697a 653d 5661 6c69 6461 746f  en_size=Validato
-0000d570: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
-0000d580: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
-0000d590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d5a0: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-0000d5b0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-0000d5c0: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
-0000d5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d5e0: 2020 2020 2020 2020 2020 2020 2020 7372                sr
-0000d5f0: 635f 7365 715f 6c65 6e67 7468 3d56 616c  c_seq_length=Val
-0000d600: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-0000d610: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
-0000d620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d630: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
-0000d640: 6571 5f6c 656e 6774 683d 5661 6c69 6461  eq_length=Valida
-0000d650: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-0000d660: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
-0000d670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d680: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0000d690: 6e5f 6472 6f70 6f75 745f 7261 7465 3d56  n_dropout_rate=V
-0000d6a0: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
-0000d6b0: 6f6e 5f6e 6567 6174 6976 655f 666c 6f61  on_negative_floa
-0000d6c0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0000d6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d6e0: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
-0000d6f0: 745f 7261 7465 3d56 616c 6964 6174 6f72  t_rate=Validator
-0000d700: 2e63 6865 636b 5f6e 6f6e 5f6e 6567 6174  .check_non_negat
-0000d710: 6976 655f 666c 6f61 742c 0a20 2020 2020  ive_float,.     
-0000d720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d730: 2020 2020 2020 2020 2020 2063 6f6d 7075             compu
-0000d740: 7465 5f64 7479 7065 3d5f 7661 6c69 645f  te_dtype=_valid_
-0000d750: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
-0000d760: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
-0000d770: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-0000d780: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
-0000d790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000af70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000af80: 2020 2020 2020 6e61 6d65 3d27 656d 6265        name='embe
+0000af90: 6464 696e 675f 7461 626c 6527 2c20 7061  dding_table', pa
+0000afa0: 7261 6c6c 656c 5f6f 7074 696d 697a 6572  rallel_optimizer
+0000afb0: 3d46 616c 7365 290a 0a20 2020 2020 2020  =False)..       
+0000afc0: 2069 6620 7061 7261 6c6c 656c 5f63 6f6e   if parallel_con
+0000afd0: 6669 672e 766f 6361 625f 656d 625f 6470  fig.vocab_emb_dp
+0000afe0: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+0000aff0: 6c66 2e67 6174 6865 7220 3d20 502e 4761  lf.gather = P.Ga
+0000b000: 7468 6572 2829 2e73 6861 7264 2828 2831  ther().shard(((1
+0000b010: 2c20 3129 2c20 2870 6172 616c 6c65 6c5f  , 1), (parallel_
+0000b020: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+0000b030: 6c6c 656c 2c20 3129 2929 0a20 2020 2020  llel, 1))).     
+0000b040: 2020 2020 2020 206c 6f67 6765 722e 696e         logger.in
+0000b050: 666f 2866 2255 7369 6e67 207b 7061 7261  fo(f"Using {para
+0000b060: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+0000b070: 5f70 6172 616c 6c65 6c7d 2064 6174 6120  _parallel} data 
+0000b080: 7061 7261 6c6c 656c 2066 6f72 2074 6865  parallel for the
+0000b090: 2065 6d62 6564 6469 6e67 206c 6f6f 6b75   embedding looku
+0000b0a0: 702e 2229 0a20 2020 2020 2020 2065 6c73  p.").        els
+0000b0b0: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
+0000b0c0: 6620 7365 6c66 2e76 6f63 6162 5f73 697a  f self.vocab_siz
+0000b0d0: 6520 2520 7061 7261 6c6c 656c 5f63 6f6e  e % parallel_con
+0000b0e0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0000b0f0: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
+0000b100: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
+0000b110: 616c 7565 4572 726f 7228 6622 5468 6520  alueError(f"The 
+0000b120: 766f 6361 6220 7369 7a65 206f 6620 7468  vocab size of th
+0000b130: 6520 656d 6265 6464 696e 6720 7b73 656c  e embedding {sel
+0000b140: 662e 766f 6361 625f 7369 7a65 7d20 6d75  f.vocab_size} mu
+0000b150: 7374 2062 6520 6120 220a 2020 2020 2020  st be a ".      
+0000b160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b170: 2020 2020 2020 2020 2020 2066 226d 756c             f"mul
+0000b180: 7469 706c 6520 6f66 2070 6172 616c 6c65  tiple of paralle
+0000b190: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0000b1a0: 6172 616c 6c65 6c20 7b70 6172 616c 6c65  arallel {paralle
+0000b1b0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0000b1c0: 6172 616c 6c65 6c7d 2e22 290a 2020 2020  arallel}.").    
+0000b1d0: 2020 2020 2020 2020 7365 6c66 2e67 6174          self.gat
+0000b1e0: 6865 7220 3d20 502e 4761 7468 6572 2829  her = P.Gather()
+0000b1f0: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
+0000b200: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0000b210: 6172 616c 6c65 6c2c 2031 292c 2028 7061  arallel, 1), (pa
+0000b220: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+0000b230: 7461 5f70 6172 616c 6c65 6c2c 2031 2929  ta_parallel, 1))
+0000b240: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
+0000b250: 6767 6572 2e69 6e66 6f28 6622 5573 696e  gger.info(f"Usin
+0000b260: 6720 7b70 6172 616c 6c65 6c5f 636f 6e66  g {parallel_conf
+0000b270: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+0000b280: 7d20 6461 7461 2070 6172 616c 6c65 6c20  } data parallel 
+0000b290: 616e 6420 7b70 6172 616c 6c65 6c5f 636f  and {parallel_co
+0000b2a0: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+0000b2b0: 6c65 6c7d 2022 0a20 2020 2020 2020 2020  lel} ".         
+0000b2c0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+0000b2d0: 226d 6f64 656c 2070 6172 616c 6c65 6c20  "model parallel 
+0000b2e0: 666f 7220 7468 6520 656d 6265 6464 696e  for the embeddin
+0000b2f0: 6720 6c6f 6f6b 7570 2e22 290a 0a20 2020  g lookup.")..   
+0000b300: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
+0000b310: 656c 662c 2069 6e70 7574 5f69 6473 293a  elf, input_ids):
+0000b320: 0a20 2020 2020 2020 205f 6368 6563 6b5f  .        _check_
+0000b330: 696e 7075 745f 6474 7970 6528 462e 6474  input_dtype(F.dt
+0000b340: 7970 6528 696e 7075 745f 6964 7329 2c20  ype(input_ids), 
+0000b350: 2269 6e70 7574 5f69 6473 222c 205b 6d73  "input_ids", [ms
+0000b360: 7479 7065 2e69 6e74 3332 5d2c 2073 656c  type.int32], sel
+0000b370: 662e 636c 735f 6e61 6d65 290a 2020 2020  f.cls_name).    
+0000b380: 2020 2020 6f75 7470 7574 203d 2073 656c      output = sel
+0000b390: 662e 6761 7468 6572 2873 656c 662e 656d  f.gather(self.em
+0000b3a0: 6265 6464 696e 675f 7461 626c 652c 2069  bedding_table, i
+0000b3b0: 6e70 7574 5f69 6473 2c20 3029 0a20 2020  nput_ids, 0).   
+0000b3c0: 2020 2020 2072 6574 7572 6e20 6f75 7470       return outp
+0000b3d0: 7574 2c20 7365 6c66 2e65 6d62 6564 6469  ut, self.embeddi
+0000b3e0: 6e67 5f74 6162 6c65 2e76 616c 7565 2829  ng_table.value()
+0000b3f0: 0a0a 0a63 6c61 7373 204d 756c 7469 4865  ...class MultiHe
+0000b400: 6164 4174 7465 6e74 696f 6e28 4365 6c6c  adAttention(Cell
+0000b410: 293a 0a20 2020 2072 2222 220a 2020 2020  ):.    r""".    
+0000b420: 2020 2020 5468 6973 2069 7320 616e 2069      This is an i
+0000b430: 6d70 6c65 6d65 6e74 6174 696f 6e20 6f66  mplementation of
+0000b440: 206d 756c 7469 6865 6164 2061 7474 656e   multihead atten
+0000b450: 7469 6f6e 2069 6e20 7468 6520 7061 7065  tion in the pape
+0000b460: 7220 6041 7474 656e 7469 6f6e 2069 7320  r `Attention is 
+0000b470: 616c 6c20 796f 7520 6e65 6564 0a20 2020  all you need.   
+0000b480: 2020 2020 203c 6874 7470 733a 2f2f 6172       <https://ar
+0000b490: 7869 762e 6f72 672f 7064 662f 3137 3036  xiv.org/pdf/1706
+0000b4a0: 2e30 3337 3632 7635 2e70 6466 3e60 5f2e  .03762v5.pdf>`_.
+0000b4b0: 2047 6976 656e 2074 6865 2071 7565 7279   Given the query
+0000b4c0: 2076 6563 746f 7220 7769 7468 2073 6f75   vector with sou
+0000b4d0: 7263 6520 6c65 6e67 7468 2c20 616e 6420  rce length, and 
+0000b4e0: 7468 650a 2020 2020 2020 2020 6b65 7920  the.        key 
+0000b4f0: 616e 6420 7661 6c75 6520 7665 6374 6f72  and value vector
+0000b500: 2077 6974 6820 7461 7267 6574 206c 656e   with target len
+0000b510: 6774 682c 2074 6865 2061 7474 656e 7469  gth, the attenti
+0000b520: 6f6e 2077 696c 6c20 6265 2070 6572 666f  on will be perfo
+0000b530: 726d 6564 2061 7320 7468 6520 666f 6c6c  rmed as the foll
+0000b540: 6f77 696e 670a 0a20 2020 2020 2020 202e  owing..        .
+0000b550: 2e20 6d61 7468 3a3a 0a20 2020 2020 2020  . math::.       
+0000b560: 2020 2020 2020 2020 4d75 6c74 6948 6561          MultiHea
+0000b570: 6441 7474 656e 7469 6f6e 2871 7565 7279  dAttention(query
+0000b580: 2c20 6b65 792c 2076 6563 746f 7229 203d  , key, vector) =
+0000b590: 2043 6f6e 6361 7428 6865 6164 5f31 2c20   Concat(head_1, 
+0000b5a0: 5c64 6f74 732c 2068 6561 645f 6829 575e  \dots, head_h)W^
+0000b5b0: 4f0a 0a20 2020 2020 2020 2077 6865 7265  O..        where
+0000b5c0: 203a 6d61 7468 3a60 6865 6164 5f69 203d   :math:`head_i =
+0000b5d0: 2041 7474 656e 7469 6f6e 2851 575f 695e   Attention(QW_i^
+0000b5e0: 512c 204b 575f 695e 4b2c 2056 575f 695e  Q, KW_i^K, VW_i^
+0000b5f0: 5629 602e 2054 6865 2064 6566 6175 6c74  V)`. The default
+0000b600: 2069 7320 7769 7468 2061 2062 6961 732e   is with a bias.
+0000b610: 0a0a 2020 2020 2020 2020 6966 2071 7565  ..        if que
+0000b620: 7279 2c20 6b65 7920 616e 6420 7661 6c75  ry, key and valu
+0000b630: 6520 7465 6e73 6f72 2069 7320 7361 6d65  e tensor is same
+0000b640: 2c20 7468 656e 2069 7420 7769 6c6c 2062  , then it will b
+0000b650: 6520 7365 6c66 2061 7474 656e 7469 6f6e  e self attention
+0000b660: 2e0a 0a20 2020 2020 2020 2041 7267 733a  ...        Args:
+0000b670: 0a20 2020 2020 2020 2020 2020 2062 6174  .            bat
+0000b680: 6368 5f73 697a 6528 696e 7429 3a20 5468  ch_size(int): Th
+0000b690: 6520 6261 7463 6820 7369 7a65 206f 6620  e batch size of 
+0000b6a0: 7468 6520 696e 7075 7420 7465 6e73 6f72  the input tensor
+0000b6b0: 2077 6865 6e20 646f 2069 6e63 7265 6e6d   when do increnm
+0000b6c0: 656e 7461 6c20 7072 6564 6963 7469 6f6e  ental prediction
+0000b6d0: 2e20 5368 6f75 6c64 2062 6520 6120 706f  . Should be a po
+0000b6e0: 7369 7469 7665 0a20 2020 2020 2020 2020  sitive.         
+0000b6f0: 2020 2020 2020 2076 616c 7565 2e20 5768         value. Wh
+0000b700: 656e 2064 6f20 7472 6169 6e69 6e67 206f  en do training o
+0000b710: 7220 7072 6564 6963 7469 6f6e 2c20 7468  r prediction, th
+0000b720: 6520 6172 6775 6d65 6e74 2077 696c 6c20  e argument will 
+0000b730: 6e6f 7420 776f 726b 2061 6e64 2074 6865  not work and the
+0000b740: 2075 7365 7220 6361 6e20 6a75 7374 2070   user can just p
+0000b750: 6173 7320 4e6f 6e65 2074 6f0a 2020 2020  ass None to.    
+0000b760: 2020 2020 2020 2020 2020 2020 7468 6520              the 
+0000b770: 6172 6775 6d65 6e74 2e0a 2020 2020 2020  argument..      
+0000b780: 2020 2020 2020 7372 635f 7365 715f 6c65        src_seq_le
+0000b790: 6e67 7468 2869 6e74 293a 2054 6865 2073  ngth(int): The s
+0000b7a0: 6571 7565 6e63 6520 6c65 6e67 7468 206f  equence length o
+0000b7b0: 6620 7468 6520 7175 6572 7920 7665 6374  f the query vect
+0000b7c0: 6f72 2e0a 2020 2020 2020 2020 2020 2020  or..            
+0000b7d0: 7467 745f 7365 715f 6c65 6e67 7468 2869  tgt_seq_length(i
+0000b7e0: 6e74 293a 2054 6865 2073 6571 7565 6e63  nt): The sequenc
+0000b7f0: 6520 6c65 6e67 7468 206f 6620 7468 6520  e length of the 
+0000b800: 6b65 7920 616e 6420 7661 6c75 6520 7665  key and value ve
+0000b810: 6374 6f72 2e0a 2020 2020 2020 2020 2020  ctor..          
+0000b820: 2020 6869 6464 656e 5f73 697a 6528 696e    hidden_size(in
+0000b830: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+0000b840: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
+0000b850: 2e0a 2020 2020 2020 2020 2020 2020 6e75  ..            nu
+0000b860: 6d5f 6865 6164 7328 696e 7429 3a20 5468  m_heads(int): Th
+0000b870: 6520 6e75 6d62 6572 206f 6620 7468 6520  e number of the 
+0000b880: 6865 6164 732e 0a20 2020 2020 2020 2020  heads..         
+0000b890: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
+0000b8a0: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
+0000b8b0: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
+0000b8c0: 6f66 2074 6865 2066 696e 616c 206f 7574  of the final out
+0000b8d0: 7075 7420 6f66 2074 6865 206c 6179 6572  put of the layer
+0000b8e0: 2e20 4465 6661 756c 743a 302e 312e 0a20  . Default:0.1.. 
+0000b8f0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+0000b900: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+0000b910: 6528 666c 6f61 7429 3a20 5468 6520 6472  e(float): The dr
+0000b920: 6f70 6f75 7420 7261 7465 206f 6620 7468  opout rate of th
+0000b930: 6520 6174 7465 6e74 696f 6e20 7363 6f72  e attention scor
+0000b940: 6573 2e20 4465 6661 756c 743a 302e 312e  es. Default:0.1.
+0000b950: 0a20 2020 2020 2020 2020 2020 2063 6f6d  .            com
+0000b960: 7075 7465 5f64 7479 7065 2864 7479 7065  pute_dtype(dtype
+0000b970: 2e4e 756d 6265 7229 3a20 5468 6520 636f  .Number): The co
+0000b980: 6d70 7574 6174 696f 6e20 7479 7065 206f  mputation type o
+0000b990: 6620 6465 6e73 652e 2044 6566 6175 6c74  f dense. Default
+0000b9a0: 206d 7374 7970 652e 666c 6f61 7431 362e   mstype.float16.
+0000b9b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000b9c0: 2053 686f 756c 6420 6265 206d 7374 7970   Should be mstyp
+0000b9d0: 652e 666c 6f61 7433 3220 6f72 206d 7374  e.float32 or mst
+0000b9e0: 7970 652e 666c 6f61 7431 362e 0a20 2020  ype.float16..   
+0000b9f0: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+0000ba00: 5f63 6f6d 7075 7465 5f74 7970 6528 6474  _compute_type(dt
+0000ba10: 7970 652e 4e75 6d62 6572 293a 2054 6865  ype.Number): The
+0000ba20: 2074 7970 6520 6f66 2073 6f66 746d 6178   type of softmax
+0000ba30: 2063 6f6d 7075 7461 7469 6f6e 206d 6f64   computation mod
+0000ba40: 756c 652e 2044 6566 6175 6c74 206d 7374  ule. Default mst
+0000ba50: 7970 652e 666c 6f61 7433 322e 0a20 2020  ype.float32..   
+0000ba60: 2020 2020 2020 2020 2020 2020 2053 686f               Sho
+0000ba70: 756c 6420 6265 206d 7374 7970 652e 666c  uld be mstype.fl
+0000ba80: 6f61 7433 3220 6f72 206d 7374 7970 652e  oat32 or mstype.
+0000ba90: 666c 6f61 7431 362e 0a20 2020 2020 2020  float16..       
+0000baa0: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+0000bab0: 7479 7065 2864 7479 7065 2e4e 756d 6265  type(dtype.Numbe
+0000bac0: 7229 3a20 5468 6520 7061 7261 6d65 7465  r): The paramete
+0000bad0: 7220 696e 6974 6961 6c69 7a61 7469 6f6e  r initialization
+0000bae0: 2074 7970 6520 6f66 2074 6865 206d 6f64   type of the mod
+0000baf0: 756c 652e 2044 6566 6175 6c74 206d 7374  ule. Default mst
+0000bb00: 7970 652e 666c 6f61 7433 322e 0a20 2020  ype.float32..   
+0000bb10: 2020 2020 2020 2020 2020 2020 2053 686f               Sho
+0000bb20: 756c 6420 6265 206d 7374 7970 652e 666c  uld be mstype.fl
+0000bb30: 6f61 7433 3220 6f72 206d 7374 7970 652e  oat32 or mstype.
+0000bb40: 666c 6f61 7431 362e 0a20 2020 2020 2020  float16..       
+0000bb50: 2020 2020 2075 7365 5f70 6173 7428 626f       use_past(bo
+0000bb60: 6f6c 293a 2055 7365 2074 6865 2070 6173  ol): Use the pas
+0000bb70: 7420 7374 6174 6520 746f 2063 6f6d 7075  t state to compu
+0000bb80: 7465 2c20 7573 6564 2066 6f72 2069 6e63  te, used for inc
+0000bb90: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
+0000bba0: 696f 6e2e 2046 6f72 2065 7861 6d70 6c65  ion. For example
+0000bbb0: 2c20 6966 2077 6520 6861 7665 2074 776f  , if we have two
+0000bbc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000bbd0: 2077 6f72 6473 2061 6e64 2077 616e 7420   words and want 
+0000bbe0: 746f 2067 656e 6572 6174 6520 7468 6520  to generate the 
+0000bbf0: 7465 6e20 6d6f 7265 2077 6f72 6473 2e20  ten more words. 
+0000bc00: 5765 206a 7573 7420 6e65 6564 2074 6f20  We just need to 
+0000bc10: 636f 6d70 7574 6520 7468 6520 7477 6f20  compute the two 
+0000bc20: 776f 7264 7327 2073 7461 7465 206f 6e6c  words' state onl
+0000bc30: 7920 6f6e 6365 2c0a 2020 2020 2020 2020  y once,.        
+0000bc40: 2020 2020 2020 2020 616e 6420 6765 6e65          and gene
+0000bc50: 7261 7465 2074 6865 206e 6578 7420 776f  rate the next wo
+0000bc60: 7264 206f 6e65 2062 7920 6f6e 652e 2057  rd one by one. W
+0000bc70: 6865 6e20 7573 655f 7061 7374 2069 7320  hen use_past is 
+0000bc80: 5472 7565 2c20 7468 6572 6520 6172 6520  True, there are 
+0000bc90: 7477 6f20 7374 6570 7320 746f 2072 756e  two steps to run
+0000bca0: 2074 6865 2070 7265 6469 6374 696f 6e2e   the prediction.
+0000bcb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000bcc0: 2049 6e20 7468 6520 6669 7273 7420 7374   In the first st
+0000bcd0: 6570 2c20 7365 7420 7468 6520 6973 5f66  ep, set the is_f
+0000bce0: 6972 7374 5f69 7465 7261 7469 6f6e 2074  irst_iteration t
+0000bcf0: 6f20 6265 2054 7275 6520 6279 0a20 2020  o be True by.   
+0000bd00: 2020 2020 2020 2020 2020 2020 2060 6d6f               `mo
+0000bd10: 6465 6c2e 6164 645f 666c 6167 735f 7265  del.add_flags_re
+0000bd20: 6375 7273 6976 6528 6973 5f66 6972 7374  cursive(is_first
+0000bd30: 5f69 7465 7261 7469 6f6e 3d54 7275 6529  _iteration=True)
+0000bd40: 602c 2061 6e64 2070 6173 7320 7468 6520  `, and pass the 
+0000bd50: 6675 6c6c 2069 6e70 7574 732e 2054 6865  full inputs. The
+0000bd60: 6e2c 2073 6574 2074 6865 0a20 2020 2020  n, set the.     
+0000bd70: 2020 2020 2020 2020 2020 2069 735f 6669             is_fi
+0000bd80: 7273 745f 6974 6572 6174 696f 6e20 746f  rst_iteration to
+0000bd90: 2062 6520 4661 6c73 6520 6279 2060 6d6f   be False by `mo
+0000bda0: 6465 6c2e 6164 645f 666c 6167 735f 7265  del.add_flags_re
+0000bdb0: 6375 7273 6976 6528 6973 5f66 6972 7374  cursive(is_first
+0000bdc0: 5f69 7465 7261 7469 6f6e 3d46 616c 7365  _iteration=False
+0000bdd0: 2960 2e20 4174 2074 6869 7320 6d6f 6d65  )`. At this mome
+0000bde0: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
+0000bdf0: 2020 2020 7061 7373 2074 6865 2073 696e      pass the sin
+0000be00: 676c 6520 7374 6570 2773 2069 6e70 7574  gle step's input
+0000be10: 2074 656e 736f 722c 2061 6e64 206c 6f6f   tensor, and loo
+0000be20: 7020 6974 2e20 4465 6661 756c 7420 4661  p it. Default Fa
+0000be30: 6c73 652e 0a20 2020 2020 2020 2020 2020  lse..           
+0000be40: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0000be50: 284f 7050 6172 616c 6c65 6c43 6f6e 6669  (OpParallelConfi
+0000be60: 6729 3a20 5468 6520 7061 7261 6c6c 656c  g): The parallel
+0000be70: 2063 6f6e 6669 6775 7265 2e20 4465 6661   configure. Defa
+0000be80: 756c 7420 6064 6566 6175 6c74 5f64 706d  ult `default_dpm
+0000be90: 705f 636f 6e66 6967 602c 0a20 2020 2020  p_config`,.     
+0000bea0: 2020 2020 2020 2020 2020 2061 6e20 696e             an in
+0000beb0: 7374 616e 6365 206f 6620 604f 7050 6172  stance of `OpPar
+0000bec0: 616c 6c65 6c43 6f6e 6669 6760 2077 6974  allelConfig` wit
+0000bed0: 6820 6465 6661 756c 7420 6172 6773 2e0a  h default args..
+0000bee0: 0a20 2020 2020 2020 2049 6e70 7574 733a  .        Inputs:
+0000bef0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+0000bf00: 2a71 7565 7279 5f74 656e 736f 722a 2a20  *query_tensor** 
+0000bf10: 2854 656e 736f 7229 202d 2054 6865 2071  (Tensor) - The q
+0000bf20: 7565 7279 2076 6563 746f 7220 7769 7468  uery vector with
+0000bf30: 2073 6861 7065 2028 6261 7463 685f 7369   shape (batch_si
+0000bf40: 7a65 2c20 7372 635f 7365 715f 6c65 6e67  ze, src_seq_leng
+0000bf50: 7468 2c20 6869 6464 656e 5f73 697a 6529  th, hidden_size)
+0000bf60: 206f 720a 2020 2020 2020 2020 2020 2020   or.            
+0000bf70: 2020 2862 6174 6368 5f73 697a 6520 2a20    (batch_size * 
+0000bf80: 7372 635f 7365 715f 6c65 6e67 7468 2c20  src_seq_length, 
+0000bf90: 6869 6464 656e 5f73 697a 6529 2c20 6966  hidden_size), if
+0000bfa0: 2074 6865 2075 7365 5f70 6173 7420 6973   the use_past is
+0000bfb0: 2046 616c 7365 206f 7220 6973 5f66 6972   False or is_fir
+0000bfc0: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
+0000bfd0: 652e 0a20 2020 2020 2020 2020 2020 2020  e..             
+0000bfe0: 204f 7468 6572 7769 7365 2c20 6d75 7374   Otherwise, must
+0000bff0: 2062 6520 2862 6174 6368 5f73 697a 652c   be (batch_size,
+0000c000: 2031 2c20 6869 6464 656e 5f73 697a 6529   1, hidden_size)
+0000c010: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+0000c020: 2a6b 6579 5f74 656e 736f 722a 2a20 2854  *key_tensor** (T
+0000c030: 656e 736f 7229 202d 2054 6865 206b 6579  ensor) - The key
+0000c040: 2076 6563 746f 7220 7769 7468 2073 6861   vector with sha
+0000c050: 7065 2028 6261 7463 685f 7369 7a65 2c20  pe (batch_size, 
+0000c060: 7467 745f 7365 715f 6c65 6e67 7468 2c20  tgt_seq_length, 
+0000c070: 6869 6464 656e 5f73 697a 6529 206f 720a  hidden_size) or.
+0000c080: 2020 2020 2020 2020 2020 2020 2020 2862                (b
+0000c090: 6174 6368 5f73 697a 6520 2a20 7467 745f  atch_size * tgt_
+0000c0a0: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+0000c0b0: 656e 5f73 697a 6529 2c20 6966 2074 6865  en_size), if the
+0000c0c0: 2075 7365 5f70 6173 7420 6973 2046 616c   use_past is Fal
+0000c0d0: 7365 206f 7220 6973 5f66 6972 7374 5f69  se or is_first_i
+0000c0e0: 7465 7261 7469 6f6e 3d54 7275 652e 0a20  teration=True.. 
+0000c0f0: 2020 2020 2020 2020 2020 2020 204f 7468               Oth
+0000c100: 6572 7769 7365 2c20 6d75 7374 2062 6520  erwise, must be 
+0000c110: 2862 6174 6368 5f73 697a 652c 2031 2c20  (batch_size, 1, 
+0000c120: 6869 6464 656e 5f73 697a 6529 0a20 2020  hidden_size).   
+0000c130: 2020 2020 2020 2020 202d 202a 2a76 616c           - **val
+0000c140: 7565 5f74 656e 736f 722a 2a20 2854 656e  ue_tensor** (Ten
+0000c150: 736f 7229 202d 2054 6865 2076 616c 7565  sor) - The value
+0000c160: 2076 6563 746f 7220 7769 7468 2073 6861   vector with sha
+0000c170: 7065 2028 6261 7463 685f 7369 7a65 2c20  pe (batch_size, 
+0000c180: 7467 745f 7365 715f 6c65 6e67 7468 2c20  tgt_seq_length, 
+0000c190: 6869 6464 656e 5f73 697a 6529 206f 720a  hidden_size) or.
+0000c1a0: 2020 2020 2020 2020 2020 2020 2020 2862                (b
+0000c1b0: 6174 6368 5f73 697a 6520 2a20 7467 745f  atch_size * tgt_
+0000c1c0: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+0000c1d0: 656e 5f73 697a 6529 2c20 6966 2074 6865  en_size), if the
+0000c1e0: 2075 7365 5f70 6173 7420 6973 2046 616c   use_past is Fal
+0000c1f0: 7365 206f 7220 6973 5f66 6972 7374 5f69  se or is_first_i
+0000c200: 7465 7261 7469 6f6e 3d54 7275 652e 0a20  teration=True.. 
+0000c210: 2020 2020 2020 2020 2020 2020 204f 7468               Oth
+0000c220: 6572 7769 7365 2c20 6d75 7374 2062 6520  erwise, must be 
+0000c230: 2862 6174 6368 5f73 697a 652c 2031 2c20  (batch_size, 1, 
+0000c240: 6869 6464 656e 5f73 697a 6529 0a20 2020  hidden_size).   
+0000c250: 2020 2020 2020 2020 202d 202a 2a61 7474           - **att
+0000c260: 656e 7469 6f6e 5f6d 6173 6b2a 2a20 2854  ention_mask** (T
+0000c270: 656e 736f 7229 202d 2049 6620 7468 6520  ensor) - If the 
+0000c280: 7573 655f 7061 7374 2069 7320 4661 6c73  use_past is Fals
+0000c290: 6520 6f72 2069 735f 6669 7273 745f 6974  e or is_first_it
+0000c2a0: 6572 6174 696f 6e3d 5472 7565 2c20 7468  eration=True, th
+0000c2b0: 6520 6174 7465 6e74 696f 6e20 6d61 736b  e attention mask
+0000c2c0: 0a20 2020 2020 2020 2020 2020 2020 206d  .              m
+0000c2d0: 6174 7269 7820 7368 6f75 6c64 2062 6120  atrix should ba 
+0000c2e0: 2862 6174 6368 5f73 697a 652c 2073 7263  (batch_size, src
+0000c2f0: 5f73 6571 5f6c 656e 6774 682c 2074 6774  _seq_length, tgt
+0000c300: 5f73 6571 5f6c 656e 6774 6829 2c20 6f72  _seq_length), or
+0000c310: 204e 6f6e 652e 204e 6f6e 6520 6d65 616e   None. None mean
+0000c320: 7320 7468 6572 6520 7769 6c6c 2062 6520  s there will be 
+0000c330: 6e6f 206d 6173 6b0a 2020 2020 2020 2020  no mask.        
+0000c340: 2020 2020 2020 696e 2073 6f66 746d 6178        in softmax
+0000c350: 2063 6f6d 7075 7461 7469 6f6e 2e20 4f74   computation. Ot
+0000c360: 6865 7277 6973 652c 2074 6865 206d 6173  herwise, the mas
+0000c370: 6b20 6d75 7374 2062 6520 2862 6174 6368  k must be (batch
+0000c380: 5f73 697a 652c 2031 2c20 7467 745f 7365  _size, 1, tgt_se
+0000c390: 715f 6c65 6e67 7468 290a 2020 2020 2020  q_length).      
+0000c3a0: 2020 2020 2020 2d20 2a2a 6b65 795f 7061        - **key_pa
+0000c3b0: 7374 2a2a 2028 5465 6e73 6f72 2920 2d20  st** (Tensor) - 
+0000c3c0: 466c 6f61 7431 3620 7465 6e73 6f72 2077  Float16 tensor w
+0000c3d0: 6974 6820 7368 6170 6520 2862 6174 6368  ith shape (batch
+0000c3e0: 5f73 697a 652c 206e 756d 5f68 6561 6473  _size, num_heads
+0000c3f0: 2c20 7369 7a65 5f70 6572 5f68 6561 642c  , size_per_head,
+0000c400: 2074 6774 5f73 6571 5f6c 656e 6774 6829   tgt_seq_length)
+0000c410: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0000c420: 5468 6520 7061 7374 2063 616c 6375 6c61  The past calcula
+0000c430: 7465 6420 6b65 7920 7665 6374 6f72 2e20  ted key vector. 
+0000c440: 5573 6564 2066 6f72 2069 6e63 7265 6d65  Used for increme
+0000c450: 6e74 616c 2070 7265 6469 6374 696f 6e20  ntal prediction 
+0000c460: 7768 656e 2074 6865 2075 7365 5f70 6173  when the use_pas
+0000c470: 7420 6973 2054 7275 652e 0a20 2020 2020  t is True..     
+0000c480: 2020 2020 2020 2020 2044 6566 6175 6c74           Default
+0000c490: 204e 6f6e 652e 0a20 2020 2020 2020 2020   None..         
+0000c4a0: 2020 202d 202a 2a76 616c 7565 5f70 6173     - **value_pas
+0000c4b0: 742a 2a20 2854 656e 736f 7229 202d 2046  t** (Tensor) - F
+0000c4c0: 6c6f 6174 3136 2074 656e 736f 7220 7769  loat16 tensor wi
+0000c4d0: 7468 2073 6861 7065 0a20 2020 2020 2020  th shape.       
+0000c4e0: 2020 2020 2020 2028 6261 7463 685f 7369         (batch_si
+0000c4f0: 7a65 2c20 6e75 6d5f 6865 6164 732c 2074  ze, num_heads, t
+0000c500: 6774 5f73 6571 5f6c 656e 6774 682c 2073  gt_seq_length, s
+0000c510: 697a 655f 7065 725f 6865 6164 292e 0a20  ize_per_head).. 
+0000c520: 2020 2020 2020 2020 2020 2020 2054 6865               The
+0000c530: 2070 6173 7420 6361 6c63 756c 6174 6564   past calculated
+0000c540: 2076 616c 7565 2076 6563 746f 722e 2055   value vector. U
+0000c550: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
+0000c560: 7461 6c20 7072 6564 6963 7469 6f6e 2077  tal prediction w
+0000c570: 6865 6e20 7468 6520 7573 655f 7061 7374  hen the use_past
+0000c580: 2069 7320 5472 7565 2e0a 2020 2020 2020   is True..      
+0000c590: 2020 2020 2020 2020 4465 6661 756c 7420          Default 
+0000c5a0: 4e6f 6e65 2e0a 2020 2020 2020 2020 2020  None..          
+0000c5b0: 2020 2d20 2a2a 6261 7463 685f 7661 6c69    - **batch_vali
+0000c5c0: 645f 6c65 6e67 7468 2a2a 2028 5465 6e73  d_length** (Tens
+0000c5d0: 6f72 2920 2d20 496e 7433 3220 7465 6e73  or) - Int32 tens
+0000c5e0: 6f72 2077 6974 6820 7368 6170 6520 2862  or with shape (b
+0000c5f0: 6174 6368 5f73 697a 652c 2920 7468 6520  atch_size,) the 
+0000c600: 7061 7374 2063 616c 6375 6c61 7465 6420  past calculated 
+0000c610: 7468 6520 696e 6465 782e 0a20 2020 2020  the index..     
+0000c620: 2020 2020 2020 2020 2055 7365 6420 666f           Used fo
+0000c630: 7220 696e 6372 656d 656e 7461 6c20 7072  r incremental pr
+0000c640: 6564 6963 7469 6f6e 2077 6865 6e20 7468  ediction when th
+0000c650: 6520 7573 655f 7061 7374 2069 7320 5472  e use_past is Tr
+0000c660: 7565 2e20 4465 6661 756c 7420 4e6f 6e65  ue. Default None
+0000c670: 2e0a 0a20 2020 2020 2020 204f 7574 7075  ...        Outpu
+0000c680: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
+0000c690: 5475 706c 652c 2061 2074 7570 6c65 2063  Tuple, a tuple c
+0000c6a0: 6f6e 7461 696e 7328 606f 7574 7075 7460  ontains(`output`
+0000c6b0: 2c20 606c 6179 6572 5f70 7265 7365 6e74  , `layer_present
+0000c6c0: 6029 0a0a 2020 2020 2020 2020 2020 2020  `)..            
+0000c6d0: 2d20 2a2a 6f75 7470 7574 2a2a 2028 5465  - **output** (Te
+0000c6e0: 6e73 6f72 2920 2d20 5465 6e73 6f72 2c20  nsor) - Tensor, 
+0000c6f0: 7468 6520 666c 6f61 7420 7465 6e73 6f72  the float tensor
+0000c700: 206f 6620 7468 6520 6f75 7470 7574 206f   of the output o
+0000c710: 6620 7468 6520 6c61 7965 7220 7769 7468  f the layer with
+0000c720: 0a20 2020 2020 2020 2020 2020 2020 2073  .              s
+0000c730: 6861 7065 2028 6261 7463 685f 7369 7a65  hape (batch_size
+0000c740: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
+0000c750: 2c20 6869 6464 656e 5f73 697a 6529 206f  , hidden_size) o
+0000c760: 7220 2862 6174 6368 5f73 697a 6520 2a20  r (batch_size * 
+0000c770: 7372 635f 7365 715f 6c65 6e67 7468 2c20  src_seq_length, 
+0000c780: 6869 6464 656e 5f73 697a 6529 2c0a 2020  hidden_size),.  
+0000c790: 2020 2020 2020 2020 2020 2020 6966 2074              if t
+0000c7a0: 6865 2075 7365 5f70 6173 7420 6973 2046  he use_past is F
+0000c7b0: 616c 7365 206f 7220 6973 5f66 6972 7374  alse or is_first
+0000c7c0: 5f69 7465 7261 7469 6f6e 3d54 7275 652e  _iteration=True.
+0000c7d0: 204f 7468 6572 7769 7365 2c20 6974 2077   Otherwise, it w
+0000c7e0: 696c 6c20 6265 2028 6261 7463 685f 7369  ill be (batch_si
+0000c7f0: 7a65 2c20 312c 2068 6964 6465 6e5f 7369  ze, 1, hidden_si
+0000c800: 7a65 292e 0a0a 2020 2020 2020 2020 2020  ze)...          
+0000c810: 2020 2d20 2a2a 6c61 7965 725f 7072 6573    - **layer_pres
+0000c820: 656e 742a 2a20 2854 7570 6c65 2920 2d20  ent** (Tuple) - 
+0000c830: 4120 7475 706c 6520 6f66 2074 6865 2054  A tuple of the T
+0000c840: 656e 736f 7220 6f66 2074 6865 2070 726f  ensor of the pro
+0000c850: 6a65 6374 6564 206b 6579 2061 6e64 2076  jected key and v
+0000c860: 616c 7565 2076 6563 746f 7220 7769 7468  alue vector with
+0000c870: 0a20 2020 2020 2020 2020 2020 2020 2028  .              (
+0000c880: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
+0000c890: 5f68 6561 6473 2c20 7369 7a65 5f70 6572  _heads, size_per
+0000c8a0: 5f68 6561 642c 2074 6774 5f73 6571 5f6c  _head, tgt_seq_l
+0000c8b0: 656e 6774 6829 2c0a 2020 2020 2020 2020  ength),.        
+0000c8c0: 2020 2020 2020 2862 6174 6368 5f73 697a        (batch_siz
+0000c8d0: 652c 206e 756d 5f68 6561 6473 2c20 7467  e, num_heads, tg
+0000c8e0: 745f 7365 715f 6c65 6e67 7468 2c20 7369  t_seq_length, si
+0000c8f0: 7a65 5f70 6572 5f68 6561 6429 292e 0a0a  ze_per_head))...
+0000c900: 2020 2020 2020 2020 5375 7070 6f72 7465          Supporte
+0000c910: 6420 506c 6174 666f 726d 733a 0a20 2020  d Platforms:.   
+0000c920: 2020 2020 2020 2020 2060 6041 7363 656e           ``Ascen
+0000c930: 6460 6020 6060 4750 5560 600a 0a20 2020  d`` ``GPU``..   
+0000c940: 2020 2020 2045 7861 6d70 6c65 733a 0a20       Examples:. 
+0000c950: 2020 2020 2020 2020 2020 203e 3e3e 2069             >>> i
+0000c960: 6d70 6f72 7420 6e75 6d70 7920 6173 206e  mport numpy as n
+0000c970: 700a 2020 2020 2020 2020 2020 2020 3e3e  p.            >>
+0000c980: 3e20 6672 6f6d 206d 696e 6466 6f72 6d65  > from mindforme
+0000c990: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+0000c9a0: 666f 726d 6572 2069 6d70 6f72 7420 4d75  former import Mu
+0000c9b0: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000c9c0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000c9d0: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
+0000c9e0: 696d 706f 7274 2064 7479 7065 2061 7320  import dtype as 
+0000c9f0: 6d73 7479 7065 0a20 2020 2020 2020 2020  mstype.         
+0000ca00: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
+0000ca10: 7370 6f72 6520 696d 706f 7274 2054 656e  spore import Ten
+0000ca20: 736f 720a 2020 2020 2020 2020 2020 2020  sor.            
+0000ca30: 3e3e 3e20 6d6f 6465 6c20 3d20 4d75 6c74  >>> model = Mult
+0000ca40: 6948 6561 6441 7474 656e 7469 6f6e 2862  iHeadAttention(b
+0000ca50: 6174 6368 5f73 697a 653d 4e6f 6e65 2c20  atch_size=None, 
+0000ca60: 6869 6464 656e 5f73 697a 653d 3135 2c20  hidden_size=15, 
+0000ca70: 7372 635f 7365 715f 6c65 6e67 7468 3d32  src_seq_length=2
+0000ca80: 302c 2074 6774 5f73 6571 5f6c 656e 6774  0, tgt_seq_lengt
+0000ca90: 683d 3230 2c0a 2020 2020 2020 2020 2020  h=20,.          
+0000caa0: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
+0000cab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000cac0: 206e 756d 5f68 6561 6473 3d33 290a 2020   num_heads=3).  
+0000cad0: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
+0000cae0: 6f6d 5f74 656e 736f 7220 3d20 5465 6e73  om_tensor = Tens
+0000caf0: 6f72 286e 702e 6f6e 6573 2828 322c 2032  or(np.ones((2, 2
+0000cb00: 302c 2031 3529 292c 206d 7374 7970 652e  0, 15)), mstype.
+0000cb10: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
+0000cb20: 2020 2020 203e 3e3e 2074 6f5f 7465 6e73       >>> to_tens
+0000cb30: 6f72 203d 2054 656e 736f 7228 6e70 2e6f  or = Tensor(np.o
+0000cb40: 6e65 7328 2832 2c20 3230 2c20 3135 2929  nes((2, 20, 15))
+0000cb50: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+0000cb60: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+0000cb70: 3e20 6174 7465 6e74 696f 6e5f 6d61 736b  > attention_mask
+0000cb80: 203d 2054 656e 736f 7228 6e70 2e6f 6e65   = Tensor(np.one
+0000cb90: 7328 2832 2c20 3230 2c20 3230 2929 2c20  s((2, 20, 20)), 
+0000cba0: 6d73 7479 7065 2e66 6c6f 6174 3136 290a  mstype.float16).
+0000cbb0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000cbc0: 6174 746e 5f6f 7574 2c20 7061 7374 203d  attn_out, past =
+0000cbd0: 206d 6f64 656c 2866 726f 6d5f 7465 6e73   model(from_tens
+0000cbe0: 6f72 2c20 746f 5f74 656e 736f 722c 2074  or, to_tensor, t
+0000cbf0: 6f5f 7465 6e73 6f72 2c20 6174 7465 6e74  o_tensor, attent
+0000cc00: 696f 6e5f 6d61 736b 290a 2020 2020 2020  ion_mask).      
+0000cc10: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+0000cc20: 6174 746e 5f6f 7574 2e73 6861 7065 290a  attn_out.shape).
+0000cc30: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
+0000cc40: 3230 2c20 3135 290a 2020 2020 2020 2020  20, 15).        
+0000cc50: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
+0000cc60: 7374 5b30 5d2e 7368 6170 6529 0a20 2020  st[0].shape).   
+0000cc70: 2020 2020 2020 2020 2028 322c 2033 2c20           (2, 3, 
+0000cc80: 352c 2032 3029 0a20 2020 2020 2020 2020  5, 20).         
+0000cc90: 2020 203e 3e3e 2070 7269 6e74 2870 6173     >>> print(pas
+0000cca0: 745b 315d 2e73 6861 7065 290a 2020 2020  t[1].shape).    
+0000ccb0: 2020 2020 2020 2020 2832 2c20 332c 2032          (2, 3, 2
+0000ccc0: 302c 2035 290a 2020 2020 2020 2020 2020  0, 5).          
+0000ccd0: 2020 3e3e 3e20 2320 5768 656e 2075 7365    >>> # When use
+0000cce0: 2075 7365 5f70 6173 743d 5472 7565 2c20   use_past=True, 
+0000ccf0: 6974 2069 6e63 6c75 6465 7320 7477 6f20  it includes two 
+0000cd00: 7374 6570 7320 746f 2069 6d70 6c65 6d65  steps to impleme
+0000cd10: 6e74 2074 6865 2069 6e63 7265 6d65 6e74  nt the increment
+0000cd20: 616c 2070 7265 6469 6374 696f 6e2e 0a20  al prediction.. 
+0000cd30: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
+0000cd40: 2053 7465 7020 313a 2073 6574 2069 735f   Step 1: set is_
+0000cd50: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
+0000cd60: 5472 7565 2c20 616e 6420 696e 7075 7420  True, and input 
+0000cd70: 7468 6520 6675 6c6c 2073 6571 7565 6e63  the full sequenc
+0000cd80: 6520 6c65 6e67 7468 2773 2073 7461 7465  e length's state
+0000cd90: 2e0a 2020 2020 2020 2020 2020 2020 3e3e  ..            >>
+0000cda0: 3e20 2320 5765 206e 6565 6420 746f 2070  > # We need to p
+0000cdb0: 7265 7061 7265 2074 6865 206d 656d 6f72  repare the memor
+0000cdc0: 7920 7061 7261 6d65 7465 7273 2066 6f72  y parameters for
+0000cdd0: 2073 6176 696e 6720 6b65 7920 616e 6420   saving key and 
+0000cde0: 7661 6c75 6520 7374 6174 6573 2066 6972  value states fir
+0000cdf0: 7374 6c79 2e0a 2020 2020 2020 2020 2020  stly..          
+0000ce00: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 4d75    >>> model = Mu
+0000ce10: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000ce20: 2862 6174 6368 5f73 697a 653d 322c 2068  (batch_size=2, h
+0000ce30: 6964 6465 6e5f 7369 7a65 3d31 352c 2073  idden_size=15, s
+0000ce40: 7263 5f73 6571 5f6c 656e 6774 683d 3230  rc_seq_length=20
+0000ce50: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
+0000ce60: 3d32 302c 0a20 2020 2020 2020 2020 2020  =20,.           
+0000ce70: 202e 2e2e 2020 2020 2020 2020 2020 2020   ...            
+0000ce80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ce90: 6e75 6d5f 6865 6164 733d 332c 2075 7365  num_heads=3, use
+0000cea0: 5f70 6173 743d 5472 7565 290a 2020 2020  _past=True).    
+0000ceb0: 2020 2020 2020 2020 3e3e 3e20 6b65 795f          >>> key_
+0000cec0: 7061 7374 203d 2054 656e 736f 7228 6e70  past = Tensor(np
+0000ced0: 2e7a 6572 6f73 2873 6861 7065 3d28 322c  .zeros(shape=(2,
+0000cee0: 2033 2c20 352c 2032 3029 292c 206d 7374   3, 5, 20)), mst
+0000cef0: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
+0000cf00: 2020 2020 2020 2020 203e 3e3e 2076 616c           >>> val
+0000cf10: 7565 5f70 6173 7420 3d20 5465 6e73 6f72  ue_past = Tensor
+0000cf20: 286e 702e 7a65 726f 7328 7368 6170 653d  (np.zeros(shape=
+0000cf30: 2832 2c20 332c 2032 302c 2035 2929 2c20  (2, 3, 20, 5)), 
+0000cf40: 6d73 7479 7065 2e66 6c6f 6174 3136 290a  mstype.float16).
+0000cf50: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000cf60: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+0000cf70: 7468 203d 2054 656e 736f 7228 6e70 2e6f  th = Tensor(np.o
+0000cf80: 6e65 7328 2832 2c29 292c 206d 7374 7970  nes((2,)), mstyp
+0000cf90: 652e 696e 7433 3229 0a20 2020 2020 2020  e.int32).       
+0000cfa0: 2020 2020 203e 3e3e 2023 2053 6574 2069       >>> # Set i
+0000cfb0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+0000cfc0: 6e3d 5472 7565 2074 6f20 6765 6e65 7261  n=True to genera
+0000cfd0: 7465 2074 6865 2066 756c 6c20 6d65 6d6f  te the full memo
+0000cfe0: 7279 2073 7461 7465 730a 2020 2020 2020  ry states.      
+0000cff0: 2020 2020 2020 3e3e 3e20 6d6f 6465 6c2e        >>> model.
+0000d000: 6164 645f 666c 6167 735f 7265 6375 7273  add_flags_recurs
+0000d010: 6976 6528 6973 5f66 6972 7374 5f69 7465  ive(is_first_ite
+0000d020: 7261 7469 6f6e 3d54 7275 6529 0a20 2020  ration=True).   
+0000d030: 2020 2020 2020 2020 203e 3e3e 2061 7474           >>> att
+0000d040: 6e5f 6f75 742c 2070 6173 7420 3d20 6d6f  n_out, past = mo
+0000d050: 6465 6c28 6672 6f6d 5f74 656e 736f 722c  del(from_tensor,
+0000d060: 2074 6f5f 7465 6e73 6f72 2c20 746f 5f74   to_tensor, to_t
+0000d070: 656e 736f 722c 2061 7474 656e 7469 6f6e  ensor, attention
+0000d080: 5f6d 6173 6b2c 206b 6579 5f70 6173 742c  _mask, key_past,
+0000d090: 2076 616c 7565 5f70 6173 742c 0a20 2020   value_past,.   
+0000d0a0: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
+0000d0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d0c0: 2020 2020 6261 7463 685f 7661 6c69 645f      batch_valid_
+0000d0d0: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
+0000d0e0: 2020 2020 3e3e 3e20 7072 696e 7428 6174      >>> print(at
+0000d0f0: 746e 5f6f 7574 2e73 6861 7065 290a 2020  tn_out.shape).  
+0000d100: 2020 2020 2020 2020 2020 2832 2c20 3230            (2, 20
+0000d110: 2c20 3135 290a 2020 2020 2020 2020 2020  , 15).          
+0000d120: 2020 3e3e 3e20 7072 696e 7428 7061 7374    >>> print(past
+0000d130: 5b30 5d2e 7368 6170 6529 0a20 2020 2020  [0].shape).     
+0000d140: 2020 2020 2020 2028 322c 2033 2c20 352c         (2, 3, 5,
+0000d150: 2032 3029 0a20 2020 2020 2020 2020 2020   20).           
+0000d160: 203e 3e3e 2070 7269 6e74 2870 6173 745b   >>> print(past[
+0000d170: 315d 2e73 6861 7065 290a 2020 2020 2020  1].shape).      
+0000d180: 2020 2020 2020 2832 2c20 332c 2032 302c        (2, 3, 20,
+0000d190: 2035 290a 2020 2020 2020 2020 2020 2020   5).            
+0000d1a0: 3e3e 3e20 6672 6f6d 5f74 656e 736f 7220  >>> from_tensor 
+0000d1b0: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
+0000d1c0: 2828 322c 2031 2c20 3135 2929 2c20 6d73  ((2, 1, 15)), ms
+0000d1d0: 7479 7065 2e66 6c6f 6174 3332 290a 2020  type.float32).  
+0000d1e0: 2020 2020 2020 2020 2020 3e3e 3e20 746f            >>> to
+0000d1f0: 5f74 656e 736f 7220 3d20 5465 6e73 6f72  _tensor = Tensor
+0000d200: 286e 702e 6f6e 6573 2828 322c 2031 2c20  (np.ones((2, 1, 
+0000d210: 3135 2929 2c20 6d73 7479 7065 2e66 6c6f  15)), mstype.flo
+0000d220: 6174 3136 290a 2020 2020 2020 2020 2020  at16).          
+0000d230: 2020 3e3e 3e20 6174 7465 6e74 696f 6e5f    >>> attention_
+0000d240: 6d61 736b 203d 2054 656e 736f 7228 6e70  mask = Tensor(np
+0000d250: 2e6f 6e65 7328 2832 2c20 312c 2032 3029  .ones((2, 1, 20)
+0000d260: 292c 206d 7374 7970 652e 666c 6f61 7431  ), mstype.float1
+0000d270: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
+0000d280: 3e3e 2023 2053 7465 7020 323a 2073 6574  >> # Step 2: set
+0000d290: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
+0000d2a0: 696f 6e3d 4661 6c73 652c 2061 6e64 2070  ion=False, and p
+0000d2b0: 6173 7320 7468 6520 7369 6e67 6c65 2077  ass the single w
+0000d2c0: 6f72 6420 746f 2072 756e 2074 6865 2070  ord to run the p
+0000d2d0: 7265 6469 6374 696f 6e20 7261 7468 6572  rediction rather
+0000d2e0: 2074 6861 6e20 7468 650a 2020 2020 2020   than the.      
+0000d2f0: 2020 2020 2020 3e3e 3e20 2320 6675 6c6c        >>> # full
+0000d300: 2073 6571 7565 6e63 652e 0a20 2020 2020   sequence..     
+0000d310: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
+0000d320: 2e61 6464 5f66 6c61 6773 5f72 6563 7572  .add_flags_recur
+0000d330: 7369 7665 2869 735f 6669 7273 745f 6974  sive(is_first_it
+0000d340: 6572 6174 696f 6e3d 4661 6c73 6529 0a20  eration=False). 
+0000d350: 2020 2020 2020 2020 2020 203e 3e3e 2061             >>> a
+0000d360: 7474 6e5f 6f75 742c 2070 6173 7420 3d20  ttn_out, past = 
+0000d370: 6d6f 6465 6c28 6672 6f6d 5f74 656e 736f  model(from_tenso
+0000d380: 722c 2074 6f5f 7465 6e73 6f72 2c20 746f  r, to_tensor, to
+0000d390: 5f74 656e 736f 722c 2061 7474 656e 7469  _tensor, attenti
+0000d3a0: 6f6e 5f6d 6173 6b2c 206b 6579 5f70 6173  on_mask, key_pas
+0000d3b0: 742c 2076 616c 7565 5f70 6173 742c 0a20  t, value_past,. 
+0000d3c0: 2020 2020 2020 2020 2020 202e 2e2e 2020             ...  
+0000d3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d3e0: 2020 2020 2020 6261 7463 685f 7661 6c69        batch_vali
+0000d3f0: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
+0000d400: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+0000d410: 6174 746e 5f6f 7574 2e73 6861 7065 290a  attn_out.shape).
+0000d420: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
+0000d430: 312c 2031 3529 0a20 2020 2020 2020 2020  1, 15).         
+0000d440: 2020 203e 3e3e 2070 7269 6e74 2870 6173     >>> print(pas
+0000d450: 745b 305d 2e73 6861 7065 290a 2020 2020  t[0].shape).    
+0000d460: 2020 2020 2020 2020 2832 2c20 332c 2035          (2, 3, 5
+0000d470: 2c20 3230 290a 2020 2020 2020 2020 2020  , 20).          
+0000d480: 2020 3e3e 3e20 7072 696e 7428 7061 7374    >>> print(past
+0000d490: 5b31 5d2e 7368 6170 6529 0a20 2020 2020  [1].shape).     
+0000d4a0: 2020 2020 2020 2028 322c 2033 2c20 3230         (2, 3, 20
+0000d4b0: 2c20 3529 0a20 2020 2022 2222 0a0a 2020  , 5).    """..  
+0000d4c0: 2020 405f 4c6f 6741 6374 696f 6e4f 6e63    @_LogActionOnc
+0000d4d0: 6528 6d5f 6c6f 6767 6572 3d6c 6f67 6765  e(m_logger=logge
+0000d4e0: 722c 206b 6579 3d27 4d75 6c74 6948 6561  r, key='MultiHea
+0000d4f0: 6441 7474 656e 7469 6f6e 272c 0a20 2020  dAttention',.   
+0000d500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d510: 206e 6f5f 7761 726e 696e 673d 5f67 6574   no_warning=_get
+0000d520: 5f70 6172 616c 6c65 6c5f 6d6f 6465 2829  _parallel_mode()
+0000d530: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
+0000d540: 652e 5354 414e 445f 414c 4f4e 452c 2929  e.STAND_ALONE,))
+0000d550: 0a20 2020 2040 5f61 7267 735f 7479 7065  .    @_args_type
+0000d560: 5f76 616c 6964 6174 6f72 5f63 6865 636b  _validator_check
+0000d570: 2868 6964 6465 6e5f 7369 7a65 3d56 616c  (hidden_size=Val
+0000d580: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
+0000d590: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
+0000d5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d5b0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+0000d5c0: 6561 6473 3d56 616c 6964 6174 6f72 2e63  eads=Validator.c
+0000d5d0: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+0000d5e0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+0000d5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d600: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
+0000d610: 683d 5661 6c69 6461 746f 722e 6368 6563  h=Validator.chec
+0000d620: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+0000d630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d650: 7467 745f 7365 715f 6c65 6e67 7468 3d56  tgt_seq_length=V
+0000d660: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+0000d670: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
+0000d680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d690: 2020 2020 2020 2020 2020 2020 2061 7474               att
+0000d6a0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0000d6b0: 6174 653d 5661 6c69 6461 746f 722e 6368  ate=Validator.ch
+0000d6c0: 6563 6b5f 6e6f 6e5f 6e65 6761 7469 7665  eck_non_negative
+0000d6d0: 5f66 6c6f 6174 2c0a 2020 2020 2020 2020  _float,.        
+0000d6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d6f0: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
+0000d700: 726f 706f 7574 5f72 6174 653d 5661 6c69  ropout_rate=Vali
+0000d710: 6461 746f 722e 6368 6563 6b5f 6e6f 6e5f  dator.check_non_
+0000d720: 6e65 6761 7469 7665 5f66 6c6f 6174 2c0a  negative_float,.
+0000d730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d750: 636f 6d70 7574 655f 6474 7970 653d 5f76  compute_dtype=_v
+0000d760: 616c 6964 5f76 616c 7565 5f63 6865 636b  alid_value_check
+0000d770: 7328 5b6d 7374 7970 652e 666c 6f61 7433  s([mstype.float3
+0000d780: 322c 206d 7374 7970 652e 666c 6f61 7431  2, mstype.float1
+0000d790: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
+0000d7a0: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
 0000d7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000d7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d7d0: 2020 224d 756c 7469 4865 6164 4174 7465    "MultiHeadAtte
-0000d7e0: 6e74 696f 6e22 292c 0a20 2020 2020 2020  ntion"),.       
-0000d7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d800: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
-0000d810: 5f63 6f6d 7075 7465 5f74 7970 653d 5f76  _compute_type=_v
-0000d820: 616c 6964 5f76 616c 7565 5f63 6865 636b  alid_value_check
-0000d830: 7328 5b6d 7374 7970 652e 666c 6f61 7433  s([mstype.float3
-0000d840: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
-0000d850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d7e0: 2020 2020 2020 2022 4d75 6c74 6948 6561         "MultiHea
+0000d7f0: 6441 7474 656e 7469 6f6e 2229 2c0a 2020  dAttention"),.  
+0000d800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d810: 2020 2020 2020 2020 2020 2020 2020 736f                so
+0000d820: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+0000d830: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
+0000d840: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
+0000d850: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
 0000d860: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000d870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d880: 2020 2020 2020 2020 2020 2020 206d 7374               mst
-0000d890: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
-0000d8a0: 7970 652e 6266 6c6f 6174 3136 5d2c 0a20  ype.bfloat16],. 
-0000d8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d8a0: 2020 6d73 7479 7065 2e66 6c6f 6174 3136    mstype.float16
+0000d8b0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
+0000d8c0: 365d 2c0a 2020 2020 2020 2020 2020 2020  6],.            
 0000d8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000d8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d8f0: 2020 2020 2020 2020 224d 756c 7469 4865          "MultiHe
-0000d900: 6164 4174 7465 6e74 696f 6e22 292c 0a20  adAttention"),. 
-0000d910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d920: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0000d930: 6172 616d 5f69 6e69 745f 7479 7065 3d5f  aram_init_type=_
-0000d940: 7661 6c69 645f 7661 6c75 655f 6368 6563  valid_value_chec
-0000d950: 6b73 285b 6d73 7479 7065 2e66 6c6f 6174  ks([mstype.float
-0000d960: 3332 2c20 6d73 7479 7065 2e66 6c6f 6174  32, mstype.float
-0000d970: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
-0000d980: 7431 365d 2c0a 2020 2020 2020 2020 2020  t16],.          
-0000d990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d900: 2020 2020 2020 2020 2020 2020 2022 4d75               "Mu
+0000d910: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000d920: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
+0000d930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d940: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+0000d950: 7970 653d 5f76 616c 6964 5f76 616c 7565  ype=_valid_value
+0000d960: 5f63 6865 636b 7328 5b6d 7374 7970 652e  _checks([mstype.
+0000d970: 666c 6f61 7433 322c 206d 7374 7970 652e  float32, mstype.
+0000d980: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+0000d990: 6266 6c6f 6174 3136 5d2c 0a20 2020 2020  bfloat16],.     
 0000d9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000d9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d9c0: 2020 2020 2020 2020 2020 224d 756c 7469            "Multi
-0000d9d0: 4865 6164 4174 7465 6e74 696f 6e22 292c  HeadAttention"),
-0000d9e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000d9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000da00: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0000da10: 3d5f 7661 6c69 645f 7479 7065 5f63 6865  =_valid_type_che
-0000da20: 636b 7328 5b4f 7050 6172 616c 6c65 6c43  cks([OpParallelC
-0000da30: 6f6e 6669 675d 2c0a 2020 2020 2020 2020  onfig],.        
-0000da40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d9d0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0000d9e0: 4d75 6c74 6948 6561 6441 7474 656e 7469  MultiHeadAttenti
+0000d9f0: 6f6e 2229 2c0a 2020 2020 2020 2020 2020  on"),.          
+0000da00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000da10: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+0000da20: 6f6e 6669 673d 5f76 616c 6964 5f74 7970  onfig=_valid_typ
+0000da30: 655f 6368 6563 6b73 285b 4f70 5061 7261  e_checks([OpPara
+0000da40: 6c6c 656c 436f 6e66 6967 5d2c 0a20 2020  llelConfig],.   
 0000da50: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000da60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000da70: 2020 2020 2020 2020 2020 2022 4d75 6c74             "Mult
-0000da80: 6948 6561 6441 7474 656e 7469 6f6e 2229  iHeadAttention")
-0000da90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000daa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000dab0: 2020 7573 655f 7061 7374 3d56 616c 6964    use_past=Valid
-0000dac0: 6174 6f72 2e63 6865 636b 5f62 6f6f 6c2c  ator.check_bool,
-0000dad0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000dae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000daf0: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
-0000db00: 7469 6f6e 3d56 616c 6964 6174 6f72 2e63  tion=Validator.c
-0000db10: 6865 636b 5f62 6f6f 6c29 0a20 2020 2064  heck_bool).    d
-0000db20: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
-0000db30: 2c20 6261 7463 685f 7369 7a65 2c0a 2020  , batch_size,.  
-0000db40: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0000db50: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
-0000db60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000db70: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
-0000db80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000db90: 2068 6964 6465 6e5f 7369 7a65 2c0a 2020   hidden_size,.  
-0000dba0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-0000dbb0: 756d 5f68 6561 6473 2c0a 2020 2020 2020  um_heads,.      
-0000dbc0: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0000dbd0: 6e5f 6472 6f70 6f75 745f 7261 7465 3d30  n_dropout_rate=0
-0000dbe0: 2e31 2c0a 2020 2020 2020 2020 2020 2020  .1,.            
-0000dbf0: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-0000dc00: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
-0000dc10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000dc20: 2020 636f 6d70 7574 655f 6474 7970 653d    compute_dtype=
-0000dc30: 6d73 7479 7065 2e66 6c6f 6174 3136 2c0a  mstype.float16,.
-0000dc40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000dc50: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-0000dc60: 5f74 7970 653d 6d73 7479 7065 2e66 6c6f  _type=mstype.flo
-0000dc70: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-0000dc80: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
-0000dc90: 745f 7479 7065 3d6d 7374 7970 652e 666c  t_type=mstype.fl
-0000dca0: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-0000dcb0: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-0000dcc0: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-0000dcd0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-0000dce0: 6c5f 636f 6e66 6967 3d64 6566 6175 6c74  l_config=default
-0000dcf0: 5f64 706d 705f 636f 6e66 6967 2c0a 2020  _dpmp_config,.  
-0000dd00: 2020 2020 2020 2020 2020 2020 2020 2075                 u
-0000dd10: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
-0000dd20: 6f6e 3d46 616c 7365 293a 0a20 2020 2020  on=False):.     
-0000dd30: 2020 2073 7570 6572 284d 756c 7469 4865     super(MultiHe
-0000dd40: 6164 4174 7465 6e74 696f 6e2c 2073 656c  adAttention, sel
-0000dd50: 6629 2e5f 5f69 6e69 745f 5f28 290a 2020  f).__init__().  
-0000dd60: 2020 2020 2020 7365 6c66 2e5f 6973 5f61        self._is_a
-0000dd70: 7363 656e 6420 3d20 636f 6e74 6578 742e  scend = context.
-0000dd80: 6765 745f 636f 6e74 6578 7428 2764 6576  get_context('dev
-0000dd90: 6963 655f 7461 7267 6574 2729 2069 6e20  ice_target') in 
-0000dda0: 5b22 4173 6365 6e64 225d 0a20 2020 2020  ["Ascend"].     
-0000ddb0: 2020 2073 656c 662e 6470 203d 2070 6172     self.dp = par
-0000ddc0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-0000ddd0: 615f 7061 7261 6c6c 656c 0a20 2020 2020  a_parallel.     
-0000dde0: 2020 2073 656c 662e 6973 5f70 6172 616c     self.is_paral
-0000ddf0: 6c65 6c5f 6d6f 6465 203d 205f 6765 745f  lel_mode = _get_
-0000de00: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
-0000de10: 696e 2028 0a20 2020 2020 2020 2020 2020  in (.           
-0000de20: 2050 6172 616c 6c65 6c4d 6f64 652e 5345   ParallelMode.SE
-0000de30: 4d49 5f41 5554 4f5f 5041 5241 4c4c 454c  MI_AUTO_PARALLEL
-0000de40: 2c20 5061 7261 6c6c 656c 4d6f 6465 2e41  , ParallelMode.A
-0000de50: 5554 4f5f 5041 5241 4c4c 454c 290a 2020  UTO_PARALLEL).  
-0000de60: 2020 2020 2020 6966 2062 6174 6368 5f73        if batch_s
-0000de70: 697a 653a 0a20 2020 2020 2020 2020 2020  ize:.           
-0000de80: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-0000de90: 5f70 6f73 6974 6976 655f 696e 7428 6261  _positive_int(ba
-0000dea0: 7463 685f 7369 7a65 290a 2020 2020 2020  tch_size).      
-0000deb0: 2020 6966 205f 6765 745f 7061 7261 6c6c    if _get_parall
-0000dec0: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
-0000ded0: 7261 6c6c 656c 4d6f 6465 2e41 5554 4f5f  rallelMode.AUTO_
-0000dee0: 5041 5241 4c4c 454c 2c29 3a0a 2020 2020  PARALLEL,):.    
-0000def0: 2020 2020 2020 2020 5f63 6865 636b 5f63          _check_c
-0000df00: 6f6e 6669 6728 7061 7261 6c6c 656c 5f63  onfig(parallel_c
-0000df10: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
-0000df20: 2020 2073 656c 662e 7372 635f 7365 715f     self.src_seq_
-0000df30: 6c65 6e67 7468 203d 2073 7263 5f73 6571  length = src_seq
-0000df40: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
-0000df50: 2020 2020 7365 6c66 2e74 6774 5f73 6571      self.tgt_seq
-0000df60: 5f6c 656e 6774 6820 3d20 7467 745f 7365  _length = tgt_se
-0000df70: 715f 6c65 6e67 7468 0a20 2020 2020 2020  q_length.       
-0000df80: 2020 2020 2073 656c 662e 6869 6464 656e       self.hidden
-0000df90: 5f73 697a 6520 3d20 6869 6464 656e 5f73  _size = hidden_s
-0000dfa0: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
-0000dfb0: 7365 6c66 2e62 6174 6368 5f73 697a 6520  self.batch_size 
-0000dfc0: 3d20 6261 7463 685f 7369 7a65 0a20 2020  = batch_size.   
-0000dfd0: 2020 2020 2020 2020 2069 6620 6869 6464           if hidd
-0000dfe0: 656e 5f64 726f 706f 7574 5f72 6174 6520  en_dropout_rate 
-0000dff0: 3c20 3020 6f72 2068 6964 6465 6e5f 6472  < 0 or hidden_dr
-0000e000: 6f70 6f75 745f 7261 7465 203e 3d20 313a  opout_rate >= 1:
-0000e010: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000e020: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-0000e030: 7228 2246 6f72 2027 4d75 6c74 6948 6561  r("For 'MultiHea
-0000e040: 6441 7474 656e 7469 6f6e 272c 2074 6865  dAttention', the
-0000e050: 2063 6c61 7373 2076 6172 6961 626c 6520   class variable 
-0000e060: 2768 6964 6465 6e5f 6472 6f70 6f75 745f  'hidden_dropout_
-0000e070: 7261 7465 2720 6d75 7374 2062 6520 220a  rate' must be ".
-0000e080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e0a0: 2022 696e 2072 616e 6765 205b 302c 2031   "in range [0, 1
-0000e0b0: 2e30 292c 2062 7574 2067 6f74 2074 6865  .0), but got the
-0000e0c0: 2076 616c 7565 203a 207b 7d2e 222e 666f   value : {}.".fo
-0000e0d0: 726d 6174 2868 6964 6465 6e5f 6472 6f70  rmat(hidden_drop
-0000e0e0: 6f75 745f 7261 7465 2929 0a20 2020 2020  out_rate)).     
-0000e0f0: 2020 2020 2020 2069 6620 6174 7465 6e74         if attent
-0000e100: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-0000e110: 203c 2030 206f 7220 6174 7465 6e74 696f   < 0 or attentio
-0000e120: 6e5f 6472 6f70 6f75 745f 7261 7465 203e  n_dropout_rate >
-0000e130: 3d20 313a 0a20 2020 2020 2020 2020 2020  = 1:.           
-0000e140: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0000e150: 4572 726f 7228 2246 6f72 2027 4d75 6c74  Error("For 'Mult
-0000e160: 6948 6561 6441 7474 656e 7469 6f6e 272c  iHeadAttention',
-0000e170: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
-0000e180: 626c 6520 2761 7474 656e 7469 6f6e 5f64  ble 'attention_d
-0000e190: 726f 706f 7574 5f72 6174 6527 206d 7573  ropout_rate' mus
-0000e1a0: 7420 6265 2022 0a20 2020 2020 2020 2020  t be ".         
-0000e1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e1c0: 2020 2020 2020 2020 2269 6e20 7261 6e67          "in rang
-0000e1d0: 6520 5b30 2c20 312e 3029 2c20 6275 7420  e [0, 1.0), but 
-0000e1e0: 676f 7420 7468 6520 7661 6c75 6520 3a20  got the value : 
-0000e1f0: 7b7d 2e22 2e66 6f72 6d61 7428 6174 7465  {}.".format(atte
-0000e200: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-0000e210: 7465 2929 0a20 2020 2020 2020 2020 2020  te)).           
-0000e220: 2069 6620 6869 6464 656e 5f73 697a 6520   if hidden_size 
-0000e230: 2520 6e75 6d5f 6865 6164 7320 213d 2030  % num_heads != 0
-0000e240: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0000e250: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0000e260: 6f72 2822 466f 7220 274d 756c 7469 4865  or("For 'MultiHe
-0000e270: 6164 4174 7465 6e74 696f 6e27 2c20 7468  adAttention', th
-0000e280: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
-0000e290: 2027 6869 6464 656e 5f73 697a 6527 206d   'hidden_size' m
-0000e2a0: 7573 7420 6265 2061 206d 756c 7469 706c  ust be a multipl
-0000e2b0: 6520 220a 2020 2020 2020 2020 2020 2020  e ".            
-0000e2c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e2d0: 2020 2020 2022 6f66 2027 6e75 6d5f 6865       "of 'num_he
-0000e2e0: 6164 7327 2c20 6275 7420 676f 7420 7468  ads', but got th
-0000e2f0: 6520 6869 6464 656e 5f73 697a 6520 6973  e hidden_size is
-0000e300: 207b 7d20 616e 6420 7468 6520 6e75 6d5f   {} and the num_
-0000e310: 6865 6164 7320 6973 207b 7d2e 220a 2020  heads is {}.".  
-0000e320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e330: 2020 2020 2020 2020 2020 2020 2020 202e                 .
-0000e340: 666f 726d 6174 2868 6964 6465 6e5f 7369  format(hidden_si
-0000e350: 7a65 2c20 6e75 6d5f 6865 6164 7329 290a  ze, num_heads)).
-0000e360: 2020 2020 2020 2020 2020 2020 6966 206e              if n
-0000e370: 756d 5f68 6561 6473 2025 2070 6172 616c  um_heads % paral
-0000e380: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0000e390: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
-0000e3a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e3b0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000e3c0: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
-0000e3d0: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
-0000e3e0: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
-0000e3f0: 6e75 6d5f 6865 6164 7327 206d 7573 7420  num_heads' must 
-0000e400: 6265 2061 206d 756c 7469 706c 6520 6f66  be a multiple of
-0000e410: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-0000e420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e430: 2020 2020 2227 7061 7261 6c6c 656c 5f63      "'parallel_c
-0000e440: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-0000e450: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
-0000e460: 6865 206e 756d 5f68 6561 6473 2069 7320  he num_heads is 
-0000e470: 7b7d 2022 0a20 2020 2020 2020 2020 2020  {} ".           
-0000e480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e490: 2020 2020 2020 2261 6e64 2074 6865 2070        "and the p
-0000e4a0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0000e4b0: 6f64 656c 5f70 6172 616c 6c65 6c20 2069  odel_parallel  i
-0000e4c0: 7320 7b7d 2e22 0a20 2020 2020 2020 2020  s {}.".         
-0000e4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e4e0: 2020 2020 2020 2020 2e66 6f72 6d61 7428          .format(
-0000e4f0: 6e75 6d5f 6865 6164 732c 2070 6172 616c  num_heads, paral
-0000e500: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0000e510: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
-0000e520: 2020 2020 2020 2020 7365 6c66 2e69 735f          self.is_
-0000e530: 6669 7273 745f 6974 6572 6174 696f 6e20  first_iteration 
-0000e540: 3d20 5472 7565 0a20 2020 2020 2020 2020  = True.         
-0000e550: 2020 2023 204f 7574 7075 7420 6c61 7965     # Output laye
-0000e560: 720a 2020 2020 2020 2020 2020 2020 7365  r.            se
-0000e570: 6c66 2e70 726f 6a65 6374 696f 6e20 3d20  lf.projection = 
-0000e580: 4c69 6e65 6172 2869 6e5f 6368 616e 6e65  Linear(in_channe
-0000e590: 6c73 3d68 6964 6465 6e5f 7369 7a65 2c0a  ls=hidden_size,.
-0000e5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e5c0: 2020 2020 206f 7574 5f63 6861 6e6e 656c       out_channel
-0000e5d0: 733d 6869 6464 656e 5f73 697a 652c 0a20  s=hidden_size,. 
-0000e5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e600: 2020 2020 7472 616e 7370 6f73 655f 623d      transpose_b=
-0000e610: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-0000e620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e630: 2020 2020 2020 2020 2020 2020 636f 6d70              comp
-0000e640: 7574 655f 6474 7970 653d 636f 6d70 7574  ute_dtype=comput
-0000e650: 655f 6474 7970 652c 0a20 2020 2020 2020  e_dtype,.       
-0000e660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e670: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0000e680: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-0000e690: 7261 6d5f 696e 6974 5f74 7970 6529 0a20  ram_init_type). 
-0000e6a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000e6b0: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
-0000e6c0: 2873 7472 6174 6567 795f 6269 6173 3d28  (strategy_bias=(
-0000e6d0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0000e6e0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-0000e6f0: 3129 2c20 2831 2c29 292c 0a20 2020 2020  1), (1,)),.     
-0000e700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e710: 2020 2020 2020 2020 2020 2020 2073 7472               str
-0000e720: 6174 6567 795f 6d61 746d 756c 3d28 2870  ategy_matmul=((p
-0000e730: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-0000e740: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
-0000e750: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0000e760: 6465 6c5f 7061 7261 6c6c 656c 292c 0a20  del_parallel),. 
-0000e770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000da70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000da80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000da90: 224d 756c 7469 4865 6164 4174 7465 6e74  "MultiHeadAttent
+0000daa0: 696f 6e22 292c 0a20 2020 2020 2020 2020  ion"),.         
+0000dab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000dac0: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+0000dad0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+0000dae0: 626f 6f6c 2c0a 2020 2020 2020 2020 2020  bool,.          
+0000daf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000db00: 2020 2020 2020 7573 655f 666c 6173 685f        use_flash_
+0000db10: 6174 7465 6e74 696f 6e3d 5661 6c69 6461  attention=Valida
+0000db20: 746f 722e 6368 6563 6b5f 626f 6f6c 290a  tor.check_bool).
+0000db30: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+0000db40: 2873 656c 662c 2062 6174 6368 5f73 697a  (self, batch_siz
+0000db50: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0000db60: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
+0000db70: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
+0000db80: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
+0000db90: 6774 682c 0a20 2020 2020 2020 2020 2020  gth,.           
+0000dba0: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
+0000dbb0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0000dbc0: 2020 2020 6e75 6d5f 6865 6164 732c 0a20      num_heads,. 
+0000dbd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000dbe0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+0000dbf0: 6174 653d 302e 312c 0a20 2020 2020 2020  ate=0.1,.       
+0000dc00: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+0000dc10: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0000dc20: 3d30 2e31 2c0a 2020 2020 2020 2020 2020  =0.1,.          
+0000dc30: 2020 2020 2020 2063 6f6d 7075 7465 5f64         compute_d
+0000dc40: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
+0000dc50: 7431 362c 0a20 2020 2020 2020 2020 2020  t16,.           
+0000dc60: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
+0000dc70: 6d70 7574 655f 7479 7065 3d6d 7374 7970  mpute_type=mstyp
+0000dc80: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
+0000dc90: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0000dca0: 6d5f 696e 6974 5f74 7970 653d 6d73 7479  m_init_type=msty
+0000dcb0: 7065 2e66 6c6f 6174 3332 2c0a 2020 2020  pe.float32,.    
+0000dcc0: 2020 2020 2020 2020 2020 2020 2075 7365               use
+0000dcd0: 5f70 6173 743d 4661 6c73 652c 0a20 2020  _past=False,.   
+0000dce0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0000dcf0: 7261 6c6c 656c 5f63 6f6e 6669 673d 6465  rallel_config=de
+0000dd00: 6661 756c 745f 6470 6d70 5f63 6f6e 6669  fault_dpmp_confi
+0000dd10: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
+0000dd20: 2020 2020 7573 655f 666c 6173 685f 6174      use_flash_at
+0000dd30: 7465 6e74 696f 6e3d 4661 6c73 6529 3a0a  tention=False):.
+0000dd40: 2020 2020 2020 2020 7375 7065 7228 4d75          super(Mu
+0000dd50: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000dd60: 2c20 7365 6c66 292e 5f5f 696e 6974 5f5f  , self).__init__
+0000dd70: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
+0000dd80: 5f69 735f 6173 6365 6e64 203d 2063 6f6e  _is_ascend = con
+0000dd90: 7465 7874 2e67 6574 5f63 6f6e 7465 7874  text.get_context
+0000dda0: 2827 6465 7669 6365 5f74 6172 6765 7427  ('device_target'
+0000ddb0: 2920 696e 205b 2241 7363 656e 6422 5d0a  ) in ["Ascend"].
+0000ddc0: 2020 2020 2020 2020 7365 6c66 2e64 7020          self.dp 
+0000ddd0: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
+0000dde0: 672e 6461 7461 5f70 6172 616c 6c65 6c0a  g.data_parallel.
+0000ddf0: 2020 2020 2020 2020 7365 6c66 2e69 735f          self.is_
+0000de00: 7061 7261 6c6c 656c 5f6d 6f64 6520 3d20  parallel_mode = 
+0000de10: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
+0000de20: 6465 2829 2069 6e20 280a 2020 2020 2020  de() in (.      
+0000de30: 2020 2020 2020 5061 7261 6c6c 656c 4d6f        ParallelMo
+0000de40: 6465 2e53 454d 495f 4155 544f 5f50 4152  de.SEMI_AUTO_PAR
+0000de50: 414c 4c45 4c2c 2050 6172 616c 6c65 6c4d  ALLEL, ParallelM
+0000de60: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
+0000de70: 4c29 0a20 2020 2020 2020 2069 6620 6261  L).        if ba
+0000de80: 7463 685f 7369 7a65 3a0a 2020 2020 2020  tch_size:.      
+0000de90: 2020 2020 2020 5661 6c69 6461 746f 722e        Validator.
+0000dea0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+0000deb0: 6e74 2862 6174 6368 5f73 697a 6529 0a20  nt(batch_size). 
+0000dec0: 2020 2020 2020 2069 6620 5f67 6574 5f70         if _get_p
+0000ded0: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
+0000dee0: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
+0000def0: 4155 544f 5f50 4152 414c 4c45 4c2c 293a  AUTO_PARALLEL,):
+0000df00: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+0000df10: 6563 6b5f 636f 6e66 6967 2870 6172 616c  eck_config(paral
+0000df20: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+0000df30: 2020 2020 2020 2020 7365 6c66 2e73 7263          self.src
+0000df40: 5f73 6571 5f6c 656e 6774 6820 3d20 7372  _seq_length = sr
+0000df50: 635f 7365 715f 6c65 6e67 7468 0a20 2020  c_seq_length.   
+0000df60: 2020 2020 2020 2020 2073 656c 662e 7467           self.tg
+0000df70: 745f 7365 715f 6c65 6e67 7468 203d 2074  t_seq_length = t
+0000df80: 6774 5f73 6571 5f6c 656e 6774 680a 2020  gt_seq_length.  
+0000df90: 2020 2020 2020 2020 2020 7365 6c66 2e68            self.h
+0000dfa0: 6964 6465 6e5f 7369 7a65 203d 2068 6964  idden_size = hid
+0000dfb0: 6465 6e5f 7369 7a65 0a20 2020 2020 2020  den_size.       
+0000dfc0: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
+0000dfd0: 7369 7a65 203d 2062 6174 6368 5f73 697a  size = batch_siz
+0000dfe0: 650a 2020 2020 2020 2020 2020 2020 6966  e.            if
+0000dff0: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+0000e000: 7261 7465 203c 2030 206f 7220 6869 6464  rate < 0 or hidd
+0000e010: 656e 5f64 726f 706f 7574 5f72 6174 6520  en_dropout_rate 
+0000e020: 3e3d 2031 3a0a 2020 2020 2020 2020 2020  >= 1:.          
+0000e030: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+0000e040: 6545 7272 6f72 2822 466f 7220 274d 756c  eError("For 'Mul
+0000e050: 7469 4865 6164 4174 7465 6e74 696f 6e27  tiHeadAttention'
+0000e060: 2c20 7468 6520 636c 6173 7320 7661 7269  , the class vari
+0000e070: 6162 6c65 2027 6869 6464 656e 5f64 726f  able 'hidden_dro
+0000e080: 706f 7574 5f72 6174 6527 206d 7573 7420  pout_rate' must 
+0000e090: 6265 2022 0a20 2020 2020 2020 2020 2020  be ".           
+0000e0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e0b0: 2020 2020 2020 2269 6e20 7261 6e67 6520        "in range 
+0000e0c0: 5b30 2c20 312e 3029 2c20 6275 7420 676f  [0, 1.0), but go
+0000e0d0: 7420 7468 6520 7661 6c75 6520 3a20 7b7d  t the value : {}
+0000e0e0: 2e22 2e66 6f72 6d61 7428 6869 6464 656e  .".format(hidden
+0000e0f0: 5f64 726f 706f 7574 5f72 6174 6529 290a  _dropout_rate)).
+0000e100: 2020 2020 2020 2020 2020 2020 6966 2061              if a
+0000e110: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
+0000e120: 5f72 6174 6520 3c20 3020 6f72 2061 7474  _rate < 0 or att
+0000e130: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0000e140: 6174 6520 3e3d 2031 3a0a 2020 2020 2020  ate >= 1:.      
+0000e150: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+0000e160: 5661 6c75 6545 7272 6f72 2822 466f 7220  ValueError("For 
+0000e170: 274d 756c 7469 4865 6164 4174 7465 6e74  'MultiHeadAttent
+0000e180: 696f 6e27 2c20 7468 6520 636c 6173 7320  ion', the class 
+0000e190: 7661 7269 6162 6c65 2027 6174 7465 6e74  variable 'attent
+0000e1a0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0000e1b0: 2720 6d75 7374 2062 6520 220a 2020 2020  ' must be ".    
+0000e1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e1d0: 2020 2020 2020 2020 2020 2020 2022 696e               "in
+0000e1e0: 2072 616e 6765 205b 302c 2031 2e30 292c   range [0, 1.0),
+0000e1f0: 2062 7574 2067 6f74 2074 6865 2076 616c   but got the val
+0000e200: 7565 203a 207b 7d2e 222e 666f 726d 6174  ue : {}.".format
+0000e210: 2861 7474 656e 7469 6f6e 5f64 726f 706f  (attention_dropo
+0000e220: 7574 5f72 6174 6529 290a 2020 2020 2020  ut_rate)).      
+0000e230: 2020 2020 2020 6966 2068 6964 6465 6e5f        if hidden_
+0000e240: 7369 7a65 2025 206e 756d 5f68 6561 6473  size % num_heads
+0000e250: 2021 3d20 303a 0a20 2020 2020 2020 2020   != 0:.         
+0000e260: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+0000e270: 7565 4572 726f 7228 2246 6f72 2027 4d75  ueError("For 'Mu
+0000e280: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000e290: 272c 2074 6865 2063 6c61 7373 2076 6172  ', the class var
+0000e2a0: 6961 626c 6520 2768 6964 6465 6e5f 7369  iable 'hidden_si
+0000e2b0: 7a65 2720 6d75 7374 2062 6520 6120 6d75  ze' must be a mu
+0000e2c0: 6c74 6970 6c65 2022 0a20 2020 2020 2020  ltiple ".       
+0000e2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e2e0: 2020 2020 2020 2020 2020 226f 6620 276e            "of 'n
+0000e2f0: 756d 5f68 6561 6473 272c 2062 7574 2067  um_heads', but g
+0000e300: 6f74 2074 6865 2068 6964 6465 6e5f 7369  ot the hidden_si
+0000e310: 7a65 2069 7320 7b7d 2061 6e64 2074 6865  ze is {} and the
+0000e320: 206e 756d 5f68 6561 6473 2069 7320 7b7d   num_heads is {}
+0000e330: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
+0000e340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e350: 2020 2020 2e66 6f72 6d61 7428 6869 6464      .format(hidd
+0000e360: 656e 5f73 697a 652c 206e 756d 5f68 6561  en_size, num_hea
+0000e370: 6473 2929 0a20 2020 2020 2020 2020 2020  ds)).           
+0000e380: 2069 6620 6e75 6d5f 6865 6164 7320 2520   if num_heads % 
+0000e390: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000e3a0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2021  model_parallel !
+0000e3b0: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+0000e3c0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0000e3d0: 4572 726f 7228 2246 6f72 2027 4d75 6c74  Error("For 'Mult
+0000e3e0: 6948 6561 6441 7474 656e 7469 6f6e 272c  iHeadAttention',
+0000e3f0: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
+0000e400: 626c 6520 276e 756d 5f68 6561 6473 2720  ble 'num_heads' 
+0000e410: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
+0000e420: 6c65 206f 6620 220a 2020 2020 2020 2020  le of ".        
+0000e430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e440: 2020 2020 2020 2020 2022 2770 6172 616c           "'paral
+0000e450: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0000e460: 5f70 6172 616c 6c65 6c27 2c20 6275 7420  _parallel', but 
+0000e470: 676f 7420 7468 6520 6e75 6d5f 6865 6164  got the num_head
+0000e480: 7320 6973 207b 7d20 220a 2020 2020 2020  s is {} ".      
+0000e490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e4a0: 2020 2020 2020 2020 2020 2022 616e 6420             "and 
+0000e4b0: 7468 6520 7061 7261 6c6c 656c 5f63 6f6e  the parallel_con
+0000e4c0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0000e4d0: 656c 2020 6973 207b 7d2e 220a 2020 2020  el  is {}.".    
+0000e4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e4f0: 2020 2020 2020 2020 2020 2020 202e 666f               .fo
+0000e500: 726d 6174 286e 756d 5f68 6561 6473 2c20  rmat(num_heads, 
+0000e510: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000e520: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
+0000e530: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000e540: 662e 6973 5f66 6972 7374 5f69 7465 7261  f.is_first_itera
+0000e550: 7469 6f6e 203d 2054 7275 650a 2020 2020  tion = True.    
+0000e560: 2020 2020 2020 2020 2320 4f75 7470 7574          # Output
+0000e570: 206c 6179 6572 0a20 2020 2020 2020 2020   layer.         
+0000e580: 2020 2073 656c 662e 7072 6f6a 6563 7469     self.projecti
+0000e590: 6f6e 203d 204c 696e 6561 7228 696e 5f63  on = Linear(in_c
+0000e5a0: 6861 6e6e 656c 733d 6869 6464 656e 5f73  hannels=hidden_s
+0000e5b0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+0000e5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e5d0: 2020 2020 2020 2020 2020 6f75 745f 6368            out_ch
+0000e5e0: 616e 6e65 6c73 3d68 6964 6465 6e5f 7369  annels=hidden_si
+0000e5f0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+0000e600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e610: 2020 2020 2020 2020 2074 7261 6e73 706f           transpo
+0000e620: 7365 5f62 3d46 616c 7365 2c0a 2020 2020  se_b=False,.    
+0000e630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e650: 2063 6f6d 7075 7465 5f64 7479 7065 3d63   compute_dtype=c
+0000e660: 6f6d 7075 7465 5f64 7479 7065 2c0a 2020  ompute_dtype,.  
+0000e670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e690: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+0000e6a0: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
+0000e6b0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0000e6c0: 7365 6c66 2e70 726f 6a65 6374 696f 6e2e  self.projection.
+0000e6d0: 7368 6172 6428 7374 7261 7465 6779 5f62  shard(strategy_b
+0000e6e0: 6961 733d 2828 7061 7261 6c6c 656c 5f63  ias=((parallel_c
+0000e6f0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+0000e700: 6c65 6c2c 2031 292c 2028 312c 2929 2c0a  lel, 1), (1,)),.
+0000e710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e730: 2020 7374 7261 7465 6779 5f6d 6174 6d75    strategy_matmu
+0000e740: 6c3d 2828 7061 7261 6c6c 656c 5f63 6f6e  l=((parallel_con
+0000e750: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+0000e760: 6c2c 2070 6172 616c 6c65 6c5f 636f 6e66  l, parallel_conf
+0000e770: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+0000e780: 6c29 2c0a 2020 2020 2020 2020 2020 2020  l),.            
 0000e790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e7a0: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
-0000e7b0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-0000e7c0: 6c2c 2031 2929 290a 2020 2020 2020 2020  l, 1))).        
-0000e7d0: 2020 2020 7365 6c66 2e70 726f 6a65 6374      self.project
-0000e7e0: 696f 6e2e 6269 6173 2e70 6172 616c 6c65  ion.bias.paralle
-0000e7f0: 6c5f 6f70 7469 6d69 7a65 7220 3d20 4661  l_optimizer = Fa
-0000e800: 6c73 650a 2020 2020 2020 2020 2020 2020  lse.            
-0000e810: 7365 6c66 2e74 7261 6e73 706f 7365 203d  self.transpose =
-0000e820: 2050 2e54 7261 6e73 706f 7365 2829 0a20   P.Transpose(). 
-0000e830: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000e840: 6d65 7267 6572 5f68 6561 645f 7472 616e  merger_head_tran
-0000e850: 7370 6f73 6520 3d20 502e 5472 616e 7370  spose = P.Transp
-0000e860: 6f73 6528 290a 2020 2020 2020 2020 2020  ose().          
-0000e870: 2020 7365 6c66 2e72 6573 6861 7065 203d    self.reshape =
-0000e880: 2050 2e52 6573 6861 7065 2829 0a20 2020   P.Reshape().   
-0000e890: 2020 2020 2020 2020 2073 656c 662e 6e5f           self.n_
-0000e8a0: 6865 6164 203d 206e 756d 5f68 6561 6473  head = num_heads
-0000e8b0: 0a20 2020 2020 2020 2020 2020 2023 2065  .            # e
-0000e8c0: 6d62 6564 6469 6e67 2073 697a 6520 7065  mbedding size pe
-0000e8d0: 7220 6865 6164 0a20 2020 2020 2020 2020  r head.         
-0000e8e0: 2020 2073 656c 662e 7369 7a65 5f70 6572     self.size_per
-0000e8f0: 5f68 6561 6420 3d20 6869 6464 656e 5f73  _head = hidden_s
-0000e900: 697a 6520 2f2f 2073 656c 662e 6e5f 6865  ize // self.n_he
-0000e910: 6164 0a20 2020 2020 2020 2020 2020 2073  ad.            s
-0000e920: 656c 662e 636f 6e63 6174 5f6b 203d 2050  elf.concat_k = P
-0000e930: 2e43 6f6e 6361 7428 6178 6973 3d33 290a  .Concat(axis=3).
-0000e940: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000e950: 2e63 6f6e 6361 745f 7620 3d20 502e 436f  .concat_v = P.Co
-0000e960: 6e63 6174 2861 7869 733d 3229 0a20 2020  ncat(axis=2).   
-0000e970: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
-0000e980: 6c74 6970 6c79 5f64 6174 6120 3d20 5465  ltiply_data = Te
-0000e990: 6e73 6f72 285b 0a20 2020 2020 2020 2020  nsor([.         
-0000e9a0: 2020 2020 2020 202d 3130 3030 302e 302c         -10000.0,
-0000e9b0: 0a20 2020 2020 2020 2020 2020 205d 2c20  .            ], 
-0000e9c0: 6474 7970 653d 736f 6674 6d61 785f 636f  dtype=softmax_co
-0000e9d0: 6d70 7574 655f 7479 7065 290a 2020 2020  mpute_type).    
-0000e9e0: 2020 2020 2020 2020 7365 6c66 2e62 6174          self.bat
-0000e9f0: 6368 5f6d 6174 6d75 6c20 3d20 502e 4261  ch_matmul = P.Ba
-0000ea00: 7463 684d 6174 4d75 6c28 290a 2020 2020  tchMatMul().    
-0000ea10: 2020 2020 2020 2020 7365 6c66 2e72 6561          self.rea
-0000ea20: 6c5f 6469 7620 3d20 502e 5265 616c 4469  l_div = P.RealDi
-0000ea30: 7628 290a 2020 2020 2020 2020 2020 2020  v().            
-0000ea40: 7365 6c66 2e73 7562 203d 2050 2e53 7562  self.sub = P.Sub
-0000ea50: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-0000ea60: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
-0000ea70: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000ea80: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
-0000ea90: 0a20 2020 2020 2020 2020 2020 2023 204e  .            # N
-0000eaa0: 6f72 6d61 6c69 7a65 2066 6163 746f 7220  ormalize factor 
-0000eab0: 666f 7220 6174 7465 6e74 696f 6e2c 2073  for attention, s
-0000eac0: 7172 7428 646b 2920 6173 2077 6964 656c  qrt(dk) as widel
-0000ead0: 7920 7573 6564 0a20 2020 2020 2020 2020  y used.         
-0000eae0: 2020 2073 656c 662e 7363 616c 655f 6661     self.scale_fa
-0000eaf0: 6374 6f72 203d 2054 656e 736f 7228 6d61  ctor = Tensor(ma
-0000eb00: 7468 2e73 7172 7428 6d61 7468 2e73 7172  th.sqrt(math.sqr
-0000eb10: 7428 7365 6c66 2e73 697a 655f 7065 725f  t(self.size_per_
-0000eb20: 6865 6164 2929 290a 2020 2020 2020 2020  head))).        
-0000eb30: 2020 2020 7365 6c66 2e75 7365 5f70 6173      self.use_pas
-0000eb40: 7420 3d20 7573 655f 7061 7374 0a20 2020  t = use_past.   
-0000eb50: 2020 2020 2020 2020 2073 656c 662e 6472           self.dr
-0000eb60: 6f70 6f75 7420 3d20 6765 745f 6472 6f70  opout = get_drop
-0000eb70: 6f75 7428 6869 6464 656e 5f64 726f 706f  out(hidden_dropo
-0000eb80: 7574 5f72 6174 6529 0a20 2020 2020 2020  ut_rate).       
-0000eb90: 2020 2020 2073 656c 662e 7072 6f62 5f64       self.prob_d
-0000eba0: 726f 706f 7574 203d 2067 6574 5f64 726f  ropout = get_dro
-0000ebb0: 706f 7574 2861 7474 656e 7469 6f6e 5f64  pout(attention_d
-0000ebc0: 726f 706f 7574 5f72 6174 6529 0a20 2020  ropout_rate).   
-0000ebd0: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
-0000ebe0: 6674 6d61 7820 3d20 6e6e 2e53 6f66 746d  ftmax = nn.Softm
-0000ebf0: 6178 2829 2e74 6f5f 666c 6f61 7428 736f  ax().to_float(so
-0000ec00: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-0000ec10: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-0000ec20: 7365 6c66 2e73 6f66 746d 6178 5f33 6420  self.softmax_3d 
-0000ec30: 3d20 6e6e 2e53 6f66 746d 6178 2829 2e74  = nn.Softmax().t
-0000ec40: 6f5f 666c 6f61 7428 736f 6674 6d61 785f  o_float(softmax_
-0000ec50: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
-0000ec60: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-0000ec70: 6f66 746d 6178 5f63 6173 7420 3d20 502e  oftmax_cast = P.
-0000ec80: 4361 7374 2829 0a20 2020 2020 2020 2020  Cast().         
-0000ec90: 2020 2073 656c 662e 736f 6674 6d61 785f     self.softmax_
-0000eca0: 7265 7368 6170 6520 3d20 502e 5265 7368  reshape = P.Resh
-0000ecb0: 6170 6528 290a 2020 2020 2020 2020 2020  ape().          
-0000ecc0: 2020 7365 6c66 2e65 7870 616e 645f 6469    self.expand_di
-0000ecd0: 6d73 203d 2050 2e45 7870 616e 6444 696d  ms = P.ExpandDim
-0000ece0: 7328 290a 0a20 2020 2020 2020 2020 2020  s()..           
-0000ecf0: 2023 2051 7565 7279 0a20 2020 2020 2020   # Query.       
-0000ed00: 2020 2020 2073 656c 662e 6465 6e73 6531       self.dense1
-0000ed10: 203d 204c 696e 6561 7228 6869 6464 656e   = Linear(hidden
-0000ed20: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-0000ed30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ed40: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-0000ed50: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0000ed60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ed70: 2020 2020 2020 636f 6d70 7574 655f 6474        compute_dt
-0000ed80: 7970 653d 636f 6d70 7574 655f 6474 7970  ype=compute_dtyp
-0000ed90: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0000eda0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000edb0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-0000edc0: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-0000edd0: 7970 6529 0a20 2020 2020 2020 2020 2020  ype).           
-0000ede0: 2023 204b 6579 0a20 2020 2020 2020 2020   # Key.         
-0000edf0: 2020 2073 656c 662e 6465 6e73 6532 203d     self.dense2 =
-0000ee00: 204c 696e 6561 7228 6869 6464 656e 5f73   Linear(hidden_s
-0000ee10: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0000ee20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee30: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-0000ee40: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0000ee50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee60: 2020 2020 636f 6d70 7574 655f 6474 7970      compute_dtyp
-0000ee70: 653d 636f 6d70 7574 655f 6474 7970 652c  e=compute_dtype,
-0000ee80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000ee90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eea0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0000eeb0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-0000eec0: 6529 0a20 2020 2020 2020 2020 2020 2023  e).            #
-0000eed0: 2056 616c 7565 0a20 2020 2020 2020 2020   Value.         
-0000eee0: 2020 2073 656c 662e 6465 6e73 6533 203d     self.dense3 =
-0000eef0: 204c 696e 6561 7228 6869 6464 656e 5f73   Linear(hidden_s
-0000ef00: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0000ef10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ef20: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-0000ef30: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0000ef40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ef50: 2020 2020 636f 6d70 7574 655f 6474 7970      compute_dtyp
-0000ef60: 653d 636f 6d70 7574 655f 6474 7970 652c  e=compute_dtype,
-0000ef70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000ef80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ef90: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0000efa0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-0000efb0: 6529 0a0a 2020 2020 2020 2020 2020 2020  e)..            
-0000efc0: 7365 6c66 2e64 7479 7065 203d 2063 6f6d  self.dtype = com
-0000efd0: 7075 7465 5f64 7479 7065 0a20 2020 2020  pute_dtype.     
-0000efe0: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
-0000eff0: 6d61 785f 6474 7970 6520 3d20 736f 6674  max_dtype = soft
-0000f000: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-0000f010: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0000f020: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-0000f030: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-0000f040: 206f 7065 7261 746f 7273 2075 7365 6420   operators used 
-0000f050: 666f 7220 7374 6174 6520 7265 7573 650a  for state reuse.
-0000f060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f070: 7365 715f 7261 6e67 6520 3d20 6e70 2e61  seq_range = np.a
-0000f080: 7261 6e67 6528 7372 635f 7365 715f 6c65  range(src_seq_le
-0000f090: 6e67 7468 292e 7265 7368 6170 6528 312c  ngth).reshape(1,
-0000f0a0: 2031 2c20 2d31 290a 2020 2020 2020 2020   1, -1).        
-0000f0b0: 2020 2020 2020 2020 7365 6c66 2e72 616e          self.ran
-0000f0c0: 6765 203d 2054 656e 736f 7228 6e70 2e74  ge = Tensor(np.t
-0000f0d0: 696c 6528 7365 715f 7261 6e67 652c 2028  ile(seq_range, (
-0000f0e0: 6261 7463 685f 7369 7a65 2c20 312c 2031  batch_size, 1, 1
-0000f0f0: 2929 2c20 6d73 7479 7065 2e69 6e74 3332  )), mstype.int32
-0000f100: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0000f110: 2020 7365 6c66 2e73 6571 5f6c 656e 6774    self.seq_lengt
-0000f120: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
-0000f130: 7468 0a20 2020 2020 2020 2020 2020 2020  th.             
-0000f140: 2020 2073 656c 662e 6174 7465 6e74 696f     self.attentio
-0000f150: 6e5f 6d61 736b 203d 2054 656e 736f 7228  n_mask = Tensor(
-0000f160: 6e70 2e74 7269 6c28 6e70 2e6f 6e65 7328  np.tril(np.ones(
-0000f170: 7368 6170 653d 2873 656c 662e 7365 715f  shape=(self.seq_
-0000f180: 6c65 6e67 7468 2c20 7365 6c66 2e73 6571  length, self.seq
-0000f190: 5f6c 656e 6774 6829 2929 2c20 6d73 7479  _length))), msty
-0000f1a0: 7065 2e69 6e74 3332 290a 2020 2020 2020  pe.int32).      
-0000f1b0: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-0000f1c0: 6c69 6365 203d 2050 2e53 7472 6964 6564  lice = P.Strided
-0000f1d0: 536c 6963 6528 292e 7368 6172 6428 2828  Slice().shard(((
-0000f1e0: 312c 2031 2c20 312c 2031 292c 2929 0a20  1, 1, 1, 1),)). 
-0000f1f0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0000f200: 656c 662e 6e6f 745f 6571 7561 6c20 3d20  elf.not_equal = 
-0000f210: 502e 4e6f 7445 7175 616c 2829 2e73 6861  P.NotEqual().sha
-0000f220: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
-0000f230: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
-0000f240: 2020 2020 2020 2073 656c 662e 7265 6475         self.redu
-0000f250: 6365 7375 6d20 3d20 502e 5265 6475 6365  cesum = P.Reduce
-0000f260: 5375 6d28 292e 7368 6172 6428 2828 312c  Sum().shard(((1,
-0000f270: 2031 2c20 312c 2031 292c 2929 0a20 2020   1, 1, 1),)).   
-0000f280: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-0000f290: 662e 6578 7061 6e64 5f64 696d 7320 3d20  f.expand_dims = 
-0000f2a0: 502e 4578 7061 6e64 4469 6d73 2829 2e73  P.ExpandDims().s
-0000f2b0: 6861 7264 2828 2831 2c20 312c 2031 292c  hard(((1, 1, 1),
-0000f2c0: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-0000f2d0: 2020 2073 656c 662e 7465 6e73 6f72 5f6c     self.tensor_l
-0000f2e0: 6520 3d20 502e 4c65 7373 4571 7561 6c28  e = P.LessEqual(
-0000f2f0: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-0000f300: 3129 2c20 2831 2c20 312c 2031 2929 290a  1), (1, 1, 1))).
-0000f310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f320: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
-0000f330: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-0000f340: 2031 2c20 3129 2c20 2831 2c20 312c 2031   1, 1), (1, 1, 1
-0000f350: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
-0000f360: 2020 2020 2020 2073 656c 662e 6571 7561         self.equa
-0000f370: 6c20 3d20 502e 4571 7561 6c28 292e 7368  l = P.Equal().sh
-0000f380: 6172 6428 2828 312c 2031 2c20 3129 2c20  ard(((1, 1, 1), 
-0000f390: 2831 2c20 312c 2031 2929 290a 2020 2020  (1, 1, 1))).    
-0000f3a0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000f3b0: 2e73 7562 3120 3d20 502e 5375 6228 292e  .sub1 = P.Sub().
-0000f3c0: 7368 6172 6428 2828 312c 292c 2028 2929  shard(((1,), ())
-0000f3d0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0000f3e0: 2020 7365 6c66 2e74 696c 6520 3d20 502e    self.tile = P.
-0000f3f0: 5469 6c65 2829 2e73 6861 7264 2828 2831  Tile().shard(((1
-0000f400: 2c20 312c 2031 2c20 3129 2c29 290a 2020  , 1, 1, 1),)).  
-0000f410: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0000f420: 6c66 2e6c 6573 7320 3d20 502e 4c65 7373  lf.less = P.Less
-0000f430: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-0000f440: 2031 292c 2028 312c 2031 2c20 3129 2929   1), (1, 1, 1)))
-0000f450: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000f460: 2073 656c 662e 6d75 6c31 203d 2050 2e4d   self.mul1 = P.M
-0000f470: 756c 2829 2e73 6861 7264 2828 2831 2c20  ul().shard(((1, 
-0000f480: 312c 2031 2c20 3129 2c20 2831 2c20 312c  1, 1, 1), (1, 1,
-0000f490: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-0000f4a0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-0000f4b0: 2020 205f 6368 6563 6b5f 636f 6e66 6967     _check_config
-0000f4c0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0000f4d0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000f4e0: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
-0000f4f0: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
-0000f500: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
-0000f510: 656c 662e 7467 745f 7365 715f 6c65 6e67  elf.tgt_seq_leng
-0000f520: 7468 203d 2074 6774 5f73 6571 5f6c 656e  th = tgt_seq_len
-0000f530: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
-0000f540: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
-0000f550: 203d 2068 6964 6465 6e5f 7369 7a65 0a20   = hidden_size. 
-0000f560: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000f570: 6261 7463 685f 7369 7a65 203d 2062 6174  batch_size = bat
-0000f580: 6368 5f73 697a 650a 2020 2020 2020 2020  ch_size.        
-0000f590: 2020 2020 6966 2068 6964 6465 6e5f 6472      if hidden_dr
-0000f5a0: 6f70 6f75 745f 7261 7465 203c 2030 206f  opout_rate < 0 o
-0000f5b0: 7220 6869 6464 656e 5f64 726f 706f 7574  r hidden_dropout
-0000f5c0: 5f72 6174 6520 3e3d 2031 3a0a 2020 2020  _rate >= 1:.    
-0000f5d0: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-0000f5e0: 6520 5661 6c75 6545 7272 6f72 2822 466f  e ValueError("Fo
-0000f5f0: 7220 274d 756c 7469 4865 6164 4174 7465  r 'MultiHeadAtte
-0000f600: 6e74 696f 6e27 2c20 7468 6520 636c 6173  ntion', the clas
-0000f610: 7320 7661 7269 6162 6c65 2027 6869 6464  s variable 'hidd
-0000f620: 656e 5f64 726f 706f 7574 5f72 6174 6527  en_dropout_rate'
-0000f630: 206d 7573 7420 6265 2022 0a20 2020 2020   must be ".     
-0000f640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f650: 2020 2020 2020 2020 2020 2020 2269 6e20              "in 
-0000f660: 7261 6e67 6520 5b30 2c20 312e 3029 2c20  range [0, 1.0), 
-0000f670: 6275 7420 676f 7420 7468 6520 7661 6c75  but got the valu
-0000f680: 6520 3a20 7b7d 2e22 2e66 6f72 6d61 7428  e : {}.".format(
-0000f690: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-0000f6a0: 6174 6529 290a 2020 2020 2020 2020 2020  ate)).          
-0000f6b0: 2020 6966 2061 7474 656e 7469 6f6e 5f64    if attention_d
-0000f6c0: 726f 706f 7574 5f72 6174 6520 3c20 3020  ropout_rate < 0 
-0000f6d0: 6f72 2061 7474 656e 7469 6f6e 5f64 726f  or attention_dro
-0000f6e0: 706f 7574 5f72 6174 6520 3e3d 2031 3a0a  pout_rate >= 1:.
-0000f6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f700: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000f710: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
-0000f720: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
-0000f730: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
-0000f740: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-0000f750: 745f 7261 7465 2720 6d75 7374 2062 6520  t_rate' must be 
-0000f760: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-0000f770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f780: 2020 2022 696e 2072 616e 6765 205b 302c     "in range [0,
-0000f790: 2031 2e30 292c 2062 7574 2067 6f74 2074   1.0), but got t
-0000f7a0: 6865 2076 616c 7565 203a 207b 7d2e 222e  he value : {}.".
-0000f7b0: 666f 726d 6174 2861 7474 656e 7469 6f6e  format(attention
-0000f7c0: 5f64 726f 706f 7574 5f72 6174 6529 290a  _dropout_rate)).
-0000f7d0: 2020 2020 2020 2020 2020 2020 6966 2068              if h
-0000f7e0: 6964 6465 6e5f 7369 7a65 2025 206e 756d  idden_size % num
-0000f7f0: 5f68 6561 6473 2021 3d20 303a 0a20 2020  _heads != 0:.   
-0000f800: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-0000f810: 7365 2056 616c 7565 4572 726f 7228 2246  se ValueError("F
-0000f820: 6f72 2027 4d75 6c74 6948 6561 6441 7474  or 'MultiHeadAtt
-0000f830: 656e 7469 6f6e 272c 2074 6865 2063 6c61  ention', the cla
-0000f840: 7373 2076 6172 6961 626c 6520 2768 6964  ss variable 'hid
-0000f850: 6465 6e5f 7369 7a65 2720 6d75 7374 2062  den_size' must b
-0000f860: 6520 6120 6d75 6c74 6970 6c65 2022 0a20  e a multiple ". 
-0000f870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f890: 226f 6620 276e 756d 5f68 6561 6473 272c  "of 'num_heads',
-0000f8a0: 2062 7574 2067 6f74 2074 6865 2068 6964   but got the hid
-0000f8b0: 6465 6e5f 7369 7a65 2069 7320 7b7d 2061  den_size is {} a
-0000f8c0: 6e64 2074 6865 206e 756d 5f68 6561 6473  nd the num_heads
-0000f8d0: 2069 7320 7b7d 2e22 0a20 2020 2020 2020   is {}.".       
-0000f8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f8f0: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
-0000f900: 7428 6869 6464 656e 5f73 697a 652c 206e  t(hidden_size, n
-0000f910: 756d 5f68 6561 6473 2929 0a20 2020 2020  um_heads)).     
-0000f920: 2020 2020 2020 2069 6620 6e75 6d5f 6865         if num_he
-0000f930: 6164 7320 2520 7061 7261 6c6c 656c 5f63  ads % parallel_c
-0000f940: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-0000f950: 6c6c 656c 2021 3d20 303a 0a20 2020 2020  llel != 0:.     
-0000f960: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-0000f970: 2056 616c 7565 4572 726f 7228 2246 6f72   ValueError("For
-0000f980: 2027 4d75 6c74 6948 6561 6441 7474 656e   'MultiHeadAtten
-0000f990: 7469 6f6e 272c 2074 6865 2063 6c61 7373  tion', the class
-0000f9a0: 2076 6172 6961 626c 6520 276e 756d 5f68   variable 'num_h
-0000f9b0: 6561 6473 2720 6d75 7374 2062 6520 6120  eads' must be a 
-0000f9c0: 6d75 6c74 6970 6c65 206f 6620 220a 2020  multiple of ".  
-0000f9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f9e0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0000f9f0: 2770 6172 616c 6c65 6c5f 636f 6e66 6967  'parallel_config
-0000fa00: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c27  .model_parallel'
-0000fa10: 2c20 6275 7420 676f 7420 7468 6520 6e75  , but got the nu
-0000fa20: 6d5f 6865 6164 7320 6973 207b 7d20 220a  m_heads is {} ".
-0000fa30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa50: 2022 616e 6420 7468 6520 7061 7261 6c6c   "and the parall
-0000fa60: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0000fa70: 7061 7261 6c6c 656c 2020 6973 207b 7d2e  parallel  is {}.
-0000fa80: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-0000fa90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000faa0: 2020 202e 666f 726d 6174 286e 756d 5f68     .format(num_h
-0000fab0: 6561 6473 2c20 7061 7261 6c6c 656c 5f63  eads, parallel_c
-0000fac0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-0000fad0: 6c6c 656c 2929 0a20 2020 2020 2020 2020  llel)).         
-0000fae0: 2020 2073 656c 662e 6973 5f66 6972 7374     self.is_first
-0000faf0: 5f69 7465 7261 7469 6f6e 203d 2054 7275  _iteration = Tru
-0000fb00: 650a 2020 2020 2020 2020 2020 2020 2320  e.            # 
-0000fb10: 4f75 7470 7574 206c 6179 6572 0a20 2020  Output layer.   
-0000fb20: 2020 2020 2020 2020 2073 656c 662e 7072           self.pr
-0000fb30: 6f6a 6563 7469 6f6e 203d 204c 696e 6561  ojection = Linea
-0000fb40: 7228 696e 5f63 6861 6e6e 656c 733d 6869  r(in_channels=hi
-0000fb50: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-0000fb60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e7b0: 2020 2020 2020 2028 7061 7261 6c6c 656c         (parallel
+0000e7c0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+0000e7d0: 7261 6c6c 656c 2c20 3129 2929 0a20 2020  rallel, 1))).   
+0000e7e0: 2020 2020 2020 2020 2073 656c 662e 7072           self.pr
+0000e7f0: 6f6a 6563 7469 6f6e 2e62 6961 732e 7061  ojection.bias.pa
+0000e800: 7261 6c6c 656c 5f6f 7074 696d 697a 6572  rallel_optimizer
+0000e810: 203d 2046 616c 7365 0a20 2020 2020 2020   = False.       
+0000e820: 2020 2020 2073 656c 662e 7472 616e 7370       self.transp
+0000e830: 6f73 6520 3d20 502e 5472 616e 7370 6f73  ose = P.Transpos
+0000e840: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
+0000e850: 7365 6c66 2e6d 6572 6765 725f 6865 6164  self.merger_head
+0000e860: 5f74 7261 6e73 706f 7365 203d 2050 2e54  _transpose = P.T
+0000e870: 7261 6e73 706f 7365 2829 0a20 2020 2020  ranspose().     
+0000e880: 2020 2020 2020 2073 656c 662e 7265 7368         self.resh
+0000e890: 6170 6520 3d20 502e 5265 7368 6170 6528  ape = P.Reshape(
+0000e8a0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000e8b0: 6c66 2e6e 5f68 6561 6420 3d20 6e75 6d5f  lf.n_head = num_
+0000e8c0: 6865 6164 730a 2020 2020 2020 2020 2020  heads.          
+0000e8d0: 2020 2320 656d 6265 6464 696e 6720 7369    # embedding si
+0000e8e0: 7a65 2070 6572 2068 6561 640a 2020 2020  ze per head.    
+0000e8f0: 2020 2020 2020 2020 7365 6c66 2e73 697a          self.siz
+0000e900: 655f 7065 725f 6865 6164 203d 2068 6964  e_per_head = hid
+0000e910: 6465 6e5f 7369 7a65 202f 2f20 7365 6c66  den_size // self
+0000e920: 2e6e 5f68 6561 640a 2020 2020 2020 2020  .n_head.        
+0000e930: 2020 2020 7365 6c66 2e63 6f6e 6361 745f      self.concat_
+0000e940: 6b20 3d20 502e 436f 6e63 6174 2861 7869  k = P.Concat(axi
+0000e950: 733d 3329 0a20 2020 2020 2020 2020 2020  s=3).           
+0000e960: 2073 656c 662e 636f 6e63 6174 5f76 203d   self.concat_v =
+0000e970: 2050 2e43 6f6e 6361 7428 6178 6973 3d32   P.Concat(axis=2
+0000e980: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000e990: 6c66 2e6d 756c 7469 706c 795f 6461 7461  lf.multiply_data
+0000e9a0: 203d 2054 656e 736f 7228 5b0a 2020 2020   = Tensor([.    
+0000e9b0: 2020 2020 2020 2020 2020 2020 2d31 3030              -100
+0000e9c0: 3030 2e30 2c0a 2020 2020 2020 2020 2020  00.0,.          
+0000e9d0: 2020 5d2c 2064 7479 7065 3d73 6f66 746d    ], dtype=softm
+0000e9e0: 6178 5f63 6f6d 7075 7465 5f74 7970 6529  ax_compute_type)
+0000e9f0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000ea00: 662e 6261 7463 685f 6d61 746d 756c 203d  f.batch_matmul =
+0000ea10: 2050 2e42 6174 6368 4d61 744d 756c 2829   P.BatchMatMul()
+0000ea20: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000ea30: 662e 7265 616c 5f64 6976 203d 2050 2e52  f.real_div = P.R
+0000ea40: 6561 6c44 6976 2829 0a20 2020 2020 2020  ealDiv().       
+0000ea50: 2020 2020 2073 656c 662e 7375 6220 3d20       self.sub = 
+0000ea60: 502e 5375 6228 290a 2020 2020 2020 2020  P.Sub().        
+0000ea70: 2020 2020 7365 6c66 2e6d 756c 203d 2050      self.mul = P
+0000ea80: 2e4d 756c 2829 0a20 2020 2020 2020 2020  .Mul().         
+0000ea90: 2020 2073 656c 662e 6164 6420 3d20 502e     self.add = P.
+0000eaa0: 4164 6428 290a 2020 2020 2020 2020 2020  Add().          
+0000eab0: 2020 2320 4e6f 726d 616c 697a 6520 6661    # Normalize fa
+0000eac0: 6374 6f72 2066 6f72 2061 7474 656e 7469  ctor for attenti
+0000ead0: 6f6e 2c20 7371 7274 2864 6b29 2061 7320  on, sqrt(dk) as 
+0000eae0: 7769 6465 6c79 2075 7365 640a 2020 2020  widely used.    
+0000eaf0: 2020 2020 2020 2020 7365 6c66 2e73 6361          self.sca
+0000eb00: 6c65 5f66 6163 746f 7220 3d20 5465 6e73  le_factor = Tens
+0000eb10: 6f72 286d 6174 682e 7371 7274 286d 6174  or(math.sqrt(mat
+0000eb20: 682e 7371 7274 2873 656c 662e 7369 7a65  h.sqrt(self.size
+0000eb30: 5f70 6572 5f68 6561 6429 2929 0a20 2020  _per_head))).   
+0000eb40: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
+0000eb50: 655f 7061 7374 203d 2075 7365 5f70 6173  e_past = use_pas
+0000eb60: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
+0000eb70: 6c66 2e64 726f 706f 7574 203d 2067 6574  lf.dropout = get
+0000eb80: 5f64 726f 706f 7574 2868 6964 6465 6e5f  _dropout(hidden_
+0000eb90: 6472 6f70 6f75 745f 7261 7465 290a 2020  dropout_rate).  
+0000eba0: 2020 2020 2020 2020 2020 7365 6c66 2e70            self.p
+0000ebb0: 726f 625f 6472 6f70 6f75 7420 3d20 6765  rob_dropout = ge
+0000ebc0: 745f 6472 6f70 6f75 7428 6174 7465 6e74  t_dropout(attent
+0000ebd0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0000ebe0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000ebf0: 6c66 2e73 6f66 746d 6178 203d 206e 6e2e  lf.softmax = nn.
+0000ec00: 536f 6674 6d61 7828 292e 746f 5f66 6c6f  Softmax().to_flo
+0000ec10: 6174 2873 6f66 746d 6178 5f63 6f6d 7075  at(softmax_compu
+0000ec20: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
+0000ec30: 2020 2020 2073 656c 662e 736f 6674 6d61       self.softma
+0000ec40: 785f 3364 203d 206e 6e2e 536f 6674 6d61  x_3d = nn.Softma
+0000ec50: 7828 292e 746f 5f66 6c6f 6174 2873 6f66  x().to_float(sof
+0000ec60: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0000ec70: 6529 0a20 2020 2020 2020 2020 2020 2073  e).            s
+0000ec80: 656c 662e 736f 6674 6d61 785f 6361 7374  elf.softmax_cast
+0000ec90: 203d 2050 2e43 6173 7428 290a 2020 2020   = P.Cast().    
+0000eca0: 2020 2020 2020 2020 7365 6c66 2e73 6f66          self.sof
+0000ecb0: 746d 6178 5f72 6573 6861 7065 203d 2050  tmax_reshape = P
+0000ecc0: 2e52 6573 6861 7065 2829 0a20 2020 2020  .Reshape().     
+0000ecd0: 2020 2020 2020 2073 656c 662e 6578 7061         self.expa
+0000ece0: 6e64 5f64 696d 7320 3d20 502e 4578 7061  nd_dims = P.Expa
+0000ecf0: 6e64 4469 6d73 2829 0a0a 2020 2020 2020  ndDims()..      
+0000ed00: 2020 2020 2020 2320 5175 6572 790a 2020        # Query.  
+0000ed10: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
+0000ed20: 656e 7365 3120 3d20 4c69 6e65 6172 2868  ense1 = Linear(h
+0000ed30: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0000ed40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ed50: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+0000ed60: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0000ed70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ed80: 2020 2020 2020 2020 2020 2063 6f6d 7075             compu
+0000ed90: 7465 5f64 7479 7065 3d63 6f6d 7075 7465  te_dtype=compute
+0000eda0: 5f64 7479 7065 2c0a 2020 2020 2020 2020  _dtype,.        
+0000edb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000edc0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+0000edd0: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
+0000ede0: 6e69 745f 7479 7065 290a 2020 2020 2020  nit_type).      
+0000edf0: 2020 2020 2020 2320 4b65 790a 2020 2020        # Key.    
+0000ee00: 2020 2020 2020 2020 7365 6c66 2e64 656e          self.den
+0000ee10: 7365 3220 3d20 4c69 6e65 6172 2868 6964  se2 = Linear(hid
+0000ee20: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0000ee30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ee40: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+0000ee50: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+0000ee60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ee70: 2020 2020 2020 2020 2063 6f6d 7075 7465           compute
+0000ee80: 5f64 7479 7065 3d63 6f6d 7075 7465 5f64  _dtype=compute_d
+0000ee90: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+0000eea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eeb0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+0000eec0: 745f 7479 7065 3d70 6172 616d 5f69 6e69  t_type=param_ini
+0000eed0: 745f 7479 7065 290a 2020 2020 2020 2020  t_type).        
+0000eee0: 2020 2020 2320 5661 6c75 650a 2020 2020      # Value.    
+0000eef0: 2020 2020 2020 2020 7365 6c66 2e64 656e          self.den
+0000ef00: 7365 3320 3d20 4c69 6e65 6172 2868 6964  se3 = Linear(hid
+0000ef10: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0000ef20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ef30: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+0000ef40: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+0000ef50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ef60: 2020 2020 2020 2020 2063 6f6d 7075 7465           compute
+0000ef70: 5f64 7479 7065 3d63 6f6d 7075 7465 5f64  _dtype=compute_d
+0000ef80: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+0000ef90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000efa0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+0000efb0: 745f 7479 7065 3d70 6172 616d 5f69 6e69  t_type=param_ini
+0000efc0: 745f 7479 7065 290a 0a20 2020 2020 2020  t_type)..       
+0000efd0: 2020 2020 2073 656c 662e 6474 7970 6520       self.dtype 
+0000efe0: 3d20 636f 6d70 7574 655f 6474 7970 650a  = compute_dtype.
+0000eff0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000f000: 2e73 6f66 746d 6178 5f64 7479 7065 203d  .softmax_dtype =
+0000f010: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+0000f020: 5f74 7970 650a 2020 2020 2020 2020 2020  _type.          
+0000f030: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
+0000f040: 7374 3a0a 2020 2020 2020 2020 2020 2020  st:.            
+0000f050: 2020 2020 2320 6f70 6572 6174 6f72 7320      # operators 
+0000f060: 7573 6564 2066 6f72 2073 7461 7465 2072  used for state r
+0000f070: 6575 7365 0a20 2020 2020 2020 2020 2020  euse.           
+0000f080: 2020 2020 2073 6571 5f72 616e 6765 203d       seq_range =
+0000f090: 206e 702e 6172 616e 6765 2873 7263 5f73   np.arange(src_s
+0000f0a0: 6571 5f6c 656e 6774 6829 2e72 6573 6861  eq_length).resha
+0000f0b0: 7065 2831 2c20 312c 202d 3129 0a20 2020  pe(1, 1, -1).   
+0000f0c0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0000f0d0: 662e 7261 6e67 6520 3d20 5465 6e73 6f72  f.range = Tensor
+0000f0e0: 286e 702e 7469 6c65 2873 6571 5f72 616e  (np.tile(seq_ran
+0000f0f0: 6765 2c20 2862 6174 6368 5f73 697a 652c  ge, (batch_size,
+0000f100: 2031 2c20 3129 292c 206d 7374 7970 652e   1, 1)), mstype.
+0000f110: 696e 7433 3229 0a20 2020 2020 2020 2020  int32).         
+0000f120: 2020 2020 2020 2073 656c 662e 7365 715f         self.seq_
+0000f130: 6c65 6e67 7468 203d 2073 7263 5f73 6571  length = src_seq
+0000f140: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
+0000f150: 2020 2020 2020 2020 7365 6c66 2e61 7474          self.att
+0000f160: 656e 7469 6f6e 5f6d 6173 6b20 3d20 5465  ention_mask = Te
+0000f170: 6e73 6f72 286e 702e 7472 696c 286e 702e  nsor(np.tril(np.
+0000f180: 6f6e 6573 2873 6861 7065 3d28 7365 6c66  ones(shape=(self
+0000f190: 2e73 6571 5f6c 656e 6774 682c 2073 656c  .seq_length, sel
+0000f1a0: 662e 7365 715f 6c65 6e67 7468 2929 292c  f.seq_length))),
+0000f1b0: 206d 7374 7970 652e 696e 7433 3229 0a20   mstype.int32). 
+0000f1c0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0000f1d0: 656c 662e 736c 6963 6520 3d20 502e 5374  elf.slice = P.St
+0000f1e0: 7269 6465 6453 6c69 6365 2829 2e73 6861  ridedSlice().sha
+0000f1f0: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
+0000f200: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+0000f210: 2020 2020 7365 6c66 2e6e 6f74 5f65 7175      self.not_equ
+0000f220: 616c 203d 2050 2e4e 6f74 4571 7561 6c28  al = P.NotEqual(
+0000f230: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
+0000f240: 312c 2031 292c 2028 2929 290a 2020 2020  1, 1), ())).    
+0000f250: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000f260: 2e72 6564 7563 6573 756d 203d 2050 2e52  .reducesum = P.R
+0000f270: 6564 7563 6553 756d 2829 2e73 6861 7264  educeSum().shard
+0000f280: 2828 2831 2c20 312c 2031 2c20 3129 2c29  (((1, 1, 1, 1),)
+0000f290: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0000f2a0: 2020 7365 6c66 2e65 7870 616e 645f 6469    self.expand_di
+0000f2b0: 6d73 203d 2050 2e45 7870 616e 6444 696d  ms = P.ExpandDim
+0000f2c0: 7328 292e 7368 6172 6428 2828 312c 2031  s().shard(((1, 1
+0000f2d0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0000f2e0: 2020 2020 2020 2020 7365 6c66 2e74 656e          self.ten
+0000f2f0: 736f 725f 6c65 203d 2050 2e4c 6573 7345  sor_le = P.LessE
+0000f300: 7175 616c 2829 2e73 6861 7264 2828 2831  qual().shard(((1
+0000f310: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
+0000f320: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+0000f330: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
+0000f340: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
+0000f350: 312c 2031 2c20 312c 2031 292c 2028 312c  1, 1, 1, 1), (1,
+0000f360: 2031 2c20 312c 2031 2929 290a 2020 2020   1, 1, 1))).    
+0000f370: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000f380: 2e65 7175 616c 203d 2050 2e45 7175 616c  .equal = P.Equal
+0000f390: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+0000f3a0: 2031 292c 2028 312c 2031 2c20 3129 2929   1), (1, 1, 1)))
+0000f3b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000f3c0: 2073 656c 662e 7375 6231 203d 2050 2e53   self.sub1 = P.S
+0000f3d0: 7562 2829 2e73 6861 7264 2828 2831 2c29  ub().shard(((1,)
+0000f3e0: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
+0000f3f0: 2020 2020 2020 2073 656c 662e 7469 6c65         self.tile
+0000f400: 203d 2050 2e54 696c 6528 292e 7368 6172   = P.Tile().shar
+0000f410: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+0000f420: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+0000f430: 2020 2073 656c 662e 6c65 7373 203d 2050     self.less = P
+0000f440: 2e4c 6573 7328 292e 7368 6172 6428 2828  .Less().shard(((
+0000f450: 312c 2031 2c20 3129 2c20 2831 2c20 312c  1, 1, 1), (1, 1,
+0000f460: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
+0000f470: 2020 2020 2020 7365 6c66 2e6d 756c 3120        self.mul1 
+0000f480: 3d20 502e 4d75 6c28 292e 7368 6172 6428  = P.Mul().shard(
+0000f490: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
+0000f4a0: 312c 2031 2c20 312c 2031 2929 290a 2020  1, 1, 1, 1))).  
+0000f4b0: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+0000f4c0: 2020 2020 2020 2020 5f63 6865 636b 5f63          _check_c
+0000f4d0: 6f6e 6669 6728 7061 7261 6c6c 656c 5f63  onfig(parallel_c
+0000f4e0: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+0000f4f0: 2020 2073 656c 662e 7372 635f 7365 715f     self.src_seq_
+0000f500: 6c65 6e67 7468 203d 2073 7263 5f73 6571  length = src_seq
+0000f510: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
+0000f520: 2020 2020 7365 6c66 2e74 6774 5f73 6571      self.tgt_seq
+0000f530: 5f6c 656e 6774 6820 3d20 7467 745f 7365  _length = tgt_se
+0000f540: 715f 6c65 6e67 7468 0a20 2020 2020 2020  q_length.       
+0000f550: 2020 2020 2073 656c 662e 6869 6464 656e       self.hidden
+0000f560: 5f73 697a 6520 3d20 6869 6464 656e 5f73  _size = hidden_s
+0000f570: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
+0000f580: 7365 6c66 2e62 6174 6368 5f73 697a 6520  self.batch_size 
+0000f590: 3d20 6261 7463 685f 7369 7a65 0a20 2020  = batch_size.   
+0000f5a0: 2020 2020 2020 2020 2069 6620 6869 6464           if hidd
+0000f5b0: 656e 5f64 726f 706f 7574 5f72 6174 6520  en_dropout_rate 
+0000f5c0: 3c20 3020 6f72 2068 6964 6465 6e5f 6472  < 0 or hidden_dr
+0000f5d0: 6f70 6f75 745f 7261 7465 203e 3d20 313a  opout_rate >= 1:
+0000f5e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000f5f0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+0000f600: 7228 2246 6f72 2027 4d75 6c74 6948 6561  r("For 'MultiHea
+0000f610: 6441 7474 656e 7469 6f6e 272c 2074 6865  dAttention', the
+0000f620: 2063 6c61 7373 2076 6172 6961 626c 6520   class variable 
+0000f630: 2768 6964 6465 6e5f 6472 6f70 6f75 745f  'hidden_dropout_
+0000f640: 7261 7465 2720 6d75 7374 2062 6520 220a  rate' must be ".
+0000f650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f670: 2022 696e 2072 616e 6765 205b 302c 2031   "in range [0, 1
+0000f680: 2e30 292c 2062 7574 2067 6f74 2074 6865  .0), but got the
+0000f690: 2076 616c 7565 203a 207b 7d2e 222e 666f   value : {}.".fo
+0000f6a0: 726d 6174 2868 6964 6465 6e5f 6472 6f70  rmat(hidden_drop
+0000f6b0: 6f75 745f 7261 7465 2929 0a20 2020 2020  out_rate)).     
+0000f6c0: 2020 2020 2020 2069 6620 6174 7465 6e74         if attent
+0000f6d0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0000f6e0: 203c 2030 206f 7220 6174 7465 6e74 696f   < 0 or attentio
+0000f6f0: 6e5f 6472 6f70 6f75 745f 7261 7465 203e  n_dropout_rate >
+0000f700: 3d20 313a 0a20 2020 2020 2020 2020 2020  = 1:.           
+0000f710: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0000f720: 4572 726f 7228 2246 6f72 2027 4d75 6c74  Error("For 'Mult
+0000f730: 6948 6561 6441 7474 656e 7469 6f6e 272c  iHeadAttention',
+0000f740: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
+0000f750: 626c 6520 2761 7474 656e 7469 6f6e 5f64  ble 'attention_d
+0000f760: 726f 706f 7574 5f72 6174 6527 206d 7573  ropout_rate' mus
+0000f770: 7420 6265 2022 0a20 2020 2020 2020 2020  t be ".         
+0000f780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f790: 2020 2020 2020 2020 2269 6e20 7261 6e67          "in rang
+0000f7a0: 6520 5b30 2c20 312e 3029 2c20 6275 7420  e [0, 1.0), but 
+0000f7b0: 676f 7420 7468 6520 7661 6c75 6520 3a20  got the value : 
+0000f7c0: 7b7d 2e22 2e66 6f72 6d61 7428 6174 7465  {}.".format(atte
+0000f7d0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+0000f7e0: 7465 2929 0a20 2020 2020 2020 2020 2020  te)).           
+0000f7f0: 2069 6620 6869 6464 656e 5f73 697a 6520   if hidden_size 
+0000f800: 2520 6e75 6d5f 6865 6164 7320 213d 2030  % num_heads != 0
+0000f810: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0000f820: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0000f830: 6f72 2822 466f 7220 274d 756c 7469 4865  or("For 'MultiHe
+0000f840: 6164 4174 7465 6e74 696f 6e27 2c20 7468  adAttention', th
+0000f850: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
+0000f860: 2027 6869 6464 656e 5f73 697a 6527 206d   'hidden_size' m
+0000f870: 7573 7420 6265 2061 206d 756c 7469 706c  ust be a multipl
+0000f880: 6520 220a 2020 2020 2020 2020 2020 2020  e ".            
+0000f890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f8a0: 2020 2020 2022 6f66 2027 6e75 6d5f 6865       "of 'num_he
+0000f8b0: 6164 7327 2c20 6275 7420 676f 7420 7468  ads', but got th
+0000f8c0: 6520 6869 6464 656e 5f73 697a 6520 6973  e hidden_size is
+0000f8d0: 207b 7d20 616e 6420 7468 6520 6e75 6d5f   {} and the num_
+0000f8e0: 6865 6164 7320 6973 207b 7d2e 220a 2020  heads is {}.".  
+0000f8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f900: 2020 2020 2020 2020 2020 2020 2020 202e                 .
+0000f910: 666f 726d 6174 2868 6964 6465 6e5f 7369  format(hidden_si
+0000f920: 7a65 2c20 6e75 6d5f 6865 6164 7329 290a  ze, num_heads)).
+0000f930: 2020 2020 2020 2020 2020 2020 6966 206e              if n
+0000f940: 756d 5f68 6561 6473 2025 2070 6172 616c  um_heads % paral
+0000f950: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0000f960: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
+0000f970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f980: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0000f990: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
+0000f9a0: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
+0000f9b0: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
+0000f9c0: 6e75 6d5f 6865 6164 7327 206d 7573 7420  num_heads' must 
+0000f9d0: 6265 2061 206d 756c 7469 706c 6520 6f66  be a multiple of
+0000f9e0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
+0000f9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fa00: 2020 2020 2227 7061 7261 6c6c 656c 5f63      "'parallel_c
+0000fa10: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0000fa20: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
+0000fa30: 6865 206e 756d 5f68 6561 6473 2069 7320  he num_heads is 
+0000fa40: 7b7d 2022 0a20 2020 2020 2020 2020 2020  {} ".           
+0000fa50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fa60: 2020 2020 2020 2261 6e64 2074 6865 2070        "and the p
+0000fa70: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+0000fa80: 6f64 656c 5f70 6172 616c 6c65 6c20 2069  odel_parallel  i
+0000fa90: 7320 7b7d 2e22 0a20 2020 2020 2020 2020  s {}.".         
+0000faa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fab0: 2020 2020 2020 2020 2e66 6f72 6d61 7428          .format(
+0000fac0: 6e75 6d5f 6865 6164 732c 2070 6172 616c  num_heads, paral
+0000fad0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0000fae0: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
+0000faf0: 2020 2020 2020 2020 7365 6c66 2e69 735f          self.is_
+0000fb00: 6669 7273 745f 6974 6572 6174 696f 6e20  first_iteration 
+0000fb10: 3d20 5472 7565 0a20 2020 2020 2020 2020  = True.         
+0000fb20: 2020 2023 204f 7574 7075 7420 6c61 7965     # Output laye
+0000fb30: 720a 2020 2020 2020 2020 2020 2020 7365  r.            se
+0000fb40: 6c66 2e70 726f 6a65 6374 696f 6e20 3d20  lf.projection = 
+0000fb50: 4c69 6e65 6172 2869 6e5f 6368 616e 6e65  Linear(in_channe
+0000fb60: 6c73 3d68 6964 6465 6e5f 7369 7a65 2c0a  ls=hidden_size,.
 0000fb70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fb80: 6f75 745f 6368 616e 6e65 6c73 3d68 6964  out_channels=hid
-0000fb90: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
-0000fba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fbb0: 2020 2020 2020 2020 2020 2020 2020 2074                 t
-0000fbc0: 7261 6e73 706f 7365 5f62 3d46 616c 7365  ranspose_b=False
-0000fbd0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000fbe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fbf0: 2020 2020 2020 2063 6f6d 7075 7465 5f64         compute_d
-0000fc00: 7479 7065 3d63 6f6d 7075 7465 5f64 7479  type=compute_dty
-0000fc10: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
-0000fc20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fc30: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-0000fc40: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-0000fc50: 6e69 745f 7479 7065 290a 2020 2020 2020  nit_type).      
-0000fc60: 2020 2020 2020 7365 6c66 2e70 726f 6a65        self.proje
-0000fc70: 6374 696f 6e2e 7368 6172 6428 7374 7261  ction.shard(stra
-0000fc80: 7465 6779 5f62 6961 733d 2828 7061 7261  tegy_bias=((para
-0000fc90: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-0000fca0: 5f70 6172 616c 6c65 6c2c 2031 292c 2028  _parallel, 1), (
-0000fcb0: 312c 2929 2c0a 2020 2020 2020 2020 2020  1,)),.          
-0000fcc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fcd0: 2020 2020 2020 2020 7374 7261 7465 6779          strategy
-0000fce0: 5f6d 6174 6d75 6c3d 2828 7061 7261 6c6c  _matmul=((parall
-0000fcf0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-0000fd00: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
-0000fd10: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0000fd20: 6172 616c 6c65 6c29 2c0a 2020 2020 2020  arallel),.      
-0000fd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fb80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fb90: 2020 2020 206f 7574 5f63 6861 6e6e 656c       out_channel
+0000fba0: 733d 6869 6464 656e 5f73 697a 652c 0a20  s=hidden_size,. 
+0000fbb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fbd0: 2020 2020 7472 616e 7370 6f73 655f 623d      transpose_b=
+0000fbe0: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
+0000fbf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fc00: 2020 2020 2020 2020 2020 2020 636f 6d70              comp
+0000fc10: 7574 655f 6474 7970 653d 636f 6d70 7574  ute_dtype=comput
+0000fc20: 655f 6474 7970 652c 0a20 2020 2020 2020  e_dtype,.       
+0000fc30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fc40: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0000fc50: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
+0000fc60: 7261 6d5f 696e 6974 5f74 7970 6529 0a20  ram_init_type). 
+0000fc70: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000fc80: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
+0000fc90: 2873 7472 6174 6567 795f 6269 6173 3d28  (strategy_bias=(
+0000fca0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+0000fcb0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+0000fcc0: 3129 2c20 2831 2c29 292c 0a20 2020 2020  1), (1,)),.     
+0000fcd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fce0: 2020 2020 2020 2020 2020 2020 2073 7472               str
+0000fcf0: 6174 6567 795f 6d61 746d 756c 3d28 2870  ategy_matmul=((p
+0000fd00: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+0000fd10: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
+0000fd20: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+0000fd30: 6465 6c5f 7061 7261 6c6c 656c 292c 0a20  del_parallel),. 
 0000fd40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fd50: 2020 2020 2020 2020 2020 2020 2028 7061               (pa
-0000fd60: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0000fd70: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
-0000fd80: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0000fd90: 656c 662e 7072 6f6a 6563 7469 6f6e 2e62  elf.projection.b
-0000fda0: 6961 732e 7061 7261 6c6c 656c 5f6f 7074  ias.parallel_opt
-0000fdb0: 696d 697a 6572 203d 2046 616c 7365 0a20  imizer = False. 
-0000fdc0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000fdd0: 7472 616e 7370 6f73 6520 3d20 502e 5472  transpose = P.Tr
-0000fde0: 616e 7370 6f73 6528 292e 7368 6172 6428  anspose().shard(
-0000fdf0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000fe00: 2028 2870 6172 616c 6c65 6c5f 636f 6e66   ((parallel_conf
-0000fe10: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-0000fe20: 2c20 312c 2070 6172 616c 6c65 6c5f 636f  , 1, parallel_co
-0000fe30: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-0000fe40: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
-0000fe50: 2020 2020 2020 2073 656c 662e 6d65 7267         self.merg
-0000fe60: 6572 5f68 6561 645f 7472 616e 7370 6f73  er_head_transpos
-0000fe70: 6520 3d20 502e 5472 616e 7370 6f73 6528  e = P.Transpose(
-0000fe80: 292e 7368 6172 6428 0a20 2020 2020 2020  ).shard(.       
-0000fe90: 2020 2020 2020 2020 2028 2870 6172 616c           ((paral
-0000fea0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-0000feb0: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
-0000fec0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0000fed0: 7061 7261 6c6c 656c 2c20 312c 2031 292c  parallel, 1, 1),
-0000fee0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0000fef0: 656c 662e 7265 7368 6170 6520 3d20 502e  elf.reshape = P.
-0000ff00: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
-0000ff10: 2020 2020 2020 7365 6c66 2e6e 5f68 6561        self.n_hea
-0000ff20: 6420 3d20 6e75 6d5f 6865 6164 730a 2020  d = num_heads.  
-0000ff30: 2020 2020 2020 2020 2020 2320 656d 6265            # embe
-0000ff40: 6464 696e 6720 7369 7a65 2070 6572 2068  dding size per h
-0000ff50: 6561 640a 2020 2020 2020 2020 2020 2020  ead.            
-0000ff60: 7365 6c66 2e73 697a 655f 7065 725f 6865  self.size_per_he
-0000ff70: 6164 203d 2068 6964 6465 6e5f 7369 7a65  ad = hidden_size
-0000ff80: 202f 2f20 7365 6c66 2e6e 5f68 6561 640a   // self.n_head.
-0000ff90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000ffa0: 2e63 6f6e 6361 745f 6b20 3d20 502e 436f  .concat_k = P.Co
-0000ffb0: 6e63 6174 2861 7869 733d 3329 0a20 2020  ncat(axis=3).   
-0000ffc0: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
-0000ffd0: 6e63 6174 5f76 203d 2050 2e43 6f6e 6361  ncat_v = P.Conca
-0000ffe0: 7428 6178 6973 3d32 290a 2020 2020 2020  t(axis=2).      
-0000fff0: 2020 2020 2020 7365 6c66 2e6d 756c 7469        self.multi
-00010000: 706c 795f 6461 7461 203d 2054 656e 736f  ply_data = Tenso
-00010010: 7228 5b0a 2020 2020 2020 2020 2020 2020  r([.            
-00010020: 2020 2020 2d31 3030 3030 2e30 2c0a 2020      -10000.0,.  
-00010030: 2020 2020 2020 2020 2020 5d2c 2064 7479            ], dty
-00010040: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
-00010050: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
-00010060: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
-00010070: 6d61 746d 756c 203d 2050 2e42 6174 6368  matmul = P.Batch
-00010080: 4d61 744d 756c 2829 2e73 6861 7264 280a  MatMul().shard(.
-00010090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000100a0: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-000100b0: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-000100c0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-000100d0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-000100e0: 2031 2c20 3129 2c0a 2020 2020 2020 2020   1, 1),.        
-000100f0: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
-00010100: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-00010110: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
-00010120: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00010130: 6172 616c 6c65 6c2c 2031 2c20 3129 2929  arallel, 1, 1)))
-00010140: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00010150: 662e 7265 616c 5f64 6976 203d 2050 2e52  f.real_div = P.R
-00010160: 6561 6c44 6976 2829 2e73 6861 7264 280a  ealDiv().shard(.
-00010170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010180: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00010190: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-000101a0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-000101b0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-000101c0: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
-000101d0: 2020 2020 2020 2020 2073 656c 662e 7375           self.su
-000101e0: 6220 3d20 502e 5375 6228 292e 7368 6172  b = P.Sub().shar
-000101f0: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
-00010200: 2020 2028 2831 2c29 2c20 2870 6172 616c     ((1,), (paral
-00010210: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00010220: 7061 7261 6c6c 656c 2c20 312c 2031 2c20  parallel, 1, 1, 
-00010230: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
-00010240: 2073 656c 662e 6d75 6c20 3d20 502e 4d75   self.mul = P.Mu
-00010250: 6c28 292e 7368 6172 6428 0a20 2020 2020  l().shard(.     
-00010260: 2020 2020 2020 2020 2020 2028 2870 6172             ((par
-00010270: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00010280: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
-00010290: 2c20 3129 2c20 2831 2c29 2929 0a20 2020  , 1), (1,))).   
-000102a0: 2020 2020 2020 2020 2073 656c 662e 6164           self.ad
-000102b0: 6420 3d20 502e 4164 6428 292e 7368 6172  d = P.Add().shar
-000102c0: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
-000102d0: 2020 2028 2870 6172 616c 6c65 6c5f 636f     ((parallel_co
-000102e0: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-000102f0: 656c 2c20 312c 2031 2c20 3129 2c0a 2020  el, 1, 1, 1),.  
-00010300: 2020 2020 2020 2020 2020 2020 2020 2028                 (
-00010310: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00010320: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
-00010330: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00010340: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-00010350: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
-00010360: 2020 2023 204e 6f72 6d61 6c69 7a65 2066     # Normalize f
-00010370: 6163 746f 7220 666f 7220 6174 7465 6e74  actor for attent
-00010380: 696f 6e2c 2073 7172 7428 646b 2920 6173  ion, sqrt(dk) as
-00010390: 2077 6964 656c 7920 7573 6564 0a20 2020   widely used.   
-000103a0: 2020 2020 2020 2020 2073 656c 662e 7363           self.sc
-000103b0: 616c 655f 6661 6374 6f72 203d 2054 656e  ale_factor = Ten
-000103c0: 736f 7228 6d61 7468 2e73 7172 7428 6d61  sor(math.sqrt(ma
-000103d0: 7468 2e73 7172 7428 7365 6c66 2e73 697a  th.sqrt(self.siz
-000103e0: 655f 7065 725f 6865 6164 2929 290a 2020  e_per_head))).  
-000103f0: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
-00010400: 7365 5f70 6173 7420 3d20 7573 655f 7061  se_past = use_pa
-00010410: 7374 0a20 2020 2020 2020 2020 2020 2073  st.            s
-00010420: 656c 662e 6472 6f70 6f75 7420 3d20 6765  elf.dropout = ge
-00010430: 745f 6472 6f70 6f75 7428 6869 6464 656e  t_dropout(hidden
-00010440: 5f64 726f 706f 7574 5f72 6174 6529 0a20  _dropout_rate). 
-00010450: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00010460: 7072 6f62 5f64 726f 706f 7574 203d 2067  prob_dropout = g
-00010470: 6574 5f64 726f 706f 7574 2861 7474 656e  et_dropout(atten
-00010480: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00010490: 6529 0a20 2020 2020 2020 2020 2020 2073  e).            s
-000104a0: 656c 662e 6472 6f70 6f75 742e 6472 6f70  elf.dropout.drop
-000104b0: 6f75 742e 7368 6172 6428 2828 7061 7261  out.shard(((para
-000104c0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-000104d0: 5f70 6172 616c 6c65 6c2c 2031 292c 2929  _parallel, 1),))
-000104e0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000104f0: 662e 7072 6f62 5f64 726f 706f 7574 2e64  f.prob_dropout.d
-00010500: 726f 706f 7574 2e73 6861 7264 280a 2020  ropout.shard(.  
-00010510: 2020 2020 2020 2020 2020 2020 2020 2828                ((
-00010520: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00010530: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
-00010540: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00010550: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-00010560: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-00010570: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
-00010580: 203d 206e 6e2e 536f 6674 6d61 7828 292e   = nn.Softmax().
-00010590: 746f 5f66 6c6f 6174 2873 6f66 746d 6178  to_float(softmax
-000105a0: 5f63 6f6d 7075 7465 5f74 7970 6529 0a20  _compute_type). 
-000105b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000105c0: 736f 6674 6d61 782e 736f 6674 6d61 782e  softmax.softmax.
-000105d0: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
-000105e0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-000105f0: 616c 6c65 6c2c 2070 6172 616c 6c65 6c5f  allel, parallel_
-00010600: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-00010610: 616c 6c65 6c2c 2031 2c20 3129 2c29 290a  allel, 1, 1),)).
-00010620: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00010630: 2e73 6f66 746d 6178 5f33 6420 3d20 6e6e  .softmax_3d = nn
-00010640: 2e53 6f66 746d 6178 2829 2e74 6f5f 666c  .Softmax().to_fl
-00010650: 6f61 7428 736f 6674 6d61 785f 636f 6d70  oat(softmax_comp
-00010660: 7574 655f 7479 7065 290a 2020 2020 2020  ute_type).      
-00010670: 2020 2020 2020 7365 6c66 2e73 6f66 746d        self.softm
-00010680: 6178 5f33 642e 736f 6674 6d61 782e 7368  ax_3d.softmax.sh
-00010690: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
-000106a0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-000106b0: 6c65 6c2c 2070 6172 616c 6c65 6c5f 636f  lel, parallel_co
-000106c0: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-000106d0: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
-000106e0: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
-000106f0: 6d61 785f 6361 7374 203d 2050 2e43 6173  max_cast = P.Cas
-00010700: 7428 290a 2020 2020 2020 2020 2020 2020  t().            
-00010710: 7365 6c66 2e73 6f66 746d 6178 5f72 6573  self.softmax_res
-00010720: 6861 7065 203d 2050 2e52 6573 6861 7065  hape = P.Reshape
-00010730: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-00010740: 656c 662e 6578 7061 6e64 5f64 696d 7320  elf.expand_dims 
-00010750: 3d20 502e 4578 7061 6e64 4469 6d73 2829  = P.ExpandDims()
-00010760: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
-00010770: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00010780: 7261 6c6c 656c 2c20 312c 2031 292c 2929  rallel, 1, 1),))
-00010790: 0a0a 2020 2020 2020 2020 2020 2020 2320  ..            # 
-000107a0: 5175 6572 790a 2020 2020 2020 2020 2020  Query.          
-000107b0: 2020 7365 6c66 2e64 656e 7365 3120 3d20    self.dense1 = 
-000107c0: 4c69 6e65 6172 2868 6964 6465 6e5f 7369  Linear(hidden_si
-000107d0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-000107e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000107f0: 2020 2020 2068 6964 6465 6e5f 7369 7a65       hidden_size
-00010800: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00010810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010820: 2020 2063 6f6d 7075 7465 5f64 7479 7065     compute_dtype
-00010830: 3d63 6f6d 7075 7465 5f64 7479 7065 2c0a  =compute_dtype,.
-00010840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010850: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010860: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00010870: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00010880: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00010890: 6c66 2e64 656e 7365 312e 7368 6172 6428  lf.dense1.shard(
-000108a0: 7374 7261 7465 6779 5f6d 6174 6d75 6c3d  strategy_matmul=
-000108b0: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-000108c0: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-000108d0: 2031 292c 0a20 2020 2020 2020 2020 2020   1),.           
-000108e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fd60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fd70: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
+0000fd80: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+0000fd90: 6c2c 2031 2929 290a 2020 2020 2020 2020  l, 1))).        
+0000fda0: 2020 2020 7365 6c66 2e70 726f 6a65 6374      self.project
+0000fdb0: 696f 6e2e 6269 6173 2e70 6172 616c 6c65  ion.bias.paralle
+0000fdc0: 6c5f 6f70 7469 6d69 7a65 7220 3d20 4661  l_optimizer = Fa
+0000fdd0: 6c73 650a 2020 2020 2020 2020 2020 2020  lse.            
+0000fde0: 7365 6c66 2e74 7261 6e73 706f 7365 203d  self.transpose =
+0000fdf0: 2050 2e54 7261 6e73 706f 7365 2829 2e73   P.Transpose().s
+0000fe00: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
+0000fe10: 2020 2020 2020 2828 7061 7261 6c6c 656c        ((parallel
+0000fe20: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+0000fe30: 616c 6c65 6c2c 2031 2c20 7061 7261 6c6c  allel, 1, parall
+0000fe40: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+0000fe50: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
+0000fe60: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000fe70: 2e6d 6572 6765 725f 6865 6164 5f74 7261  .merger_head_tra
+0000fe80: 6e73 706f 7365 203d 2050 2e54 7261 6e73  nspose = P.Trans
+0000fe90: 706f 7365 2829 2e73 6861 7264 280a 2020  pose().shard(.  
+0000fea0: 2020 2020 2020 2020 2020 2020 2020 2828                ((
+0000feb0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000fec0: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
+0000fed0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+0000fee0: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
+0000fef0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0000ff00: 2020 2020 7365 6c66 2e72 6573 6861 7065      self.reshape
+0000ff10: 203d 2050 2e52 6573 6861 7065 2829 0a20   = P.Reshape(). 
+0000ff20: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000ff30: 6e5f 6865 6164 203d 206e 756d 5f68 6561  n_head = num_hea
+0000ff40: 6473 0a20 2020 2020 2020 2020 2020 2023  ds.            #
+0000ff50: 2065 6d62 6564 6469 6e67 2073 697a 6520   embedding size 
+0000ff60: 7065 7220 6865 6164 0a20 2020 2020 2020  per head.       
+0000ff70: 2020 2020 2073 656c 662e 7369 7a65 5f70       self.size_p
+0000ff80: 6572 5f68 6561 6420 3d20 6869 6464 656e  er_head = hidden
+0000ff90: 5f73 697a 6520 2f2f 2073 656c 662e 6e5f  _size // self.n_
+0000ffa0: 6865 6164 0a20 2020 2020 2020 2020 2020  head.           
+0000ffb0: 2073 656c 662e 636f 6e63 6174 5f6b 203d   self.concat_k =
+0000ffc0: 2050 2e43 6f6e 6361 7428 6178 6973 3d33   P.Concat(axis=3
+0000ffd0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000ffe0: 6c66 2e63 6f6e 6361 745f 7620 3d20 502e  lf.concat_v = P.
+0000fff0: 436f 6e63 6174 2861 7869 733d 3229 0a20  Concat(axis=2). 
+00010000: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00010010: 6d75 6c74 6970 6c79 5f64 6174 6120 3d20  multiply_data = 
+00010020: 5465 6e73 6f72 285b 0a20 2020 2020 2020  Tensor([.       
+00010030: 2020 2020 2020 2020 202d 3130 3030 302e           -10000.
+00010040: 302c 0a20 2020 2020 2020 2020 2020 205d  0,.            ]
+00010050: 2c20 6474 7970 653d 736f 6674 6d61 785f  , dtype=softmax_
+00010060: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
+00010070: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
+00010080: 6174 6368 5f6d 6174 6d75 6c20 3d20 502e  atch_matmul = P.
+00010090: 4261 7463 684d 6174 4d75 6c28 292e 7368  BatchMatMul().sh
+000100a0: 6172 6428 0a20 2020 2020 2020 2020 2020  ard(.           
+000100b0: 2020 2020 2028 2870 6172 616c 6c65 6c5f       ((parallel_
+000100c0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+000100d0: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+000100e0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+000100f0: 6c6c 656c 2c20 312c 2031 292c 0a20 2020  llel, 1, 1),.   
+00010100: 2020 2020 2020 2020 2020 2020 2020 2870                (p
+00010110: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00010120: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
+00010130: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00010140: 6465 6c5f 7061 7261 6c6c 656c 2c20 312c  del_parallel, 1,
+00010150: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
+00010160: 2020 7365 6c66 2e72 6561 6c5f 6469 7620    self.real_div 
+00010170: 3d20 502e 5265 616c 4469 7628 292e 7368  = P.RealDiv().sh
+00010180: 6172 6428 0a20 2020 2020 2020 2020 2020  ard(.           
+00010190: 2020 2020 2028 2870 6172 616c 6c65 6c5f       ((parallel_
+000101a0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+000101b0: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+000101c0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+000101d0: 6c6c 656c 2c20 312c 2031 292c 2028 2929  llel, 1, 1), ())
+000101e0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+000101f0: 6c66 2e73 7562 203d 2050 2e53 7562 2829  lf.sub = P.Sub()
+00010200: 2e73 6861 7264 280a 2020 2020 2020 2020  .shard(.        
+00010210: 2020 2020 2020 2020 2828 312c 292c 2028          ((1,), (
+00010220: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00010230: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
+00010240: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
+00010250: 2020 2020 2020 7365 6c66 2e6d 756c 203d        self.mul =
+00010260: 2050 2e4d 756c 2829 2e73 6861 7264 280a   P.Mul().shard(.
+00010270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010280: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+00010290: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
+000102a0: 2031 2c20 312c 2031 292c 2028 312c 2929   1, 1, 1), (1,))
+000102b0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+000102c0: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
+000102d0: 2e73 6861 7264 280a 2020 2020 2020 2020  .shard(.        
+000102e0: 2020 2020 2020 2020 2828 7061 7261 6c6c          ((parall
+000102f0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+00010300: 6172 616c 6c65 6c2c 2031 2c20 312c 2031  arallel, 1, 1, 1
+00010310: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00010320: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
+00010330: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00010340: 656c 2c20 7061 7261 6c6c 656c 5f63 6f6e  el, parallel_con
+00010350: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+00010360: 656c 2c20 312c 2031 2929 290a 2020 2020  el, 1, 1))).    
+00010370: 2020 2020 2020 2020 2320 4e6f 726d 616c          # Normal
+00010380: 697a 6520 6661 6374 6f72 2066 6f72 2061  ize factor for a
+00010390: 7474 656e 7469 6f6e 2c20 7371 7274 2864  ttention, sqrt(d
+000103a0: 6b29 2061 7320 7769 6465 6c79 2075 7365  k) as widely use
+000103b0: 640a 2020 2020 2020 2020 2020 2020 7365  d.            se
+000103c0: 6c66 2e73 6361 6c65 5f66 6163 746f 7220  lf.scale_factor 
+000103d0: 3d20 5465 6e73 6f72 286d 6174 682e 7371  = Tensor(math.sq
+000103e0: 7274 286d 6174 682e 7371 7274 2873 656c  rt(math.sqrt(sel
+000103f0: 662e 7369 7a65 5f70 6572 5f68 6561 6429  f.size_per_head)
+00010400: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+00010410: 656c 662e 7573 655f 7061 7374 203d 2075  elf.use_past = u
+00010420: 7365 5f70 6173 740a 2020 2020 2020 2020  se_past.        
+00010430: 2020 2020 7365 6c66 2e64 726f 706f 7574      self.dropout
+00010440: 203d 2067 6574 5f64 726f 706f 7574 2868   = get_dropout(h
+00010450: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+00010460: 7465 290a 2020 2020 2020 2020 2020 2020  te).            
+00010470: 7365 6c66 2e70 726f 625f 6472 6f70 6f75  self.prob_dropou
+00010480: 7420 3d20 6765 745f 6472 6f70 6f75 7428  t = get_dropout(
+00010490: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+000104a0: 745f 7261 7465 290a 2020 2020 2020 2020  t_rate).        
+000104b0: 2020 2020 7365 6c66 2e64 726f 706f 7574      self.dropout
+000104c0: 2e64 726f 706f 7574 2e73 6861 7264 2828  .dropout.shard((
+000104d0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+000104e0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+000104f0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+00010500: 2020 7365 6c66 2e70 726f 625f 6472 6f70    self.prob_drop
+00010510: 6f75 742e 6472 6f70 6f75 742e 7368 6172  out.dropout.shar
+00010520: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
+00010530: 2020 2028 2870 6172 616c 6c65 6c5f 636f     ((parallel_co
+00010540: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00010550: 656c 2c20 7061 7261 6c6c 656c 5f63 6f6e  el, parallel_con
+00010560: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+00010570: 656c 2c20 312c 2031 292c 2929 0a20 2020  el, 1, 1),)).   
+00010580: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
+00010590: 6674 6d61 7820 3d20 6e6e 2e53 6f66 746d  ftmax = nn.Softm
+000105a0: 6178 2829 2e74 6f5f 666c 6f61 7428 736f  ax().to_float(so
+000105b0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+000105c0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+000105d0: 7365 6c66 2e73 6f66 746d 6178 2e73 6f66  self.softmax.sof
+000105e0: 746d 6178 2e73 6861 7264 2828 2870 6172  tmax.shard(((par
+000105f0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00010600: 615f 7061 7261 6c6c 656c 2c20 7061 7261  a_parallel, para
+00010610: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+00010620: 6c5f 7061 7261 6c6c 656c 2c20 312c 2031  l_parallel, 1, 1
+00010630: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00010640: 2073 656c 662e 736f 6674 6d61 785f 3364   self.softmax_3d
+00010650: 203d 206e 6e2e 536f 6674 6d61 7828 292e   = nn.Softmax().
+00010660: 746f 5f66 6c6f 6174 2873 6f66 746d 6178  to_float(softmax
+00010670: 5f63 6f6d 7075 7465 5f74 7970 6529 0a20  _compute_type). 
+00010680: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00010690: 736f 6674 6d61 785f 3364 2e73 6f66 746d  softmax_3d.softm
+000106a0: 6178 2e73 6861 7264 2828 2870 6172 616c  ax.shard(((paral
+000106b0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+000106c0: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
+000106d0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+000106e0: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
+000106f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00010700: 2e73 6f66 746d 6178 5f63 6173 7420 3d20  .softmax_cast = 
+00010710: 502e 4361 7374 2829 0a20 2020 2020 2020  P.Cast().       
+00010720: 2020 2020 2073 656c 662e 736f 6674 6d61       self.softma
+00010730: 785f 7265 7368 6170 6520 3d20 502e 5265  x_reshape = P.Re
+00010740: 7368 6170 6528 290a 2020 2020 2020 2020  shape().        
+00010750: 2020 2020 7365 6c66 2e65 7870 616e 645f      self.expand_
+00010760: 6469 6d73 203d 2050 2e45 7870 616e 6444  dims = P.ExpandD
+00010770: 696d 7328 292e 7368 6172 6428 2828 7061  ims().shard(((pa
+00010780: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+00010790: 7461 5f70 6172 616c 6c65 6c2c 2031 2c20  ta_parallel, 1, 
+000107a0: 3129 2c29 290a 0a20 2020 2020 2020 2020  1),))..         
+000107b0: 2020 2023 2051 7565 7279 0a20 2020 2020     # Query.     
+000107c0: 2020 2020 2020 2073 656c 662e 6465 6e73         self.dens
+000107d0: 6531 203d 204c 696e 6561 7228 6869 6464  e1 = Linear(hidd
+000107e0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+000107f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010800: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00010810: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+00010820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010830: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
+00010840: 6474 7970 653d 636f 6d70 7574 655f 6474  dtype=compute_dt
+00010850: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+00010860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010870: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+00010880: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+00010890: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
+000108a0: 2020 2073 656c 662e 6465 6e73 6531 2e73     self.dense1.s
+000108b0: 6861 7264 2873 7472 6174 6567 795f 6d61  hard(strategy_ma
+000108c0: 746d 756c 3d28 2870 6172 616c 6c65 6c5f  tmul=((parallel_
+000108d0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+000108e0: 6c6c 656c 2c20 3129 2c0a 2020 2020 2020  llel, 1),.      
 000108f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010900: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
-00010910: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00010920: 6c65 6c2c 2031 2929 2c0a 2020 2020 2020  lel, 1)),.      
-00010930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010940: 2020 2020 2020 2020 7374 7261 7465 6779          strategy
-00010950: 5f62 6961 733d 2828 7061 7261 6c6c 656c  _bias=((parallel
-00010960: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-00010970: 616c 6c65 6c2c 2070 6172 616c 6c65 6c5f  allel, parallel_
-00010980: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-00010990: 616c 6c65 6c29 2c0a 2020 2020 2020 2020  allel),.        
-000109a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010910: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
+00010920: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00010930: 7061 7261 6c6c 656c 2c20 3129 292c 0a20  parallel, 1)),. 
+00010940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010950: 2020 2020 2020 2020 2020 2020 2073 7472               str
+00010960: 6174 6567 795f 6269 6173 3d28 2870 6172  ategy_bias=((par
+00010970: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00010980: 615f 7061 7261 6c6c 656c 2c20 7061 7261  a_parallel, para
+00010990: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+000109a0: 6c5f 7061 7261 6c6c 656c 292c 0a20 2020  l_parallel),.   
 000109b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000109c0: 2020 2020 2028 7061 7261 6c6c 656c 5f63       (parallel_c
-000109d0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-000109e0: 6c6c 656c 2c29 2929 0a20 2020 2020 2020  llel,))).       
-000109f0: 2020 2020 2023 204b 6579 0a20 2020 2020       # Key.     
-00010a00: 2020 2020 2020 2073 656c 662e 6465 6e73         self.dens
-00010a10: 6532 203d 204c 696e 6561 7228 6869 6464  e2 = Linear(hidd
-00010a20: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-00010a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010a40: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-00010a50: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00010a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010a70: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
-00010a80: 6474 7970 653d 636f 6d70 7574 655f 6474  dtype=compute_dt
-00010a90: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-00010aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010ab0: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00010ac0: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
-00010ad0: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
-00010ae0: 2020 2073 656c 662e 6465 6e73 6532 2e73     self.dense2.s
-00010af0: 6861 7264 2873 7472 6174 6567 795f 6d61  hard(strategy_ma
-00010b00: 746d 756c 3d28 2870 6172 616c 6c65 6c5f  tmul=((parallel_
-00010b10: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-00010b20: 6c6c 656c 2c20 3129 2c0a 2020 2020 2020  llel, 1),.      
-00010b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000109c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000109d0: 2020 2020 2020 2020 2020 2870 6172 616c            (paral
+000109e0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+000109f0: 5f70 6172 616c 6c65 6c2c 2929 290a 2020  _parallel,))).  
+00010a00: 2020 2020 2020 2020 2020 2320 4b65 790a            # Key.
+00010a10: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00010a20: 2e64 656e 7365 3220 3d20 4c69 6e65 6172  .dense2 = Linear
+00010a30: 2868 6964 6465 6e5f 7369 7a65 2c0a 2020  (hidden_size,.  
+00010a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010a50: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+00010a60: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+00010a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010a80: 2020 2020 2020 2020 2020 2020 2063 6f6d               com
+00010a90: 7075 7465 5f64 7479 7065 3d63 6f6d 7075  pute_dtype=compu
+00010aa0: 7465 5f64 7479 7065 2c0a 2020 2020 2020  te_dtype,.      
+00010ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010ac0: 2020 2020 2020 2020 2020 2070 6172 616d             param
+00010ad0: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
+00010ae0: 5f69 6e69 745f 7479 7065 290a 2020 2020  _init_type).    
+00010af0: 2020 2020 2020 2020 7365 6c66 2e64 656e          self.den
+00010b00: 7365 322e 7368 6172 6428 7374 7261 7465  se2.shard(strate
+00010b10: 6779 5f6d 6174 6d75 6c3d 2828 7061 7261  gy_matmul=((para
+00010b20: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+00010b30: 5f70 6172 616c 6c65 6c2c 2031 292c 0a20  _parallel, 1),. 
 00010b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010b50: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
-00010b60: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00010b70: 7061 7261 6c6c 656c 2c20 3129 292c 0a20  parallel, 1)),. 
-00010b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010b90: 2020 2020 2020 2020 2020 2020 2073 7472               str
-00010ba0: 6174 6567 795f 6269 6173 3d28 2870 6172  ategy_bias=((par
-00010bb0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00010bc0: 615f 7061 7261 6c6c 656c 2c20 7061 7261  a_parallel, para
-00010bd0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00010be0: 6c5f 7061 7261 6c6c 656c 292c 0a20 2020  l_parallel),.   
-00010bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010c10: 2020 2020 2020 2020 2020 2870 6172 616c            (paral
-00010c20: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-00010c30: 5f70 6172 616c 6c65 6c2c 2929 290a 0a20  _parallel,))).. 
-00010c40: 2020 2020 2020 2020 2020 2023 2056 616c             # Val
-00010c50: 7565 0a20 2020 2020 2020 2020 2020 2073  ue.            s
-00010c60: 656c 662e 6465 6e73 6533 203d 204c 696e  elf.dense3 = Lin
-00010c70: 6561 7228 6869 6464 656e 5f73 697a 652c  ear(hidden_size,
-00010c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010ca0: 2020 6869 6464 656e 5f73 697a 652c 0a20    hidden_size,. 
-00010cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010cd0: 636f 6d70 7574 655f 6474 7970 653d 636f  compute_dtype=co
-00010ce0: 6d70 7574 655f 6474 7970 652c 0a20 2020  mpute_dtype,.   
-00010cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010d00: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00010d10: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-00010d20: 7261 6d5f 696e 6974 5f74 7970 6529 0a20  ram_init_type). 
-00010d30: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00010d40: 6465 6e73 6533 2e73 6861 7264 2873 7472  dense3.shard(str
-00010d50: 6174 6567 795f 6d61 746d 756c 3d28 2870  ategy_matmul=((p
-00010d60: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00010d70: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
-00010d80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00010d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010b60: 2020 2020 2020 2020 2020 2020 2020 2870                (p
+00010b70: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00010b80: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
+00010b90: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
+00010ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010bb0: 2020 7374 7261 7465 6779 5f62 6961 733d    strategy_bias=
+00010bc0: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+00010bd0: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
+00010be0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00010bf0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c29  .model_parallel)
+00010c00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00010c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010c20: 2020 2020 2020 2020 2020 2020 2020 2028                 (
+00010c30: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00010c40: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c29  model_parallel,)
+00010c50: 2929 0a0a 2020 2020 2020 2020 2020 2020  ))..            
+00010c60: 2320 5661 6c75 650a 2020 2020 2020 2020  # Value.        
+00010c70: 2020 2020 7365 6c66 2e64 656e 7365 3320      self.dense3 
+00010c80: 3d20 4c69 6e65 6172 2868 6964 6465 6e5f  = Linear(hidden_
+00010c90: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00010ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010cb0: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
+00010cc0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00010cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010ce0: 2020 2020 2063 6f6d 7075 7465 5f64 7479       compute_dty
+00010cf0: 7065 3d63 6f6d 7075 7465 5f64 7479 7065  pe=compute_dtype
+00010d00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00010d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010d20: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+00010d30: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
+00010d40: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00010d50: 7365 6c66 2e64 656e 7365 332e 7368 6172  self.dense3.shar
+00010d60: 6428 7374 7261 7465 6779 5f6d 6174 6d75  d(strategy_matmu
+00010d70: 6c3d 2828 7061 7261 6c6c 656c 5f63 6f6e  l=((parallel_con
+00010d80: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00010d90: 6c2c 2031 292c 0a20 2020 2020 2020 2020  l, 1),.         
 00010da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010db0: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
-00010dc0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00010dd0: 2c20 3129 292c 0a20 2020 2020 2020 2020  , 1)),.         
-00010de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010df0: 2020 2020 2073 7472 6174 6567 795f 6269       strategy_bi
-00010e00: 6173 3d28 2870 6172 616c 6c65 6c5f 636f  as=((parallel_co
-00010e10: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-00010e20: 656c 2c20 7061 7261 6c6c 656c 5f63 6f6e  el, parallel_con
-00010e30: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-00010e40: 656c 292c 0a20 2020 2020 2020 2020 2020  el),.           
-00010e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010dc0: 2020 2020 2020 2870 6172 616c 6c65 6c5f        (parallel_
+00010dd0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00010de0: 616c 6c65 6c2c 2031 2929 2c0a 2020 2020  allel, 1)),.    
+00010df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e00: 2020 2020 2020 2020 2020 7374 7261 7465            strate
+00010e10: 6779 5f62 6961 733d 2828 7061 7261 6c6c  gy_bias=((parall
+00010e20: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+00010e30: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
+00010e40: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+00010e50: 6172 616c 6c65 6c29 2c0a 2020 2020 2020  arallel),.      
 00010e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010e70: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
-00010e80: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-00010e90: 6c2c 2929 290a 2020 2020 2020 2020 2020  l,))).          
-00010ea0: 2020 7365 6c66 2e64 7479 7065 203d 2063    self.dtype = c
-00010eb0: 6f6d 7075 7465 5f64 7479 7065 0a20 2020  ompute_dtype.   
-00010ec0: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
-00010ed0: 6674 6d61 785f 6474 7970 6520 3d20 736f  ftmax_dtype = so
-00010ee0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-00010ef0: 7065 0a20 2020 2020 2020 2020 2020 2069  pe.            i
-00010f00: 6620 7365 6c66 2e75 7365 5f70 6173 743a  f self.use_past:
-00010f10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010f20: 2023 206f 7065 7261 746f 7273 2075 7365   # operators use
-00010f30: 6420 666f 7220 7374 6174 6520 7265 7573  d for state reus
-00010f40: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-00010f50: 2020 7365 715f 7261 6e67 6520 3d20 6e70    seq_range = np
-00010f60: 2e61 7261 6e67 6528 7372 635f 7365 715f  .arange(src_seq_
-00010f70: 6c65 6e67 7468 292e 7265 7368 6170 6528  length).reshape(
-00010f80: 312c 2031 2c20 2d31 290a 2020 2020 2020  1, 1, -1).      
-00010f90: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-00010fa0: 616e 6765 203d 2054 656e 736f 7228 6e70  ange = Tensor(np
-00010fb0: 2e74 696c 6528 7365 715f 7261 6e67 652c  .tile(seq_range,
-00010fc0: 2028 6261 7463 685f 7369 7a65 2c20 312c   (batch_size, 1,
-00010fd0: 2031 2929 2c20 6d73 7479 7065 2e69 6e74   1)), mstype.int
-00010fe0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
-00010ff0: 2020 2020 7365 6c66 2e73 6571 5f6c 656e      self.seq_len
-00011000: 6774 6820 3d20 7372 635f 7365 715f 6c65  gth = src_seq_le
-00011010: 6e67 7468 0a20 2020 2020 2020 2020 2020  ngth.           
-00011020: 2020 2020 2073 656c 662e 6174 7465 6e74       self.attent
-00011030: 696f 6e5f 6d61 736b 203d 2054 656e 736f  ion_mask = Tenso
-00011040: 7228 6e70 2e74 7269 6c28 6e70 2e6f 6e65  r(np.tril(np.one
-00011050: 7328 7368 6170 653d 2873 656c 662e 7365  s(shape=(self.se
-00011060: 715f 6c65 6e67 7468 2c20 7365 6c66 2e73  q_length, self.s
-00011070: 6571 5f6c 656e 6774 6829 2929 2c20 6d73  eq_length))), ms
-00011080: 7479 7065 2e69 6e74 3332 290a 2020 2020  type.int32).    
-00011090: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000110a0: 2e73 6c69 6365 203d 2050 2e53 7472 6964  .slice = P.Strid
-000110b0: 6564 536c 6963 6528 292e 7368 6172 6428  edSlice().shard(
-000110c0: 2828 312c 2031 2c20 312c 2031 292c 2929  ((1, 1, 1, 1),))
-000110d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000110e0: 2073 656c 662e 6e6f 745f 6571 7561 6c20   self.not_equal 
-000110f0: 3d20 502e 4e6f 7445 7175 616c 2829 2e73  = P.NotEqual().s
-00011100: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
-00011110: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
-00011120: 2020 2020 2020 2020 2073 656c 662e 7265           self.re
-00011130: 6475 6365 7375 6d20 3d20 502e 5265 6475  ducesum = P.Redu
-00011140: 6365 5375 6d28 292e 7368 6172 6428 2828  ceSum().shard(((
-00011150: 312c 2031 2c20 312c 2031 292c 2929 0a20  1, 1, 1, 1),)). 
-00011160: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00011170: 656c 662e 6578 7061 6e64 5f64 696d 7320  elf.expand_dims 
-00011180: 3d20 502e 4578 7061 6e64 4469 6d73 2829  = P.ExpandDims()
-00011190: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
-000111a0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-000111b0: 2020 2020 2073 656c 662e 7465 6e73 6f72       self.tensor
-000111c0: 5f6c 6520 3d20 502e 4c65 7373 4571 7561  _le = P.LessEqua
-000111d0: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
-000111e0: 2c20 3129 2c20 2831 2c20 312c 2031 2929  , 1), (1, 1, 1))
-000111f0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00011200: 2020 7365 6c66 2e61 6464 203d 2050 2e41    self.add = P.A
-00011210: 6464 2829 2e73 6861 7264 2828 2831 2c20  dd().shard(((1, 
-00011220: 312c 2031 2c20 3129 2c20 2831 2c20 312c  1, 1, 1), (1, 1,
-00011230: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-00011240: 2020 2020 2020 2020 2073 656c 662e 6571           self.eq
-00011250: 7561 6c20 3d20 502e 4571 7561 6c28 292e  ual = P.Equal().
-00011260: 7368 6172 6428 2828 312c 2031 2c20 3129  shard(((1, 1, 1)
-00011270: 2c20 2831 2c20 312c 2031 2929 290a 2020  , (1, 1, 1))).  
-00011280: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00011290: 6c66 2e73 7562 3120 3d20 502e 5375 6228  lf.sub1 = P.Sub(
-000112a0: 292e 7368 6172 6428 2828 312c 292c 2028  ).shard(((1,), (
-000112b0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-000112c0: 2020 2020 7365 6c66 2e74 696c 6520 3d20      self.tile = 
-000112d0: 502e 5469 6c65 2829 2e73 6861 7264 2828  P.Tile().shard((
-000112e0: 2831 2c20 312c 2031 2c20 3129 2c29 290a  (1, 1, 1, 1),)).
-000112f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011300: 7365 6c66 2e6c 6573 7320 3d20 502e 4c65  self.less = P.Le
-00011310: 7373 2829 2e73 6861 7264 2828 2831 2c20  ss().shard(((1, 
-00011320: 312c 2031 292c 2028 312c 2031 2c20 3129  1, 1), (1, 1, 1)
-00011330: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-00011340: 2020 2073 656c 662e 6d75 6c31 203d 2050     self.mul1 = P
-00011350: 2e4d 756c 2829 2e73 6861 7264 2828 2831  .Mul().shard(((1
-00011360: 2c20 312c 2031 2c20 3129 2c20 2831 2c20  , 1, 1, 1), (1, 
-00011370: 312c 2031 2c20 3129 2929 0a0a 2020 2020  1, 1, 1)))..    
-00011380: 2020 2020 2020 2020 6966 2070 6172 616c          if paral
-00011390: 6c65 6c5f 636f 6e66 6967 2e75 7365 5f73  lel_config.use_s
-000113a0: 6571 5f70 6172 616c 6c65 6c3a 0a20 2020  eq_parallel:.   
-000113b0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-000113c0: 662e 6472 6f70 6f75 742e 6472 6f70 6f75  f.dropout.dropou
-000113d0: 742e 7368 6172 6428 2828 7061 7261 6c6c  t.shard(((parall
-000113e0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000113f0: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
-00011400: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00011410: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
-00011420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011430: 7365 6c66 2e70 726f 6a65 6374 696f 6e2e  self.projection.
-00011440: 7368 6172 6428 0a20 2020 2020 2020 2020  shard(.         
-00011450: 2020 2020 2020 2020 2020 2073 7472 6174             strat
-00011460: 6567 795f 6269 6173 3d28 2870 6172 616c  egy_bias=((paral
-00011470: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00011480: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
-00011490: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-000114a0: 5f70 6172 616c 6c65 6c2c 2031 292c 2028  _parallel, 1), (
-000114b0: 312c 2929 2c0a 2020 2020 2020 2020 2020  1,)),.          
-000114c0: 2020 2020 2020 2020 2020 7374 7261 7465            strate
-000114d0: 6779 5f6d 6174 6d75 6c3d 2828 7061 7261  gy_matmul=((para
-000114e0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-000114f0: 5f70 6172 616c 6c65 6c2c 2070 6172 616c  _parallel, paral
-00011500: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-00011510: 5f70 6172 616c 6c65 6c29 2c0a 2020 2020  _parallel),.    
-00011520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011540: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
-00011550: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00011560: 2c20 3129 292c 0a20 2020 2020 2020 2020  , 1)),.         
-00011570: 2020 2020 2020 2020 2020 206f 7574 5f73             out_s
-00011580: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
-00011590: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-000115a0: 2e64 6174 615f 7061 7261 6c6c 656c 202a  .data_parallel *
-000115b0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-000115c0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-000115d0: 2031 292c 2929 0a0a 2020 2020 2020 2020   1),))..        
-000115e0: 7365 6c66 2e75 7365 5f66 6c61 7368 5f61  self.use_flash_a
-000115f0: 7474 656e 7469 6f6e 203d 2075 7365 5f66  ttention = use_f
-00011600: 6c61 7368 5f61 7474 656e 7469 6f6e 0a20  lash_attention. 
-00011610: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
-00011620: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
-00011630: 6f6e 2061 6e64 206e 6f74 2063 6865 636b  on and not check
-00011640: 5f76 616c 6964 5f66 6c61 7368 5f61 7474  _valid_flash_att
-00011650: 656e 7469 6f6e 2846 4c41 5348 4154 5445  ention(FLASHATTE
-00011660: 4e54 494f 4e5f 5641 4c49 4429 3a0a 2020  NTION_VALID):.  
-00011670: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
-00011680: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
-00011690: 6f6e 203d 2046 616c 7365 0a20 2020 2020  on = False.     
-000116a0: 2020 2020 2020 206c 6f67 2e69 6e66 6f28         log.info(
-000116b0: 2243 7572 7265 6e74 204d 696e 6453 706f  "Current MindSpo
-000116c0: 7265 2064 6f20 6e6f 7420 7375 7070 6f72  re do not suppor
-000116d0: 7420 666c 6173 6820 6174 7465 6e74 696f  t flash attentio
-000116e0: 6e2c 2070 6c65 6173 6520 7570 6772 6164  n, please upgrad
-000116f0: 6520 746f 2032 2e32 2e30 206f 7220 6869  e to 2.2.0 or hi
-00011700: 6768 6572 2229 0a0a 2020 2020 2020 2020  gher")..        
-00011710: 6966 2073 656c 662e 7573 655f 666c 6173  if self.use_flas
-00011720: 685f 6174 7465 6e74 696f 6e3a 0a20 2020  h_attention:.   
-00011730: 2020 2020 2020 2020 2073 656c 662e 666c           self.fl
-00011740: 6173 685f 6174 7465 6e74 696f 6e20 3d20  ash_attention = 
-00011750: 466c 6173 6841 7474 656e 7469 6f6e 280a  FlashAttention(.
-00011760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011770: 7365 6c66 2e73 697a 655f 7065 725f 6865  self.size_per_he
-00011780: 6164 2c0a 2020 2020 2020 2020 2020 2020  ad,.            
-00011790: 2020 2020 6e75 6d5f 6865 6164 732c 0a20      num_heads,. 
-000117a0: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-000117b0: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
-000117c0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-000117d0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-000117e0: 2020 2020 6470 3d70 6172 616c 6c65 6c5f      dp=parallel_
-000117f0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-00011800: 6c6c 656c 2c0a 2020 2020 2020 2020 2020  llel,.          
-00011810: 2020 2020 2020 6d70 3d70 6172 616c 6c65        mp=paralle
-00011820: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00011830: 6172 616c 6c65 6c2c 0a20 2020 2020 2020  arallel,.       
-00011840: 2020 2020 2020 2020 206e 6578 745f 626c           next_bl
-00011850: 6f63 6b5f 6e75 6d3d 302c 0a20 2020 2020  ock_num=0,.     
-00011860: 2020 2020 2020 2020 2020 2070 7265 765f             prev_
-00011870: 626c 6f63 6b5f 6e75 6d3d 3635 3533 362c  block_num=65536,
-00011880: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00011890: 2068 6967 685f 7072 6563 6973 696f 6e3d   high_precision=
-000118a0: 5472 7565 0a20 2020 2020 2020 2020 2020  True.           
-000118b0: 2029 0a20 2020 2020 2020 2020 2020 2073   ).            s
-000118c0: 656c 662e 7375 6220 3d20 502e 5375 6228  elf.sub = P.Sub(
-000118d0: 292e 7368 6172 6428 0a20 2020 2020 2020  ).shard(.       
-000118e0: 2020 2020 2020 2020 2028 2831 2c29 2c20           ((1,), 
-000118f0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00011900: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00011910: 312c 2031 2929 290a 0a20 2020 2020 2020  1, 1)))..       
-00011920: 2020 2020 2073 656c 662e 6f6e 6520 3d20       self.one = 
-00011930: 5465 6e73 6f72 285b 312e 305d 2c20 6474  Tensor([1.0], dt
-00011940: 7970 653d 636f 6d70 7574 655f 6474 7970  ype=compute_dtyp
-00011950: 6529 0a0a 2020 2020 2020 2020 6966 2070  e)..        if p
-00011960: 6172 616c 6c65 6c5f 636f 6e66 6967 2e73  arallel_config.s
-00011970: 656c 6563 745f 7265 636f 6d70 7574 653a  elect_recompute:
-00011980: 0a20 2020 2020 2020 2020 2020 2023 205f  .            # _
-00011990: 6174 746e e4b8 ade6 b689 e58f 8ae7 9a84  attn............
-000119a0: e585 b3e9 94ae e7ae 97e5 ad90 e4bd bfe7  ................
-000119b0: 94a8 e987 8de8 aea1 e7ae 97e9 80bb e8be  ................
-000119c0: 91ef bc8c e980 9ae5 b8b8 e985 8de5 9088  ................
-000119d0: e5ba 8fe5 8897 e5b9 b6e8 a18c e4bd bfe7  ................
-000119e0: 94a8 efbc 8ce4 bc9a e69c 89e8 be83 e5a5  ................
-000119f0: bde7 9a84 e680 a7e8 83bd e68f 90e5 8d87  ................
-00011a00: e695 88e6 9e9c 0a20 2020 2020 2020 2020  .......         
-00011a10: 2020 2073 656c 662e 6261 7463 685f 6d61     self.batch_ma
-00011a20: 746d 756c 2e72 6563 6f6d 7075 7465 2829  tmul.recompute()
-00011a30: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00011a40: 662e 7375 622e 7265 636f 6d70 7574 6528  f.sub.recompute(
-00011a50: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00011a60: 6c66 2e61 6464 2e72 6563 6f6d 7075 7465  lf.add.recompute
-00011a70: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-00011a80: 656c 662e 6d65 7267 6572 5f68 6561 645f  elf.merger_head_
-00011a90: 7472 616e 7370 6f73 652e 7265 636f 6d70  transpose.recomp
-00011aa0: 7574 6528 290a 2020 2020 2020 2020 2020  ute().          
-00011ab0: 2020 7365 6c66 2e73 6f66 746d 6178 5f72    self.softmax_r
-00011ac0: 6573 6861 7065 2e72 6563 6f6d 7075 7465  eshape.recompute
-00011ad0: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-00011ae0: 656c 662e 7072 6f62 5f64 726f 706f 7574  elf.prob_dropout
-00011af0: 2e72 6563 6f6d 7075 7465 2829 0a20 2020  .recompute().   
-00011b00: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
-00011b10: 6674 6d61 785f 6361 7374 2e72 6563 6f6d  ftmax_cast.recom
-00011b20: 7075 7465 2829 0a20 2020 2020 2020 2020  pute().         
-00011b30: 2020 2073 656c 662e 736f 6674 6d61 782e     self.softmax.
-00011b40: 736f 6674 6d61 782e 7265 636f 6d70 7574  softmax.recomput
-00011b50: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
-00011b60: 7365 6c66 2e73 6f66 746d 6178 5f33 642e  self.softmax_3d.
-00011b70: 7265 636f 6d70 7574 6528 290a 0a20 2020  recompute()..   
-00011b80: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
-00011b90: 656c 662c 2071 7565 7279 5f74 656e 736f  elf, query_tenso
-00011ba0: 722c 206b 6579 5f74 656e 736f 722c 2076  r, key_tensor, v
-00011bb0: 616c 7565 5f74 656e 736f 722c 2061 7474  alue_tensor, att
-00011bc0: 656e 7469 6f6e 5f6d 6173 6b2c 206b 6579  ention_mask, key
-00011bd0: 5f70 6173 743d 4e6f 6e65 2c0a 2020 2020  _past=None,.    
-00011be0: 2020 2020 2020 2020 2020 2020 2020 7661                va
-00011bf0: 6c75 655f 7061 7374 3d4e 6f6e 652c 2062  lue_past=None, b
-00011c00: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00011c10: 683d 4e6f 6e65 293a 0a20 2020 2020 2020  h=None):.       
-00011c20: 2022 2222 466f 7277 6172 6420 7072 6f63   """Forward proc
-00011c30: 6573 7320 6f66 2074 6865 204d 756c 7469  ess of the Multi
-00011c40: 4865 6164 4174 7465 6e74 696f 6e22 2222  HeadAttention"""
-00011c50: 0a20 2020 2020 2020 2073 656c 662e 5f63  .        self._c
-00011c60: 6865 636b 5f69 6e70 7574 7328 7175 6572  heck_inputs(quer
-00011c70: 795f 7465 6e73 6f72 2c20 6b65 795f 7465  y_tensor, key_te
-00011c80: 6e73 6f72 2c20 7661 6c75 655f 7465 6e73  nsor, value_tens
-00011c90: 6f72 2c20 6174 7465 6e74 696f 6e5f 6d61  or, attention_ma
-00011ca0: 736b 2c20 6b65 795f 7061 7374 2c0a 2020  sk, key_past,.  
-00011cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011cc0: 2020 2020 2020 2020 2076 616c 7565 5f70           value_p
-00011cd0: 6173 742c 2062 6174 6368 5f76 616c 6964  ast, batch_valid
-00011ce0: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-00011cf0: 206f 7269 5f73 6861 7065 203d 2046 2e73   ori_shape = F.s
-00011d00: 6861 7065 2871 7565 7279 5f74 656e 736f  hape(query_tenso
-00011d10: 7229 0a20 2020 2020 2020 2062 6174 6368  r).        batch
-00011d20: 5f73 697a 6520 3d20 7365 6c66 2e5f 6765  _size = self._ge
-00011d30: 745f 6261 7463 685f 7369 7a65 5f66 726f  t_batch_size_fro
-00011d40: 6d5f 7175 6572 7928 7175 6572 795f 7465  m_query(query_te
-00011d50: 6e73 6f72 290a 2020 2020 2020 2020 7175  nsor).        qu
-00011d60: 6572 795f 7465 6e73 6f72 2c20 6b65 795f  ery_tensor, key_
-00011d70: 7465 6e73 6f72 2c20 7661 6c75 655f 7465  tensor, value_te
-00011d80: 6e73 6f72 203d 2073 656c 662e 5f63 6f6e  nsor = self._con
-00011d90: 7665 7274 5f74 6f5f 3264 5f74 656e 736f  vert_to_2d_tenso
-00011da0: 7228 7175 6572 795f 7465 6e73 6f72 2c0a  r(query_tensor,.
-00011db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e80: 2020 2020 2020 2028 7061 7261 6c6c 656c         (parallel
+00010e90: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+00010ea0: 7261 6c6c 656c 2c29 2929 0a20 2020 2020  rallel,))).     
+00010eb0: 2020 2020 2020 2073 656c 662e 6474 7970         self.dtyp
+00010ec0: 6520 3d20 636f 6d70 7574 655f 6474 7970  e = compute_dtyp
+00010ed0: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
+00010ee0: 6c66 2e73 6f66 746d 6178 5f64 7479 7065  lf.softmax_dtype
+00010ef0: 203d 2073 6f66 746d 6178 5f63 6f6d 7075   = softmax_compu
+00010f00: 7465 5f74 7970 650a 2020 2020 2020 2020  te_type.        
+00010f10: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+00010f20: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+00010f30: 2020 2020 2020 2320 6f70 6572 6174 6f72        # operator
+00010f40: 7320 7573 6564 2066 6f72 2073 7461 7465  s used for state
+00010f50: 2072 6575 7365 0a20 2020 2020 2020 2020   reuse.         
+00010f60: 2020 2020 2020 2073 6571 5f72 616e 6765         seq_range
+00010f70: 203d 206e 702e 6172 616e 6765 2873 7263   = np.arange(src
+00010f80: 5f73 6571 5f6c 656e 6774 6829 2e72 6573  _seq_length).res
+00010f90: 6861 7065 2831 2c20 312c 202d 3129 0a20  hape(1, 1, -1). 
+00010fa0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00010fb0: 656c 662e 7261 6e67 6520 3d20 5465 6e73  elf.range = Tens
+00010fc0: 6f72 286e 702e 7469 6c65 2873 6571 5f72  or(np.tile(seq_r
+00010fd0: 616e 6765 2c20 2862 6174 6368 5f73 697a  ange, (batch_siz
+00010fe0: 652c 2031 2c20 3129 292c 206d 7374 7970  e, 1, 1)), mstyp
+00010ff0: 652e 696e 7433 3229 0a20 2020 2020 2020  e.int32).       
+00011000: 2020 2020 2020 2020 2073 656c 662e 7365           self.se
+00011010: 715f 6c65 6e67 7468 203d 2073 7263 5f73  q_length = src_s
+00011020: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
+00011030: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+00011040: 7474 656e 7469 6f6e 5f6d 6173 6b20 3d20  ttention_mask = 
+00011050: 5465 6e73 6f72 286e 702e 7472 696c 286e  Tensor(np.tril(n
+00011060: 702e 6f6e 6573 2873 6861 7065 3d28 7365  p.ones(shape=(se
+00011070: 6c66 2e73 6571 5f6c 656e 6774 682c 2073  lf.seq_length, s
+00011080: 656c 662e 7365 715f 6c65 6e67 7468 2929  elf.seq_length))
+00011090: 292c 206d 7374 7970 652e 696e 7433 3229  ), mstype.int32)
+000110a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000110b0: 2073 656c 662e 736c 6963 6520 3d20 502e   self.slice = P.
+000110c0: 5374 7269 6465 6453 6c69 6365 2829 2e73  StridedSlice().s
+000110d0: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
+000110e0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+000110f0: 2020 2020 2020 7365 6c66 2e6e 6f74 5f65        self.not_e
+00011100: 7175 616c 203d 2050 2e4e 6f74 4571 7561  qual = P.NotEqua
+00011110: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
+00011120: 2c20 312c 2031 292c 2028 2929 290a 2020  , 1, 1), ())).  
+00011130: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00011140: 6c66 2e72 6564 7563 6573 756d 203d 2050  lf.reducesum = P
+00011150: 2e52 6564 7563 6553 756d 2829 2e73 6861  .ReduceSum().sha
+00011160: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
+00011170: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+00011180: 2020 2020 7365 6c66 2e65 7870 616e 645f      self.expand_
+00011190: 6469 6d73 203d 2050 2e45 7870 616e 6444  dims = P.ExpandD
+000111a0: 696d 7328 292e 7368 6172 6428 2828 312c  ims().shard(((1,
+000111b0: 2031 2c20 3129 2c29 290a 2020 2020 2020   1, 1),)).      
+000111c0: 2020 2020 2020 2020 2020 7365 6c66 2e74            self.t
+000111d0: 656e 736f 725f 6c65 203d 2050 2e4c 6573  ensor_le = P.Les
+000111e0: 7345 7175 616c 2829 2e73 6861 7264 2828  sEqual().shard((
+000111f0: 2831 2c20 312c 2031 292c 2028 312c 2031  (1, 1, 1), (1, 1
+00011200: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
+00011210: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+00011220: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+00011230: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
+00011240: 312c 2031 2c20 312c 2031 2929 290a 2020  1, 1, 1, 1))).  
+00011250: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00011260: 6c66 2e65 7175 616c 203d 2050 2e45 7175  lf.equal = P.Equ
+00011270: 616c 2829 2e73 6861 7264 2828 2831 2c20  al().shard(((1, 
+00011280: 312c 2031 292c 2028 312c 2031 2c20 3129  1, 1), (1, 1, 1)
+00011290: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+000112a0: 2020 2073 656c 662e 7375 6231 203d 2050     self.sub1 = P
+000112b0: 2e53 7562 2829 2e73 6861 7264 2828 2831  .Sub().shard(((1
+000112c0: 2c29 2c20 2829 2929 0a20 2020 2020 2020  ,), ())).       
+000112d0: 2020 2020 2020 2020 2073 656c 662e 7469           self.ti
+000112e0: 6c65 203d 2050 2e54 696c 6528 292e 7368  le = P.Tile().sh
+000112f0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+00011300: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00011310: 2020 2020 2073 656c 662e 6c65 7373 203d       self.less =
+00011320: 2050 2e4c 6573 7328 292e 7368 6172 6428   P.Less().shard(
+00011330: 2828 312c 2031 2c20 3129 2c20 2831 2c20  ((1, 1, 1), (1, 
+00011340: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+00011350: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
+00011360: 3120 3d20 502e 4d75 6c28 292e 7368 6172  1 = P.Mul().shar
+00011370: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+00011380: 2028 312c 2031 2c20 312c 2031 2929 290a   (1, 1, 1, 1))).
+00011390: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+000113a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+000113b0: 7573 655f 7365 715f 7061 7261 6c6c 656c  use_seq_parallel
+000113c0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000113d0: 2020 7365 6c66 2e64 726f 706f 7574 2e64    self.dropout.d
+000113e0: 726f 706f 7574 2e73 6861 7264 2828 2870  ropout.shard(((p
+000113f0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00011400: 6174 615f 7061 7261 6c6c 656c 202a 2070  ata_parallel * p
+00011410: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00011420: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
+00011430: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00011440: 2020 2020 2073 656c 662e 7072 6f6a 6563       self.projec
+00011450: 7469 6f6e 2e73 6861 7264 280a 2020 2020  tion.shard(.    
+00011460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011470: 7374 7261 7465 6779 5f62 6961 733d 2828  strategy_bias=((
+00011480: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00011490: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
+000114a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+000114b0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+000114c0: 3129 2c20 2831 2c29 292c 0a20 2020 2020  1), (1,)),.     
+000114d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000114e0: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
+000114f0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00011500: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+00011510: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00011520: 6d6f 6465 6c5f 7061 7261 6c6c 656c 292c  model_parallel),
+00011530: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011550: 2020 2020 2020 2870 6172 616c 6c65 6c5f        (parallel_
+00011560: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00011570: 616c 6c65 6c2c 2031 2929 2c0a 2020 2020  allel, 1)),.    
+00011580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011590: 6f75 745f 7374 7261 7465 6779 5f6d 6174  out_strategy_mat
+000115a0: 6d75 6c3d 2828 7061 7261 6c6c 656c 5f63  mul=((parallel_c
+000115b0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+000115c0: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
+000115d0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+000115e0: 6c6c 656c 2c20 3129 2c29 290a 0a20 2020  llel, 1),))..   
+000115f0: 2020 2020 2073 656c 662e 7573 655f 666c       self.use_fl
+00011600: 6173 685f 6174 7465 6e74 696f 6e20 3d20  ash_attention = 
+00011610: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
+00011620: 696f 6e0a 2020 2020 2020 2020 6966 2073  ion.        if s
+00011630: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
+00011640: 7465 6e74 696f 6e20 616e 6420 6e6f 7420  tention and not 
+00011650: 6368 6563 6b5f 7661 6c69 645f 666c 6173  check_valid_flas
+00011660: 685f 6174 7465 6e74 696f 6e28 464c 4153  h_attention(FLAS
+00011670: 4841 5454 454e 5449 4f4e 5f56 414c 4944  HATTENTION_VALID
+00011680: 2c20 2746 6c61 7368 4174 7465 6e74 696f  , 'FlashAttentio
+00011690: 6e27 293a 0a20 2020 2020 2020 2020 2020  n'):.           
+000116a0: 2073 656c 662e 7573 655f 666c 6173 685f   self.use_flash_
+000116b0: 6174 7465 6e74 696f 6e20 3d20 4661 6c73  attention = Fals
+000116c0: 650a 2020 2020 2020 2020 2020 2020 6c6f  e.            lo
+000116d0: 672e 696e 666f 2822 4375 7272 656e 7420  g.info("Current 
+000116e0: 4d69 6e64 5370 6f72 6520 646f 206e 6f74  MindSpore do not
+000116f0: 2073 7570 706f 7274 2066 6c61 7368 2061   support flash a
+00011700: 7474 656e 7469 6f6e 2c20 706c 6561 7365  ttention, please
+00011710: 2075 7067 7261 6465 2074 6f20 322e 322e   upgrade to 2.2.
+00011720: 3020 6f72 2068 6967 6865 7222 290a 0a20  0 or higher").. 
+00011730: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+00011740: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
+00011750: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
+00011760: 7365 6c66 2e66 6c61 7368 5f61 7474 656e  self.flash_atten
+00011770: 7469 6f6e 203d 2046 6c61 7368 4174 7465  tion = FlashAtte
+00011780: 6e74 696f 6e28 0a20 2020 2020 2020 2020  ntion(.         
+00011790: 2020 2020 2020 2073 656c 662e 7369 7a65         self.size
+000117a0: 5f70 6572 5f68 6561 642c 0a20 2020 2020  _per_head,.     
+000117b0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+000117c0: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
+000117d0: 2020 2020 2020 6472 6f70 6f75 745f 7261        dropout_ra
+000117e0: 7465 3d61 7474 656e 7469 6f6e 5f64 726f  te=attention_dro
+000117f0: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+00011800: 2020 2020 2020 2020 2020 2064 703d 7061             dp=pa
+00011810: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+00011820: 7461 5f70 6172 616c 6c65 6c2c 0a20 2020  ta_parallel,.   
+00011830: 2020 2020 2020 2020 2020 2020 206d 703d               mp=
+00011840: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00011850: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c0a  model_parallel,.
+00011860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011870: 6e65 7874 5f62 6c6f 636b 5f6e 756d 3d30  next_block_num=0
+00011880: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00011890: 2020 7072 6576 5f62 6c6f 636b 5f6e 756d    prev_block_num
+000118a0: 3d36 3535 3336 2c0a 2020 2020 2020 2020  =65536,.        
+000118b0: 2020 2020 2020 2020 6869 6768 5f70 7265          high_pre
+000118c0: 6369 7369 6f6e 3d54 7275 650a 2020 2020  cision=True.    
+000118d0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+000118e0: 2020 2020 2020 7365 6c66 2e73 7562 203d        self.sub =
+000118f0: 2050 2e53 7562 2829 2e73 6861 7264 280a   P.Sub().shard(.
+00011900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011910: 2828 312c 292c 2028 7061 7261 6c6c 656c  ((1,), (parallel
+00011920: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00011930: 616c 6c65 6c2c 2031 2c20 3129 2929 0a0a  allel, 1, 1)))..
+00011940: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00011950: 2e6f 6e65 203d 2054 656e 736f 7228 5b31  .one = Tensor([1
+00011960: 2e30 5d2c 2064 7479 7065 3d63 6f6d 7075  .0], dtype=compu
+00011970: 7465 5f64 7479 7065 290a 0a20 2020 2020  te_dtype)..     
+00011980: 2020 2069 6620 7061 7261 6c6c 656c 5f63     if parallel_c
+00011990: 6f6e 6669 672e 7365 6c65 6374 5f72 6563  onfig.select_rec
+000119a0: 6f6d 7075 7465 3a0a 2020 2020 2020 2020  ompute:.        
+000119b0: 2020 2020 2320 5f61 7474 6ee4 b8ad e6b6      # _attn.....
+000119c0: 89e5 8f8a e79a 84e5 85b3 e994 aee7 ae97  ................
+000119d0: e5ad 90e4 bdbf e794 a8e9 878d e8ae a1e7  ................
+000119e0: ae97 e980 bbe8 be91 efbc 8ce9 809a e5b8  ................
+000119f0: b8e9 858d e590 88e5 ba8f e588 97e5 b9b6  ................
+00011a00: e8a1 8ce4 bdbf e794 a8ef bc8c e4bc 9ae6  ................
+00011a10: 9c89 e8be 83e5 a5bd e79a 84e6 80a7 e883  ................
+00011a20: bde6 8f90 e58d 87e6 9588 e69e 9c0a 2020  ..............  
+00011a30: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
+00011a40: 6174 6368 5f6d 6174 6d75 6c2e 7265 636f  atch_matmul.reco
+00011a50: 6d70 7574 6528 290a 2020 2020 2020 2020  mpute().        
+00011a60: 2020 2020 7365 6c66 2e73 7562 2e72 6563      self.sub.rec
+00011a70: 6f6d 7075 7465 2829 0a20 2020 2020 2020  ompute().       
+00011a80: 2020 2020 2073 656c 662e 6164 642e 7265       self.add.re
+00011a90: 636f 6d70 7574 6528 290a 2020 2020 2020  compute().      
+00011aa0: 2020 2020 2020 7365 6c66 2e6d 6572 6765        self.merge
+00011ab0: 725f 6865 6164 5f74 7261 6e73 706f 7365  r_head_transpose
+00011ac0: 2e72 6563 6f6d 7075 7465 2829 0a20 2020  .recompute().   
+00011ad0: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
+00011ae0: 6674 6d61 785f 7265 7368 6170 652e 7265  ftmax_reshape.re
+00011af0: 636f 6d70 7574 6528 290a 2020 2020 2020  compute().      
+00011b00: 2020 2020 2020 7365 6c66 2e70 726f 625f        self.prob_
+00011b10: 6472 6f70 6f75 742e 7265 636f 6d70 7574  dropout.recomput
+00011b20: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
+00011b30: 7365 6c66 2e73 6f66 746d 6178 5f63 6173  self.softmax_cas
+00011b40: 742e 7265 636f 6d70 7574 6528 290a 2020  t.recompute().  
+00011b50: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
+00011b60: 6f66 746d 6178 2e73 6f66 746d 6178 2e72  oftmax.softmax.r
+00011b70: 6563 6f6d 7075 7465 2829 0a20 2020 2020  ecompute().     
+00011b80: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
+00011b90: 6d61 785f 3364 2e72 6563 6f6d 7075 7465  max_3d.recompute
+00011ba0: 2829 0a0a 2020 2020 6465 6620 636f 6e73  ()..    def cons
+00011bb0: 7472 7563 7428 7365 6c66 2c20 7175 6572  truct(self, quer
+00011bc0: 795f 7465 6e73 6f72 2c20 6b65 795f 7465  y_tensor, key_te
+00011bd0: 6e73 6f72 2c20 7661 6c75 655f 7465 6e73  nsor, value_tens
+00011be0: 6f72 2c20 6174 7465 6e74 696f 6e5f 6d61  or, attention_ma
+00011bf0: 736b 2c20 6b65 795f 7061 7374 3d4e 6f6e  sk, key_past=Non
+00011c00: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00011c10: 2020 2020 2076 616c 7565 5f70 6173 743d       value_past=
+00011c20: 4e6f 6e65 2c20 6261 7463 685f 7661 6c69  None, batch_vali
+00011c30: 645f 6c65 6e67 7468 3d4e 6f6e 6529 3a0a  d_length=None):.
+00011c40: 2020 2020 2020 2020 2222 2246 6f72 7761          """Forwa
+00011c50: 7264 2070 726f 6365 7373 206f 6620 7468  rd process of th
+00011c60: 6520 4d75 6c74 6948 6561 6441 7474 656e  e MultiHeadAtten
+00011c70: 7469 6f6e 2222 220a 2020 2020 2020 2020  tion""".        
+00011c80: 7365 6c66 2e5f 6368 6563 6b5f 696e 7075  self._check_inpu
+00011c90: 7473 2871 7565 7279 5f74 656e 736f 722c  ts(query_tensor,
+00011ca0: 206b 6579 5f74 656e 736f 722c 2076 616c   key_tensor, val
+00011cb0: 7565 5f74 656e 736f 722c 2061 7474 656e  ue_tensor, atten
+00011cc0: 7469 6f6e 5f6d 6173 6b2c 206b 6579 5f70  tion_mask, key_p
+00011cd0: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+00011ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011cf0: 7661 6c75 655f 7061 7374 2c20 6261 7463  value_past, batc
+00011d00: 685f 7661 6c69 645f 6c65 6e67 7468 290a  h_valid_length).
+00011d10: 2020 2020 2020 2020 6f72 695f 7368 6170          ori_shap
+00011d20: 6520 3d20 462e 7368 6170 6528 7175 6572  e = F.shape(quer
+00011d30: 795f 7465 6e73 6f72 290a 2020 2020 2020  y_tensor).      
+00011d40: 2020 6261 7463 685f 7369 7a65 203d 2073    batch_size = s
+00011d50: 656c 662e 5f67 6574 5f62 6174 6368 5f73  elf._get_batch_s
+00011d60: 697a 655f 6672 6f6d 5f71 7565 7279 2871  ize_from_query(q
+00011d70: 7565 7279 5f74 656e 736f 7229 0a20 2020  uery_tensor).   
+00011d80: 2020 2020 2071 7565 7279 5f74 656e 736f       query_tenso
+00011d90: 722c 206b 6579 5f74 656e 736f 722c 2076  r, key_tensor, v
+00011da0: 616c 7565 5f74 656e 736f 7220 3d20 7365  alue_tensor = se
+00011db0: 6c66 2e5f 636f 6e76 6572 745f 746f 5f32  lf._convert_to_2
+00011dc0: 645f 7465 6e73 6f72 2871 7565 7279 5f74  d_tensor(query_t
+00011dd0: 656e 736f 722c 0a20 2020 2020 2020 2020  ensor,.         
 00011de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011df0: 2020 2020 2020 2020 2020 2020 6b65 795f              key_
-00011e00: 7465 6e73 6f72 2c0a 2020 2020 2020 2020  tensor,.        
+00011df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00011e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e20: 2020 206b 6579 5f74 656e 736f 722c 0a20     key_tensor,. 
 00011e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00011e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e50: 2020 2020 7661 6c75 655f 7465 6e73 6f72      value_tensor
-00011e60: 290a 2020 2020 2020 2020 6f72 695f 6474  ).        ori_dt
-00011e70: 7970 6520 3d20 462e 6474 7970 6528 7175  ype = F.dtype(qu
-00011e80: 6572 795f 7465 6e73 6f72 290a 2020 2020  ery_tensor).    
-00011e90: 2020 2020 7175 6572 795f 7465 6e73 6f72      query_tensor
-00011ea0: 203d 2046 2e63 6173 7428 7175 6572 795f   = F.cast(query_
-00011eb0: 7465 6e73 6f72 2c20 7365 6c66 2e64 7479  tensor, self.dty
-00011ec0: 7065 290a 2020 2020 2020 2020 6b65 795f  pe).        key_
-00011ed0: 7465 6e73 6f72 203d 2046 2e63 6173 7428  tensor = F.cast(
-00011ee0: 6b65 795f 7465 6e73 6f72 2c20 7365 6c66  key_tensor, self
-00011ef0: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-00011f00: 7661 6c75 655f 7465 6e73 6f72 203d 2046  value_tensor = F
-00011f10: 2e63 6173 7428 7661 6c75 655f 7465 6e73  .cast(value_tens
-00011f20: 6f72 2c20 7365 6c66 2e64 7479 7065 290a  or, self.dtype).
-00011f30: 2020 2020 2020 2020 2320 6d75 6c74 6920          # multi 
-00011f40: 6865 6164 2061 7474 656e 7469 6f6e 3a20  head attention: 
-00011f50: 7175 6572 792c 206b 6579 2c20 7661 6c75  query, key, valu
-00011f60: 6520 6172 6520 6465 7269 7665 6420 6672  e are derived fr
-00011f70: 6f6d 2074 6865 2073 616d 6520 696e 7075  om the same inpu
-00011f80: 7473 0a20 2020 2020 2020 2071 7565 7279  ts.        query
-00011f90: 203d 2073 656c 662e 6465 6e73 6531 2871   = self.dense1(q
-00011fa0: 7565 7279 5f74 656e 736f 7229 0a20 2020  uery_tensor).   
-00011fb0: 2020 2020 206b 6579 203d 2073 656c 662e       key = self.
-00011fc0: 6465 6e73 6532 286b 6579 5f74 656e 736f  dense2(key_tenso
-00011fd0: 7229 0a20 2020 2020 2020 2076 616c 7565  r).        value
-00011fe0: 203d 2073 656c 662e 6465 6e73 6533 2876   = self.dense3(v
-00011ff0: 616c 7565 5f74 656e 736f 7229 0a20 2020  alue_tensor).   
-00012000: 2020 2020 2023 2074 6865 2072 6574 7572       # the retur
-00012010: 6e65 6420 7368 6170 6520 6973 205b 6273  ned shape is [bs
-00012020: 2c20 6e75 6d5f 6865 6164 732c 2073 6571  , num_heads, seq
-00012030: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
-00012040: 725f 6865 6164 5d0a 2020 2020 2020 2020  r_head].        
-00012050: 7175 6572 7920 3d20 7365 6c66 2e74 7261  query = self.tra
-00012060: 6e73 706f 7365 280a 2020 2020 2020 2020  nspose(.        
-00012070: 2020 2020 462e 7265 7368 6170 6528 0a20      F.reshape(. 
-00012080: 2020 2020 2020 2020 2020 2020 2020 2071                 q
-00012090: 7565 7279 2c0a 2020 2020 2020 2020 2020  uery,.          
-000120a0: 2020 2020 2020 2862 6174 6368 5f73 697a        (batch_siz
-000120b0: 652c 2073 656c 662e 5f67 6574 5f73 6571  e, self._get_seq
-000120c0: 5f6c 656e 6774 685f 756e 6465 725f 696e  _length_under_in
-000120d0: 6372 656d 656e 7461 6c28 7365 6c66 2e73  cremental(self.s
-000120e0: 7263 5f73 6571 5f6c 656e 6774 6829 2c0a  rc_seq_length),.
-000120f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012100: 2073 656c 662e 6e5f 6865 6164 2c20 7365   self.n_head, se
-00012110: 6c66 2e73 697a 655f 7065 725f 6865 6164  lf.size_per_head
-00012120: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
-00012130: 2830 2c20 322c 2031 2c20 3329 290a 2020  (0, 2, 1, 3)).  
-00012140: 2020 2020 2020 2320 4641 3a20 5b62 732c        # FA: [bs,
-00012150: 206e 756d 5f68 6561 6473 2c20 7365 715f   num_heads, seq_
-00012160: 6c65 6e67 7468 2c20 7369 7a65 5f70 6572  length, size_per
-00012170: 5f68 6561 645d 206f 7220 5b62 732c 206e  _head] or [bs, n
-00012180: 756d 5f68 6561 6473 2c20 7369 7a65 5f70  um_heads, size_p
-00012190: 6572 5f68 6561 642c 2073 6571 5f6c 656e  er_head, seq_len
-000121a0: 6774 685d 0a20 2020 2020 2020 206b 6579  gth].        key
-000121b0: 5f74 7261 6e73 706f 7365 5f73 6861 7065  _transpose_shape
-000121c0: 203d 2028 302c 2032 2c20 312c 2033 2920   = (0, 2, 1, 3) 
-000121d0: 6966 2073 656c 662e 7573 655f 666c 6173  if self.use_flas
-000121e0: 685f 6174 7465 6e74 696f 6e20 656c 7365  h_attention else
-000121f0: 2028 302c 2032 2c20 332c 2031 290a 2020   (0, 2, 3, 1).  
-00012200: 2020 2020 2020 6b65 7920 3d20 7365 6c66        key = self
-00012210: 2e74 7261 6e73 706f 7365 280a 2020 2020  .transpose(.    
-00012220: 2020 2020 2020 2020 462e 7265 7368 6170          F.reshap
-00012230: 6528 0a20 2020 2020 2020 2020 2020 2020  e(.             
-00012240: 2020 206b 6579 2c20 2862 6174 6368 5f73     key, (batch_s
-00012250: 697a 652c 2073 656c 662e 5f67 6574 5f73  ize, self._get_s
-00012260: 6571 5f6c 656e 6774 685f 756e 6465 725f  eq_length_under_
-00012270: 696e 6372 656d 656e 7461 6c28 7365 6c66  incremental(self
-00012280: 2e74 6774 5f73 6571 5f6c 656e 6774 6829  .tgt_seq_length)
-00012290: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000122a0: 2020 2020 2020 2020 7365 6c66 2e6e 5f68          self.n_h
-000122b0: 6561 642c 2073 656c 662e 7369 7a65 5f70  ead, self.size_p
-000122c0: 6572 5f68 6561 6429 292c 0a20 2020 2020  er_head)),.     
-000122d0: 2020 2020 2020 206b 6579 5f74 7261 6e73         key_trans
-000122e0: 706f 7365 5f73 6861 7065 290a 2020 2020  pose_shape).    
-000122f0: 2020 2020 2320 7468 6520 7265 7475 726e      # the return
-00012300: 6564 2073 6861 7065 2069 7320 5b62 732c  ed shape is [bs,
-00012310: 206e 756d 5f68 6561 6473 2c20 7365 715f   num_heads, seq_
-00012320: 6c65 6e67 7468 2c20 7369 7a65 5f70 6572  length, size_per
-00012330: 5f68 6561 645d 0a20 2020 2020 2020 2076  _head].        v
-00012340: 616c 7565 203d 2073 656c 662e 7472 616e  alue = self.tran
-00012350: 7370 6f73 6528 0a20 2020 2020 2020 2020  spose(.         
-00012360: 2020 2046 2e72 6573 6861 7065 280a 2020     F.reshape(.  
-00012370: 2020 2020 2020 2020 2020 2020 2020 7661                va
-00012380: 6c75 652c 0a20 2020 2020 2020 2020 2020  lue,.           
-00012390: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
-000123a0: 2c20 7365 6c66 2e5f 6765 745f 7365 715f  , self._get_seq_
-000123b0: 6c65 6e67 7468 5f75 6e64 6572 5f69 6e63  length_under_inc
-000123c0: 7265 6d65 6e74 616c 2873 656c 662e 7467  remental(self.tg
-000123d0: 745f 7365 715f 6c65 6e67 7468 292c 0a20  t_seq_length),. 
-000123e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000123f0: 7365 6c66 2e6e 5f68 6561 642c 2073 656c  self.n_head, sel
-00012400: 662e 7369 7a65 5f70 6572 5f68 6561 6429  f.size_per_head)
-00012410: 292c 0a20 2020 2020 2020 2020 2020 2028  ),.            (
-00012420: 302c 2032 2c20 312c 2033 2929 0a20 2020  0, 2, 1, 3)).   
-00012430: 2020 2020 2023 2073 7570 706f 7274 2069       # support i
-00012440: 6e70 7574 2073 6861 7065 2069 7320 5b62  nput shape is [b
-00012450: 732c 2073 6571 2c20 7365 715d 206f 7220  s, seq, seq] or 
-00012460: 5b62 732c 2068 6561 6473 2c20 7365 712c  [bs, heads, seq,
-00012470: 2073 6571 5d0a 2020 2020 2020 2020 6966   seq].        if
-00012480: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
-00012490: 6973 206e 6f74 204e 6f6e 6520 616e 6420  is not None and 
-000124a0: 6c65 6e28 462e 7368 6170 6528 6174 7465  len(F.shape(atte
-000124b0: 6e74 696f 6e5f 6d61 736b 2929 203d 3d20  ntion_mask)) == 
-000124c0: 3320 616e 6420 6e6f 7420 7365 6c66 2e75  3 and not self.u
-000124d0: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
-000124e0: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-000124f0: 2320 6578 7061 6e64 2061 7474 656e 7469  # expand attenti
-00012500: 6f6e 206d 6173 6b20 6672 6f6d 205b 6273  on mask from [bs
-00012510: 2c20 7365 712c 2073 6571 5d20 2d3e 205b  , seq, seq] -> [
-00012520: 6273 2c20 312c 2073 6571 2c20 7365 715d  bs, 1, seq, seq]
-00012530: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
-00012540: 656e 7469 6f6e 5f6d 6173 6b20 3d20 7365  ention_mask = se
-00012550: 6c66 2e65 7870 616e 645f 6469 6d73 2861  lf.expand_dims(a
-00012560: 7474 656e 7469 6f6e 5f6d 6173 6b2c 2031  ttention_mask, 1
-00012570: 290a 2020 2020 2020 2020 2320 6b65 7920  ).        # key 
-00012580: 616e 6420 7661 6c75 6520 666f 7220 6375  and value for cu
-00012590: 7272 656e 7420 746f 6b65 6e28 7329 0a20  rrent token(s). 
-000125a0: 2020 2020 2020 206b 6579 5f70 7265 7365         key_prese
-000125b0: 6e74 203d 206b 6579 0a20 2020 2020 2020  nt = key.       
-000125c0: 2076 616c 7565 5f70 7265 7365 6e74 203d   value_present =
-000125d0: 2076 616c 7565 0a20 2020 2020 2020 2069   value.        i
-000125e0: 6620 7365 6c66 2e75 7365 5f70 6173 743a  f self.use_past:
-000125f0: 0a20 2020 2020 2020 2020 2020 2023 2054  .            # T
-00012600: 6865 2066 6972 7374 2067 7261 7068 2077  he first graph w
-00012610: 6974 6820 7468 6520 696e 7075 7420 7369  ith the input si
-00012620: 7a65 206f 6620 2862 732c 2073 6571 5f6c  ze of (bs, seq_l
-00012630: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
-00012640: 2020 2069 6620 7365 6c66 2e69 735f 6669     if self.is_fi
-00012650: 7273 745f 6974 6572 6174 696f 6e3a 0a20  rst_iteration:. 
-00012660: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-00012670: 2047 6574 2074 6865 2076 616c 6964 2069   Get the valid i
-00012680: 6e70 7574 206c 656e 6774 6820 7769 7468  nput length with
-00012690: 6f75 7420 7061 6464 696e 670a 2020 2020  out padding.    
-000126a0: 2020 2020 2020 2020 2020 2020 7661 6c69              vali
-000126b0: 645f 6c65 6e67 7468 5f76 6563 746f 7220  d_length_vector 
-000126c0: 3d20 462e 6361 7374 2873 656c 662e 6c65  = F.cast(self.le
-000126d0: 7373 2873 656c 662e 7261 6e67 652c 2062  ss(self.range, b
-000126e0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-000126f0: 682e 7669 6577 282d 312c 2031 2c20 3129  h.view(-1, 1, 1)
-00012700: 292c 2073 656c 662e 6474 7970 6529 0a20  ), self.dtype). 
-00012710: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-00012720: 2043 6f76 6572 2074 6865 206b 6579 2061   Cover the key a
-00012730: 6e64 2076 616c 7565 206e 756d 6265 7273  nd value numbers
-00012740: 2063 6f72 7265 7370 6f6e 6469 6e67 2074   corresponding t
-00012750: 6f20 7468 6520 7061 6464 696e 6720 706f  o the padding po
-00012760: 7369 7469 6f6e 0a20 2020 2020 2020 2020  sition.         
-00012770: 2020 2020 2020 206b 6579 5f70 7265 7365         key_prese
-00012780: 6e74 203d 2073 656c 662e 6d75 6c31 286b  nt = self.mul1(k
-00012790: 6579 2c20 7365 6c66 2e65 7870 616e 645f  ey, self.expand_
-000127a0: 6469 6d73 2876 616c 6964 5f6c 656e 6774  dims(valid_lengt
-000127b0: 685f 7665 6374 6f72 2c20 3229 290a 2020  h_vector, 2)).  
-000127c0: 2020 2020 2020 2020 2020 2020 2020 7661                va
-000127d0: 6c75 655f 7072 6573 656e 7420 3d20 7365  lue_present = se
-000127e0: 6c66 2e6d 756c 3128 7661 6c75 652c 2073  lf.mul1(value, s
-000127f0: 656c 662e 6578 7061 6e64 5f64 696d 7328  elf.expand_dims(
-00012800: 7661 6c69 645f 6c65 6e67 7468 5f76 6563  valid_length_vec
-00012810: 746f 722c 2033 2929 0a20 2020 2020 2020  tor, 3)).       
-00012820: 2020 2020 2023 2054 6865 2073 6563 6f6e       # The secon
-00012830: 6420 6772 6170 6820 7769 7468 2074 6865  d graph with the
-00012840: 2069 6e70 7573 2073 697a 6520 6f66 2028   inpus size of (
-00012850: 6273 2c20 3129 0a20 2020 2020 2020 2020  bs, 1).         
-00012860: 2020 2023 2074 6865 2073 6861 7065 206f     # the shape o
-00012870: 6620 7175 6572 7920 6973 2028 6273 2c20  f query is (bs, 
-00012880: 6e75 6d5f 6865 6164 732c 2031 2c20 7369  num_heads, 1, si
-00012890: 7a65 5f70 6572 5f68 6561 6429 0a20 2020  ze_per_head).   
-000128a0: 2020 2020 2020 2020 2023 2074 6865 2073           # the s
-000128b0: 6861 7065 206f 6620 6b65 7920 6973 2020  hape of key is  
-000128c0: 2028 6273 2c20 6e75 6d5f 6865 6164 732c   (bs, num_heads,
-000128d0: 2073 697a 655f 7065 725f 6865 6164 2c20   size_per_head, 
-000128e0: 3129 0a20 2020 2020 2020 2020 2020 2023  1).            #
-000128f0: 2074 6865 2073 6861 7065 206f 6620 7661   the shape of va
-00012900: 6c75 6520 6973 2028 6273 2c20 6e75 6d5f  lue is (bs, num_
-00012910: 6865 6164 732c 2031 2c20 7369 7a65 5f70  heads, 1, size_p
-00012920: 6572 5f68 6561 6429 0a20 2020 2020 2020  er_head).       
-00012930: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00012940: 2020 2020 2020 2020 2020 2023 2047 6574             # Get
-00012950: 2074 6865 2063 7572 7265 6e74 2074 6f6b   the current tok
-00012960: 656e 2070 6f73 6974 696f 6e20 696e 6465  en position inde
-00012970: 780a 2020 2020 2020 2020 2020 2020 2020  x.              
-00012980: 2020 7661 6c69 645f 6c65 6e67 7468 203d    valid_length =
-00012990: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-000129a0: 6774 6820 2d20 310a 2020 2020 2020 2020  gth - 1.        
-000129b0: 2020 2020 2020 2020 7661 6c69 645f 6c65          valid_le
-000129c0: 6e67 7468 203d 2073 656c 662e 7265 7368  ngth = self.resh
-000129d0: 6170 6528 7661 6c69 645f 6c65 6e67 7468  ape(valid_length
-000129e0: 2c20 282d 312c 2031 2c20 3129 290a 2020  , (-1, 1, 1)).  
-000129f0: 2020 2020 2020 2020 2020 2020 2020 7661                va
-00012a00: 6c69 645f 6c65 6e67 7468 5f76 6563 746f  lid_length_vecto
-00012a10: 7220 3d20 462e 6361 7374 2873 656c 662e  r = F.cast(self.
-00012a20: 6571 7561 6c28 7661 6c69 645f 6c65 6e67  equal(valid_leng
-00012a30: 7468 2c20 7365 6c66 2e72 616e 6765 292c  th, self.range),
-00012a40: 2073 656c 662e 6474 7970 6529 0a20 2020   self.dtype).   
-00012a50: 2020 2020 2020 2020 2020 2020 2023 2050               # P
-00012a60: 6164 2074 6865 206b 6579 2061 6e64 2076  ad the key and v
-00012a70: 616c 7565 2074 6f20 7365 715f 6c65 6e67  alue to seq_leng
-00012a80: 7468 2077 6974 6820 6f6e 6c79 2074 6865  th with only the
-00012a90: 2070 6f73 6974 696f 6e20 696e 6465 7820   position index 
-00012aa0: 6e6f 7420 7a65 726f 0a20 2020 2020 2020  not zero.       
-00012ab0: 2020 2020 2020 2020 2063 7572 7265 6e74           current
-00012ac0: 5f6b 6579 203d 2073 656c 662e 6d75 6c31  _key = self.mul1
-00012ad0: 2873 656c 662e 7469 6c65 286b 6579 2c20  (self.tile(key, 
-00012ae0: 2831 2c20 312c 2031 2c20 7365 6c66 2e73  (1, 1, 1, self.s
-00012af0: 6571 5f6c 656e 6774 6829 292c 0a20 2020  eq_length)),.   
-00012b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012b20: 2020 2020 2073 656c 662e 6578 7061 6e64       self.expand
-00012b30: 5f64 696d 7328 7661 6c69 645f 6c65 6e67  _dims(valid_leng
-00012b40: 7468 5f76 6563 746f 722c 2032 2929 0a20  th_vector, 2)). 
-00012b50: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-00012b60: 7572 7265 6e74 5f76 616c 7565 203d 2073  urrent_value = s
-00012b70: 656c 662e 6d75 6c31 2873 656c 662e 7469  elf.mul1(self.ti
-00012b80: 6c65 2876 616c 7565 2c20 2831 2c20 312c  le(value, (1, 1,
-00012b90: 2073 656c 662e 7365 715f 6c65 6e67 7468   self.seq_length
-00012ba0: 2c20 3129 292c 0a20 2020 2020 2020 2020  , 1)),.         
-00012bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012bd0: 2073 656c 662e 6578 7061 6e64 5f64 696d   self.expand_dim
-00012be0: 7328 7661 6c69 645f 6c65 6e67 7468 5f76  s(valid_length_v
-00012bf0: 6563 746f 722c 2033 2929 0a20 2020 2020  ector, 3)).     
-00012c00: 2020 2020 2020 2020 2020 2023 2043 6f6e             # Con
-00012c10: 6361 7420 7468 6520 7072 6576 696f 7573  cat the previous
-00012c20: 2073 6176 6564 2073 7461 7465 2061 6e64   saved state and
-00012c30: 2063 7572 7265 6e74 2073 7461 7465 0a20   current state. 
-00012c40: 2020 2020 2020 2020 2020 2020 2020 206b                 k
-00012c50: 6579 203d 2073 656c 662e 6164 6428 6b65  ey = self.add(ke
-00012c60: 795f 7061 7374 2c20 6375 7272 656e 745f  y_past, current_
-00012c70: 6b65 7929 0a20 2020 2020 2020 2020 2020  key).           
-00012c80: 2020 2020 2076 616c 7565 203d 2073 656c       value = sel
-00012c90: 662e 6164 6428 7661 6c75 655f 7061 7374  f.add(value_past
-00012ca0: 2c20 6375 7272 656e 745f 7661 6c75 6529  , current_value)
-00012cb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00012cc0: 2023 2055 7064 6174 6520 6b65 795f 7072   # Update key_pr
-00012cd0: 6573 656e 7420 616e 6420 7661 6c75 655f  esent and value_
-00012ce0: 7072 6573 656e 7420 666f 7220 7374 6174  present for stat
-00012cf0: 6520 7570 6461 7465 0a20 2020 2020 2020  e update.       
-00012d00: 2020 2020 2020 2020 206b 6579 5f70 7265           key_pre
-00012d10: 7365 6e74 203d 206b 6579 0a20 2020 2020  sent = key.     
-00012d20: 2020 2020 2020 2020 2020 2076 616c 7565             value
-00012d30: 5f70 7265 7365 6e74 203d 2076 616c 7565  _present = value
-00012d40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00012d50: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
-00012d60: 3d20 462e 7265 7368 6170 6528 7365 6c66  = F.reshape(self
-00012d70: 2e61 7474 656e 7469 6f6e 5f6d 6173 6b2c  .attention_mask,
-00012d80: 2028 7365 6c66 2e73 6571 5f6c 656e 6774   (self.seq_lengt
-00012d90: 682c 2073 656c 662e 7365 715f 6c65 6e67  h, self.seq_leng
-00012da0: 7468 2c20 312c 2031 2929 0a0a 2020 2020  th, 1, 1))..    
-00012db0: 2020 2020 6c61 7965 725f 7072 6573 656e      layer_presen
-00012dc0: 7420 3d20 286b 6579 5f70 7265 7365 6e74  t = (key_present
-00012dd0: 2c20 7661 6c75 655f 7072 6573 656e 7429  , value_present)
-00012de0: 0a20 2020 2020 2020 2023 206d 756c 7469  .        # multi
-00012df0: 2068 6561 6420 6174 7465 6e74 696f 6e20   head attention 
-00012e00: 636f 6e73 6964 6572 696e 6720 6174 7465  considering atte
-00012e10: 6e74 696f 6e20 6d61 736b 0a20 2020 2020  ntion mask.     
-00012e20: 2020 2023 2074 6865 2072 6574 7572 6e20     # the return 
-00012e30: 7368 6170 6520 6973 205b 6273 202a 2073  shape is [bs * s
-00012e40: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-00012e50: 6e5f 7369 7a65 5d0a 2020 2020 2020 2020  n_size].        
-00012e60: 6966 2073 656c 662e 7573 655f 666c 6173  if self.use_flas
-00012e70: 685f 6174 7465 6e74 696f 6e3a 0a20 2020  h_attention:.   
-00012e80: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-00012e90: 6f6e 203d 2073 656c 662e 5f66 6c61 7368  on = self._flash
-00012ea0: 5f61 7474 6e28 7175 6572 792c 206b 6579  _attn(query, key
-00012eb0: 2c20 7661 6c75 652c 2061 7474 656e 7469  , value, attenti
-00012ec0: 6f6e 5f6d 6173 6b29 0a20 2020 2020 2020  on_mask).       
-00012ed0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-00012ee0: 2020 2061 7474 656e 7469 6f6e 203d 2073     attention = s
-00012ef0: 656c 662e 5f61 7474 6e28 7175 6572 792c  elf._attn(query,
-00012f00: 206b 6579 2c20 7661 6c75 652c 2061 7474   key, value, att
-00012f10: 656e 7469 6f6e 5f6d 6173 6b29 0a20 2020  ention_mask).   
-00012f20: 2020 2020 2023 204f 7574 7075 740a 2020       # Output.  
-00012f30: 2020 2020 2020 6f75 7470 7574 203d 2073        output = s
-00012f40: 656c 662e 7072 6f6a 6563 7469 6f6e 2861  elf.projection(a
-00012f50: 7474 656e 7469 6f6e 290a 2020 2020 2020  ttention).      
-00012f60: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
-00012f70: 6472 6f70 6f75 7428 6f75 7470 7574 290a  dropout(output).
-00012f80: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
-00012f90: 2046 2e72 6573 6861 7065 286f 7574 7075   F.reshape(outpu
-00012fa0: 742c 206f 7269 5f73 6861 7065 290a 2020  t, ori_shape).  
-00012fb0: 2020 2020 2020 6f75 7470 7574 203d 2046        output = F
-00012fc0: 2e63 6173 7428 6f75 7470 7574 2c20 6f72  .cast(output, or
-00012fd0: 695f 6474 7970 6529 0a20 2020 2020 2020  i_dtype).       
-00012fe0: 2072 6574 7572 6e20 6f75 7470 7574 2c20   return output, 
-00012ff0: 6c61 7965 725f 7072 6573 656e 740a 0a20  layer_present.. 
-00013000: 2020 2064 6566 205f 6765 745f 6261 7463     def _get_batc
-00013010: 685f 7369 7a65 5f66 726f 6d5f 7175 6572  h_size_from_quer
-00013020: 7928 7365 6c66 2c20 7175 6572 7929 3a0a  y(self, query):.
-00013030: 2020 2020 2020 2020 7222 2222 4765 7420          r"""Get 
-00013040: 7468 6520 6261 7463 6820 7369 7a65 2066  the batch size f
-00013050: 726f 6d20 7175 6572 7920 7465 6e73 6f72  rom query tensor
-00013060: 2222 220a 2020 2020 2020 2020 2320 466f  """.        # Fo
-00013070: 7220 7468 6520 696e 6372 656d 656e 7461  r the incrementa
-00013080: 6c20 7072 6564 6963 7469 6f6e 2c20 7468  l prediction, th
-00013090: 6520 7365 7120 6c65 6e67 7468 2066 6f72  e seq length for
-000130a0: 2074 6865 2069 6e70 7574 2069 7320 312e   the input is 1.
-000130b0: 0a20 2020 2020 2020 2069 6620 6c65 6e28  .        if len(
-000130c0: 462e 7368 6170 6528 7175 6572 7929 2920  F.shape(query)) 
-000130d0: 3d3d 2032 2061 6e64 2028 2873 656c 662e  == 2 and ((self.
-000130e0: 7573 655f 7061 7374 2061 6e64 2073 656c  use_past and sel
-000130f0: 662e 6973 5f66 6972 7374 5f69 7465 7261  f.is_first_itera
-00013100: 7469 6f6e 2920 6f72 2028 6e6f 7420 7365  tion) or (not se
-00013110: 6c66 2e75 7365 5f70 6173 7429 293a 0a20  lf.use_past)):. 
-00013120: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-00013130: 6e20 462e 7368 6170 6528 7175 6572 7929  n F.shape(query)
-00013140: 5b30 5d20 2f2f 2073 656c 662e 7372 635f  [0] // self.src_
-00013150: 7365 715f 6c65 6e67 7468 0a20 2020 2020  seq_length.     
-00013160: 2020 2072 6574 7572 6e20 462e 7368 6170     return F.shap
-00013170: 6528 7175 6572 7929 5b30 5d0a 0a20 2020  e(query)[0]..   
-00013180: 2064 6566 205f 6765 745f 7365 715f 6c65   def _get_seq_le
-00013190: 6e67 7468 5f75 6e64 6572 5f69 6e63 7265  ngth_under_incre
-000131a0: 6d65 6e74 616c 2873 656c 662c 206c 656e  mental(self, len
-000131b0: 6774 6829 3a0a 2020 2020 2020 2020 7222  gth):.        r"
-000131c0: 2222 5265 7475 726e 2074 6865 206c 656e  ""Return the len
-000131d0: 6774 6820 6f66 2074 6865 2074 656e 736f  gth of the tenso
-000131e0: 722e 0a20 2020 2020 2020 2020 2020 2046  r..            F
-000131f0: 6f72 2074 6865 2069 6e63 7265 6d65 6e74  or the increment
-00013200: 616c 2070 7265 6469 6374 696f 6e2c 2074  al prediction, t
-00013210: 6865 2073 6571 206c 656e 6774 6820 666f  he seq length fo
-00013220: 7220 7468 6520 696e 7075 7420 6973 2031  r the input is 1
-00013230: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
-00013240: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
-00013250: 655f 7061 7374 2061 6e64 206e 6f74 2073  e_past and not s
-00013260: 656c 662e 6973 5f66 6972 7374 5f69 7465  elf.is_first_ite
-00013270: 7261 7469 6f6e 3a0a 2020 2020 2020 2020  ration:.        
-00013280: 2020 2020 7265 7475 726e 2031 0a20 2020      return 1.   
-00013290: 2020 2020 2072 6574 7572 6e20 6c65 6e67       return leng
-000132a0: 7468 0a0a 2020 2020 6465 6620 5f63 6865  th..    def _che
-000132b0: 636b 5f69 6e70 7574 7328 7365 6c66 2c20  ck_inputs(self, 
-000132c0: 7175 6572 795f 7465 6e73 6f72 2c20 6b65  query_tensor, ke
-000132d0: 795f 7465 6e73 6f72 2c20 7661 6c75 655f  y_tensor, value_
-000132e0: 7465 6e73 6f72 2c20 6174 7465 6e74 696f  tensor, attentio
-000132f0: 6e5f 6d61 736b 2c20 6b65 795f 7061 7374  n_mask, key_past
-00013300: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
-00013310: 2020 2020 2020 2020 2020 2020 2076 616c               val
-00013320: 7565 5f70 6173 743d 4e6f 6e65 2c20 6261  ue_past=None, ba
-00013330: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-00013340: 3d4e 6f6e 6529 3a0a 2020 2020 2020 2020  =None):.        
-00013350: 7222 2222 4368 6563 6b20 696e 7075 7473  r"""Check inputs
-00013360: 2222 220a 2020 2020 2020 2020 5f63 6865  """.        _che
-00013370: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
-00013380: 2e64 7479 7065 2871 7565 7279 5f74 656e  .dtype(query_ten
-00013390: 736f 7229 2c20 2271 7565 7279 5f74 656e  sor), "query_ten
-000133a0: 736f 7222 2c0a 2020 2020 2020 2020 2020  sor",.          
-000133b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000133c0: 205b 6d73 7479 7065 2e66 6c6f 6174 3332   [mstype.float32
-000133d0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-000133e0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-000133f0: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
-00013400: 6529 0a20 2020 2020 2020 205f 6368 6563  e).        _chec
-00013410: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
-00013420: 6474 7970 6528 6b65 795f 7465 6e73 6f72  dtype(key_tensor
-00013430: 292c 2022 6b65 795f 7465 6e73 6f72 222c  ), "key_tensor",
-00013440: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013450: 2020 2020 2020 2020 2020 2020 5b6d 7374              [mst
-00013460: 7970 652e 666c 6f61 7433 322c 206d 7374  ype.float32, mst
-00013470: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
-00013480: 7970 652e 6266 6c6f 6174 3136 5d2c 2073  ype.bfloat16], s
-00013490: 656c 662e 636c 735f 6e61 6d65 290a 2020  elf.cls_name).  
-000134a0: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
-000134b0: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
-000134c0: 2876 616c 7565 5f74 656e 736f 7229 2c20  (value_tensor), 
-000134d0: 2276 616c 7565 5f74 656e 736f 7222 2c0a  "value_tensor",.
-000134e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000134f0: 2020 2020 2020 2020 2020 205b 6d73 7479             [msty
-00013500: 7065 2e66 6c6f 6174 3332 2c20 6d73 7479  pe.float32, msty
-00013510: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
-00013520: 7065 2e62 666c 6f61 7431 365d 2c20 7365  pe.bfloat16], se
-00013530: 6c66 2e63 6c73 5f6e 616d 6529 0a20 2020  lf.cls_name).   
-00013540: 2020 2020 2069 6620 6174 7465 6e74 696f       if attentio
-00013550: 6e5f 6d61 736b 2069 7320 6e6f 7420 4e6f  n_mask is not No
-00013560: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
-00013570: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
-00013580: 7065 2846 2e64 7479 7065 2861 7474 656e  pe(F.dtype(atten
-00013590: 7469 6f6e 5f6d 6173 6b29 2c20 2261 7474  tion_mask), "att
-000135a0: 656e 7469 6f6e 5f6d 6173 6b22 2c0a 2020  ention_mask",.  
-000135b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000135c0: 2020 2020 2020 2020 2020 2020 205b 6d73               [ms
-000135d0: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
-000135e0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-000135f0: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
-00013600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013610: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00013620: 656c 662e 636c 735f 6e61 6d65 290a 0a20  elf.cls_name).. 
-00013630: 2020 2020 2020 2062 6174 6368 5f76 616c         batch_val
-00013640: 6964 5f6c 656e 6774 685f 6973 5f74 656e  id_length_is_ten
-00013650: 736f 7220 3d20 6973 696e 7374 616e 6365  sor = isinstance
-00013660: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
-00013670: 6774 682c 2054 656e 736f 7229 0a20 2020  gth, Tensor).   
-00013680: 2020 2020 2062 6174 6368 5f69 735f 6465       batch_is_de
-00013690: 6661 756c 7420 3d20 6261 7463 685f 7661  fault = batch_va
-000136a0: 6c69 645f 6c65 6e67 7468 2069 7320 4e6f  lid_length is No
-000136b0: 6e65 0a20 2020 2020 2020 205f 6368 6563  ne.        _chec
-000136c0: 6b5f 7061 7374 5f6e 6f6e 655f 696e 7075  k_past_none_inpu
-000136d0: 745f 6e6f 6e65 2873 656c 662e 7573 655f  t_none(self.use_
-000136e0: 7061 7374 2c20 2262 6174 6368 5f76 616c  past, "batch_val
-000136f0: 6964 5f6c 656e 6774 6822 2c20 7365 6c66  id_length", self
-00013700: 2e63 6c73 5f6e 616d 652c 204e 6f6e 652c  .cls_name, None,
-00013710: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013730: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
-00013740: 5f6c 656e 6774 685f 6973 5f74 656e 736f  _length_is_tenso
-00013750: 722c 2062 6174 6368 5f69 735f 6465 6661  r, batch_is_defa
-00013760: 756c 7429 0a20 2020 2020 2020 2069 6620  ult).        if 
-00013770: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-00013780: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
-00013790: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
-000137a0: 6474 7970 6528 6b65 795f 7061 7374 292c  dtype(key_past),
-000137b0: 2022 6b65 795f 7061 7374 222c 205b 6d73   "key_past", [ms
-000137c0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-000137d0: 7479 7065 2e62 666c 6f61 7431 365d 2c20  type.bfloat16], 
-000137e0: 7365 6c66 2e63 6c73 5f6e 616d 6529 0a20  self.cls_name). 
-000137f0: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
-00013800: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
-00013810: 6474 7970 6528 7661 6c75 655f 7061 7374  dtype(value_past
-00013820: 292c 2022 7661 6c75 655f 7061 7374 222c  ), "value_past",
-00013830: 205b 6d73 7479 7065 2e66 6c6f 6174 3136   [mstype.float16
-00013840: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-00013850: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
-00013860: 6529 0a20 2020 2020 2020 2020 2020 205f  e).            _
-00013870: 6368 6563 6b5f 696e 7075 745f 6474 7970  check_input_dtyp
-00013880: 6528 462e 6474 7970 6528 6261 7463 685f  e(F.dtype(batch_
-00013890: 7661 6c69 645f 6c65 6e67 7468 292c 2022  valid_length), "
-000138a0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-000138b0: 7468 222c 205b 6d73 7479 7065 2e69 6e74  th", [mstype.int
-000138c0: 3332 5d2c 2073 656c 662e 636c 735f 6e61  32], self.cls_na
-000138d0: 6d65 290a 2020 2020 2020 2020 7265 7475  me).        retu
-000138e0: 726e 2054 7275 650a 0a20 2020 2064 6566  rn True..    def
-000138f0: 205f 636f 6e76 6572 745f 746f 5f32 645f   _convert_to_2d_
-00013900: 7465 6e73 6f72 2873 656c 662c 2071 7565  tensor(self, que
-00013910: 7279 5f74 656e 736f 722c 206b 6579 5f74  ry_tensor, key_t
-00013920: 656e 736f 722c 2076 616c 7565 5f74 656e  ensor, value_ten
-00013930: 736f 7229 3a0a 2020 2020 2020 2020 2222  sor):.        ""
-00013940: 2263 6f6e 7665 7274 2061 206e 6420 7465  "convert a nd te
-00013950: 6e73 6f72 2074 6f20 6120 3264 2074 656e  nsor to a 2d ten
-00013960: 736f 7222 2222 0a20 2020 2020 2020 2071  sor""".        q
-00013970: 7565 7279 5f73 6861 7065 203d 2046 2e73  uery_shape = F.s
-00013980: 6861 7065 2871 7565 7279 5f74 656e 736f  hape(query_tenso
-00013990: 7229 0a20 2020 2020 2020 2071 7565 7279  r).        query
-000139a0: 5f74 656e 736f 7220 3d20 462e 7265 7368  _tensor = F.resh
-000139b0: 6170 6528 7175 6572 795f 7465 6e73 6f72  ape(query_tensor
-000139c0: 2c20 282d 312c 2071 7565 7279 5f73 6861  , (-1, query_sha
-000139d0: 7065 5b2d 315d 2929 0a20 2020 2020 2020  pe[-1])).       
-000139e0: 206b 6579 5f73 6861 7065 203d 2046 2e73   key_shape = F.s
-000139f0: 6861 7065 286b 6579 5f74 656e 736f 7229  hape(key_tensor)
-00013a00: 0a20 2020 2020 2020 206b 6579 5f74 656e  .        key_ten
-00013a10: 736f 7220 3d20 462e 7265 7368 6170 6528  sor = F.reshape(
-00013a20: 6b65 795f 7465 6e73 6f72 2c20 282d 312c  key_tensor, (-1,
-00013a30: 206b 6579 5f73 6861 7065 5b2d 315d 2929   key_shape[-1]))
-00013a40: 0a20 2020 2020 2020 2076 616c 7565 5f73  .        value_s
-00013a50: 6861 7065 203d 2046 2e73 6861 7065 2876  hape = F.shape(v
-00013a60: 616c 7565 5f74 656e 736f 7229 0a20 2020  alue_tensor).   
-00013a70: 2020 2020 2076 616c 7565 5f74 656e 736f       value_tenso
-00013a80: 7220 3d20 462e 7265 7368 6170 6528 7661  r = F.reshape(va
-00013a90: 6c75 655f 7465 6e73 6f72 2c20 282d 312c  lue_tensor, (-1,
-00013aa0: 2076 616c 7565 5f73 6861 7065 5b2d 315d   value_shape[-1]
-00013ab0: 2929 0a0a 2020 2020 2020 2020 7265 7475  ))..        retu
-00013ac0: 726e 2071 7565 7279 5f74 656e 736f 722c  rn query_tensor,
-00013ad0: 206b 6579 5f74 656e 736f 722c 2076 616c   key_tensor, val
-00013ae0: 7565 5f74 656e 736f 720a 0a20 2020 2064  ue_tensor..    d
-00013af0: 6566 205f 6d65 7267 655f 6865 6164 7328  ef _merge_heads(
-00013b00: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
-00013b10: 2020 2222 220a 2020 2020 2020 2020 636f    """.        co
-00013b20: 6e76 6572 7420 6120 3464 2069 6e70 7574  nvert a 4d input
-00013b30: 2074 6f20 6120 3264 206f 7574 7075 740a   to a 2d output.
-00013b40: 0a20 2020 2020 2020 2049 6e70 7574 733a  .        Inputs:
-00013b50: 0a20 2020 2020 2020 2020 2020 2078 3a20  .            x: 
-00013b60: 696e 7075 7420 7465 6e73 6f72 0a0a 2020  input tensor..  
-00013b70: 2020 2020 2020 4f75 7470 7574 3a0a 2020        Output:.  
-00013b80: 2020 2020 2020 2020 2020 785f 6d65 7267            x_merg
-00013b90: 653a 2074 6865 2032 6420 6f75 7470 7574  e: the 2d output
-00013ba0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00013bb0: 2020 2020 2078 203d 2073 656c 662e 6d65       x = self.me
-00013bc0: 7267 6572 5f68 6561 645f 7472 616e 7370  rger_head_transp
-00013bd0: 6f73 6528 0a20 2020 2020 2020 2020 2020  ose(.           
-00013be0: 2078 2c20 2830 2c20 322c 2031 2c20 3329   x, (0, 2, 1, 3)
-00013bf0: 2920 2023 2062 732c 2073 6571 5f6c 656e  )  # bs, seq_len
-00013c00: 6774 682c 2068 6561 642c 2073 697a 655f  gth, head, size_
-00013c10: 7065 725f 6865 6164 0a20 2020 2020 2020  per_head.       
-00013c20: 2078 5f73 6861 7065 203d 2050 2e53 6861   x_shape = P.Sha
-00013c30: 7065 2829 2878 290a 2020 2020 2020 2020  pe()(x).        
-00013c40: 6e65 775f 7368 6170 6520 3d20 282d 312c  new_shape = (-1,
-00013c50: 2078 5f73 6861 7065 5b2d 325d 202a 2078   x_shape[-2] * x
-00013c60: 5f73 6861 7065 5b2d 315d 290a 2020 2020  _shape[-1]).    
-00013c70: 2020 2020 785f 6d65 7267 6520 3d20 7365      x_merge = se
-00013c80: 6c66 2e72 6573 6861 7065 2878 2c20 6e65  lf.reshape(x, ne
-00013c90: 775f 7368 6170 6529 0a20 2020 2020 2020  w_shape).       
-00013ca0: 2072 6574 7572 6e20 785f 6d65 7267 650a   return x_merge.
-00013cb0: 0a20 2020 2064 6566 205f 736f 6674 6d61  .    def _softma
-00013cc0: 7828 7365 6c66 2c20 6174 7465 6e74 696f  x(self, attentio
-00013cd0: 6e5f 7363 6f72 6573 293a 0a20 2020 2020  n_scores):.     
-00013ce0: 2020 2022 2222 0a20 2020 2020 2020 2046     """.        F
-00013cf0: 6f72 2074 6865 2063 6f6e 7369 6465 7261  or the considera
-00013d00: 7469 6f6e 206f 6620 7468 6520 7065 7266  tion of the perf
-00013d10: 6f72 6d61 6e63 652c 2064 6f20 736f 6674  ormance, do soft
-00013d20: 6d61 7820 6163 636f 7264 696e 6720 746f  max according to
-00013d30: 2064 6966 6665 7265 6e74 2073 6974 7561   different situa
-00013d40: 7469 6f6e 730a 2020 2020 2020 2020 3a70  tions.        :p
-00013d50: 6172 616d 2061 7474 656e 7469 6f6e 5f73  aram attention_s
-00013d60: 636f 7265 733a 2061 2033 6420 7465 6e73  cores: a 3d tens
-00013d70: 6f72 2062 6566 6f72 6520 736f 6674 6d61  or before softma
-00013d80: 780a 2020 2020 2020 2020 3a72 6574 7572  x.        :retur
-00013d90: 6e3a 2074 6865 2061 7474 656e 7469 6f6e  n: the attention
-00013da0: 2073 636f 7265 732e 0a20 2020 2020 2020   scores..       
-00013db0: 2022 2222 0a0a 2020 2020 2020 2020 6966   """..        if
-00013dc0: 2073 656c 662e 5f69 735f 6173 6365 6e64   self._is_ascend
-00013dd0: 2061 6e64 2073 656c 662e 736f 6674 6d61   and self.softma
-00013de0: 785f 6474 7970 6520 3d3d 206d 7374 7970  x_dtype == mstyp
-00013df0: 652e 666c 6f61 7431 3620 6f72 206e 6f74  e.float16 or not
-00013e00: 2073 656c 662e 5f69 735f 6173 6365 6e64   self._is_ascend
-00013e10: 3a0a 2020 2020 2020 2020 2020 2020 6174  :.            at
-00013e20: 7465 6e74 696f 6e5f 7072 6f62 7320 3d20  tention_probs = 
-00013e30: 7365 6c66 2e73 6f66 746d 6178 2861 7474  self.softmax(att
-00013e40: 656e 7469 6f6e 5f73 636f 7265 7329 0a20  ention_scores). 
-00013e50: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-00013e60: 2020 2020 2020 2020 2073 6861 7065 203d           shape =
-00013e70: 2046 2e73 6861 7065 2861 7474 656e 7469   F.shape(attenti
-00013e80: 6f6e 5f73 636f 7265 7329 0a20 2020 2020  on_scores).     
-00013e90: 2020 2020 2020 2023 2061 7474 656e 7469         # attenti
-00013ea0: 6f6e 2070 726f 6273 0a20 2020 2020 2020  on probs.       
-00013eb0: 2020 2020 2061 7474 656e 7469 6f6e 5f70       attention_p
-00013ec0: 726f 6273 203d 2073 656c 662e 736f 6674  robs = self.soft
-00013ed0: 6d61 785f 3364 280a 2020 2020 2020 2020  max_3d(.        
-00013ee0: 2020 2020 2020 2020 7365 6c66 2e73 6f66          self.sof
-00013ef0: 746d 6178 5f72 6573 6861 7065 2861 7474  tmax_reshape(att
-00013f00: 656e 7469 6f6e 5f73 636f 7265 732c 2028  ention_scores, (
-00013f10: 7368 6170 655b 305d 2c20 2d31 2c20 7368  shape[0], -1, sh
-00013f20: 6170 655b 2d31 5d29 2929 0a20 2020 2020  ape[-1]))).     
-00013f30: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-00013f40: 5f70 726f 6273 203d 2073 656c 662e 736f  _probs = self.so
-00013f50: 6674 6d61 785f 7265 7368 6170 6528 6174  ftmax_reshape(at
-00013f60: 7465 6e74 696f 6e5f 7072 6f62 732c 2073  tention_probs, s
-00013f70: 6861 7065 290a 2020 2020 2020 2020 7265  hape).        re
-00013f80: 7475 726e 2061 7474 656e 7469 6f6e 5f70  turn attention_p
-00013f90: 726f 6273 0a0a 2020 2020 6465 6620 5f66  robs..    def _f
-00013fa0: 6c61 7368 5f61 7474 6e28 7365 6c66 2c20  lash_attn(self, 
-00013fb0: 7175 6572 792c 206b 6579 2c20 7661 6c75  query, key, valu
-00013fc0: 652c 2061 7474 656e 7469 6f6e 5f6d 6173  e, attention_mas
-00013fd0: 6b29 3a0a 2020 2020 2020 2020 2222 220a  k):.        """.
-00013fe0: 2020 2020 2020 2020 666c 6173 6820 6174          flash at
-00013ff0: 7465 6e74 696f 6e0a 2020 2020 2020 2020  tention.        
-00014000: 2222 220a 2020 2020 2020 2020 6966 2061  """.        if a
-00014010: 7474 656e 7469 6f6e 5f6d 6173 6b20 6973  ttention_mask is
-00014020: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
-00014030: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-00014040: 5f6d 6173 6b5f 6474 7970 6520 3d20 6368  _mask_dtype = ch
-00014050: 6f6f 7365 5f66 6c61 7368 5f61 7474 656e  oose_flash_atten
-00014060: 7469 6f6e 5f64 7479 7065 2829 0a20 2020  tion_dtype().   
-00014070: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-00014080: 6f6e 5f6d 6173 6b20 3d20 7365 6c66 2e73  on_mask = self.s
-00014090: 7562 280a 2020 2020 2020 2020 2020 2020  ub(.            
-000140a0: 2020 2020 502e 4361 7374 2829 2873 656c      P.Cast()(sel
-000140b0: 662e 6f6e 652c 2061 7474 656e 7469 6f6e  f.one, attention
-000140c0: 5f6d 6173 6b5f 6474 7970 6529 2c0a 2020  _mask_dtype),.  
-000140d0: 2020 2020 2020 2020 2020 2020 2020 502e                P.
-000140e0: 4361 7374 2829 2861 7474 656e 7469 6f6e  Cast()(attention
-000140f0: 5f6d 6173 6b2c 2061 7474 656e 7469 6f6e  _mask, attention
-00014100: 5f6d 6173 6b5f 6474 7970 6529 290a 0a20  _mask_dtype)).. 
-00014110: 2020 2020 2020 2077 6569 6768 7465 645f         weighted_
-00014120: 7661 6c75 6573 203d 2073 656c 662e 666c  values = self.fl
-00014130: 6173 685f 6174 7465 6e74 696f 6e28 7175  ash_attention(qu
-00014140: 6572 792c 206b 6579 2c20 7661 6c75 652c  ery, key, value,
-00014150: 2061 7474 656e 7469 6f6e 5f6d 6173 6b29   attention_mask)
-00014160: 0a20 2020 2020 2020 2061 7474 656e 7469  .        attenti
-00014170: 6f6e 5f6d 6572 6765 203d 2073 656c 662e  on_merge = self.
-00014180: 5f6d 6572 6765 5f68 6561 6473 2877 6569  _merge_heads(wei
-00014190: 6768 7465 645f 7661 6c75 6573 290a 2020  ghted_values).  
-000141a0: 2020 2020 2020 7265 7475 726e 2061 7474        return att
-000141b0: 656e 7469 6f6e 5f6d 6572 6765 0a0a 2020  ention_merge..  
-000141c0: 2020 6465 6620 5f61 7474 6e28 7365 6c66    def _attn(self
-000141d0: 2c20 7175 6572 792c 206b 6579 2c20 7661  , query, key, va
-000141e0: 6c75 652c 2061 7474 656e 7469 6f6e 5f6d  lue, attention_m
-000141f0: 6173 6b29 3a0a 2020 2020 2020 2020 2222  ask):.        ""
-00014200: 220a 2020 2020 2020 2020 4765 7420 7468  ".        Get th
-00014210: 6520 7765 6967 6874 6564 2073 636f 7265  e weighted score
-00014220: 2061 6c6f 6e67 2074 6865 2073 6571 5f6c   along the seq_l
-00014230: 656e 6774 680a 0a20 2020 2020 2020 2049  ength..        I
-00014240: 6e70 7574 733a 0a20 2020 2020 2020 2020  nputs:.         
-00014250: 2020 2071 7565 7279 3a20 7468 6520 7175     query: the qu
-00014260: 6572 7920 6d61 7472 6978 0a20 2020 2020  ery matrix.     
-00014270: 2020 2020 2020 206b 6579 3a20 7468 6520         key: the 
-00014280: 6b65 7920 6d61 7472 6978 0a20 2020 2020  key matrix.     
-00014290: 2020 2020 2020 2076 616c 7565 3a20 7468         value: th
-000142a0: 6520 7661 6c75 6520 6d61 7472 6978 0a20  e value matrix. 
-000142b0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-000142c0: 7469 6f6e 5f6d 6173 6b3a 2074 6865 2061  tion_mask: the a
-000142d0: 7474 656e 7469 6f6e 206d 6173 6b20 6d61  ttention mask ma
-000142e0: 7472 6978 2077 6974 6820 7368 6170 6520  trix with shape 
-000142f0: 2862 6174 6368 5f73 697a 652c 0a20 2020  (batch_size,.   
-00014300: 2020 2020 2020 2020 2031 2c20 7365 715f           1, seq_
-00014310: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
-00014320: 7468 290a 2020 2020 2020 2020 4f75 7470  th).        Outp
-00014330: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-00014340: 2077 6569 6768 7465 645f 7661 6c75 6573   weighted_values
-00014350: 3a20 5465 6e73 6f72 2c20 7468 6520 7765  : Tensor, the we
-00014360: 6967 6874 6564 2073 756d 2073 636f 7265  ighted sum score
-00014370: 730a 2020 2020 2020 2020 2222 220a 2020  s.        """.  
-00014380: 2020 2020 2020 2320 4e6f 726d 616c 697a        # Normaliz
-00014390: 6520 7175 6572 7920 616e 6420 6b65 7920  e query and key 
-000143a0: 6265 666f 7265 204d 6174 4d75 6c2c 2064  before MatMul, d
-000143b0: 6566 6175 6c74 206f 6666 0a20 2020 2020  efault off.     
-000143c0: 2020 2023 2041 7474 656e 7469 6f6e 2073     # Attention s
-000143d0: 636f 7265 205b 6273 2c20 6e75 6d5f 6865  core [bs, num_he
-000143e0: 6164 732c 2073 6571 5f6c 656e 6774 682c  ads, seq_length,
-000143f0: 2073 6571 5f6c 656e 6774 685d 0a20 2020   seq_length].   
-00014400: 2020 2020 2066 6163 746f 7220 3d20 502e       factor = P.
-00014410: 4361 7374 2829 2873 656c 662e 7363 616c  Cast()(self.scal
-00014420: 655f 6661 6374 6f72 2c20 502e 4454 7970  e_factor, P.DTyp
-00014430: 6528 2928 7175 6572 7929 290a 2020 2020  e()(query)).    
-00014440: 2020 2020 7175 6572 7920 3d20 7365 6c66      query = self
-00014450: 2e72 6561 6c5f 6469 7628 7175 6572 792c  .real_div(query,
-00014460: 2066 6163 746f 7229 0a20 2020 2020 2020   factor).       
-00014470: 206b 6579 203d 2073 656c 662e 7265 616c   key = self.real
-00014480: 5f64 6976 286b 6579 2c20 6661 6374 6f72  _div(key, factor
-00014490: 290a 2020 2020 2020 2020 7363 6f72 6520  ).        score 
-000144a0: 3d20 7365 6c66 2e62 6174 6368 5f6d 6174  = self.batch_mat
-000144b0: 6d75 6c28 7175 6572 792c 206b 6579 290a  mul(query, key).
-000144c0: 0a20 2020 2020 2020 206f 7269 5f64 7479  .        ori_dty
-000144d0: 7065 203d 2050 2e44 5479 7065 2829 2873  pe = P.DType()(s
-000144e0: 636f 7265 290a 2020 2020 2020 2020 6174  core).        at
-000144f0: 7465 6e74 696f 6e5f 7363 6f72 6573 203d  tention_scores =
-00014500: 2073 656c 662e 736f 6674 6d61 785f 6361   self.softmax_ca
-00014510: 7374 2873 636f 7265 2c20 7365 6c66 2e73  st(score, self.s
-00014520: 6f66 746d 6178 5f64 7479 7065 290a 0a20  oftmax_dtype).. 
-00014530: 2020 2020 2020 2023 2066 6f72 2069 6e70         # for inp
-00014540: 7574 2073 697a 6520 6f66 2028 6273 2c20  ut size of (bs, 
-00014550: 3129 206e 616d 656c 7920 7468 6520 7365  1) namely the se
-00014560: 636f 6e64 2067 7261 7068 2c0a 2020 2020  cond graph,.    
-00014570: 2020 2020 2320 7468 6520 7368 6170 6520      # the shape 
-00014580: 6f66 2061 7474 656e 7469 6f6e 5f6d 6173  of attention_mas
-00014590: 6b20 6d61 7472 6978 2073 686f 756c 6420  k matrix should 
-000145a0: 6265 2028 6273 2c20 312c 2031 2c20 7365  be (bs, 1, 1, se
-000145b0: 715f 6c65 6e67 7468 290a 2020 2020 2020  q_length).      
-000145c0: 2020 6966 2061 7474 656e 7469 6f6e 5f6d    if attention_m
-000145d0: 6173 6b20 6973 206e 6f74 204e 6f6e 653a  ask is not None:
-000145e0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-000145f0: 7365 6c66 2e75 7365 5f70 6173 7420 616e  self.use_past an
-00014600: 6420 6e6f 7420 7365 6c66 2e69 735f 6669  d not self.is_fi
-00014610: 7273 745f 6974 6572 6174 696f 6e3a 0a20  rst_iteration:. 
-00014620: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-00014630: 2043 616c 6375 6c61 7465 2074 6865 2063   Calculate the c
-00014640: 7572 7265 6e74 2074 6f74 616c 2074 6f6b  urrent total tok
-00014650: 656e 0a20 2020 2020 2020 2020 2020 2020  en.             
-00014660: 2020 2063 7572 7265 6e74 5f69 6e64 6578     current_index
-00014670: 203d 2073 656c 662e 7265 6475 6365 7375   = self.reducesu
-00014680: 6d28 462e 6361 7374 2873 656c 662e 6e6f  m(F.cast(self.no
-00014690: 745f 6571 7561 6c28 7365 6c66 2e73 6c69  t_equal(self.sli
-000146a0: 6365 286b 6579 2c20 2830 2c20 302c 2030  ce(key, (0, 0, 0
-000146b0: 2c20 3029 2c0a 2020 2020 2020 2020 2020  , 0),.          
-000146c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000146d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e70: 2020 2020 2020 2020 2020 2076 616c 7565             value
+00011e80: 5f74 656e 736f 7229 0a20 2020 2020 2020  _tensor).       
+00011e90: 206f 7269 5f64 7479 7065 203d 2046 2e64   ori_dtype = F.d
+00011ea0: 7479 7065 2871 7565 7279 5f74 656e 736f  type(query_tenso
+00011eb0: 7229 0a20 2020 2020 2020 2071 7565 7279  r).        query
+00011ec0: 5f74 656e 736f 7220 3d20 462e 6361 7374  _tensor = F.cast
+00011ed0: 2871 7565 7279 5f74 656e 736f 722c 2073  (query_tensor, s
+00011ee0: 656c 662e 6474 7970 6529 0a20 2020 2020  elf.dtype).     
+00011ef0: 2020 206b 6579 5f74 656e 736f 7220 3d20     key_tensor = 
+00011f00: 462e 6361 7374 286b 6579 5f74 656e 736f  F.cast(key_tenso
+00011f10: 722c 2073 656c 662e 6474 7970 6529 0a20  r, self.dtype). 
+00011f20: 2020 2020 2020 2076 616c 7565 5f74 656e         value_ten
+00011f30: 736f 7220 3d20 462e 6361 7374 2876 616c  sor = F.cast(val
+00011f40: 7565 5f74 656e 736f 722c 2073 656c 662e  ue_tensor, self.
+00011f50: 6474 7970 6529 0a20 2020 2020 2020 2023  dtype).        #
+00011f60: 206d 756c 7469 2068 6561 6420 6174 7465   multi head atte
+00011f70: 6e74 696f 6e3a 2071 7565 7279 2c20 6b65  ntion: query, ke
+00011f80: 792c 2076 616c 7565 2061 7265 2064 6572  y, value are der
+00011f90: 6976 6564 2066 726f 6d20 7468 6520 7361  ived from the sa
+00011fa0: 6d65 2069 6e70 7574 730a 2020 2020 2020  me inputs.      
+00011fb0: 2020 7175 6572 7920 3d20 7365 6c66 2e64    query = self.d
+00011fc0: 656e 7365 3128 7175 6572 795f 7465 6e73  ense1(query_tens
+00011fd0: 6f72 290a 2020 2020 2020 2020 6b65 7920  or).        key 
+00011fe0: 3d20 7365 6c66 2e64 656e 7365 3228 6b65  = self.dense2(ke
+00011ff0: 795f 7465 6e73 6f72 290a 2020 2020 2020  y_tensor).      
+00012000: 2020 7661 6c75 6520 3d20 7365 6c66 2e64    value = self.d
+00012010: 656e 7365 3328 7661 6c75 655f 7465 6e73  ense3(value_tens
+00012020: 6f72 290a 2020 2020 2020 2020 2320 7468  or).        # th
+00012030: 6520 7265 7475 726e 6564 2073 6861 7065  e returned shape
+00012040: 2069 7320 5b62 732c 206e 756d 5f68 6561   is [bs, num_hea
+00012050: 6473 2c20 7365 715f 6c65 6e67 7468 2c20  ds, seq_length, 
+00012060: 7369 7a65 5f70 6572 5f68 6561 645d 0a20  size_per_head]. 
+00012070: 2020 2020 2020 2071 7565 7279 203d 2073         query = s
+00012080: 656c 662e 7472 616e 7370 6f73 6528 0a20  elf.transpose(. 
+00012090: 2020 2020 2020 2020 2020 2046 2e72 6573             F.res
+000120a0: 6861 7065 280a 2020 2020 2020 2020 2020  hape(.          
+000120b0: 2020 2020 2020 7175 6572 792c 0a20 2020        query,.   
+000120c0: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
+000120d0: 7463 685f 7369 7a65 2c20 7365 6c66 2e5f  tch_size, self._
+000120e0: 6765 745f 7365 715f 6c65 6e67 7468 5f75  get_seq_length_u
+000120f0: 6e64 6572 5f69 6e63 7265 6d65 6e74 616c  nder_incremental
+00012100: 2873 656c 662e 7372 635f 7365 715f 6c65  (self.src_seq_le
+00012110: 6e67 7468 292c 0a20 2020 2020 2020 2020  ngth),.         
+00012120: 2020 2020 2020 2020 7365 6c66 2e6e 5f68          self.n_h
+00012130: 6561 642c 2073 656c 662e 7369 7a65 5f70  ead, self.size_p
+00012140: 6572 5f68 6561 6429 292c 0a20 2020 2020  er_head)),.     
+00012150: 2020 2020 2020 2028 302c 2032 2c20 312c         (0, 2, 1,
+00012160: 2033 2929 0a20 2020 2020 2020 2023 2046   3)).        # F
+00012170: 413a 205b 6273 2c20 6e75 6d5f 6865 6164  A: [bs, num_head
+00012180: 732c 2073 6571 5f6c 656e 6774 682c 2073  s, seq_length, s
+00012190: 697a 655f 7065 725f 6865 6164 5d20 6f72  ize_per_head] or
+000121a0: 205b 6273 2c20 6e75 6d5f 6865 6164 732c   [bs, num_heads,
+000121b0: 2073 697a 655f 7065 725f 6865 6164 2c20   size_per_head, 
+000121c0: 7365 715f 6c65 6e67 7468 5d0a 2020 2020  seq_length].    
+000121d0: 2020 2020 6b65 795f 7472 616e 7370 6f73      key_transpos
+000121e0: 655f 7368 6170 6520 3d20 2830 2c20 322c  e_shape = (0, 2,
+000121f0: 2031 2c20 3329 2069 6620 7365 6c66 2e75   1, 3) if self.u
+00012200: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
+00012210: 6f6e 2065 6c73 6520 2830 2c20 322c 2033  on else (0, 2, 3
+00012220: 2c20 3129 0a20 2020 2020 2020 206b 6579  , 1).        key
+00012230: 203d 2073 656c 662e 7472 616e 7370 6f73   = self.transpos
+00012240: 6528 0a20 2020 2020 2020 2020 2020 2046  e(.            F
+00012250: 2e72 6573 6861 7065 280a 2020 2020 2020  .reshape(.      
+00012260: 2020 2020 2020 2020 2020 6b65 792c 2028            key, (
+00012270: 6261 7463 685f 7369 7a65 2c20 7365 6c66  batch_size, self
+00012280: 2e5f 6765 745f 7365 715f 6c65 6e67 7468  ._get_seq_length
+00012290: 5f75 6e64 6572 5f69 6e63 7265 6d65 6e74  _under_increment
+000122a0: 616c 2873 656c 662e 7467 745f 7365 715f  al(self.tgt_seq_
+000122b0: 6c65 6e67 7468 292c 0a20 2020 2020 2020  length),.       
+000122c0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000122d0: 656c 662e 6e5f 6865 6164 2c20 7365 6c66  elf.n_head, self
+000122e0: 2e73 697a 655f 7065 725f 6865 6164 2929  .size_per_head))
+000122f0: 2c0a 2020 2020 2020 2020 2020 2020 6b65  ,.            ke
+00012300: 795f 7472 616e 7370 6f73 655f 7368 6170  y_transpose_shap
+00012310: 6529 0a20 2020 2020 2020 2023 2074 6865  e).        # the
+00012320: 2072 6574 7572 6e65 6420 7368 6170 6520   returned shape 
+00012330: 6973 205b 6273 2c20 6e75 6d5f 6865 6164  is [bs, num_head
+00012340: 732c 2073 6571 5f6c 656e 6774 682c 2073  s, seq_length, s
+00012350: 697a 655f 7065 725f 6865 6164 5d0a 2020  ize_per_head].  
+00012360: 2020 2020 2020 7661 6c75 6520 3d20 7365        value = se
+00012370: 6c66 2e74 7261 6e73 706f 7365 280a 2020  lf.transpose(.  
+00012380: 2020 2020 2020 2020 2020 462e 7265 7368            F.resh
+00012390: 6170 6528 0a20 2020 2020 2020 2020 2020  ape(.           
+000123a0: 2020 2020 2076 616c 7565 2c0a 2020 2020       value,.    
+000123b0: 2020 2020 2020 2020 2020 2020 2862 6174              (bat
+000123c0: 6368 5f73 697a 652c 2073 656c 662e 5f67  ch_size, self._g
+000123d0: 6574 5f73 6571 5f6c 656e 6774 685f 756e  et_seq_length_un
+000123e0: 6465 725f 696e 6372 656d 656e 7461 6c28  der_incremental(
+000123f0: 7365 6c66 2e74 6774 5f73 6571 5f6c 656e  self.tgt_seq_len
+00012400: 6774 6829 2c0a 2020 2020 2020 2020 2020  gth),.          
+00012410: 2020 2020 2020 2073 656c 662e 6e5f 6865         self.n_he
+00012420: 6164 2c20 7365 6c66 2e73 697a 655f 7065  ad, self.size_pe
+00012430: 725f 6865 6164 2929 2c0a 2020 2020 2020  r_head)),.      
+00012440: 2020 2020 2020 2830 2c20 322c 2031 2c20        (0, 2, 1, 
+00012450: 3329 290a 2020 2020 2020 2020 2320 7375  3)).        # su
+00012460: 7070 6f72 7420 696e 7075 7420 7368 6170  pport input shap
+00012470: 6520 6973 205b 6273 2c20 7365 712c 2073  e is [bs, seq, s
+00012480: 6571 5d20 6f72 205b 6273 2c20 6865 6164  eq] or [bs, head
+00012490: 732c 2073 6571 2c20 7365 715d 0a20 2020  s, seq, seq].   
+000124a0: 2020 2020 2069 6620 6174 7465 6e74 696f       if attentio
+000124b0: 6e5f 6d61 736b 2069 7320 6e6f 7420 4e6f  n_mask is not No
+000124c0: 6e65 2061 6e64 206c 656e 2846 2e73 6861  ne and len(F.sha
+000124d0: 7065 2861 7474 656e 7469 6f6e 5f6d 6173  pe(attention_mas
+000124e0: 6b29 2920 3d3d 2033 2061 6e64 206e 6f74  k)) == 3 and not
+000124f0: 2073 656c 662e 7573 655f 666c 6173 685f   self.use_flash_
+00012500: 6174 7465 6e74 696f 6e3a 0a20 2020 2020  attention:.     
+00012510: 2020 2020 2020 2023 2065 7870 616e 6420         # expand 
+00012520: 6174 7465 6e74 696f 6e20 6d61 736b 2066  attention mask f
+00012530: 726f 6d20 5b62 732c 2073 6571 2c20 7365  rom [bs, seq, se
+00012540: 715d 202d 3e20 5b62 732c 2031 2c20 7365  q] -> [bs, 1, se
+00012550: 712c 2073 6571 5d0a 2020 2020 2020 2020  q, seq].        
+00012560: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
+00012570: 736b 203d 2073 656c 662e 6578 7061 6e64  sk = self.expand
+00012580: 5f64 696d 7328 6174 7465 6e74 696f 6e5f  _dims(attention_
+00012590: 6d61 736b 2c20 3129 0a20 2020 2020 2020  mask, 1).       
+000125a0: 2023 206b 6579 2061 6e64 2076 616c 7565   # key and value
+000125b0: 2066 6f72 2063 7572 7265 6e74 2074 6f6b   for current tok
+000125c0: 656e 2873 290a 2020 2020 2020 2020 6b65  en(s).        ke
+000125d0: 795f 7072 6573 656e 7420 3d20 6b65 790a  y_present = key.
+000125e0: 2020 2020 2020 2020 7661 6c75 655f 7072          value_pr
+000125f0: 6573 656e 7420 3d20 7661 6c75 650a 2020  esent = value.  
+00012600: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
+00012610: 655f 7061 7374 3a0a 2020 2020 2020 2020  e_past:.        
+00012620: 2020 2020 2320 5468 6520 6669 7273 7420      # The first 
+00012630: 6772 6170 6820 7769 7468 2074 6865 2069  graph with the i
+00012640: 6e70 7574 2073 697a 6520 6f66 2028 6273  nput size of (bs
+00012650: 2c20 7365 715f 6c65 6e67 7468 290a 2020  , seq_length).  
+00012660: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+00012670: 662e 6973 5f66 6972 7374 5f69 7465 7261  f.is_first_itera
+00012680: 7469 6f6e 3a0a 2020 2020 2020 2020 2020  tion:.          
+00012690: 2020 2020 2020 2320 4765 7420 7468 6520        # Get the 
+000126a0: 7661 6c69 6420 696e 7075 7420 6c65 6e67  valid input leng
+000126b0: 7468 2077 6974 686f 7574 2070 6164 6469  th without paddi
+000126c0: 6e67 0a20 2020 2020 2020 2020 2020 2020  ng.             
+000126d0: 2020 2076 616c 6964 5f6c 656e 6774 685f     valid_length_
+000126e0: 7665 6374 6f72 203d 2046 2e63 6173 7428  vector = F.cast(
+000126f0: 7365 6c66 2e6c 6573 7328 7365 6c66 2e72  self.less(self.r
+00012700: 616e 6765 2c20 6261 7463 685f 7661 6c69  ange, batch_vali
+00012710: 645f 6c65 6e67 7468 2e76 6965 7728 2d31  d_length.view(-1
+00012720: 2c20 312c 2031 2929 2c20 7365 6c66 2e64  , 1, 1)), self.d
+00012730: 7479 7065 290a 2020 2020 2020 2020 2020  type).          
+00012740: 2020 2020 2020 2320 436f 7665 7220 7468        # Cover th
+00012750: 6520 6b65 7920 616e 6420 7661 6c75 6520  e key and value 
+00012760: 6e75 6d62 6572 7320 636f 7272 6573 706f  numbers correspo
+00012770: 6e64 696e 6720 746f 2074 6865 2070 6164  nding to the pad
+00012780: 6469 6e67 2070 6f73 6974 696f 6e0a 2020  ding position.  
+00012790: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
+000127a0: 795f 7072 6573 656e 7420 3d20 7365 6c66  y_present = self
+000127b0: 2e6d 756c 3128 6b65 792c 2073 656c 662e  .mul1(key, self.
+000127c0: 6578 7061 6e64 5f64 696d 7328 7661 6c69  expand_dims(vali
+000127d0: 645f 6c65 6e67 7468 5f76 6563 746f 722c  d_length_vector,
+000127e0: 2032 2929 0a20 2020 2020 2020 2020 2020   2)).           
+000127f0: 2020 2020 2076 616c 7565 5f70 7265 7365       value_prese
+00012800: 6e74 203d 2073 656c 662e 6d75 6c31 2876  nt = self.mul1(v
+00012810: 616c 7565 2c20 7365 6c66 2e65 7870 616e  alue, self.expan
+00012820: 645f 6469 6d73 2876 616c 6964 5f6c 656e  d_dims(valid_len
+00012830: 6774 685f 7665 6374 6f72 2c20 3329 290a  gth_vector, 3)).
+00012840: 2020 2020 2020 2020 2020 2020 2320 5468              # Th
+00012850: 6520 7365 636f 6e64 2067 7261 7068 2077  e second graph w
+00012860: 6974 6820 7468 6520 696e 7075 7320 7369  ith the inpus si
+00012870: 7a65 206f 6620 2862 732c 2031 290a 2020  ze of (bs, 1).  
+00012880: 2020 2020 2020 2020 2020 2320 7468 6520            # the 
+00012890: 7368 6170 6520 6f66 2071 7565 7279 2069  shape of query i
+000128a0: 7320 2862 732c 206e 756d 5f68 6561 6473  s (bs, num_heads
+000128b0: 2c20 312c 2073 697a 655f 7065 725f 6865  , 1, size_per_he
+000128c0: 6164 290a 2020 2020 2020 2020 2020 2020  ad).            
+000128d0: 2320 7468 6520 7368 6170 6520 6f66 206b  # the shape of k
+000128e0: 6579 2069 7320 2020 2862 732c 206e 756d  ey is   (bs, num
+000128f0: 5f68 6561 6473 2c20 7369 7a65 5f70 6572  _heads, size_per
+00012900: 5f68 6561 642c 2031 290a 2020 2020 2020  _head, 1).      
+00012910: 2020 2020 2020 2320 7468 6520 7368 6170        # the shap
+00012920: 6520 6f66 2076 616c 7565 2069 7320 2862  e of value is (b
+00012930: 732c 206e 756d 5f68 6561 6473 2c20 312c  s, num_heads, 1,
+00012940: 2073 697a 655f 7065 725f 6865 6164 290a   size_per_head).
+00012950: 2020 2020 2020 2020 2020 2020 656c 7365              else
+00012960: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00012970: 2020 2320 4765 7420 7468 6520 6375 7272    # Get the curr
+00012980: 656e 7420 746f 6b65 6e20 706f 7369 7469  ent token positi
+00012990: 6f6e 2069 6e64 6578 0a20 2020 2020 2020  on index.       
+000129a0: 2020 2020 2020 2020 2076 616c 6964 5f6c           valid_l
+000129b0: 656e 6774 6820 3d20 6261 7463 685f 7661  ength = batch_va
+000129c0: 6c69 645f 6c65 6e67 7468 202d 2031 0a20  lid_length - 1. 
+000129d0: 2020 2020 2020 2020 2020 2020 2020 2076                 v
+000129e0: 616c 6964 5f6c 656e 6774 6820 3d20 7365  alid_length = se
+000129f0: 6c66 2e72 6573 6861 7065 2876 616c 6964  lf.reshape(valid
+00012a00: 5f6c 656e 6774 682c 2028 2d31 2c20 312c  _length, (-1, 1,
+00012a10: 2031 2929 0a20 2020 2020 2020 2020 2020   1)).           
+00012a20: 2020 2020 2076 616c 6964 5f6c 656e 6774       valid_lengt
+00012a30: 685f 7665 6374 6f72 203d 2046 2e63 6173  h_vector = F.cas
+00012a40: 7428 7365 6c66 2e65 7175 616c 2876 616c  t(self.equal(val
+00012a50: 6964 5f6c 656e 6774 682c 2073 656c 662e  id_length, self.
+00012a60: 7261 6e67 6529 2c20 7365 6c66 2e64 7479  range), self.dty
+00012a70: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00012a80: 2020 2020 2320 5061 6420 7468 6520 6b65      # Pad the ke
+00012a90: 7920 616e 6420 7661 6c75 6520 746f 2073  y and value to s
+00012aa0: 6571 5f6c 656e 6774 6820 7769 7468 206f  eq_length with o
+00012ab0: 6e6c 7920 7468 6520 706f 7369 7469 6f6e  nly the position
+00012ac0: 2069 6e64 6578 206e 6f74 207a 6572 6f0a   index not zero.
+00012ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012ae0: 6375 7272 656e 745f 6b65 7920 3d20 7365  current_key = se
+00012af0: 6c66 2e6d 756c 3128 7365 6c66 2e74 696c  lf.mul1(self.til
+00012b00: 6528 6b65 792c 2028 312c 2031 2c20 312c  e(key, (1, 1, 1,
+00012b10: 2073 656c 662e 7365 715f 6c65 6e67 7468   self.seq_length
+00012b20: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
+00012b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012b40: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00012b50: 2e65 7870 616e 645f 6469 6d73 2876 616c  .expand_dims(val
+00012b60: 6964 5f6c 656e 6774 685f 7665 6374 6f72  id_length_vector
+00012b70: 2c20 3229 290a 2020 2020 2020 2020 2020  , 2)).          
+00012b80: 2020 2020 2020 6375 7272 656e 745f 7661        current_va
+00012b90: 6c75 6520 3d20 7365 6c66 2e6d 756c 3128  lue = self.mul1(
+00012ba0: 7365 6c66 2e74 696c 6528 7661 6c75 652c  self.tile(value,
+00012bb0: 2028 312c 2031 2c20 7365 6c66 2e73 6571   (1, 1, self.seq
+00012bc0: 5f6c 656e 6774 682c 2031 2929 2c0a 2020  _length, 1)),.  
+00012bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012bf0: 2020 2020 2020 2020 7365 6c66 2e65 7870          self.exp
+00012c00: 616e 645f 6469 6d73 2876 616c 6964 5f6c  and_dims(valid_l
+00012c10: 656e 6774 685f 7665 6374 6f72 2c20 3329  ength_vector, 3)
+00012c20: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00012c30: 2020 2320 436f 6e63 6174 2074 6865 2070    # Concat the p
+00012c40: 7265 7669 6f75 7320 7361 7665 6420 7374  revious saved st
+00012c50: 6174 6520 616e 6420 6375 7272 656e 7420  ate and current 
+00012c60: 7374 6174 650a 2020 2020 2020 2020 2020  state.          
+00012c70: 2020 2020 2020 6b65 7920 3d20 7365 6c66        key = self
+00012c80: 2e61 6464 286b 6579 5f70 6173 742c 2063  .add(key_past, c
+00012c90: 7572 7265 6e74 5f6b 6579 290a 2020 2020  urrent_key).    
+00012ca0: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
+00012cb0: 6520 3d20 7365 6c66 2e61 6464 2876 616c  e = self.add(val
+00012cc0: 7565 5f70 6173 742c 2063 7572 7265 6e74  ue_past, current
+00012cd0: 5f76 616c 7565 290a 2020 2020 2020 2020  _value).        
+00012ce0: 2020 2020 2020 2020 2320 5570 6461 7465          # Update
+00012cf0: 206b 6579 5f70 7265 7365 6e74 2061 6e64   key_present and
+00012d00: 2076 616c 7565 5f70 7265 7365 6e74 2066   value_present f
+00012d10: 6f72 2073 7461 7465 2075 7064 6174 650a  or state update.
+00012d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012d30: 6b65 795f 7072 6573 656e 7420 3d20 6b65  key_present = ke
+00012d40: 790a 2020 2020 2020 2020 2020 2020 2020  y.              
+00012d50: 2020 7661 6c75 655f 7072 6573 656e 7420    value_present 
+00012d60: 3d20 7661 6c75 650a 2020 2020 2020 2020  = value.        
+00012d70: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+00012d80: 6e5f 6d61 736b 203d 2046 2e72 6573 6861  n_mask = F.resha
+00012d90: 7065 2873 656c 662e 6174 7465 6e74 696f  pe(self.attentio
+00012da0: 6e5f 6d61 736b 2c20 2873 656c 662e 7365  n_mask, (self.se
+00012db0: 715f 6c65 6e67 7468 2c20 7365 6c66 2e73  q_length, self.s
+00012dc0: 6571 5f6c 656e 6774 682c 2031 2c20 3129  eq_length, 1, 1)
+00012dd0: 290a 0a20 2020 2020 2020 206c 6179 6572  )..        layer
+00012de0: 5f70 7265 7365 6e74 203d 2028 6b65 795f  _present = (key_
+00012df0: 7072 6573 656e 742c 2076 616c 7565 5f70  present, value_p
+00012e00: 7265 7365 6e74 290a 2020 2020 2020 2020  resent).        
+00012e10: 2320 6d75 6c74 6920 6865 6164 2061 7474  # multi head att
+00012e20: 656e 7469 6f6e 2063 6f6e 7369 6465 7269  ention consideri
+00012e30: 6e67 2061 7474 656e 7469 6f6e 206d 6173  ng attention mas
+00012e40: 6b0a 2020 2020 2020 2020 2320 7468 6520  k.        # the 
+00012e50: 7265 7475 726e 2073 6861 7065 2069 7320  return shape is 
+00012e60: 5b62 7320 2a20 7365 715f 6c65 6e67 7468  [bs * seq_length
+00012e70: 2c20 6869 6464 656e 5f73 697a 655d 0a20  , hidden_size]. 
+00012e80: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+00012e90: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
+00012ea0: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
+00012eb0: 6174 7465 6e74 696f 6e20 3d20 7365 6c66  attention = self
+00012ec0: 2e5f 666c 6173 685f 6174 746e 2871 7565  ._flash_attn(que
+00012ed0: 7279 2c20 6b65 792c 2076 616c 7565 2c20  ry, key, value, 
+00012ee0: 6174 7465 6e74 696f 6e5f 6d61 736b 290a  attention_mask).
+00012ef0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00012f00: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+00012f10: 696f 6e20 3d20 7365 6c66 2e5f 6174 746e  ion = self._attn
+00012f20: 2871 7565 7279 2c20 6b65 792c 2076 616c  (query, key, val
+00012f30: 7565 2c20 6174 7465 6e74 696f 6e5f 6d61  ue, attention_ma
+00012f40: 736b 290a 2020 2020 2020 2020 2320 4f75  sk).        # Ou
+00012f50: 7470 7574 0a20 2020 2020 2020 206f 7574  tput.        out
+00012f60: 7075 7420 3d20 7365 6c66 2e70 726f 6a65  put = self.proje
+00012f70: 6374 696f 6e28 6174 7465 6e74 696f 6e29  ction(attention)
+00012f80: 0a20 2020 2020 2020 206f 7574 7075 7420  .        output 
+00012f90: 3d20 7365 6c66 2e64 726f 706f 7574 286f  = self.dropout(o
+00012fa0: 7574 7075 7429 0a20 2020 2020 2020 206f  utput).        o
+00012fb0: 7574 7075 7420 3d20 462e 7265 7368 6170  utput = F.reshap
+00012fc0: 6528 6f75 7470 7574 2c20 6f72 695f 7368  e(output, ori_sh
+00012fd0: 6170 6529 0a20 2020 2020 2020 206f 7574  ape).        out
+00012fe0: 7075 7420 3d20 462e 6361 7374 286f 7574  put = F.cast(out
+00012ff0: 7075 742c 206f 7269 5f64 7479 7065 290a  put, ori_dtype).
+00013000: 2020 2020 2020 2020 7265 7475 726e 206f          return o
+00013010: 7574 7075 742c 206c 6179 6572 5f70 7265  utput, layer_pre
+00013020: 7365 6e74 0a0a 2020 2020 6465 6620 5f67  sent..    def _g
+00013030: 6574 5f62 6174 6368 5f73 697a 655f 6672  et_batch_size_fr
+00013040: 6f6d 5f71 7565 7279 2873 656c 662c 2071  om_query(self, q
+00013050: 7565 7279 293a 0a20 2020 2020 2020 2072  uery):.        r
+00013060: 2222 2247 6574 2074 6865 2062 6174 6368  """Get the batch
+00013070: 2073 697a 6520 6672 6f6d 2071 7565 7279   size from query
+00013080: 2074 656e 736f 7222 2222 0a20 2020 2020   tensor""".     
+00013090: 2020 2023 2046 6f72 2074 6865 2069 6e63     # For the inc
+000130a0: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
+000130b0: 696f 6e2c 2074 6865 2073 6571 206c 656e  ion, the seq len
+000130c0: 6774 6820 666f 7220 7468 6520 696e 7075  gth for the inpu
+000130d0: 7420 6973 2031 2e0a 2020 2020 2020 2020  t is 1..        
+000130e0: 6966 206c 656e 2846 2e73 6861 7065 2871  if len(F.shape(q
+000130f0: 7565 7279 2929 203d 3d20 3220 616e 6420  uery)) == 2 and 
+00013100: 2828 7365 6c66 2e75 7365 5f70 6173 7420  ((self.use_past 
+00013110: 616e 6420 7365 6c66 2e69 735f 6669 7273  and self.is_firs
+00013120: 745f 6974 6572 6174 696f 6e29 206f 7220  t_iteration) or 
+00013130: 286e 6f74 2073 656c 662e 7573 655f 7061  (not self.use_pa
+00013140: 7374 2929 3a0a 2020 2020 2020 2020 2020  st)):.          
+00013150: 2020 7265 7475 726e 2046 2e73 6861 7065    return F.shape
+00013160: 2871 7565 7279 295b 305d 202f 2f20 7365  (query)[0] // se
+00013170: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
+00013180: 680a 2020 2020 2020 2020 7265 7475 726e  h.        return
+00013190: 2046 2e73 6861 7065 2871 7565 7279 295b   F.shape(query)[
+000131a0: 305d 0a0a 2020 2020 6465 6620 5f67 6574  0]..    def _get
+000131b0: 5f73 6571 5f6c 656e 6774 685f 756e 6465  _seq_length_unde
+000131c0: 725f 696e 6372 656d 656e 7461 6c28 7365  r_incremental(se
+000131d0: 6c66 2c20 6c65 6e67 7468 293a 0a20 2020  lf, length):.   
+000131e0: 2020 2020 2072 2222 2252 6574 7572 6e20       r"""Return 
+000131f0: 7468 6520 6c65 6e67 7468 206f 6620 7468  the length of th
+00013200: 6520 7465 6e73 6f72 2e0a 2020 2020 2020  e tensor..      
+00013210: 2020 2020 2020 466f 7220 7468 6520 696e        For the in
+00013220: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+00013230: 7469 6f6e 2c20 7468 6520 7365 7120 6c65  tion, the seq le
+00013240: 6e67 7468 2066 6f72 2074 6865 2069 6e70  ngth for the inp
+00013250: 7574 2069 7320 312e 0a20 2020 2020 2020  ut is 1..       
+00013260: 2022 2222 0a20 2020 2020 2020 2069 6620   """.        if 
+00013270: 7365 6c66 2e75 7365 5f70 6173 7420 616e  self.use_past an
+00013280: 6420 6e6f 7420 7365 6c66 2e69 735f 6669  d not self.is_fi
+00013290: 7273 745f 6974 6572 6174 696f 6e3a 0a20  rst_iteration:. 
+000132a0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+000132b0: 6e20 310a 2020 2020 2020 2020 7265 7475  n 1.        retu
+000132c0: 726e 206c 656e 6774 680a 0a20 2020 2064  rn length..    d
+000132d0: 6566 205f 6368 6563 6b5f 696e 7075 7473  ef _check_inputs
+000132e0: 2873 656c 662c 2071 7565 7279 5f74 656e  (self, query_ten
+000132f0: 736f 722c 206b 6579 5f74 656e 736f 722c  sor, key_tensor,
+00013300: 2076 616c 7565 5f74 656e 736f 722c 2061   value_tensor, a
+00013310: 7474 656e 7469 6f6e 5f6d 6173 6b2c 206b  ttention_mask, k
+00013320: 6579 5f70 6173 743d 4e6f 6e65 2c0a 2020  ey_past=None,.  
+00013330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013340: 2020 2020 7661 6c75 655f 7061 7374 3d4e      value_past=N
+00013350: 6f6e 652c 2062 6174 6368 5f76 616c 6964  one, batch_valid
+00013360: 5f6c 656e 6774 683d 4e6f 6e65 293a 0a20  _length=None):. 
+00013370: 2020 2020 2020 2072 2222 2243 6865 636b         r"""Check
+00013380: 2069 6e70 7574 7322 2222 0a20 2020 2020   inputs""".     
+00013390: 2020 205f 6368 6563 6b5f 696e 7075 745f     _check_input_
+000133a0: 6474 7970 6528 462e 6474 7970 6528 7175  dtype(F.dtype(qu
+000133b0: 6572 795f 7465 6e73 6f72 292c 2022 7175  ery_tensor), "qu
+000133c0: 6572 795f 7465 6e73 6f72 222c 0a20 2020  ery_tensor",.   
+000133d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000133e0: 2020 2020 2020 2020 5b6d 7374 7970 652e          [mstype.
+000133f0: 666c 6f61 7433 322c 206d 7374 7970 652e  float32, mstype.
+00013400: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+00013410: 6266 6c6f 6174 3136 5d2c 2073 656c 662e  bfloat16], self.
+00013420: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+00013430: 2020 5f63 6865 636b 5f69 6e70 7574 5f64    _check_input_d
+00013440: 7479 7065 2846 2e64 7479 7065 286b 6579  type(F.dtype(key
+00013450: 5f74 656e 736f 7229 2c20 226b 6579 5f74  _tensor), "key_t
+00013460: 656e 736f 7222 2c0a 2020 2020 2020 2020  ensor",.        
+00013470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013480: 2020 205b 6d73 7479 7065 2e66 6c6f 6174     [mstype.float
+00013490: 3332 2c20 6d73 7479 7065 2e66 6c6f 6174  32, mstype.float
+000134a0: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
+000134b0: 7431 365d 2c20 7365 6c66 2e63 6c73 5f6e  t16], self.cls_n
+000134c0: 616d 6529 0a20 2020 2020 2020 205f 6368  ame).        _ch
+000134d0: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
+000134e0: 462e 6474 7970 6528 7661 6c75 655f 7465  F.dtype(value_te
+000134f0: 6e73 6f72 292c 2022 7661 6c75 655f 7465  nsor), "value_te
+00013500: 6e73 6f72 222c 0a20 2020 2020 2020 2020  nsor",.         
+00013510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013520: 2020 5b6d 7374 7970 652e 666c 6f61 7433    [mstype.float3
+00013530: 322c 206d 7374 7970 652e 666c 6f61 7431  2, mstype.float1
+00013540: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
+00013550: 3136 5d2c 2073 656c 662e 636c 735f 6e61  16], self.cls_na
+00013560: 6d65 290a 2020 2020 2020 2020 6966 2061  me).        if a
+00013570: 7474 656e 7469 6f6e 5f6d 6173 6b20 6973  ttention_mask is
+00013580: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
+00013590: 2020 2020 2020 205f 6368 6563 6b5f 696e         _check_in
+000135a0: 7075 745f 6474 7970 6528 462e 6474 7970  put_dtype(F.dtyp
+000135b0: 6528 6174 7465 6e74 696f 6e5f 6d61 736b  e(attention_mask
+000135c0: 292c 2022 6174 7465 6e74 696f 6e5f 6d61  ), "attention_ma
+000135d0: 736b 222c 0a20 2020 2020 2020 2020 2020  sk",.           
+000135e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000135f0: 2020 2020 5b6d 7374 7970 652e 666c 6f61      [mstype.floa
+00013600: 7433 322c 206d 7374 7970 652e 666c 6f61  t32, mstype.floa
+00013610: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
+00013620: 6174 3136 5d2c 0a20 2020 2020 2020 2020  at16],.         
+00013630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013640: 2020 2020 2020 7365 6c66 2e63 6c73 5f6e        self.cls_n
+00013650: 616d 6529 0a0a 2020 2020 2020 2020 6261  ame)..        ba
+00013660: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+00013670: 5f69 735f 7465 6e73 6f72 203d 2069 7369  _is_tensor = isi
+00013680: 6e73 7461 6e63 6528 6261 7463 685f 7661  nstance(batch_va
+00013690: 6c69 645f 6c65 6e67 7468 2c20 5465 6e73  lid_length, Tens
+000136a0: 6f72 290a 2020 2020 2020 2020 6261 7463  or).        batc
+000136b0: 685f 6973 5f64 6566 6175 6c74 203d 2062  h_is_default = b
+000136c0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
+000136d0: 6820 6973 204e 6f6e 650a 2020 2020 2020  h is None.      
+000136e0: 2020 5f63 6865 636b 5f70 6173 745f 6e6f    _check_past_no
+000136f0: 6e65 5f69 6e70 7574 5f6e 6f6e 6528 7365  ne_input_none(se
+00013700: 6c66 2e75 7365 5f70 6173 742c 2022 6261  lf.use_past, "ba
+00013710: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+00013720: 222c 2073 656c 662e 636c 735f 6e61 6d65  ", self.cls_name
+00013730: 2c20 4e6f 6e65 2c0a 2020 2020 2020 2020  , None,.        
+00013740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013750: 2020 2020 2020 2020 2020 2020 6261 7463              batc
+00013760: 685f 7661 6c69 645f 6c65 6e67 7468 5f69  h_valid_length_i
+00013770: 735f 7465 6e73 6f72 2c20 6261 7463 685f  s_tensor, batch_
+00013780: 6973 5f64 6566 6175 6c74 290a 2020 2020  is_default).    
+00013790: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+000137a0: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+000137b0: 2020 5f63 6865 636b 5f69 6e70 7574 5f64    _check_input_d
+000137c0: 7479 7065 2846 2e64 7479 7065 286b 6579  type(F.dtype(key
+000137d0: 5f70 6173 7429 2c20 226b 6579 5f70 6173  _past), "key_pas
+000137e0: 7422 2c20 5b6d 7374 7970 652e 666c 6f61  t", [mstype.floa
+000137f0: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
+00013800: 6174 3136 5d2c 2073 656c 662e 636c 735f  at16], self.cls_
+00013810: 6e61 6d65 290a 2020 2020 2020 2020 2020  name).          
+00013820: 2020 5f63 6865 636b 5f69 6e70 7574 5f64    _check_input_d
+00013830: 7479 7065 2846 2e64 7479 7065 2876 616c  type(F.dtype(val
+00013840: 7565 5f70 6173 7429 2c20 2276 616c 7565  ue_past), "value
+00013850: 5f70 6173 7422 2c20 5b6d 7374 7970 652e  _past", [mstype.
+00013860: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+00013870: 6266 6c6f 6174 3136 5d2c 2073 656c 662e  bfloat16], self.
+00013880: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+00013890: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
+000138a0: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
+000138b0: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
+000138c0: 6774 6829 2c20 2262 6174 6368 5f76 616c  gth), "batch_val
+000138d0: 6964 5f6c 656e 6774 6822 2c20 5b6d 7374  id_length", [mst
+000138e0: 7970 652e 696e 7433 325d 2c20 7365 6c66  ype.int32], self
+000138f0: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
+00013900: 2020 2072 6574 7572 6e20 5472 7565 0a0a     return True..
+00013910: 2020 2020 6465 6620 5f63 6f6e 7665 7274      def _convert
+00013920: 5f74 6f5f 3264 5f74 656e 736f 7228 7365  _to_2d_tensor(se
+00013930: 6c66 2c20 7175 6572 795f 7465 6e73 6f72  lf, query_tensor
+00013940: 2c20 6b65 795f 7465 6e73 6f72 2c20 7661  , key_tensor, va
+00013950: 6c75 655f 7465 6e73 6f72 293a 0a20 2020  lue_tensor):.   
+00013960: 2020 2020 2022 2222 636f 6e76 6572 7420       """convert 
+00013970: 6120 6e64 2074 656e 736f 7220 746f 2061  a nd tensor to a
+00013980: 2032 6420 7465 6e73 6f72 2222 220a 2020   2d tensor""".  
+00013990: 2020 2020 2020 7175 6572 795f 7368 6170        query_shap
+000139a0: 6520 3d20 462e 7368 6170 6528 7175 6572  e = F.shape(quer
+000139b0: 795f 7465 6e73 6f72 290a 2020 2020 2020  y_tensor).      
+000139c0: 2020 7175 6572 795f 7465 6e73 6f72 203d    query_tensor =
+000139d0: 2046 2e72 6573 6861 7065 2871 7565 7279   F.reshape(query
+000139e0: 5f74 656e 736f 722c 2028 2d31 2c20 7175  _tensor, (-1, qu
+000139f0: 6572 795f 7368 6170 655b 2d31 5d29 290a  ery_shape[-1])).
+00013a00: 2020 2020 2020 2020 6b65 795f 7368 6170          key_shap
+00013a10: 6520 3d20 462e 7368 6170 6528 6b65 795f  e = F.shape(key_
+00013a20: 7465 6e73 6f72 290a 2020 2020 2020 2020  tensor).        
+00013a30: 6b65 795f 7465 6e73 6f72 203d 2046 2e72  key_tensor = F.r
+00013a40: 6573 6861 7065 286b 6579 5f74 656e 736f  eshape(key_tenso
+00013a50: 722c 2028 2d31 2c20 6b65 795f 7368 6170  r, (-1, key_shap
+00013a60: 655b 2d31 5d29 290a 2020 2020 2020 2020  e[-1])).        
+00013a70: 7661 6c75 655f 7368 6170 6520 3d20 462e  value_shape = F.
+00013a80: 7368 6170 6528 7661 6c75 655f 7465 6e73  shape(value_tens
+00013a90: 6f72 290a 2020 2020 2020 2020 7661 6c75  or).        valu
+00013aa0: 655f 7465 6e73 6f72 203d 2046 2e72 6573  e_tensor = F.res
+00013ab0: 6861 7065 2876 616c 7565 5f74 656e 736f  hape(value_tenso
+00013ac0: 722c 2028 2d31 2c20 7661 6c75 655f 7368  r, (-1, value_sh
+00013ad0: 6170 655b 2d31 5d29 290a 0a20 2020 2020  ape[-1]))..     
+00013ae0: 2020 2072 6574 7572 6e20 7175 6572 795f     return query_
+00013af0: 7465 6e73 6f72 2c20 6b65 795f 7465 6e73  tensor, key_tens
+00013b00: 6f72 2c20 7661 6c75 655f 7465 6e73 6f72  or, value_tensor
+00013b10: 0a0a 2020 2020 6465 6620 5f6d 6572 6765  ..    def _merge
+00013b20: 5f68 6561 6473 2873 656c 662c 2078 293a  _heads(self, x):
+00013b30: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+00013b40: 2020 2020 2063 6f6e 7665 7274 2061 2034       convert a 4
+00013b50: 6420 696e 7075 7420 746f 2061 2032 6420  d input to a 2d 
+00013b60: 6f75 7470 7574 0a0a 2020 2020 2020 2020  output..        
+00013b70: 496e 7075 7473 3a0a 2020 2020 2020 2020  Inputs:.        
+00013b80: 2020 2020 783a 2069 6e70 7574 2074 656e      x: input ten
+00013b90: 736f 720a 0a20 2020 2020 2020 204f 7574  sor..        Out
+00013ba0: 7075 743a 0a20 2020 2020 2020 2020 2020  put:.           
+00013bb0: 2078 5f6d 6572 6765 3a20 7468 6520 3264   x_merge: the 2d
+00013bc0: 206f 7574 7075 740a 2020 2020 2020 2020   output.        
+00013bd0: 2222 220a 2020 2020 2020 2020 7820 3d20  """.        x = 
+00013be0: 7365 6c66 2e6d 6572 6765 725f 6865 6164  self.merger_head
+00013bf0: 5f74 7261 6e73 706f 7365 280a 2020 2020  _transpose(.    
+00013c00: 2020 2020 2020 2020 782c 2028 302c 2032          x, (0, 2
+00013c10: 2c20 312c 2033 2929 2020 2320 6273 2c20  , 1, 3))  # bs, 
+00013c20: 7365 715f 6c65 6e67 7468 2c20 6865 6164  seq_length, head
+00013c30: 2c20 7369 7a65 5f70 6572 5f68 6561 640a  , size_per_head.
+00013c40: 2020 2020 2020 2020 785f 7368 6170 6520          x_shape 
+00013c50: 3d20 502e 5368 6170 6528 2928 7829 0a20  = P.Shape()(x). 
+00013c60: 2020 2020 2020 206e 6577 5f73 6861 7065         new_shape
+00013c70: 203d 2028 2d31 2c20 785f 7368 6170 655b   = (-1, x_shape[
+00013c80: 2d32 5d20 2a20 785f 7368 6170 655b 2d31  -2] * x_shape[-1
+00013c90: 5d29 0a20 2020 2020 2020 2078 5f6d 6572  ]).        x_mer
+00013ca0: 6765 203d 2073 656c 662e 7265 7368 6170  ge = self.reshap
+00013cb0: 6528 782c 206e 6577 5f73 6861 7065 290a  e(x, new_shape).
+00013cc0: 2020 2020 2020 2020 7265 7475 726e 2078          return x
+00013cd0: 5f6d 6572 6765 0a0a 2020 2020 6465 6620  _merge..    def 
+00013ce0: 5f73 6f66 746d 6178 2873 656c 662c 2061  _softmax(self, a
+00013cf0: 7474 656e 7469 6f6e 5f73 636f 7265 7329  ttention_scores)
+00013d00: 3a0a 2020 2020 2020 2020 2222 220a 2020  :.        """.  
+00013d10: 2020 2020 2020 466f 7220 7468 6520 636f        For the co
+00013d20: 6e73 6964 6572 6174 696f 6e20 6f66 2074  nsideration of t
+00013d30: 6865 2070 6572 666f 726d 616e 6365 2c20  he performance, 
+00013d40: 646f 2073 6f66 746d 6178 2061 6363 6f72  do softmax accor
+00013d50: 6469 6e67 2074 6f20 6469 6666 6572 656e  ding to differen
+00013d60: 7420 7369 7475 6174 696f 6e73 0a20 2020  t situations.   
+00013d70: 2020 2020 203a 7061 7261 6d20 6174 7465       :param atte
+00013d80: 6e74 696f 6e5f 7363 6f72 6573 3a20 6120  ntion_scores: a 
+00013d90: 3364 2074 656e 736f 7220 6265 666f 7265  3d tensor before
+00013da0: 2073 6f66 746d 6178 0a20 2020 2020 2020   softmax.       
+00013db0: 203a 7265 7475 726e 3a20 7468 6520 6174   :return: the at
+00013dc0: 7465 6e74 696f 6e20 7363 6f72 6573 2e0a  tention scores..
+00013dd0: 2020 2020 2020 2020 2222 220a 0a20 2020          """..   
+00013de0: 2020 2020 2069 6620 7365 6c66 2e5f 6973       if self._is
+00013df0: 5f61 7363 656e 6420 616e 6420 7365 6c66  _ascend and self
+00013e00: 2e73 6f66 746d 6178 5f64 7479 7065 203d  .softmax_dtype =
+00013e10: 3d20 6d73 7479 7065 2e66 6c6f 6174 3136  = mstype.float16
+00013e20: 206f 7220 6e6f 7420 7365 6c66 2e5f 6973   or not self._is
+00013e30: 5f61 7363 656e 643a 0a20 2020 2020 2020  _ascend:.       
+00013e40: 2020 2020 2061 7474 656e 7469 6f6e 5f70       attention_p
+00013e50: 726f 6273 203d 2073 656c 662e 736f 6674  robs = self.soft
+00013e60: 6d61 7828 6174 7465 6e74 696f 6e5f 7363  max(attention_sc
+00013e70: 6f72 6573 290a 2020 2020 2020 2020 656c  ores).        el
+00013e80: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00013e90: 7368 6170 6520 3d20 462e 7368 6170 6528  shape = F.shape(
+00013ea0: 6174 7465 6e74 696f 6e5f 7363 6f72 6573  attention_scores
+00013eb0: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
+00013ec0: 6174 7465 6e74 696f 6e20 7072 6f62 730a  attention probs.
+00013ed0: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+00013ee0: 6e74 696f 6e5f 7072 6f62 7320 3d20 7365  ntion_probs = se
+00013ef0: 6c66 2e73 6f66 746d 6178 5f33 6428 0a20  lf.softmax_3d(. 
+00013f00: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00013f10: 656c 662e 736f 6674 6d61 785f 7265 7368  elf.softmax_resh
+00013f20: 6170 6528 6174 7465 6e74 696f 6e5f 7363  ape(attention_sc
+00013f30: 6f72 6573 2c20 2873 6861 7065 5b30 5d2c  ores, (shape[0],
+00013f40: 202d 312c 2073 6861 7065 5b2d 315d 2929   -1, shape[-1]))
+00013f50: 290a 2020 2020 2020 2020 2020 2020 6174  ).            at
+00013f60: 7465 6e74 696f 6e5f 7072 6f62 7320 3d20  tention_probs = 
+00013f70: 7365 6c66 2e73 6f66 746d 6178 5f72 6573  self.softmax_res
+00013f80: 6861 7065 2861 7474 656e 7469 6f6e 5f70  hape(attention_p
+00013f90: 726f 6273 2c20 7368 6170 6529 0a20 2020  robs, shape).   
+00013fa0: 2020 2020 2072 6574 7572 6e20 6174 7465       return atte
+00013fb0: 6e74 696f 6e5f 7072 6f62 730a 0a20 2020  ntion_probs..   
+00013fc0: 2064 6566 205f 666c 6173 685f 6174 746e   def _flash_attn
+00013fd0: 2873 656c 662c 2071 7565 7279 2c20 6b65  (self, query, ke
+00013fe0: 792c 2076 616c 7565 2c20 6174 7465 6e74  y, value, attent
+00013ff0: 696f 6e5f 6d61 736b 293a 0a20 2020 2020  ion_mask):.     
+00014000: 2020 2022 2222 0a20 2020 2020 2020 2066     """.        f
+00014010: 6c61 7368 2061 7474 656e 7469 6f6e 0a20  lash attention. 
+00014020: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
+00014030: 2020 2069 6620 6174 7465 6e74 696f 6e5f     if attention_
+00014040: 6d61 736b 2069 7320 6e6f 7420 4e6f 6e65  mask is not None
+00014050: 3a0a 2020 2020 2020 2020 2020 2020 6174  :.            at
+00014060: 7465 6e74 696f 6e5f 6d61 736b 5f64 7479  tention_mask_dty
+00014070: 7065 203d 2063 686f 6f73 655f 666c 6173  pe = choose_flas
+00014080: 685f 6174 7465 6e74 696f 6e5f 6474 7970  h_attention_dtyp
+00014090: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
+000140a0: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
+000140b0: 2073 656c 662e 7375 6228 0a20 2020 2020   self.sub(.     
+000140c0: 2020 2020 2020 2020 2020 2050 2e43 6173             P.Cas
+000140d0: 7428 2928 7365 6c66 2e6f 6e65 2c20 6174  t()(self.one, at
+000140e0: 7465 6e74 696f 6e5f 6d61 736b 5f64 7479  tention_mask_dty
+000140f0: 7065 292c 0a20 2020 2020 2020 2020 2020  pe),.           
+00014100: 2020 2020 2050 2e43 6173 7428 2928 6174       P.Cast()(at
+00014110: 7465 6e74 696f 6e5f 6d61 736b 2c20 6174  tention_mask, at
+00014120: 7465 6e74 696f 6e5f 6d61 736b 5f64 7479  tention_mask_dty
+00014130: 7065 2929 0a0a 2020 2020 2020 2020 7765  pe))..        we
+00014140: 6967 6874 6564 5f76 616c 7565 7320 3d20  ighted_values = 
+00014150: 7365 6c66 2e66 6c61 7368 5f61 7474 656e  self.flash_atten
+00014160: 7469 6f6e 2871 7565 7279 2c20 6b65 792c  tion(query, key,
+00014170: 2076 616c 7565 2c20 6174 7465 6e74 696f   value, attentio
+00014180: 6e5f 6d61 736b 290a 2020 2020 2020 2020  n_mask).        
+00014190: 6174 7465 6e74 696f 6e5f 6d65 7267 6520  attention_merge 
+000141a0: 3d20 7365 6c66 2e5f 6d65 7267 655f 6865  = self._merge_he
+000141b0: 6164 7328 7765 6967 6874 6564 5f76 616c  ads(weighted_val
+000141c0: 7565 7329 0a20 2020 2020 2020 2072 6574  ues).        ret
+000141d0: 7572 6e20 6174 7465 6e74 696f 6e5f 6d65  urn attention_me
+000141e0: 7267 650a 0a20 2020 2064 6566 205f 6174  rge..    def _at
+000141f0: 746e 2873 656c 662c 2071 7565 7279 2c20  tn(self, query, 
+00014200: 6b65 792c 2076 616c 7565 2c20 6174 7465  key, value, atte
+00014210: 6e74 696f 6e5f 6d61 736b 293a 0a20 2020  ntion_mask):.   
+00014220: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
+00014230: 2047 6574 2074 6865 2077 6569 6768 7465   Get the weighte
+00014240: 6420 7363 6f72 6520 616c 6f6e 6720 7468  d score along th
+00014250: 6520 7365 715f 6c65 6e67 7468 0a0a 2020  e seq_length..  
+00014260: 2020 2020 2020 496e 7075 7473 3a0a 2020        Inputs:.  
+00014270: 2020 2020 2020 2020 2020 7175 6572 793a            query:
+00014280: 2074 6865 2071 7565 7279 206d 6174 7269   the query matri
+00014290: 780a 2020 2020 2020 2020 2020 2020 6b65  x.            ke
+000142a0: 793a 2074 6865 206b 6579 206d 6174 7269  y: the key matri
+000142b0: 780a 2020 2020 2020 2020 2020 2020 7661  x.            va
+000142c0: 6c75 653a 2074 6865 2076 616c 7565 206d  lue: the value m
+000142d0: 6174 7269 780a 2020 2020 2020 2020 2020  atrix.          
+000142e0: 2020 6174 7465 6e74 696f 6e5f 6d61 736b    attention_mask
+000142f0: 3a20 7468 6520 6174 7465 6e74 696f 6e20  : the attention 
+00014300: 6d61 736b 206d 6174 7269 7820 7769 7468  mask matrix with
+00014310: 2073 6861 7065 2028 6261 7463 685f 7369   shape (batch_si
+00014320: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00014330: 312c 2073 6571 5f6c 656e 6774 682c 2073  1, seq_length, s
+00014340: 6571 5f6c 656e 6774 6829 0a20 2020 2020  eq_length).     
+00014350: 2020 204f 7574 7075 7473 3a0a 2020 2020     Outputs:.    
+00014360: 2020 2020 2020 2020 7765 6967 6874 6564          weighted
+00014370: 5f76 616c 7565 733a 2054 656e 736f 722c  _values: Tensor,
+00014380: 2074 6865 2077 6569 6768 7465 6420 7375   the weighted su
+00014390: 6d20 7363 6f72 6573 0a20 2020 2020 2020  m scores.       
+000143a0: 2022 2222 0a20 2020 2020 2020 2023 204e   """.        # N
+000143b0: 6f72 6d61 6c69 7a65 2071 7565 7279 2061  ormalize query a
+000143c0: 6e64 206b 6579 2062 6566 6f72 6520 4d61  nd key before Ma
+000143d0: 744d 756c 2c20 6465 6661 756c 7420 6f66  tMul, default of
+000143e0: 660a 2020 2020 2020 2020 2320 4174 7465  f.        # Atte
+000143f0: 6e74 696f 6e20 7363 6f72 6520 5b62 732c  ntion score [bs,
+00014400: 206e 756d 5f68 6561 6473 2c20 7365 715f   num_heads, seq_
+00014410: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
+00014420: 7468 5d0a 2020 2020 2020 2020 6661 6374  th].        fact
+00014430: 6f72 203d 2050 2e43 6173 7428 2928 7365  or = P.Cast()(se
+00014440: 6c66 2e73 6361 6c65 5f66 6163 746f 722c  lf.scale_factor,
+00014450: 2050 2e44 5479 7065 2829 2871 7565 7279   P.DType()(query
+00014460: 2929 0a20 2020 2020 2020 2071 7565 7279  )).        query
+00014470: 203d 2073 656c 662e 7265 616c 5f64 6976   = self.real_div
+00014480: 2871 7565 7279 2c20 6661 6374 6f72 290a  (query, factor).
+00014490: 2020 2020 2020 2020 6b65 7920 3d20 7365          key = se
+000144a0: 6c66 2e72 6561 6c5f 6469 7628 6b65 792c  lf.real_div(key,
+000144b0: 2066 6163 746f 7229 0a20 2020 2020 2020   factor).       
+000144c0: 2073 636f 7265 203d 2073 656c 662e 6261   score = self.ba
+000144d0: 7463 685f 6d61 746d 756c 2871 7565 7279  tch_matmul(query
+000144e0: 2c20 6b65 7929 0a0a 2020 2020 2020 2020  , key)..        
+000144f0: 6f72 695f 6474 7970 6520 3d20 502e 4454  ori_dtype = P.DT
+00014500: 7970 6528 2928 7363 6f72 6529 0a20 2020  ype()(score).   
+00014510: 2020 2020 2061 7474 656e 7469 6f6e 5f73       attention_s
+00014520: 636f 7265 7320 3d20 7365 6c66 2e73 6f66  cores = self.sof
+00014530: 746d 6178 5f63 6173 7428 7363 6f72 652c  tmax_cast(score,
+00014540: 2073 656c 662e 736f 6674 6d61 785f 6474   self.softmax_dt
+00014550: 7970 6529 0a0a 2020 2020 2020 2020 2320  ype)..        # 
+00014560: 666f 7220 696e 7075 7420 7369 7a65 206f  for input size o
+00014570: 6620 2862 732c 2031 2920 6e61 6d65 6c79  f (bs, 1) namely
+00014580: 2074 6865 2073 6563 6f6e 6420 6772 6170   the second grap
+00014590: 682c 0a20 2020 2020 2020 2023 2074 6865  h,.        # the
+000145a0: 2073 6861 7065 206f 6620 6174 7465 6e74   shape of attent
+000145b0: 696f 6e5f 6d61 736b 206d 6174 7269 7820  ion_mask matrix 
+000145c0: 7368 6f75 6c64 2062 6520 2862 732c 2031  should be (bs, 1
+000145d0: 2c20 312c 2073 6571 5f6c 656e 6774 6829  , 1, seq_length)
+000145e0: 0a20 2020 2020 2020 2069 6620 6174 7465  .        if atte
+000145f0: 6e74 696f 6e5f 6d61 736b 2069 7320 6e6f  ntion_mask is no
+00014600: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
+00014610: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+00014620: 7061 7374 2061 6e64 206e 6f74 2073 656c  past and not sel
+00014630: 662e 6973 5f66 6972 7374 5f69 7465 7261  f.is_first_itera
+00014640: 7469 6f6e 3a0a 2020 2020 2020 2020 2020  tion:.          
+00014650: 2020 2020 2020 2320 4361 6c63 756c 6174        # Calculat
+00014660: 6520 7468 6520 6375 7272 656e 7420 746f  e the current to
+00014670: 7461 6c20 746f 6b65 6e0a 2020 2020 2020  tal token.      
+00014680: 2020 2020 2020 2020 2020 6375 7272 656e            curren
+00014690: 745f 696e 6465 7820 3d20 7365 6c66 2e72  t_index = self.r
+000146a0: 6564 7563 6573 756d 2846 2e63 6173 7428  educesum(F.cast(
+000146b0: 7365 6c66 2e6e 6f74 5f65 7175 616c 2873  self.not_equal(s
+000146c0: 656c 662e 736c 6963 6528 6b65 792c 2028  elf.slice(key, (
+000146d0: 302c 2030 2c20 302c 2030 292c 0a20 2020  0, 0, 0, 0),.   
 000146e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000146f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014700: 2020 2020 2020 2846 2e73 6861 7065 2871        (F.shape(q
-00014710: 7565 7279 295b 305d 2c20 312c 2031 2c0a  uery)[0], 1, 1,.
-00014720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014720: 2020 2020 2020 2020 2020 2020 2028 462e               (F.
+00014730: 7368 6170 6528 7175 6572 7929 5b30 5d2c  shape(query)[0],
+00014740: 2031 2c20 312c 0a20 2020 2020 2020 2020   1, 1,.         
 00014750: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00014760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014770: 2073 656c 662e 7365 715f 6c65 6e67 7468   self.seq_length
-00014780: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00014790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000147a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014790: 2020 2020 2020 2020 7365 6c66 2e73 6571          self.seq
+000147a0: 5f6c 656e 6774 6829 2c0a 2020 2020 2020  _length),.      
 000147b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000147c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000147d0: 2020 2028 312c 2031 2c20 312c 2031 2929     (1, 1, 1, 1))
-000147e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000147f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000147d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000147e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000147f0: 2020 2020 2020 2020 2020 2831 2c20 312c            (1, 1,
+00014800: 2031 2c20 3129 292c 0a20 2020 2020 2020   1, 1)),.       
 00014810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014820: 2020 2020 2020 2030 292c 206d 7374 7970         0), mstyp
-00014830: 652e 666c 6f61 7433 3229 2c20 2831 2c20  e.float32), (1, 
-00014840: 322c 2033 2929 0a20 2020 2020 2020 2020  2, 3)).         
-00014850: 2020 2020 2020 2023 2047 6574 2074 6865         # Get the
-00014860: 2070 7265 6369 7365 2070 6f73 6974 696f   precise positio
-00014870: 6e20 696e 6465 780a 2020 2020 2020 2020  n index.        
-00014880: 2020 2020 2020 2020 696e 6465 7820 3d20          index = 
-00014890: 7365 6c66 2e73 7562 3128 462e 6361 7374  self.sub1(F.cast
-000148a0: 2863 7572 7265 6e74 5f69 6e64 6578 2c20  (current_index, 
-000148b0: 6d73 7479 7065 2e69 6e74 3332 292c 2031  mstype.int32), 1
-000148c0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-000148d0: 2020 696e 6465 7820 3d20 462e 7265 7368    index = F.resh
-000148e0: 6170 6528 696e 6465 782c 2028 2d31 2c20  ape(index, (-1, 
-000148f0: 312c 2031 2929 0a20 2020 2020 2020 2020  1, 1)).         
-00014900: 2020 2020 2020 2023 2043 616c 6375 6c61         # Calcula
-00014910: 7465 2074 6865 2061 7474 656e 7469 6f6e  te the attention
-00014920: 5f6d 6173 6b20 6d61 7472 6978 2076 6961  _mask matrix via
-00014930: 2074 6865 2070 6f73 6974 696f 6e20 696e   the position in
-00014940: 6465 780a 2020 2020 2020 2020 2020 2020  dex.            
-00014950: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
-00014960: 736b 203d 2046 2e63 6173 7428 7365 6c66  sk = F.cast(self
-00014970: 2e74 656e 736f 725f 6c65 2873 656c 662e  .tensor_le(self.
-00014980: 7261 6e67 652c 2069 6e64 6578 292c 206d  range, index), m
-00014990: 7374 7970 652e 696e 7433 3229 0a20 2020  stype.int32).   
-000149a0: 2020 2020 2020 2020 2020 2020 2061 7474               att
-000149b0: 656e 7469 6f6e 5f6d 6173 6b20 3d20 7365  ention_mask = se
-000149c0: 6c66 2e65 7870 616e 645f 6469 6d73 2861  lf.expand_dims(a
-000149d0: 7474 656e 7469 6f6e 5f6d 6173 6b2c 2032  ttention_mask, 2
-000149e0: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
-000149f0: 4d69 6e75 7320 3130 3030 3020 666f 7220  Minus 10000 for 
-00014a00: 7468 6520 706f 7369 7469 6f6e 2077 6865  the position whe
-00014a10: 7265 206d 6173 6b65 6420 746f 2065 7863  re masked to exc
-00014a20: 6c75 6465 2074 6865 6d20 6672 6f6d 2073  lude them from s
-00014a30: 6f66 746d 6178 0a20 2020 2020 2020 2020  oftmax.         
-00014a40: 2020 206d 756c 7469 706c 755f 6f75 7420     multiplu_out 
-00014a50: 3d20 7365 6c66 2e73 7562 280a 2020 2020  = self.sub(.    
-00014a60: 2020 2020 2020 2020 2020 2020 502e 4361              P.Ca
-00014a70: 7374 2829 2846 2e74 7570 6c65 5f74 6f5f  st()(F.tuple_to_
-00014a80: 6172 7261 7928 2831 2e30 2c29 292c 2050  array((1.0,)), P
-00014a90: 2e44 5479 7065 2829 2861 7474 656e 7469  .DType()(attenti
-00014aa0: 6f6e 5f73 636f 7265 7329 292c 0a20 2020  on_scores)),.   
-00014ab0: 2020 2020 2020 2020 2020 2020 2050 2e43               P.C
-00014ac0: 6173 7428 2928 6174 7465 6e74 696f 6e5f  ast()(attention_
-00014ad0: 6d61 736b 2c20 502e 4454 7970 6528 2928  mask, P.DType()(
-00014ae0: 6174 7465 6e74 696f 6e5f 7363 6f72 6573  attention_scores
-00014af0: 2929 290a 0a20 2020 2020 2020 2020 2020  )))..           
-00014b00: 2061 6464 6572 203d 2073 656c 662e 6d75   adder = self.mu
-00014b10: 6c28 6d75 6c74 6970 6c75 5f6f 7574 2c20  l(multiplu_out, 
-00014b20: 7365 6c66 2e6d 756c 7469 706c 795f 6461  self.multiply_da
-00014b30: 7461 290a 2020 2020 2020 2020 2020 2020  ta).            
-00014b40: 6174 7465 6e74 696f 6e5f 7363 6f72 6573  attention_scores
-00014b50: 203d 2073 656c 662e 6164 6428 6164 6465   = self.add(adde
-00014b60: 722c 2061 7474 656e 7469 6f6e 5f73 636f  r, attention_sco
-00014b70: 7265 7329 0a0a 2020 2020 2020 2020 2320  res)..        # 
-00014b80: 6174 7465 6e74 696f 6e20 7072 6f62 730a  attention probs.
-00014b90: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-00014ba0: 6e5f 7072 6f62 7320 3d20 7365 6c66 2e5f  n_probs = self._
-00014bb0: 736f 6674 6d61 7828 6174 7465 6e74 696f  softmax(attentio
-00014bc0: 6e5f 7363 6f72 6573 290a 2020 2020 2020  n_scores).      
-00014bd0: 2020 6174 7465 6e74 696f 6e5f 7072 6f62    attention_prob
-00014be0: 7320 3d20 7365 6c66 2e73 6f66 746d 6178  s = self.softmax
-00014bf0: 5f63 6173 7428 6174 7465 6e74 696f 6e5f  _cast(attention_
-00014c00: 7072 6f62 732c 206f 7269 5f64 7479 7065  probs, ori_dtype
-00014c10: 290a 0a20 2020 2020 2020 2061 7474 656e  )..        atten
-00014c20: 7469 6f6e 5f70 726f 6273 203d 2073 656c  tion_probs = sel
-00014c30: 662e 7072 6f62 5f64 726f 706f 7574 2861  f.prob_dropout(a
-00014c40: 7474 656e 7469 6f6e 5f70 726f 6273 290a  ttention_probs).
-00014c50: 2020 2020 2020 2020 2320 5765 6967 6874          # Weight
-00014c60: 6564 2073 756d 206f 7574 7075 7420 5b62  ed sum output [b
-00014c70: 732c 206e 756d 5f68 6561 6473 2c20 7365  s, num_heads, se
-00014c80: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-00014c90: 6572 5f68 6561 645d 0a20 2020 2020 2020  er_head].       
-00014ca0: 2077 6569 6768 7465 645f 7661 6c75 6573   weighted_values
-00014cb0: 203d 2073 656c 662e 6261 7463 685f 6d61   = self.batch_ma
-00014cc0: 746d 756c 2861 7474 656e 7469 6f6e 5f70  tmul(attention_p
-00014cd0: 726f 6273 2c20 7661 6c75 6529 0a20 2020  robs, value).   
-00014ce0: 2020 2020 2061 7474 656e 7469 6f6e 5f6d       attention_m
-00014cf0: 6572 6765 203d 2073 656c 662e 5f6d 6572  erge = self._mer
-00014d00: 6765 5f68 6561 6473 2877 6569 6768 7465  ge_heads(weighte
-00014d10: 645f 7661 6c75 6573 290a 2020 2020 2020  d_values).      
-00014d20: 2020 7265 7475 726e 2061 7474 656e 7469    return attenti
-00014d30: 6f6e 5f6d 6572 6765 0a0a 0a63 6c61 7373  on_merge...class
-00014d40: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
-00014d50: 6465 724c 6179 6572 2843 656c 6c29 3a0a  derLayer(Cell):.
-00014d60: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
-00014d70: 2054 7261 6e73 666f 726d 6572 2045 6e63   Transformer Enc
-00014d80: 6f64 6572 204c 6179 6572 2e20 5468 6973  oder Layer. This
-00014d90: 2069 7320 616e 2069 6d70 6c65 6d65 6e74   is an implement
-00014da0: 6174 696f 6e20 6f66 2074 6865 2073 696e  ation of the sin
-00014db0: 676c 6520 6c61 7965 7220 6f66 2074 6865  gle layer of the
-00014dc0: 2074 7261 6e73 666f 726d 6572 0a20 2020   transformer.   
-00014dd0: 2020 2020 2065 6e63 6f64 6572 206c 6179       encoder lay
-00014de0: 6572 2c20 696e 636c 7564 696e 6720 6d75  er, including mu
-00014df0: 6c74 6968 6561 6420 6174 7465 6e74 696f  ltihead attentio
-00014e00: 6e20 616e 6420 6665 6564 7761 7264 206c  n and feedward l
-00014e10: 6179 6572 2e0a 0a20 2020 2020 2020 2041  ayer...        A
-00014e20: 7267 733a 0a20 2020 2020 2020 2020 2020  rgs:.           
-00014e30: 2062 6174 6368 5f73 697a 6528 696e 7429   batch_size(int)
-00014e40: 3a20 5468 6520 6261 7463 6820 7369 7a65  : The batch size
-00014e50: 206f 6620 7468 6520 696e 7075 7420 7465   of the input te
-00014e60: 6e73 6f72 2077 6865 6e20 646f 2069 6e63  nsor when do inc
-00014e70: 7265 6e6d 656e 7461 6c20 7072 6564 6963  renmental predic
-00014e80: 7469 6f6e 2e20 5368 6f75 6c64 2062 6520  tion. Should be 
-00014e90: 6120 706f 7369 7469 7665 0a20 2020 2020  a positive.     
-00014ea0: 2020 2020 2020 2020 2020 2076 616c 7565             value
-00014eb0: 2e20 5768 656e 2064 6f20 7472 6169 6e69  . When do traini
-00014ec0: 6e67 206f 7220 7072 6564 6963 7469 6f6e  ng or prediction
-00014ed0: 2c20 7468 6520 6172 6775 6d65 6e74 2077  , the argument w
-00014ee0: 696c 6c20 6e6f 7420 776f 726b 2061 6e64  ill not work and
-00014ef0: 2074 6865 2075 7365 7220 6361 6e20 6a75   the user can ju
-00014f00: 7374 2070 6173 7320 4e6f 6e65 2074 6f0a  st pass None to.
-00014f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014f20: 7468 6520 6172 6775 6d65 6e74 2e0a 2020  the argument..  
-00014f30: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-00014f40: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
-00014f50: 6869 6464 656e 2073 697a 6520 6f66 2074  hidden size of t
-00014f60: 6865 2069 6e70 7574 2e0a 2020 2020 2020  he input..      
-00014f70: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
-00014f80: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
-00014f90: 6869 6464 656e 2073 697a 6520 6f66 2062  hidden size of b
-00014fa0: 6f74 746c 656e 6563 6b20 696e 2074 6865  ottleneck in the
-00014fb0: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
-00014fc0: 6572 2e0a 2020 2020 2020 2020 2020 2020  er..            
-00014fd0: 6e75 6d5f 6865 6164 7328 696e 7429 3a20  num_heads(int): 
-00014fe0: 5468 6520 6e75 6d62 6572 206f 6620 7468  The number of th
-00014ff0: 6520 6865 6164 732e 0a20 2020 2020 2020  e heads..       
-00015000: 2020 2020 2073 6571 5f6c 656e 6774 6828       seq_length(
-00015010: 696e 7429 3a20 5468 6520 696e 7075 7420  int): The input 
-00015020: 7365 7175 656e 6365 206c 656e 6774 682e  sequence length.
-00015030: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
-00015040: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
-00015050: 6174 6528 666c 6f61 7429 3a20 5468 6520  ate(float): The 
-00015060: 6472 6f70 6f75 7420 7261 7465 206f 6620  dropout rate of 
-00015070: 7468 6520 6174 7465 6e74 696f 6e20 7363  the attention sc
-00015080: 6f72 6573 2e20 4465 6661 756c 743a 302e  ores. Default:0.
-00015090: 312e 0a20 2020 2020 2020 2020 2020 2068  1..            h
-000150a0: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-000150b0: 7465 2866 6c6f 6174 293a 2054 6865 2064  te(float): The d
-000150c0: 726f 706f 7574 2072 6174 6520 6f66 2074  ropout rate of t
-000150d0: 6865 2066 696e 616c 206f 7574 7075 7420  he final output 
-000150e0: 6f66 2074 6865 206c 6179 6572 2e20 4465  of the layer. De
-000150f0: 6661 756c 743a 302e 312e 0a20 2020 2020  fault:0.1..     
-00015100: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
-00015110: 726e 6f72 6d5f 7265 7369 6475 616c 2862  rnorm_residual(b
-00015120: 6f6f 6c29 3a20 446f 2072 6573 6964 7561  ool): Do residua
-00015130: 6c73 2061 6464 7320 6265 666f 7265 2074  ls adds before t
-00015140: 6865 206c 6179 6572 6e6f 726d 2e20 4465  he layernorm. De
-00015150: 6661 756c 7420 4661 6c73 652e 0a20 2020  fault False..   
-00015160: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-00015170: 726d 5f63 6f6d 7075 7465 5f74 7970 6528  rm_compute_type(
-00015180: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-00015190: 6865 2063 6f6d 7075 7461 7469 6f6e 2074  he computation t
-000151a0: 7970 6520 6f66 2074 6865 206c 6179 6572  ype of the layer
-000151b0: 6e6f 726d 2e0a 2020 2020 2020 2020 2020  norm..          
-000151c0: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
-000151d0: 6d73 7479 7065 2e66 6c6f 6174 3332 206f  mstype.float32 o
-000151e0: 7220 6d73 7479 7065 2e66 6c6f 6174 3136  r mstype.float16
-000151f0: 2e20 4465 6661 756c 7420 6d73 7479 7065  . Default mstype
-00015200: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
-00015210: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
-00015220: 6d70 7574 655f 7479 7065 2864 7479 7065  mpute_type(dtype
-00015230: 2e4e 756d 6265 7229 3a20 5468 6520 636f  .Number): The co
-00015240: 6d70 7574 6174 696f 6e20 7479 7065 206f  mputation type o
-00015250: 6620 7468 6520 736f 6674 6d61 7820 696e  f the softmax in
-00015260: 2074 6865 2061 7474 656e 7469 6f6e 2e0a   the attention..
-00015270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015280: 5368 6f75 6c64 2062 6520 6d73 7479 7065  Should be mstype
-00015290: 2e66 6c6f 6174 3332 206f 7220 6d73 7479  .float32 or msty
-000152a0: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-000152b0: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
-000152c0: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-000152d0: 7061 7261 6d5f 696e 6974 5f74 7970 6528  param_init_type(
-000152e0: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-000152f0: 6865 2070 6172 616d 6574 6572 2069 6e69  he parameter ini
-00015300: 7469 616c 697a 6174 696f 6e20 7479 7065  tialization type
-00015310: 206f 6620 7468 6520 6d6f 6475 6c65 2e0a   of the module..
-00015320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015330: 5368 6f75 6c64 2062 6520 6d73 7479 7065  Should be mstype
-00015340: 2e66 6c6f 6174 3332 206f 7220 6d73 7479  .float32 or msty
-00015350: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-00015360: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
-00015370: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-00015380: 6869 6464 656e 5f61 6374 2028 7374 722c  hidden_act (str,
-00015390: 206e 6e2e 4365 6c6c 293a 2054 6865 2061   nn.Cell): The a
-000153a0: 6374 6976 6174 696f 6e20 6f66 2074 6865  ctivation of the
-000153b0: 2069 6e74 6572 6e61 6c20 6665 6564 666f   internal feedfo
-000153c0: 7277 6172 6420 6c61 7965 722e 2053 7570  rward layer. Sup
-000153d0: 706f 7274 7320 2772 656c 7527 2c0a 2020  ports 'relu',.  
-000153e0: 2020 2020 2020 2020 2020 2020 2020 2772                'r
-000153f0: 656c 7536 272c 2027 7461 6e68 272c 2027  elu6', 'tanh', '
-00015400: 6765 6c75 272c 2027 6661 7374 5f67 656c  gelu', 'fast_gel
-00015410: 7527 2c20 2765 6c75 272c 2027 7369 676d  u', 'elu', 'sigm
-00015420: 6f69 6427 2c20 2770 7265 6c75 272c 2027  oid', 'prelu', '
-00015430: 6c65 616b 7972 656c 7527 2c20 2768 7377  leakyrelu', 'hsw
-00015440: 6973 6827 2c0a 2020 2020 2020 2020 2020  ish',.          
-00015450: 2020 2020 2020 2768 7369 676d 6f69 6427        'hsigmoid'
-00015460: 2c20 276c 6f67 7369 676d 6f69 6427 2061  , 'logsigmoid' a
-00015470: 6e64 2073 6f20 6f6e 2e20 5573 6572 2063  nd so on. User c
-00015480: 616e 2070 726f 7669 6465 2063 7573 746f  an provide custo
-00015490: 6d20 6163 7469 7669 7469 6f6e 2074 6f20  m activition to 
-000154a0: 7468 6520 6172 6775 6d65 6e74 2e0a 2020  the argument..  
-000154b0: 2020 2020 2020 2020 2020 2020 2020 4966                If
-000154c0: 2075 7365 7220 7761 6e74 7320 746f 2072   user wants to r
-000154d0: 756e 2074 6865 206e 6574 2069 6e20 7468  un the net in th
-000154e0: 6520 7061 7261 6c6c 656c 206d 6f64 652c  e parallel mode,
-000154f0: 2074 6865 2063 7573 746f 6d20 6163 7469   the custom acti
-00015500: 7661 7469 6f6e 206d 7573 7420 616c 736f  vation must also
-00015510: 2070 726f 7669 6465 0a20 2020 2020 2020   provide.       
-00015520: 2020 2020 2020 2020 2074 6865 2060 6163           the `ac
-00015530: 7469 7661 7469 6f6e 5f73 6861 7264 6020  tivation_shard` 
-00015540: 6675 6e63 7469 6f6e 2e20 506c 6561 7365  function. Please
-00015550: 2073 6565 2074 6865 2065 7861 6d70 6c65   see the example
-00015560: 7320 6f66 2074 6865 0a20 2020 2020 2020  s of the.       
-00015570: 2020 2020 2020 2020 2063 6c61 7373 3a60           class:`
-00015580: 6d69 6e64 666f 726d 6572 732e 6d6f 6475  mindformers.modu
-00015590: 6c65 732e 7472 616e 7366 6f72 6d65 722e  les.transformer.
-000155a0: 4665 6564 466f 7277 6172 6460 2e20 4465  FeedForward`. De
-000155b0: 6661 756c 743a 2067 656c 752e 0a20 2020  fault: gelu..   
-000155c0: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-000155d0: 7428 626f 6f6c 293a 2055 7365 2074 6865  t(bool): Use the
-000155e0: 2070 6173 7420 7374 6174 6520 746f 2063   past state to c
-000155f0: 6f6d 7075 7465 2c20 7573 6564 2066 6f72  ompute, used for
-00015600: 2069 6e63 7265 6d65 6e74 616c 2070 7265   incremental pre
-00015610: 6469 6374 696f 6e2e 2046 6f72 2065 7861  diction. For exa
-00015620: 6d70 6c65 2c20 6966 2077 6520 6861 7665  mple, if we have
-00015630: 2074 776f 0a20 2020 2020 2020 2020 2020   two.           
-00015640: 2020 2020 2077 6f72 6473 2061 6e64 2077       words and w
-00015650: 616e 7420 746f 2067 656e 6572 6174 6520  ant to generate 
-00015660: 7468 6520 7465 6e20 6d6f 7265 2077 6f72  the ten more wor
-00015670: 6473 2e20 5765 206a 7573 7420 6e65 6564  ds. We just need
-00015680: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
-00015690: 7477 6f20 776f 7264 7327 2073 7461 7465  two words' state
-000156a0: 206f 6e6c 7920 6f6e 6365 2c0a 2020 2020   only once,.    
-000156b0: 2020 2020 2020 2020 2020 2020 616e 6420              and 
-000156c0: 6765 6e65 7261 7465 2074 6865 206e 6578  generate the nex
-000156d0: 7420 776f 7264 206f 6e65 2062 7920 6f6e  t word one by on
-000156e0: 652e 2057 6865 6e20 7573 655f 7061 7374  e. When use_past
-000156f0: 2069 7320 5472 7565 2c20 7468 6572 6520   is True, there 
-00015700: 6172 6520 7477 6f20 7374 6570 7320 746f  are two steps to
-00015710: 2072 756e 2074 6865 2070 7265 6469 6374   run the predict
-00015720: 696f 6e2e 0a20 2020 2020 2020 2020 2020  ion..           
-00015730: 2020 2020 2049 6e20 7468 6520 6669 7273       In the firs
-00015740: 7420 7374 6570 2c20 7365 7420 7468 6520  t step, set the 
-00015750: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00015760: 6f6e 2074 6f20 6265 2054 7275 6520 6279  on to be True by
-00015770: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00015780: 2060 6d6f 6465 6c2e 6164 645f 666c 6167   `model.add_flag
-00015790: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
-000157a0: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
-000157b0: 7275 6529 602c 2061 6e64 2070 6173 7320  rue)`, and pass 
-000157c0: 7468 6520 6675 6c6c 2069 6e70 7574 732e  the full inputs.
-000157d0: 2054 6865 6e2c 2073 6574 2074 6865 0a20   Then, set the. 
-000157e0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-000157f0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00015800: 6e20 746f 2062 6520 4661 6c73 6520 6279  n to be False by
-00015810: 2060 6d6f 6465 6c2e 6164 645f 666c 6167   `model.add_flag
-00015820: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
-00015830: 6972 7374 5f69 7465 7261 7469 6f6e 3d46  irst_iteration=F
-00015840: 616c 7365 2960 2e0a 2020 2020 2020 2020  alse)`..        
-00015850: 2020 2020 2020 2020 4174 2074 6869 7320          At this 
-00015860: 6d6f 6d65 6e74 2c20 7061 7373 2074 6865  moment, pass the
-00015870: 2073 696e 676c 6520 7374 6570 2773 2069   single step's i
-00015880: 6e70 7574 2074 656e 736f 722c 2061 6e64  nput tensor, and
-00015890: 206c 6f6f 7020 6974 2e20 4465 6661 756c   loop it. Defaul
-000158a0: 7420 4661 6c73 652e 0a20 2020 2020 2020  t False..       
-000158b0: 2020 2020 206d 6f65 5f63 6f6e 6669 6728       moe_config(
-000158c0: 4d6f 4543 6f6e 6669 6729 3a20 5468 6520  MoEConfig): The 
-000158d0: 636f 6e66 6967 7572 6174 696f 6e20 6f66  configuration of
-000158e0: 204d 6f45 2028 4d69 7874 7572 6520 6f66   MoE (Mixture of
-000158f0: 2045 7870 6572 7429 2e20 4465 6661 756c   Expert). Defaul
-00015900: 7420 6973 2061 6e20 696e 7374 616e 6365  t is an instance
-00015910: 206f 6620 4d6f 4543 6f6e 6669 670a 2020   of MoEConfig.  
-00015920: 2020 2020 2020 2020 2020 2020 2020 7769                wi
-00015930: 7468 2064 6566 6175 6c74 2076 616c 7565  th default value
-00015940: 732e 2050 6c65 6173 6520 7365 6520 604d  s. Please see `M
-00015950: 6f45 436f 6e66 6967 602e 0a20 2020 2020  oEConfig`..     
-00015960: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-00015970: 636f 6e66 6967 284f 7050 6172 616c 6c65  config(OpParalle
-00015980: 6c43 6f6e 6669 672c 204d 6f45 5061 7261  lConfig, MoEPara
-00015990: 6c6c 656c 436f 6e66 6967 293a 2054 6865  llelConfig): The
-000159a0: 2070 6172 616c 6c65 6c20 636f 6e66 6967   parallel config
-000159b0: 7572 652e 2057 6865 6e20 4d6f 4520 6973  ure. When MoE is
-000159c0: 2061 7070 6c69 6564 2c0a 2020 2020 2020   applied,.      
-000159d0: 2020 2020 2020 2020 2020 4d6f 4550 6172            MoEPar
-000159e0: 616c 6c65 6c43 6f6e 6669 6720 6973 2065  allelConfig is e
-000159f0: 6666 6563 7469 7665 2c20 6f74 6865 7277  ffective, otherw
-00015a00: 6973 6520 4f70 5061 7261 6c6c 656c 436f  ise OpParallelCo
-00015a10: 6e66 6967 2069 7320 6566 6665 6374 6976  nfig is effectiv
-00015a20: 652e 2044 6566 6175 6c74 2060 6465 6661  e. Default `defa
-00015a30: 756c 745f 6470 6d70 5f63 6f6e 6669 6760  ult_dpmp_config`
-00015a40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00015a50: 2020 616e 2069 6e73 7461 6e63 6520 6f66    an instance of
-00015a60: 2060 4f70 5061 7261 6c6c 656c 436f 6e66   `OpParallelConf
-00015a70: 6967 6020 7769 7468 2064 6566 6175 6c74  ig` with default
-00015a80: 2061 7267 732e 0a0a 2020 2020 2020 2020   args...        
-00015a90: 496e 7075 7473 3a0a 2020 2020 2020 2020  Inputs:.        
-00015aa0: 2020 2020 2d20 2a2a 782a 2a20 2854 656e      - **x** (Ten
-00015ab0: 736f 7229 202d 2046 6c6f 6174 2054 656e  sor) - Float Ten
-00015ac0: 736f 722c 2073 6861 7065 2073 686f 756c  sor, shape shoul
-00015ad0: 6420 6265 205b 6261 7463 685f 7369 7a65  d be [batch_size
-00015ae0: 2c20 7365 715f 6c65 6e67 7468 2c20 6869  , seq_length, hi
-00015af0: 6464 656e 5f73 697a 655d 206f 720a 2020  dden_size] or.  
-00015b00: 2020 2020 2020 2020 2020 2020 5b62 6174              [bat
-00015b10: 6368 5f73 697a 6520 2a20 7365 715f 6c65  ch_size * seq_le
-00015b20: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
-00015b30: 655d 2c20 6966 2074 6865 2075 7365 5f70  e], if the use_p
-00015b40: 6173 7420 6973 2046 616c 7365 206f 7220  ast is False or 
-00015b50: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00015b60: 6f6e 3d54 7275 652e 204f 7468 6572 7769  on=True. Otherwi
-00015b70: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
-00015b80: 2020 7368 6f75 6c64 2062 6520 5b62 6174    should be [bat
-00015b90: 6368 5f73 697a 652c 2031 2c20 6869 6464  ch_size, 1, hidd
-00015ba0: 656e 5f73 697a 655d 0a20 2020 2020 2020  en_size].       
-00015bb0: 2020 2020 202d 202a 2a69 6e70 7574 5f6d       - **input_m
-00015bc0: 6173 6b2a 2a20 2854 656e 736f 7229 202d  ask** (Tensor) -
-00015bd0: 2046 6c6f 6174 2054 656e 736f 722c 2049   Float Tensor, I
-00015be0: 6620 7468 6520 7573 655f 7061 7374 2069  f the use_past i
-00015bf0: 7320 4661 6c73 6520 6f72 2069 735f 6669  s False or is_fi
-00015c00: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
-00015c10: 7565 2c0a 2020 2020 2020 2020 2020 2020  ue,.            
-00015c20: 2020 7468 6520 6174 7465 6e74 696f 6e20    the attention 
-00015c30: 6d61 736b 206d 6174 7269 7820 7368 6f75  mask matrix shou
-00015c40: 6c64 2062 6120 5b62 6174 6368 5f73 697a  ld ba [batch_siz
-00015c50: 652c 2073 6571 5f6c 656e 6774 682c 2073  e, seq_length, s
-00015c60: 6571 5f6c 656e 6774 685d 2c20 6f72 204e  eq_length], or N
-00015c70: 6f6e 652e 204e 6f6e 6520 6d65 616e 7320  one. None means 
-00015c80: 7468 6572 6520 7769 6c6c 0a20 2020 2020  there will.     
-00015c90: 2020 2020 2020 2020 2062 6520 6e6f 206d           be no m
-00015ca0: 6173 6b20 696e 2073 6f66 746d 6178 2063  ask in softmax c
-00015cb0: 6f6d 7075 7461 7469 6f6e 2e20 4f74 6865  omputation. Othe
-00015cc0: 7277 6973 652c 2073 686f 756c 6420 6265  rwise, should be
-00015cd0: 205b 6261 7463 685f 7369 7a65 2c20 312c   [batch_size, 1,
-00015ce0: 2068 6964 6465 6e5f 7369 7a65 5d0a 2020   hidden_size].  
-00015cf0: 2020 2020 2020 2020 2020 2d20 2a2a 696e            - **in
-00015d00: 6974 5f72 6573 6574 2a2a 2028 5465 6e73  it_reset** (Tens
-00015d10: 6f72 2920 2d20 4120 626f 6f6c 2074 656e  or) - A bool ten
-00015d20: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
-00015d30: 315d 2c20 7573 6564 2074 6f20 636c 6561  1], used to clea
-00015d40: 7220 7468 6520 7061 7374 206b 6579 2070  r the past key p
-00015d50: 6172 616d 6574 6572 2061 6e64 0a20 2020  arameter and.   
-00015d60: 2020 2020 2020 2020 2020 2070 6173 7420             past 
-00015d70: 7661 6c75 6520 7061 7261 6d65 7465 7220  value parameter 
-00015d80: 7573 6564 2069 6e20 7468 6520 696e 6372  used in the incr
-00015d90: 656d 656e 7461 6c20 7072 6564 6963 7469  emental predicti
-00015da0: 6f6e 2e20 4f6e 6c79 2076 616c 6964 2077  on. Only valid w
-00015db0: 6865 6e20 7573 655f 7061 7374 2069 7320  hen use_past is 
-00015dc0: 5472 7565 2e20 4465 6661 756c 7420 5472  True. Default Tr
-00015dd0: 7565 2e0a 2020 2020 2020 2020 2020 2020  ue..            
-00015de0: 2d20 2a2a 6261 7463 685f 7661 6c69 645f  - **batch_valid_
-00015df0: 6c65 6e67 7468 2a2a 2028 5465 6e73 6f72  length** (Tensor
-00015e00: 2920 2d20 496e 7433 3220 7465 6e73 6f72  ) - Int32 tensor
-00015e10: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
-00015e20: 6368 5f73 697a 655d 2074 6865 2070 6173  ch_size] the pas
-00015e30: 7420 6361 6c63 756c 6174 6564 2074 6865  t calculated the
-00015e40: 2069 6e64 6578 2e0a 2020 2020 2020 2020   index..        
-00015e50: 2020 2020 2020 5573 6564 2066 6f72 2069        Used for i
-00015e60: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
-00015e70: 6374 696f 6e20 7768 656e 2074 6865 2075  ction when the u
-00015e80: 7365 5f70 6173 7420 6973 2054 7275 652e  se_past is True.
-00015e90: 2044 6566 6175 6c74 204e 6f6e 652e 0a0a   Default None...
-00015ea0: 2020 2020 2020 2020 4f75 7470 7574 733a          Outputs:
-00015eb0: 0a20 2020 2020 2020 2020 2020 2054 7570  .            Tup
-00015ec0: 6c65 2c20 6120 7475 706c 6520 636f 6e74  le, a tuple cont
-00015ed0: 6169 6e73 2860 6f75 7470 7574 602c 2060  ains(`output`, `
-00015ee0: 6c61 7965 725f 7072 6573 656e 7460 292e  layer_present`).
-00015ef0: 0a0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
-00015f00: 2a2a 6f75 7470 7574 2a2a 2028 5465 6e73  **output** (Tens
-00015f10: 6f72 2920 2d20 5468 6520 666c 6f61 7420  or) - The float 
-00015f20: 7465 6e73 6f72 206f 6620 7468 6520 6f75  tensor of the ou
-00015f30: 7470 7574 206f 6620 7468 6520 6c61 7965  tput of the laye
-00015f40: 7220 7769 7468 0a20 2020 2020 2020 2020  r with.         
-00015f50: 2020 2020 2073 6861 7065 2028 6261 7463       shape (batc
-00015f60: 685f 7369 7a65 2c20 7365 715f 6c65 6e67  h_size, seq_leng
-00015f70: 7468 2c20 6869 6464 656e 5f73 697a 6529  th, hidden_size)
-00015f80: 206f 7220 2862 6174 6368 5f73 697a 6520   or (batch_size 
-00015f90: 2a20 7365 715f 6c65 6e67 7468 2c20 6869  * seq_length, hi
-00015fa0: 6464 656e 5f73 697a 6529 2c20 6966 2074  dden_size), if t
-00015fb0: 6865 2075 7365 5f70 6173 7420 6973 0a20  he use_past is. 
-00015fc0: 2020 2020 2020 2020 2020 2020 2046 616c               Fal
-00015fd0: 7365 206f 7220 6973 5f66 6972 7374 5f69  se or is_first_i
-00015fe0: 7465 7261 7469 6f6e 3d54 7275 652e 204f  teration=True. O
-00015ff0: 7468 6572 7769 7365 2c20 6974 2077 696c  therwise, it wil
-00016000: 6c20 6265 2028 6261 7463 685f 7369 7a65  l be (batch_size
-00016010: 2c20 312c 2068 6964 6465 6e5f 7369 7a65  , 1, hidden_size
-00016020: 290a 0a20 2020 2020 2020 2020 2020 202d  )..            -
-00016030: 202a 2a6c 6179 6572 5f70 7265 7365 6e74   **layer_present
-00016040: 2a2a 2028 5475 706c 6529 202d 2041 2074  ** (Tuple) - A t
-00016050: 7570 6c65 206f 6620 7468 6520 5465 6e73  uple of the Tens
-00016060: 6f72 206f 6620 7468 6520 7072 6f6a 6563  or of the projec
-00016070: 7465 6420 6b65 7920 616e 6420 7661 6c75  ted key and valu
-00016080: 6520 7665 6374 6f72 2077 6974 680a 2020  e vector with.  
-00016090: 2020 2020 2020 2020 2020 2020 2828 6261              ((ba
-000160a0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
-000160b0: 6164 732c 2073 697a 655f 7065 725f 6865  ads, size_per_he
-000160c0: 6164 2c20 7365 715f 6c65 6e67 7468 292c  ad, seq_length),
-000160d0: 0a20 2020 2020 2020 2020 2020 2020 2028  .              (
-000160e0: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
-000160f0: 6865 6164 732c 2073 6571 5f6c 656e 6774  heads, seq_lengt
-00016100: 682c 2073 697a 655f 7065 725f 6865 6164  h, size_per_head
-00016110: 2929 2e0a 0a20 2020 2020 2020 2053 7570  ))...        Sup
-00016120: 706f 7274 6564 2050 6c61 7466 6f72 6d73  ported Platforms
-00016130: 3a0a 2020 2020 2020 2020 2020 2020 6060  :.            ``
-00016140: 4173 6365 6e64 6060 2060 6047 5055 6060  Ascend`` ``GPU``
-00016150: 0a0a 2020 2020 2020 2020 4578 616d 706c  ..        Exampl
-00016160: 6573 3a0a 2020 2020 2020 2020 2020 2020  es:.            
-00016170: 3e3e 3e20 696d 706f 7274 206e 756d 7079  >>> import numpy
-00016180: 2061 7320 6e70 0a20 2020 2020 2020 2020   as np.         
-00016190: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
-000161a0: 7370 6f72 6520 696d 706f 7274 2064 7479  spore import dty
-000161b0: 7065 2061 7320 6d73 7479 7065 0a20 2020  pe as mstype.   
-000161c0: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
-000161d0: 6d20 6d69 6e64 666f 726d 6572 732e 6d6f  m mindformers.mo
-000161e0: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-000161f0: 7220 696d 706f 7274 2054 7261 6e73 666f  r import Transfo
-00016200: 726d 6572 456e 636f 6465 724c 6179 6572  rmerEncoderLayer
-00016210: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00016220: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
-00016230: 696d 706f 7274 2054 656e 736f 720a 2020  import Tensor.  
-00016240: 2020 2020 2020 2020 2020 3e3e 3e20 6d6f            >>> mo
-00016250: 6465 6c20 3d20 5472 616e 7366 6f72 6d65  del = Transforme
-00016260: 7245 6e63 6f64 6572 4c61 7965 7228 6261  rEncoderLayer(ba
-00016270: 7463 685f 7369 7a65 3d32 2c20 6869 6464  tch_size=2, hidd
-00016280: 656e 5f73 697a 653d 382c 2066 666e 5f68  en_size=8, ffn_h
-00016290: 6964 6465 6e5f 7369 7a65 3d36 342c 2073  idden_size=64, s
-000162a0: 6571 5f6c 656e 6774 683d 3136 2c0a 2020  eq_length=16,.  
-000162b0: 2020 2020 2020 2020 2020 2e2e 2e20 2020            ...   
-000162c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000162d0: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
-000162e0: 6d5f 6865 6164 733d 3229 0a20 2020 2020  m_heads=2).     
-000162f0: 2020 2020 2020 203e 3e3e 2065 6e63 6f64         >>> encod
-00016300: 6572 5f69 6e70 7574 5f76 616c 7565 203d  er_input_value =
-00016310: 2054 656e 736f 7228 6e70 2e6f 6e65 7328   Tensor(np.ones(
-00016320: 2832 2c20 3136 2c20 3829 292c 206d 7374  (2, 16, 8)), mst
-00016330: 7970 652e 666c 6f61 7433 3229 0a20 2020  ype.float32).   
-00016340: 2020 2020 2020 2020 203e 3e3e 2065 6e63           >>> enc
-00016350: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b20  oder_input_mask 
-00016360: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-00016370: 2828 322c 2031 362c 2031 3629 292c 206d  ((2, 16, 16)), m
-00016380: 7374 7970 652e 666c 6f61 7431 3629 0a20  stype.float16). 
-00016390: 2020 2020 2020 2020 2020 203e 3e3e 206f             >>> o
-000163a0: 7574 7075 742c 2070 6173 7420 3d20 6d6f  utput, past = mo
-000163b0: 6465 6c28 656e 636f 6465 725f 696e 7075  del(encoder_inpu
-000163c0: 745f 7661 6c75 652c 2065 6e63 6f64 6572  t_value, encoder
-000163d0: 5f69 6e70 7574 5f6d 6173 6b29 0a20 2020  _input_mask).   
-000163e0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-000163f0: 6e74 286f 7574 7075 742e 7368 6170 6529  nt(output.shape)
-00016400: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-00016410: 2031 362c 2038 290a 2020 2020 2020 2020   16, 8).        
-00016420: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-00016430: 7374 5b30 5d2e 7368 6170 6529 0a20 2020  st[0].shape).   
-00016440: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-00016450: 342c 2031 3629 0a20 2020 2020 2020 2020  4, 16).         
-00016460: 2020 203e 3e3e 2070 7269 6e74 2870 6173     >>> print(pas
-00016470: 745b 315d 2e73 6861 7065 290a 2020 2020  t[1].shape).    
-00016480: 2020 2020 2020 2020 2832 2c20 322c 2031          (2, 2, 1
-00016490: 362c 2034 290a 2020 2020 2020 2020 2020  6, 4).          
-000164a0: 2020 3e3e 3e20 2320 5768 656e 2075 7365    >>> # When use
-000164b0: 2075 7365 5f70 6173 743d 5472 7565 2c20   use_past=True, 
-000164c0: 6974 2069 6e63 6c75 6465 7320 7477 6f20  it includes two 
-000164d0: 7374 6570 7320 746f 2069 6d70 6c65 6d65  steps to impleme
-000164e0: 6e74 2074 6865 2069 6e63 7265 6d65 6e74  nt the increment
-000164f0: 616c 2070 7265 6469 6374 696f 6e2e 0a20  al prediction.. 
-00016500: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
-00016510: 2053 7465 7020 313a 2073 6574 2069 735f   Step 1: set is_
-00016520: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-00016530: 5472 7565 2c20 616e 6420 696e 7075 7420  True, and input 
-00016540: 7468 6520 6675 6c6c 2073 6571 7565 6e63  the full sequenc
-00016550: 6520 6c65 6e67 7468 2773 2073 7461 7465  e length's state
-00016560: 2e0a 2020 2020 2020 2020 2020 2020 3e3e  ..            >>
-00016570: 3e20 6261 7463 685f 7661 6c69 645f 6c65  > batch_valid_le
-00016580: 6e67 7468 203d 2054 656e 736f 7228 6e70  ngth = Tensor(np
-00016590: 2e6f 6e65 7328 2832 2c29 292c 206d 7374  .ones((2,)), mst
-000165a0: 7970 652e 696e 7433 3229 0a20 2020 2020  ype.int32).     
-000165b0: 2020 2020 2020 203e 3e3e 2069 6e69 745f         >>> init_
-000165c0: 7265 7365 7420 3d20 5465 6e73 6f72 285b  reset = Tensor([
-000165d0: 5472 7565 5d2c 206d 7374 7970 652e 626f  True], mstype.bo
-000165e0: 6f6c 5f29 0a20 2020 2020 2020 2020 2020  ol_).           
-000165f0: 203e 3e3e 2023 2053 6574 2069 735f 6669   >>> # Set is_fi
-00016600: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
-00016610: 7565 2074 6f20 6765 6e65 7261 7465 2074  ue to generate t
-00016620: 6865 2066 756c 6c20 6d65 6d6f 7279 2073  he full memory s
-00016630: 7461 7465 730a 2020 2020 2020 2020 2020  tates.          
-00016640: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 5472    >>> model = Tr
-00016650: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00016660: 4c61 7965 7228 6261 7463 685f 7369 7a65  Layer(batch_size
-00016670: 3d32 2c20 6869 6464 656e 5f73 697a 653d  =2, hidden_size=
-00016680: 382c 2066 666e 5f68 6964 6465 6e5f 7369  8, ffn_hidden_si
-00016690: 7a65 3d36 342c 2073 6571 5f6c 656e 6774  ze=64, seq_lengt
-000166a0: 683d 3136 2c0a 2020 2020 2020 2020 2020  h=16,.          
-000166b0: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
-000166c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000166d0: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-000166e0: 322c 2075 7365 5f70 6173 743d 5472 7565  2, use_past=True
-000166f0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-00016700: 3e20 6d6f 6465 6c2e 6164 645f 666c 6167  > model.add_flag
-00016710: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
-00016720: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
-00016730: 7275 6529 0a20 2020 2020 2020 2020 2020  rue).           
-00016740: 203e 3e3e 2068 6964 6465 6e2c 2070 6173   >>> hidden, pas
-00016750: 7420 3d20 6d6f 6465 6c28 656e 636f 6465  t = model(encode
-00016760: 725f 696e 7075 745f 7661 6c75 652c 2065  r_input_value, e
-00016770: 6e63 6f64 6572 5f69 6e70 7574 5f6d 6173  ncoder_input_mas
-00016780: 6b2c 2069 6e69 745f 7265 7365 742c 2062  k, init_reset, b
-00016790: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-000167a0: 6829 0a20 2020 2020 2020 2020 2020 203e  h).            >
-000167b0: 3e3e 2070 7269 6e74 2868 6964 6465 6e2e  >> print(hidden.
-000167c0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-000167d0: 2020 2028 322c 2031 362c 2038 290a 2020     (2, 16, 8).  
-000167e0: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
-000167f0: 696e 7428 7061 7374 5b30 5d2e 7368 6170  int(past[0].shap
-00016800: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-00016810: 322c 2032 2c20 342c 2031 3629 0a20 2020  2, 2, 4, 16).   
-00016820: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-00016830: 6e74 2870 6173 745b 315d 2e73 6861 7065  nt(past[1].shape
-00016840: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-00016850: 2c20 322c 2031 362c 2034 290a 2020 2020  , 2, 16, 4).    
-00016860: 2020 2020 2020 2020 3e3e 3e20 656e 636f          >>> enco
-00016870: 6465 725f 696e 7075 745f 7661 6c75 6520  der_input_value 
-00016880: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-00016890: 2828 322c 2031 2c20 3829 292c 206d 7374  ((2, 1, 8)), mst
-000168a0: 7970 652e 666c 6f61 7433 3229 0a20 2020  ype.float32).   
-000168b0: 2020 2020 2020 2020 203e 3e3e 2065 6e63           >>> enc
-000168c0: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b20  oder_input_mask 
-000168d0: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-000168e0: 2828 322c 2031 2c20 3136 2929 2c20 6d73  ((2, 1, 16)), ms
-000168f0: 7479 7065 2e66 6c6f 6174 3136 290a 2020  type.float16).  
-00016900: 2020 2020 2020 2020 2020 3e3e 3e20 696e            >>> in
-00016910: 6974 5f72 6573 6574 203d 2054 656e 736f  it_reset = Tenso
-00016920: 7228 5b46 616c 7365 5d2c 206d 7374 7970  r([False], mstyp
-00016930: 652e 626f 6f6c 5f29 0a20 2020 2020 2020  e.bool_).       
-00016940: 2020 2020 203e 3e3e 2023 2053 7465 7020       >>> # Step 
-00016950: 323a 2073 6574 2069 735f 6669 7273 745f  2: set is_first_
-00016960: 6974 6572 6174 696f 6e3d 4661 6c73 652c  iteration=False,
-00016970: 2061 6e64 2070 6173 7320 7468 6520 7369   and pass the si
-00016980: 6e67 6c65 2077 6f72 6420 746f 2072 756e  ngle word to run
-00016990: 2074 6865 2070 7265 6469 6374 696f 6e20   the prediction 
-000169a0: 7261 7468 6572 2074 6861 6e0a 2020 2020  rather than.    
-000169b0: 2020 2020 2020 2020 3e3e 3e20 2320 7468          >>> # th
-000169c0: 6520 6675 6c6c 2073 6571 7565 6e63 652e  e full sequence.
-000169d0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000169e0: 206d 6f64 656c 2e61 6464 5f66 6c61 6773   model.add_flags
-000169f0: 5f72 6563 7572 7369 7665 2869 735f 6669  _recursive(is_fi
-00016a00: 7273 745f 6974 6572 6174 696f 6e3d 4661  rst_iteration=Fa
-00016a10: 6c73 6529 0a20 2020 2020 2020 2020 2020  lse).           
-00016a20: 203e 3e3e 2068 6964 6465 6e2c 2070 6173   >>> hidden, pas
-00016a30: 7420 3d20 6d6f 6465 6c28 656e 636f 6465  t = model(encode
-00016a40: 725f 696e 7075 745f 7661 6c75 652c 2065  r_input_value, e
-00016a50: 6e63 6f64 6572 5f69 6e70 7574 5f6d 6173  ncoder_input_mas
-00016a60: 6b2c 2069 6e69 745f 7265 7365 742c 2062  k, init_reset, b
-00016a70: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00016a80: 6829 0a20 2020 2020 2020 2020 2020 203e  h).            >
-00016a90: 3e3e 2070 7269 6e74 2868 6964 6465 6e2e  >> print(hidden.
-00016aa0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-00016ab0: 2020 2028 322c 2031 2c20 3829 0a20 2020     (2, 1, 8).   
-00016ac0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-00016ad0: 6e74 2870 6173 745b 305d 2e73 6861 7065  nt(past[0].shape
-00016ae0: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-00016af0: 2c20 322c 2034 2c20 3136 290a 2020 2020  , 2, 4, 16).    
-00016b00: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-00016b10: 7428 7061 7374 5b31 5d2e 7368 6170 6529  t(past[1].shape)
-00016b20: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-00016b30: 2032 2c20 3136 2c20 3429 0a20 2020 2022   2, 16, 4).    "
-00016b40: 2222 0a0a 2020 2020 405f 4c6f 6741 6374  ""..    @_LogAct
-00016b50: 696f 6e4f 6e63 6528 6d5f 6c6f 6767 6572  ionOnce(m_logger
-00016b60: 3d6c 6f67 6765 722c 206b 6579 3d27 5472  =logger, key='Tr
-00016b70: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00016b80: 4c61 7965 7227 2c0a 2020 2020 2020 2020  Layer',.        
-00016b90: 2020 2020 2020 2020 2020 2020 6e6f 5f77              no_w
-00016ba0: 6172 6e69 6e67 3d5f 6765 745f 7061 7261  arning=_get_para
-00016bb0: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
-00016bc0: 5061 7261 6c6c 656c 4d6f 6465 2e53 5441  ParallelMode.STA
-00016bd0: 4e44 5f41 4c4f 4e45 2c29 290a 2020 2020  ND_ALONE,)).    
-00016be0: 405f 6172 6773 5f74 7970 655f 7661 6c69  @_args_type_vali
-00016bf0: 6461 746f 725f 6368 6563 6b28 6869 6464  dator_check(hidd
-00016c00: 656e 5f73 697a 653d 5661 6c69 6461 746f  en_size=Validato
-00016c10: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
-00016c20: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
-00016c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016c40: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-00016c50: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00016c60: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
-00016c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016c80: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-00016c90: 6e5f 6869 6464 656e 5f73 697a 653d 5661  n_hidden_size=Va
-00016ca0: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
-00016cb0: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
-00016cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016cd0: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
-00016ce0: 6c65 6e67 7468 3d56 616c 6964 6174 6f72  length=Validator
-00016cf0: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-00016d00: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
-00016d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016d20: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-00016d30: 726f 706f 7574 5f72 6174 653d 5661 6c69  ropout_rate=Vali
-00016d40: 6461 746f 722e 6368 6563 6b5f 6e6f 6e5f  dator.check_non_
-00016d50: 6e65 6761 7469 7665 5f66 6c6f 6174 2c0a  negative_float,.
-00016d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016d80: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-00016d90: 6174 653d 5661 6c69 6461 746f 722e 6368  ate=Validator.ch
-00016da0: 6563 6b5f 6e6f 6e5f 6e65 6761 7469 7665  eck_non_negative
-00016db0: 5f66 6c6f 6174 2c0a 2020 2020 2020 2020  _float,.        
-00016dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016dd0: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-00016de0: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-00016df0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00016e00: 626f 6f6c 2c0a 2020 2020 2020 2020 2020  bool,.          
-00016e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016e20: 2020 2020 2020 6c61 7965 726e 6f72 6d5f        layernorm_
-00016e30: 636f 6d70 7574 655f 7479 7065 3d5f 7661  compute_type=_va
-00016e40: 6c69 645f 7661 6c75 655f 6368 6563 6b73  lid_value_checks
-00016e50: 285b 6d73 7479 7065 2e66 6c6f 6174 3332  ([mstype.float32
-00016e60: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00016e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014840: 2020 2020 2020 2020 2020 2020 2020 3029                0)
+00014850: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+00014860: 292c 2028 312c 2032 2c20 3329 290a 2020  ), (1, 2, 3)).  
+00014870: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00014880: 4765 7420 7468 6520 7072 6563 6973 6520  Get the precise 
+00014890: 706f 7369 7469 6f6e 2069 6e64 6578 0a20  position index. 
+000148a0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+000148b0: 6e64 6578 203d 2073 656c 662e 7375 6231  ndex = self.sub1
+000148c0: 2846 2e63 6173 7428 6375 7272 656e 745f  (F.cast(current_
+000148d0: 696e 6465 782c 206d 7374 7970 652e 696e  index, mstype.in
+000148e0: 7433 3229 2c20 3129 0a20 2020 2020 2020  t32), 1).       
+000148f0: 2020 2020 2020 2020 2069 6e64 6578 203d           index =
+00014900: 2046 2e72 6573 6861 7065 2869 6e64 6578   F.reshape(index
+00014910: 2c20 282d 312c 2031 2c20 3129 290a 2020  , (-1, 1, 1)).  
+00014920: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00014930: 4361 6c63 756c 6174 6520 7468 6520 6174  Calculate the at
+00014940: 7465 6e74 696f 6e5f 6d61 736b 206d 6174  tention_mask mat
+00014950: 7269 7820 7669 6120 7468 6520 706f 7369  rix via the posi
+00014960: 7469 6f6e 2069 6e64 6578 0a20 2020 2020  tion index.     
+00014970: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00014980: 7469 6f6e 5f6d 6173 6b20 3d20 462e 6361  tion_mask = F.ca
+00014990: 7374 2873 656c 662e 7465 6e73 6f72 5f6c  st(self.tensor_l
+000149a0: 6528 7365 6c66 2e72 616e 6765 2c20 696e  e(self.range, in
+000149b0: 6465 7829 2c20 6d73 7479 7065 2e69 6e74  dex), mstype.int
+000149c0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
+000149d0: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
+000149e0: 736b 203d 2073 656c 662e 6578 7061 6e64  sk = self.expand
+000149f0: 5f64 696d 7328 6174 7465 6e74 696f 6e5f  _dims(attention_
+00014a00: 6d61 736b 2c20 3229 0a20 2020 2020 2020  mask, 2).       
+00014a10: 2020 2020 2023 204d 696e 7573 2031 3030       # Minus 100
+00014a20: 3030 2066 6f72 2074 6865 2070 6f73 6974  00 for the posit
+00014a30: 696f 6e20 7768 6572 6520 6d61 736b 6564  ion where masked
+00014a40: 2074 6f20 6578 636c 7564 6520 7468 656d   to exclude them
+00014a50: 2066 726f 6d20 736f 6674 6d61 780a 2020   from softmax.  
+00014a60: 2020 2020 2020 2020 2020 6d75 6c74 6970            multip
+00014a70: 6c75 5f6f 7574 203d 2073 656c 662e 7375  lu_out = self.su
+00014a80: 6228 0a20 2020 2020 2020 2020 2020 2020  b(.             
+00014a90: 2020 2050 2e43 6173 7428 2928 462e 7475     P.Cast()(F.tu
+00014aa0: 706c 655f 746f 5f61 7272 6179 2828 312e  ple_to_array((1.
+00014ab0: 302c 2929 2c20 502e 4454 7970 6528 2928  0,)), P.DType()(
+00014ac0: 6174 7465 6e74 696f 6e5f 7363 6f72 6573  attention_scores
+00014ad0: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
+00014ae0: 2020 2020 502e 4361 7374 2829 2861 7474      P.Cast()(att
+00014af0: 656e 7469 6f6e 5f6d 6173 6b2c 2050 2e44  ention_mask, P.D
+00014b00: 5479 7065 2829 2861 7474 656e 7469 6f6e  Type()(attention
+00014b10: 5f73 636f 7265 7329 2929 0a0a 2020 2020  _scores)))..    
+00014b20: 2020 2020 2020 2020 6164 6465 7220 3d20          adder = 
+00014b30: 7365 6c66 2e6d 756c 286d 756c 7469 706c  self.mul(multipl
+00014b40: 755f 6f75 742c 2073 656c 662e 6d75 6c74  u_out, self.mult
+00014b50: 6970 6c79 5f64 6174 6129 0a20 2020 2020  iply_data).     
+00014b60: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+00014b70: 5f73 636f 7265 7320 3d20 7365 6c66 2e61  _scores = self.a
+00014b80: 6464 2861 6464 6572 2c20 6174 7465 6e74  dd(adder, attent
+00014b90: 696f 6e5f 7363 6f72 6573 290a 0a20 2020  ion_scores)..   
+00014ba0: 2020 2020 2023 2061 7474 656e 7469 6f6e       # attention
+00014bb0: 2070 726f 6273 0a20 2020 2020 2020 2061   probs.        a
+00014bc0: 7474 656e 7469 6f6e 5f70 726f 6273 203d  ttention_probs =
+00014bd0: 2073 656c 662e 5f73 6f66 746d 6178 2861   self._softmax(a
+00014be0: 7474 656e 7469 6f6e 5f73 636f 7265 7329  ttention_scores)
+00014bf0: 0a20 2020 2020 2020 2061 7474 656e 7469  .        attenti
+00014c00: 6f6e 5f70 726f 6273 203d 2073 656c 662e  on_probs = self.
+00014c10: 736f 6674 6d61 785f 6361 7374 2861 7474  softmax_cast(att
+00014c20: 656e 7469 6f6e 5f70 726f 6273 2c20 6f72  ention_probs, or
+00014c30: 695f 6474 7970 6529 0a0a 2020 2020 2020  i_dtype)..      
+00014c40: 2020 6174 7465 6e74 696f 6e5f 7072 6f62    attention_prob
+00014c50: 7320 3d20 7365 6c66 2e70 726f 625f 6472  s = self.prob_dr
+00014c60: 6f70 6f75 7428 6174 7465 6e74 696f 6e5f  opout(attention_
+00014c70: 7072 6f62 7329 0a20 2020 2020 2020 2023  probs).        #
+00014c80: 2057 6569 6768 7465 6420 7375 6d20 6f75   Weighted sum ou
+00014c90: 7470 7574 205b 6273 2c20 6e75 6d5f 6865  tput [bs, num_he
+00014ca0: 6164 732c 2073 6571 5f6c 656e 6774 682c  ads, seq_length,
+00014cb0: 2073 697a 655f 7065 725f 6865 6164 5d0a   size_per_head].
+00014cc0: 2020 2020 2020 2020 7765 6967 6874 6564          weighted
+00014cd0: 5f76 616c 7565 7320 3d20 7365 6c66 2e62  _values = self.b
+00014ce0: 6174 6368 5f6d 6174 6d75 6c28 6174 7465  atch_matmul(atte
+00014cf0: 6e74 696f 6e5f 7072 6f62 732c 2076 616c  ntion_probs, val
+00014d00: 7565 290a 2020 2020 2020 2020 6174 7465  ue).        atte
+00014d10: 6e74 696f 6e5f 6d65 7267 6520 3d20 7365  ntion_merge = se
+00014d20: 6c66 2e5f 6d65 7267 655f 6865 6164 7328  lf._merge_heads(
+00014d30: 7765 6967 6874 6564 5f76 616c 7565 7329  weighted_values)
+00014d40: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00014d50: 6174 7465 6e74 696f 6e5f 6d65 7267 650a  attention_merge.
+00014d60: 0a0a 636c 6173 7320 5472 616e 7366 6f72  ..class Transfor
+00014d70: 6d65 7245 6e63 6f64 6572 4c61 7965 7228  merEncoderLayer(
+00014d80: 4365 6c6c 293a 0a20 2020 2072 2222 220a  Cell):.    r""".
+00014d90: 2020 2020 2020 2020 5472 616e 7366 6f72          Transfor
+00014da0: 6d65 7220 456e 636f 6465 7220 4c61 7965  mer Encoder Laye
+00014db0: 722e 2054 6869 7320 6973 2061 6e20 696d  r. This is an im
+00014dc0: 706c 656d 656e 7461 7469 6f6e 206f 6620  plementation of 
+00014dd0: 7468 6520 7369 6e67 6c65 206c 6179 6572  the single layer
+00014de0: 206f 6620 7468 6520 7472 616e 7366 6f72   of the transfor
+00014df0: 6d65 720a 2020 2020 2020 2020 656e 636f  mer.        enco
+00014e00: 6465 7220 6c61 7965 722c 2069 6e63 6c75  der layer, inclu
+00014e10: 6469 6e67 206d 756c 7469 6865 6164 2061  ding multihead a
+00014e20: 7474 656e 7469 6f6e 2061 6e64 2066 6565  ttention and fee
+00014e30: 6477 6172 6420 6c61 7965 722e 0a0a 2020  dward layer...  
+00014e40: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
+00014e50: 2020 2020 2020 2020 6261 7463 685f 7369          batch_si
+00014e60: 7a65 2869 6e74 293a 2054 6865 2062 6174  ze(int): The bat
+00014e70: 6368 2073 697a 6520 6f66 2074 6865 2069  ch size of the i
+00014e80: 6e70 7574 2074 656e 736f 7220 7768 656e  nput tensor when
+00014e90: 2064 6f20 696e 6372 656e 6d65 6e74 616c   do increnmental
+00014ea0: 2070 7265 6469 6374 696f 6e2e 2053 686f   prediction. Sho
+00014eb0: 756c 6420 6265 2061 2070 6f73 6974 6976  uld be a positiv
+00014ec0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00014ed0: 2020 7661 6c75 652e 2057 6865 6e20 646f    value. When do
+00014ee0: 2074 7261 696e 696e 6720 6f72 2070 7265   training or pre
+00014ef0: 6469 6374 696f 6e2c 2074 6865 2061 7267  diction, the arg
+00014f00: 756d 656e 7420 7769 6c6c 206e 6f74 2077  ument will not w
+00014f10: 6f72 6b20 616e 6420 7468 6520 7573 6572  ork and the user
+00014f20: 2063 616e 206a 7573 7420 7061 7373 204e   can just pass N
+00014f30: 6f6e 6520 746f 0a20 2020 2020 2020 2020  one to.         
+00014f40: 2020 2020 2020 2074 6865 2061 7267 756d         the argum
+00014f50: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
+00014f60: 2068 6964 6465 6e5f 7369 7a65 2869 6e74   hidden_size(int
+00014f70: 293a 2054 6865 2068 6964 6465 6e20 7369  ): The hidden si
+00014f80: 7a65 206f 6620 7468 6520 696e 7075 742e  ze of the input.
+00014f90: 0a20 2020 2020 2020 2020 2020 2066 666e  .            ffn
+00014fa0: 5f68 6964 6465 6e5f 7369 7a65 2869 6e74  _hidden_size(int
+00014fb0: 293a 2054 6865 2068 6964 6465 6e20 7369  ): The hidden si
+00014fc0: 7a65 206f 6620 626f 7474 6c65 6e65 636b  ze of bottleneck
+00014fd0: 2069 6e20 7468 6520 6665 6564 666f 7277   in the feedforw
+00014fe0: 6172 6420 6c61 7965 722e 0a20 2020 2020  ard layer..     
+00014ff0: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
+00015000: 2869 6e74 293a 2054 6865 206e 756d 6265  (int): The numbe
+00015010: 7220 6f66 2074 6865 2068 6561 6473 2e0a  r of the heads..
+00015020: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
+00015030: 6c65 6e67 7468 2869 6e74 293a 2054 6865  length(int): The
+00015040: 2069 6e70 7574 2073 6571 7565 6e63 6520   input sequence 
+00015050: 6c65 6e67 7468 2e0a 2020 2020 2020 2020  length..        
+00015060: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
+00015070: 6f70 6f75 745f 7261 7465 2866 6c6f 6174  opout_rate(float
+00015080: 293a 2054 6865 2064 726f 706f 7574 2072  ): The dropout r
+00015090: 6174 6520 6f66 2074 6865 2061 7474 656e  ate of the atten
+000150a0: 7469 6f6e 2073 636f 7265 732e 2044 6566  tion scores. Def
+000150b0: 6175 6c74 3a30 2e31 2e0a 2020 2020 2020  ault:0.1..      
+000150c0: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+000150d0: 706f 7574 5f72 6174 6528 666c 6f61 7429  pout_rate(float)
+000150e0: 3a20 5468 6520 6472 6f70 6f75 7420 7261  : The dropout ra
+000150f0: 7465 206f 6620 7468 6520 6669 6e61 6c20  te of the final 
+00015100: 6f75 7470 7574 206f 6620 7468 6520 6c61  output of the la
+00015110: 7965 722e 2044 6566 6175 6c74 3a30 2e31  yer. Default:0.1
+00015120: 2e0a 2020 2020 2020 2020 2020 2020 706f  ..            po
+00015130: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00015140: 6964 7561 6c28 626f 6f6c 293a 2044 6f20  idual(bool): Do 
+00015150: 7265 7369 6475 616c 7320 6164 6473 2062  residuals adds b
+00015160: 6566 6f72 6520 7468 6520 6c61 7965 726e  efore the layern
+00015170: 6f72 6d2e 2044 6566 6175 6c74 2046 616c  orm. Default Fal
+00015180: 7365 2e0a 2020 2020 2020 2020 2020 2020  se..            
+00015190: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+000151a0: 655f 7479 7065 2864 7479 7065 2e4e 756d  e_type(dtype.Num
+000151b0: 6265 7229 3a20 5468 6520 636f 6d70 7574  ber): The comput
+000151c0: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
+000151d0: 6520 6c61 7965 726e 6f72 6d2e 0a20 2020  e layernorm..   
+000151e0: 2020 2020 2020 2020 2020 2020 2053 686f               Sho
+000151f0: 756c 6420 6265 206d 7374 7970 652e 666c  uld be mstype.fl
+00015200: 6f61 7433 3220 6f72 206d 7374 7970 652e  oat32 or mstype.
+00015210: 666c 6f61 7431 362e 2044 6566 6175 6c74  float16. Default
+00015220: 206d 7374 7970 652e 666c 6f61 7433 322e   mstype.float32.
+00015230: 0a20 2020 2020 2020 2020 2020 2073 6f66  .            sof
+00015240: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+00015250: 6528 6474 7970 652e 4e75 6d62 6572 293a  e(dtype.Number):
+00015260: 2054 6865 2063 6f6d 7075 7461 7469 6f6e   The computation
+00015270: 2074 7970 6520 6f66 2074 6865 2073 6f66   type of the sof
+00015280: 746d 6178 2069 6e20 7468 6520 6174 7465  tmax in the atte
+00015290: 6e74 696f 6e2e 0a20 2020 2020 2020 2020  ntion..         
+000152a0: 2020 2020 2020 2053 686f 756c 6420 6265         Should be
+000152b0: 206d 7374 7970 652e 666c 6f61 7433 3220   mstype.float32 
+000152c0: 6f72 206d 7374 7970 652e 666c 6f61 7431  or mstype.float1
+000152d0: 362e 2044 6566 6175 6c74 206d 7374 7970  6. Default mstyp
+000152e0: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+000152f0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+00015300: 745f 7479 7065 2864 7479 7065 2e4e 756d  t_type(dtype.Num
+00015310: 6265 7229 3a20 5468 6520 7061 7261 6d65  ber): The parame
+00015320: 7465 7220 696e 6974 6961 6c69 7a61 7469  ter initializati
+00015330: 6f6e 2074 7970 6520 6f66 2074 6865 206d  on type of the m
+00015340: 6f64 756c 652e 0a20 2020 2020 2020 2020  odule..         
+00015350: 2020 2020 2020 2053 686f 756c 6420 6265         Should be
+00015360: 206d 7374 7970 652e 666c 6f61 7433 3220   mstype.float32 
+00015370: 6f72 206d 7374 7970 652e 666c 6f61 7431  or mstype.float1
+00015380: 362e 2044 6566 6175 6c74 206d 7374 7970  6. Default mstyp
+00015390: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+000153a0: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
+000153b0: 7420 2873 7472 2c20 6e6e 2e43 656c 6c29  t (str, nn.Cell)
+000153c0: 3a20 5468 6520 6163 7469 7661 7469 6f6e  : The activation
+000153d0: 206f 6620 7468 6520 696e 7465 726e 616c   of the internal
+000153e0: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
+000153f0: 6572 2e20 5375 7070 6f72 7473 2027 7265  er. Supports 're
+00015400: 6c75 272c 0a20 2020 2020 2020 2020 2020  lu',.           
+00015410: 2020 2020 2027 7265 6c75 3627 2c20 2774       'relu6', 't
+00015420: 616e 6827 2c20 2767 656c 7527 2c20 2766  anh', 'gelu', 'f
+00015430: 6173 745f 6765 6c75 272c 2027 656c 7527  ast_gelu', 'elu'
+00015440: 2c20 2773 6967 6d6f 6964 272c 2027 7072  , 'sigmoid', 'pr
+00015450: 656c 7527 2c20 276c 6561 6b79 7265 6c75  elu', 'leakyrelu
+00015460: 272c 2027 6873 7769 7368 272c 0a20 2020  ', 'hswish',.   
+00015470: 2020 2020 2020 2020 2020 2020 2027 6873               'hs
+00015480: 6967 6d6f 6964 272c 2027 6c6f 6773 6967  igmoid', 'logsig
+00015490: 6d6f 6964 2720 616e 6420 736f 206f 6e2e  moid' and so on.
+000154a0: 2055 7365 7220 6361 6e20 7072 6f76 6964   User can provid
+000154b0: 6520 6375 7374 6f6d 2061 6374 6976 6974  e custom activit
+000154c0: 696f 6e20 746f 2074 6865 2061 7267 756d  ion to the argum
+000154d0: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
+000154e0: 2020 2020 2049 6620 7573 6572 2077 616e       If user wan
+000154f0: 7473 2074 6f20 7275 6e20 7468 6520 6e65  ts to run the ne
+00015500: 7420 696e 2074 6865 2070 6172 616c 6c65  t in the paralle
+00015510: 6c20 6d6f 6465 2c20 7468 6520 6375 7374  l mode, the cust
+00015520: 6f6d 2061 6374 6976 6174 696f 6e20 6d75  om activation mu
+00015530: 7374 2061 6c73 6f20 7072 6f76 6964 650a  st also provide.
+00015540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015550: 7468 6520 6061 6374 6976 6174 696f 6e5f  the `activation_
+00015560: 7368 6172 6460 2066 756e 6374 696f 6e2e  shard` function.
+00015570: 2050 6c65 6173 6520 7365 6520 7468 6520   Please see the 
+00015580: 6578 616d 706c 6573 206f 6620 7468 650a  examples of the.
+00015590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000155a0: 636c 6173 733a 606d 696e 6466 6f72 6d65  class:`mindforme
+000155b0: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+000155c0: 666f 726d 6572 2e46 6565 6446 6f72 7761  former.FeedForwa
+000155d0: 7264 602e 2044 6566 6175 6c74 3a20 6765  rd`. Default: ge
+000155e0: 6c75 2e0a 2020 2020 2020 2020 2020 2020  lu..            
+000155f0: 7573 655f 7061 7374 2862 6f6f 6c29 3a20  use_past(bool): 
+00015600: 5573 6520 7468 6520 7061 7374 2073 7461  Use the past sta
+00015610: 7465 2074 6f20 636f 6d70 7574 652c 2075  te to compute, u
+00015620: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
+00015630: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
+00015640: 466f 7220 6578 616d 706c 652c 2069 6620  For example, if 
+00015650: 7765 2068 6176 6520 7477 6f0a 2020 2020  we have two.    
+00015660: 2020 2020 2020 2020 2020 2020 776f 7264              word
+00015670: 7320 616e 6420 7761 6e74 2074 6f20 6765  s and want to ge
+00015680: 6e65 7261 7465 2074 6865 2074 656e 206d  nerate the ten m
+00015690: 6f72 6520 776f 7264 732e 2057 6520 6a75  ore words. We ju
+000156a0: 7374 206e 6565 6420 746f 2063 6f6d 7075  st need to compu
+000156b0: 7465 2074 6865 2074 776f 2077 6f72 6473  te the two words
+000156c0: 2720 7374 6174 6520 6f6e 6c79 206f 6e63  ' state only onc
+000156d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000156e0: 2020 2061 6e64 2067 656e 6572 6174 6520     and generate 
+000156f0: 7468 6520 6e65 7874 2077 6f72 6420 6f6e  the next word on
+00015700: 6520 6279 206f 6e65 2e20 5768 656e 2075  e by one. When u
+00015710: 7365 5f70 6173 7420 6973 2054 7275 652c  se_past is True,
+00015720: 2074 6865 7265 2061 7265 2074 776f 2073   there are two s
+00015730: 7465 7073 2074 6f20 7275 6e20 7468 6520  teps to run the 
+00015740: 7072 6564 6963 7469 6f6e 2e0a 2020 2020  prediction..    
+00015750: 2020 2020 2020 2020 2020 2020 496e 2074              In t
+00015760: 6865 2066 6972 7374 2073 7465 702c 2073  he first step, s
+00015770: 6574 2074 6865 2069 735f 6669 7273 745f  et the is_first_
+00015780: 6974 6572 6174 696f 6e20 746f 2062 6520  iteration to be 
+00015790: 5472 7565 2062 790a 2020 2020 2020 2020  True by.        
+000157a0: 2020 2020 2020 2020 606d 6f64 656c 2e61          `model.a
+000157b0: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
+000157c0: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
+000157d0: 6174 696f 6e3d 5472 7565 2960 2c20 616e  ation=True)`, an
+000157e0: 6420 7061 7373 2074 6865 2066 756c 6c20  d pass the full 
+000157f0: 696e 7075 7473 2e20 5468 656e 2c20 7365  inputs. Then, se
+00015800: 7420 7468 650a 2020 2020 2020 2020 2020  t the.          
+00015810: 2020 2020 2020 6973 5f66 6972 7374 5f69        is_first_i
+00015820: 7465 7261 7469 6f6e 2074 6f20 6265 2046  teration to be F
+00015830: 616c 7365 2062 7920 606d 6f64 656c 2e61  alse by `model.a
+00015840: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
+00015850: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
+00015860: 6174 696f 6e3d 4661 6c73 6529 602e 0a20  ation=False)`.. 
+00015870: 2020 2020 2020 2020 2020 2020 2020 2041                 A
+00015880: 7420 7468 6973 206d 6f6d 656e 742c 2070  t this moment, p
+00015890: 6173 7320 7468 6520 7369 6e67 6c65 2073  ass the single s
+000158a0: 7465 7027 7320 696e 7075 7420 7465 6e73  tep's input tens
+000158b0: 6f72 2c20 616e 6420 6c6f 6f70 2069 742e  or, and loop it.
+000158c0: 2044 6566 6175 6c74 2046 616c 7365 2e0a   Default False..
+000158d0: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
+000158e0: 636f 6e66 6967 284d 6f45 436f 6e66 6967  config(MoEConfig
+000158f0: 293a 2054 6865 2063 6f6e 6669 6775 7261  ): The configura
+00015900: 7469 6f6e 206f 6620 4d6f 4520 284d 6978  tion of MoE (Mix
+00015910: 7475 7265 206f 6620 4578 7065 7274 292e  ture of Expert).
+00015920: 2044 6566 6175 6c74 2069 7320 616e 2069   Default is an i
+00015930: 6e73 7461 6e63 6520 6f66 204d 6f45 436f  nstance of MoECo
+00015940: 6e66 6967 0a20 2020 2020 2020 2020 2020  nfig.           
+00015950: 2020 2020 2077 6974 6820 6465 6661 756c       with defaul
+00015960: 7420 7661 6c75 6573 2e20 506c 6561 7365  t values. Please
+00015970: 2073 6565 2060 4d6f 4543 6f6e 6669 6760   see `MoEConfig`
+00015980: 2e0a 2020 2020 2020 2020 2020 2020 7061  ..            pa
+00015990: 7261 6c6c 656c 5f63 6f6e 6669 6728 4f70  rallel_config(Op
+000159a0: 5061 7261 6c6c 656c 436f 6e66 6967 2c20  ParallelConfig, 
+000159b0: 4d6f 4550 6172 616c 6c65 6c43 6f6e 6669  MoEParallelConfi
+000159c0: 6729 3a20 5468 6520 7061 7261 6c6c 656c  g): The parallel
+000159d0: 2063 6f6e 6669 6775 7265 2e20 5768 656e   configure. When
+000159e0: 204d 6f45 2069 7320 6170 706c 6965 642c   MoE is applied,
+000159f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015a00: 204d 6f45 5061 7261 6c6c 656c 436f 6e66   MoEParallelConf
+00015a10: 6967 2069 7320 6566 6665 6374 6976 652c  ig is effective,
+00015a20: 206f 7468 6572 7769 7365 204f 7050 6172   otherwise OpPar
+00015a30: 616c 6c65 6c43 6f6e 6669 6720 6973 2065  allelConfig is e
+00015a40: 6666 6563 7469 7665 2e20 4465 6661 756c  ffective. Defaul
+00015a50: 7420 6064 6566 6175 6c74 5f64 706d 705f  t `default_dpmp_
+00015a60: 636f 6e66 6967 602c 0a20 2020 2020 2020  config`,.       
+00015a70: 2020 2020 2020 2020 2061 6e20 696e 7374           an inst
+00015a80: 616e 6365 206f 6620 604f 7050 6172 616c  ance of `OpParal
+00015a90: 6c65 6c43 6f6e 6669 6760 2077 6974 6820  lelConfig` with 
+00015aa0: 6465 6661 756c 7420 6172 6773 2e0a 0a20  default args... 
+00015ab0: 2020 2020 2020 2049 6e70 7574 733a 0a20         Inputs:. 
+00015ac0: 2020 2020 2020 2020 2020 202d 202a 2a78             - **x
+00015ad0: 2a2a 2028 5465 6e73 6f72 2920 2d20 466c  ** (Tensor) - Fl
+00015ae0: 6f61 7420 5465 6e73 6f72 2c20 7368 6170  oat Tensor, shap
+00015af0: 6520 7368 6f75 6c64 2062 6520 5b62 6174  e should be [bat
+00015b00: 6368 5f73 697a 652c 2073 6571 5f6c 656e  ch_size, seq_len
+00015b10: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
+00015b20: 5d20 6f72 0a20 2020 2020 2020 2020 2020  ] or.           
+00015b30: 2020 205b 6261 7463 685f 7369 7a65 202a     [batch_size *
+00015b40: 2073 6571 5f6c 656e 6774 682c 2068 6964   seq_length, hid
+00015b50: 6465 6e5f 7369 7a65 5d2c 2069 6620 7468  den_size], if th
+00015b60: 6520 7573 655f 7061 7374 2069 7320 4661  e use_past is Fa
+00015b70: 6c73 6520 6f72 2069 735f 6669 7273 745f  lse or is_first_
+00015b80: 6974 6572 6174 696f 6e3d 5472 7565 2e20  iteration=True. 
+00015b90: 4f74 6865 7277 6973 652c 0a20 2020 2020  Otherwise,.     
+00015ba0: 2020 2020 2020 2020 2073 686f 756c 6420           should 
+00015bb0: 6265 205b 6261 7463 685f 7369 7a65 2c20  be [batch_size, 
+00015bc0: 312c 2068 6964 6465 6e5f 7369 7a65 5d0a  1, hidden_size].
+00015bd0: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+00015be0: 696e 7075 745f 6d61 736b 2a2a 2028 5465  input_mask** (Te
+00015bf0: 6e73 6f72 2920 2d20 466c 6f61 7420 5465  nsor) - Float Te
+00015c00: 6e73 6f72 2c20 4966 2074 6865 2075 7365  nsor, If the use
+00015c10: 5f70 6173 7420 6973 2046 616c 7365 206f  _past is False o
+00015c20: 7220 6973 5f66 6972 7374 5f69 7465 7261  r is_first_itera
+00015c30: 7469 6f6e 3d54 7275 652c 0a20 2020 2020  tion=True,.     
+00015c40: 2020 2020 2020 2020 2074 6865 2061 7474           the att
+00015c50: 656e 7469 6f6e 206d 6173 6b20 6d61 7472  ention mask matr
+00015c60: 6978 2073 686f 756c 6420 6261 205b 6261  ix should ba [ba
+00015c70: 7463 685f 7369 7a65 2c20 7365 715f 6c65  tch_size, seq_le
+00015c80: 6e67 7468 2c20 7365 715f 6c65 6e67 7468  ngth, seq_length
+00015c90: 5d2c 206f 7220 4e6f 6e65 2e20 4e6f 6e65  ], or None. None
+00015ca0: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
+00015cb0: 6c0a 2020 2020 2020 2020 2020 2020 2020  l.              
+00015cc0: 6265 206e 6f20 6d61 736b 2069 6e20 736f  be no mask in so
+00015cd0: 6674 6d61 7820 636f 6d70 7574 6174 696f  ftmax computatio
+00015ce0: 6e2e 204f 7468 6572 7769 7365 2c20 7368  n. Otherwise, sh
+00015cf0: 6f75 6c64 2062 6520 5b62 6174 6368 5f73  ould be [batch_s
+00015d00: 697a 652c 2031 2c20 6869 6464 656e 5f73  ize, 1, hidden_s
+00015d10: 697a 655d 0a20 2020 2020 2020 2020 2020  ize].           
+00015d20: 202d 202a 2a69 6e69 745f 7265 7365 742a   - **init_reset*
+00015d30: 2a20 2854 656e 736f 7229 202d 2041 2062  * (Tensor) - A b
+00015d40: 6f6f 6c20 7465 6e73 6f72 2077 6974 6820  ool tensor with 
+00015d50: 7368 6170 6520 5b31 5d2c 2075 7365 6420  shape [1], used 
+00015d60: 746f 2063 6c65 6172 2074 6865 2070 6173  to clear the pas
+00015d70: 7420 6b65 7920 7061 7261 6d65 7465 7220  t key parameter 
+00015d80: 616e 640a 2020 2020 2020 2020 2020 2020  and.            
+00015d90: 2020 7061 7374 2076 616c 7565 2070 6172    past value par
+00015da0: 616d 6574 6572 2075 7365 6420 696e 2074  ameter used in t
+00015db0: 6865 2069 6e63 7265 6d65 6e74 616c 2070  he incremental p
+00015dc0: 7265 6469 6374 696f 6e2e 204f 6e6c 7920  rediction. Only 
+00015dd0: 7661 6c69 6420 7768 656e 2075 7365 5f70  valid when use_p
+00015de0: 6173 7420 6973 2054 7275 652e 2044 6566  ast is True. Def
+00015df0: 6175 6c74 2054 7275 652e 0a20 2020 2020  ault True..     
+00015e00: 2020 2020 2020 202d 202a 2a62 6174 6368         - **batch
+00015e10: 5f76 616c 6964 5f6c 656e 6774 682a 2a20  _valid_length** 
+00015e20: 2854 656e 736f 7229 202d 2049 6e74 3332  (Tensor) - Int32
+00015e30: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
+00015e40: 7065 205b 6261 7463 685f 7369 7a65 5d20  pe [batch_size] 
+00015e50: 7468 6520 7061 7374 2063 616c 6375 6c61  the past calcula
+00015e60: 7465 6420 7468 6520 696e 6465 782e 0a20  ted the index.. 
+00015e70: 2020 2020 2020 2020 2020 2020 2055 7365               Use
+00015e80: 6420 666f 7220 696e 6372 656d 656e 7461  d for incrementa
+00015e90: 6c20 7072 6564 6963 7469 6f6e 2077 6865  l prediction whe
+00015ea0: 6e20 7468 6520 7573 655f 7061 7374 2069  n the use_past i
+00015eb0: 7320 5472 7565 2e20 4465 6661 756c 7420  s True. Default 
+00015ec0: 4e6f 6e65 2e0a 0a20 2020 2020 2020 204f  None...        O
+00015ed0: 7574 7075 7473 3a0a 2020 2020 2020 2020  utputs:.        
+00015ee0: 2020 2020 5475 706c 652c 2061 2074 7570      Tuple, a tup
+00015ef0: 6c65 2063 6f6e 7461 696e 7328 606f 7574  le contains(`out
+00015f00: 7075 7460 2c20 606c 6179 6572 5f70 7265  put`, `layer_pre
+00015f10: 7365 6e74 6029 2e0a 0a20 2020 2020 2020  sent`)...       
+00015f20: 2020 2020 202d 202a 2a6f 7574 7075 742a       - **output*
+00015f30: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+00015f40: 2066 6c6f 6174 2074 656e 736f 7220 6f66   float tensor of
+00015f50: 2074 6865 206f 7574 7075 7420 6f66 2074   the output of t
+00015f60: 6865 206c 6179 6572 2077 6974 680a 2020  he layer with.  
+00015f70: 2020 2020 2020 2020 2020 2020 7368 6170              shap
+00015f80: 6520 2862 6174 6368 5f73 697a 652c 2073  e (batch_size, s
+00015f90: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+00015fa0: 6e5f 7369 7a65 2920 6f72 2028 6261 7463  n_size) or (batc
+00015fb0: 685f 7369 7a65 202a 2073 6571 5f6c 656e  h_size * seq_len
+00015fc0: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
+00015fd0: 292c 2069 6620 7468 6520 7573 655f 7061  ), if the use_pa
+00015fe0: 7374 2069 730a 2020 2020 2020 2020 2020  st is.          
+00015ff0: 2020 2020 4661 6c73 6520 6f72 2069 735f      False or is_
+00016000: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
+00016010: 5472 7565 2e20 4f74 6865 7277 6973 652c  True. Otherwise,
+00016020: 2069 7420 7769 6c6c 2062 6520 2862 6174   it will be (bat
+00016030: 6368 5f73 697a 652c 2031 2c20 6869 6464  ch_size, 1, hidd
+00016040: 656e 5f73 697a 6529 0a0a 2020 2020 2020  en_size)..      
+00016050: 2020 2020 2020 2d20 2a2a 6c61 7965 725f        - **layer_
+00016060: 7072 6573 656e 742a 2a20 2854 7570 6c65  present** (Tuple
+00016070: 2920 2d20 4120 7475 706c 6520 6f66 2074  ) - A tuple of t
+00016080: 6865 2054 656e 736f 7220 6f66 2074 6865  he Tensor of the
+00016090: 2070 726f 6a65 6374 6564 206b 6579 2061   projected key a
+000160a0: 6e64 2076 616c 7565 2076 6563 746f 7220  nd value vector 
+000160b0: 7769 7468 0a20 2020 2020 2020 2020 2020  with.           
+000160c0: 2020 2028 2862 6174 6368 5f73 697a 652c     ((batch_size,
+000160d0: 206e 756d 5f68 6561 6473 2c20 7369 7a65   num_heads, size
+000160e0: 5f70 6572 5f68 6561 642c 2073 6571 5f6c  _per_head, seq_l
+000160f0: 656e 6774 6829 2c0a 2020 2020 2020 2020  ength),.        
+00016100: 2020 2020 2020 2862 6174 6368 5f73 697a        (batch_siz
+00016110: 652c 206e 756d 5f68 6561 6473 2c20 7365  e, num_heads, se
+00016120: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
+00016130: 6572 5f68 6561 6429 292e 0a0a 2020 2020  er_head))...    
+00016140: 2020 2020 5375 7070 6f72 7465 6420 506c      Supported Pl
+00016150: 6174 666f 726d 733a 0a20 2020 2020 2020  atforms:.       
+00016160: 2020 2020 2060 6041 7363 656e 6460 6020       ``Ascend`` 
+00016170: 6060 4750 5560 600a 0a20 2020 2020 2020  ``GPU``..       
+00016180: 2045 7861 6d70 6c65 733a 0a20 2020 2020   Examples:.     
+00016190: 2020 2020 2020 203e 3e3e 2069 6d70 6f72         >>> impor
+000161a0: 7420 6e75 6d70 7920 6173 206e 700a 2020  t numpy as np.  
+000161b0: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
+000161c0: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
+000161d0: 6f72 7420 6474 7970 6520 6173 206d 7374  ort dtype as mst
+000161e0: 7970 650a 2020 2020 2020 2020 2020 2020  ype.            
+000161f0: 3e3e 3e20 6672 6f6d 206d 696e 6466 6f72  >>> from mindfor
+00016200: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
+00016210: 6e73 666f 726d 6572 2069 6d70 6f72 7420  nsformer import 
+00016220: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
+00016230: 6572 4c61 7965 720a 2020 2020 2020 2020  erLayer.        
+00016240: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
+00016250: 6473 706f 7265 2069 6d70 6f72 7420 5465  dspore import Te
+00016260: 6e73 6f72 0a20 2020 2020 2020 2020 2020  nsor.           
+00016270: 203e 3e3e 206d 6f64 656c 203d 2054 7261   >>> model = Tra
+00016280: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
+00016290: 6179 6572 2862 6174 6368 5f73 697a 653d  ayer(batch_size=
+000162a0: 322c 2068 6964 6465 6e5f 7369 7a65 3d38  2, hidden_size=8
+000162b0: 2c20 6666 6e5f 6869 6464 656e 5f73 697a  , ffn_hidden_siz
+000162c0: 653d 3634 2c20 7365 715f 6c65 6e67 7468  e=64, seq_length
+000162d0: 3d31 362c 0a20 2020 2020 2020 2020 2020  =16,.           
+000162e0: 202e 2e2e 2020 2020 2020 2020 2020 2020   ...            
+000162f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016300: 2020 2020 206e 756d 5f68 6561 6473 3d32       num_heads=2
+00016310: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00016320: 3e20 656e 636f 6465 725f 696e 7075 745f  > encoder_input_
+00016330: 7661 6c75 6520 3d20 5465 6e73 6f72 286e  value = Tensor(n
+00016340: 702e 6f6e 6573 2828 322c 2031 362c 2038  p.ones((2, 16, 8
+00016350: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+00016360: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
+00016370: 3e3e 3e20 656e 636f 6465 725f 696e 7075  >>> encoder_inpu
+00016380: 745f 6d61 736b 203d 2054 656e 736f 7228  t_mask = Tensor(
+00016390: 6e70 2e6f 6e65 7328 2832 2c20 3136 2c20  np.ones((2, 16, 
+000163a0: 3136 2929 2c20 6d73 7479 7065 2e66 6c6f  16)), mstype.flo
+000163b0: 6174 3136 290a 2020 2020 2020 2020 2020  at16).          
+000163c0: 2020 3e3e 3e20 6f75 7470 7574 2c20 7061    >>> output, pa
+000163d0: 7374 203d 206d 6f64 656c 2865 6e63 6f64  st = model(encod
+000163e0: 6572 5f69 6e70 7574 5f76 616c 7565 2c20  er_input_value, 
+000163f0: 656e 636f 6465 725f 696e 7075 745f 6d61  encoder_input_ma
+00016400: 736b 290a 2020 2020 2020 2020 2020 2020  sk).            
+00016410: 3e3e 3e20 7072 696e 7428 6f75 7470 7574  >>> print(output
+00016420: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+00016430: 2020 2020 2832 2c20 3136 2c20 3829 0a20      (2, 16, 8). 
+00016440: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+00016450: 7269 6e74 2870 6173 745b 305d 2e73 6861  rint(past[0].sha
+00016460: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00016470: 2832 2c20 322c 2034 2c20 3136 290a 2020  (2, 2, 4, 16).  
+00016480: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+00016490: 696e 7428 7061 7374 5b31 5d2e 7368 6170  int(past[1].shap
+000164a0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+000164b0: 322c 2032 2c20 3136 2c20 3429 0a20 2020  2, 2, 16, 4).   
+000164c0: 2020 2020 2020 2020 203e 3e3e 2023 2057           >>> # W
+000164d0: 6865 6e20 7573 6520 7573 655f 7061 7374  hen use use_past
+000164e0: 3d54 7275 652c 2069 7420 696e 636c 7564  =True, it includ
+000164f0: 6573 2074 776f 2073 7465 7073 2074 6f20  es two steps to 
+00016500: 696d 706c 656d 656e 7420 7468 6520 696e  implement the in
+00016510: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+00016520: 7469 6f6e 2e0a 2020 2020 2020 2020 2020  tion..          
+00016530: 2020 3e3e 3e20 2320 5374 6570 2031 3a20    >>> # Step 1: 
+00016540: 7365 7420 6973 5f66 6972 7374 5f69 7465  set is_first_ite
+00016550: 7261 7469 6f6e 3d54 7275 652c 2061 6e64  ration=True, and
+00016560: 2069 6e70 7574 2074 6865 2066 756c 6c20   input the full 
+00016570: 7365 7175 656e 6365 206c 656e 6774 6827  sequence length'
+00016580: 7320 7374 6174 652e 0a20 2020 2020 2020  s state..       
+00016590: 2020 2020 203e 3e3e 2062 6174 6368 5f76       >>> batch_v
+000165a0: 616c 6964 5f6c 656e 6774 6820 3d20 5465  alid_length = Te
+000165b0: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
+000165c0: 2929 2c20 6d73 7479 7065 2e69 6e74 3332  )), mstype.int32
+000165d0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+000165e0: 3e20 696e 6974 5f72 6573 6574 203d 2054  > init_reset = T
+000165f0: 656e 736f 7228 5b54 7275 655d 2c20 6d73  ensor([True], ms
+00016600: 7479 7065 2e62 6f6f 6c5f 290a 2020 2020  type.bool_).    
+00016610: 2020 2020 2020 2020 3e3e 3e20 2320 5365          >>> # Se
+00016620: 7420 6973 5f66 6972 7374 5f69 7465 7261  t is_first_itera
+00016630: 7469 6f6e 3d54 7275 6520 746f 2067 656e  tion=True to gen
+00016640: 6572 6174 6520 7468 6520 6675 6c6c 206d  erate the full m
+00016650: 656d 6f72 7920 7374 6174 6573 0a20 2020  emory states.   
+00016660: 2020 2020 2020 2020 203e 3e3e 206d 6f64           >>> mod
+00016670: 656c 203d 2054 7261 6e73 666f 726d 6572  el = Transformer
+00016680: 456e 636f 6465 724c 6179 6572 2862 6174  EncoderLayer(bat
+00016690: 6368 5f73 697a 653d 322c 2068 6964 6465  ch_size=2, hidde
+000166a0: 6e5f 7369 7a65 3d38 2c20 6666 6e5f 6869  n_size=8, ffn_hi
+000166b0: 6464 656e 5f73 697a 653d 3634 2c20 7365  dden_size=64, se
+000166c0: 715f 6c65 6e67 7468 3d31 362c 0a20 2020  q_length=16,.   
+000166d0: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
+000166e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000166f0: 2020 2020 2020 2020 2020 2020 206e 756d               num
+00016700: 5f68 6561 6473 3d32 2c20 7573 655f 7061  _heads=2, use_pa
+00016710: 7374 3d54 7275 6529 0a20 2020 2020 2020  st=True).       
+00016720: 2020 2020 203e 3e3e 206d 6f64 656c 2e61       >>> model.a
+00016730: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
+00016740: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
+00016750: 6174 696f 6e3d 5472 7565 290a 2020 2020  ation=True).    
+00016760: 2020 2020 2020 2020 3e3e 3e20 6869 6464          >>> hidd
+00016770: 656e 2c20 7061 7374 203d 206d 6f64 656c  en, past = model
+00016780: 2865 6e63 6f64 6572 5f69 6e70 7574 5f76  (encoder_input_v
+00016790: 616c 7565 2c20 656e 636f 6465 725f 696e  alue, encoder_in
+000167a0: 7075 745f 6d61 736b 2c20 696e 6974 5f72  put_mask, init_r
+000167b0: 6573 6574 2c20 6261 7463 685f 7661 6c69  eset, batch_vali
+000167c0: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
+000167d0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+000167e0: 6869 6464 656e 2e73 6861 7065 290a 2020  hidden.shape).  
+000167f0: 2020 2020 2020 2020 2020 2832 2c20 3136            (2, 16
+00016800: 2c20 3829 0a20 2020 2020 2020 2020 2020  , 8).           
+00016810: 203e 3e3e 2070 7269 6e74 2870 6173 745b   >>> print(past[
+00016820: 305d 2e73 6861 7065 290a 2020 2020 2020  0].shape).      
+00016830: 2020 2020 2020 2832 2c20 322c 2034 2c20        (2, 2, 4, 
+00016840: 3136 290a 2020 2020 2020 2020 2020 2020  16).            
+00016850: 3e3e 3e20 7072 696e 7428 7061 7374 5b31  >>> print(past[1
+00016860: 5d2e 7368 6170 6529 0a20 2020 2020 2020  ].shape).       
+00016870: 2020 2020 2028 322c 2032 2c20 3136 2c20       (2, 2, 16, 
+00016880: 3429 0a20 2020 2020 2020 2020 2020 203e  4).            >
+00016890: 3e3e 2065 6e63 6f64 6572 5f69 6e70 7574  >> encoder_input
+000168a0: 5f76 616c 7565 203d 2054 656e 736f 7228  _value = Tensor(
+000168b0: 6e70 2e6f 6e65 7328 2832 2c20 312c 2038  np.ones((2, 1, 8
+000168c0: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+000168d0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
+000168e0: 3e3e 3e20 656e 636f 6465 725f 696e 7075  >>> encoder_inpu
+000168f0: 745f 6d61 736b 203d 2054 656e 736f 7228  t_mask = Tensor(
+00016900: 6e70 2e6f 6e65 7328 2832 2c20 312c 2031  np.ones((2, 1, 1
+00016910: 3629 292c 206d 7374 7970 652e 666c 6f61  6)), mstype.floa
+00016920: 7431 3629 0a20 2020 2020 2020 2020 2020  t16).           
+00016930: 203e 3e3e 2069 6e69 745f 7265 7365 7420   >>> init_reset 
+00016940: 3d20 5465 6e73 6f72 285b 4661 6c73 655d  = Tensor([False]
+00016950: 2c20 6d73 7479 7065 2e62 6f6f 6c5f 290a  , mstype.bool_).
+00016960: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00016970: 2320 5374 6570 2032 3a20 7365 7420 6973  # Step 2: set is
+00016980: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
+00016990: 3d46 616c 7365 2c20 616e 6420 7061 7373  =False, and pass
+000169a0: 2074 6865 2073 696e 676c 6520 776f 7264   the single word
+000169b0: 2074 6f20 7275 6e20 7468 6520 7072 6564   to run the pred
+000169c0: 6963 7469 6f6e 2072 6174 6865 7220 7468  iction rather th
+000169d0: 616e 0a20 2020 2020 2020 2020 2020 203e  an.            >
+000169e0: 3e3e 2023 2074 6865 2066 756c 6c20 7365  >> # the full se
+000169f0: 7175 656e 6365 2e0a 2020 2020 2020 2020  quence..        
+00016a00: 2020 2020 3e3e 3e20 6d6f 6465 6c2e 6164      >>> model.ad
+00016a10: 645f 666c 6167 735f 7265 6375 7273 6976  d_flags_recursiv
+00016a20: 6528 6973 5f66 6972 7374 5f69 7465 7261  e(is_first_itera
+00016a30: 7469 6f6e 3d46 616c 7365 290a 2020 2020  tion=False).    
+00016a40: 2020 2020 2020 2020 3e3e 3e20 6869 6464          >>> hidd
+00016a50: 656e 2c20 7061 7374 203d 206d 6f64 656c  en, past = model
+00016a60: 2865 6e63 6f64 6572 5f69 6e70 7574 5f76  (encoder_input_v
+00016a70: 616c 7565 2c20 656e 636f 6465 725f 696e  alue, encoder_in
+00016a80: 7075 745f 6d61 736b 2c20 696e 6974 5f72  put_mask, init_r
+00016a90: 6573 6574 2c20 6261 7463 685f 7661 6c69  eset, batch_vali
+00016aa0: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
+00016ab0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00016ac0: 6869 6464 656e 2e73 6861 7065 290a 2020  hidden.shape).  
+00016ad0: 2020 2020 2020 2020 2020 2832 2c20 312c            (2, 1,
+00016ae0: 2038 290a 2020 2020 2020 2020 2020 2020   8).            
+00016af0: 3e3e 3e20 7072 696e 7428 7061 7374 5b30  >>> print(past[0
+00016b00: 5d2e 7368 6170 6529 0a20 2020 2020 2020  ].shape).       
+00016b10: 2020 2020 2028 322c 2032 2c20 342c 2031       (2, 2, 4, 1
+00016b20: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
+00016b30: 3e3e 2070 7269 6e74 2870 6173 745b 315d  >> print(past[1]
+00016b40: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+00016b50: 2020 2020 2832 2c20 322c 2031 362c 2034      (2, 2, 16, 4
+00016b60: 290a 2020 2020 2222 220a 0a20 2020 2040  ).    """..    @
+00016b70: 5f4c 6f67 4163 7469 6f6e 4f6e 6365 286d  _LogActionOnce(m
+00016b80: 5f6c 6f67 6765 723d 6c6f 6767 6572 2c20  _logger=logger, 
+00016b90: 6b65 793d 2754 7261 6e73 666f 726d 6572  key='Transformer
+00016ba0: 456e 636f 6465 724c 6179 6572 272c 0a20  EncoderLayer',. 
+00016bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016bc0: 2020 206e 6f5f 7761 726e 696e 673d 5f67     no_warning=_g
+00016bd0: 6574 5f70 6172 616c 6c65 6c5f 6d6f 6465  et_parallel_mode
+00016be0: 2829 2069 6e20 2850 6172 616c 6c65 6c4d  () in (ParallelM
+00016bf0: 6f64 652e 5354 414e 445f 414c 4f4e 452c  ode.STAND_ALONE,
+00016c00: 2929 0a20 2020 2040 5f61 7267 735f 7479  )).    @_args_ty
+00016c10: 7065 5f76 616c 6964 6174 6f72 5f63 6865  pe_validator_che
+00016c20: 636b 2868 6964 6465 6e5f 7369 7a65 3d56  ck(hidden_size=V
+00016c30: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+00016c40: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
+00016c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016c60: 2020 2020 2020 2020 2020 2020 206e 756d               num
+00016c70: 5f68 6561 6473 3d56 616c 6964 6174 6f72  _heads=Validator
+00016c80: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
+00016c90: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
+00016ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016cb0: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+00016cc0: 7369 7a65 3d56 616c 6964 6174 6f72 2e63  size=Validator.c
+00016cd0: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+00016ce0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+00016cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d00: 2020 2073 6571 5f6c 656e 6774 683d 5661     seq_length=Va
+00016d10: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
+00016d20: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
+00016d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d40: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+00016d50: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+00016d60: 7465 3d56 616c 6964 6174 6f72 2e63 6865  te=Validator.che
+00016d70: 636b 5f6e 6f6e 5f6e 6567 6174 6976 655f  ck_non_negative_
+00016d80: 666c 6f61 742c 0a20 2020 2020 2020 2020  float,.         
+00016d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016da0: 2020 2020 2020 2068 6964 6465 6e5f 6472         hidden_dr
+00016db0: 6f70 6f75 745f 7261 7465 3d56 616c 6964  opout_rate=Valid
+00016dc0: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
+00016dd0: 6567 6174 6976 655f 666c 6f61 742c 0a20  egative_float,. 
+00016de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016df0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00016e00: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+00016e10: 7369 6475 616c 3d56 616c 6964 6174 6f72  sidual=Validator
+00016e20: 2e63 6865 636b 5f62 6f6f 6c2c 0a20 2020  .check_bool,.   
+00016e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e40: 2020 2020 2020 2020 2020 2020 206c 6179               lay
+00016e50: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
+00016e60: 7970 653d 5f76 616c 6964 5f76 616c 7565  ype=_valid_value
+00016e70: 5f63 6865 636b 7328 5b6d 7374 7970 652e  _checks([mstype.
+00016e80: 666c 6f61 7433 322c 0a20 2020 2020 2020  float32,.       
 00016e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016ea0: 2020 2020 2020 2020 2020 2020 2020 6d73                ms
-00016eb0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-00016ec0: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
-00016ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016ed0: 2020 2020 206d 7374 7970 652e 666c 6f61       mstype.floa
+00016ee0: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
+00016ef0: 6174 3136 5d2c 0a20 2020 2020 2020 2020  at16],.         
 00016f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016f10: 2020 2020 2020 2020 2020 2022 5472 616e             "Tran
-00016f20: 7366 6f72 6d65 7245 6e63 6f64 6572 4c61  sformerEncoderLa
-00016f30: 7965 7222 292c 0a20 2020 2020 2020 2020  yer"),.         
-00016f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016f50: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-00016f60: 6f6d 7075 7465 5f74 7970 653d 5f76 616c  ompute_type=_val
-00016f70: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
-00016f80: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00016f90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00016fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f40: 2020 2254 7261 6e73 666f 726d 6572 456e    "TransformerEn
+00016f50: 636f 6465 724c 6179 6572 2229 2c0a 2020  coderLayer"),.  
+00016f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f70: 2020 2020 2020 2020 2020 2020 2020 736f                so
+00016f80: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+00016f90: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
+00016fa0: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
+00016fb0: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
 00016fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016fd0: 2020 2020 2020 2020 2020 206d 7374 7970             mstyp
-00016fe0: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
-00016ff0: 652e 6266 6c6f 6174 3136 5d2c 0a20 2020  e.bfloat16],.   
-00017000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017000: 2020 6d73 7479 7065 2e66 6c6f 6174 3136    mstype.float16
+00017010: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
+00017020: 365d 2c0a 2020 2020 2020 2020 2020 2020  6],.            
 00017030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017040: 2020 2020 2020 2254 7261 6e73 666f 726d        "Transform
-00017050: 6572 456e 636f 6465 724c 6179 6572 2229  erEncoderLayer")
-00017060: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00017070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017080: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-00017090: 653d 5f76 616c 6964 5f76 616c 7565 5f63  e=_valid_value_c
-000170a0: 6865 636b 7328 5b6d 7374 7970 652e 666c  hecks([mstype.fl
-000170b0: 6f61 7433 322c 206d 7374 7970 652e 666c  oat32, mstype.fl
-000170c0: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-000170d0: 6c6f 6174 3136 5d2c 0a20 2020 2020 2020  loat16],.       
-000170e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000170f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017060: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
+00017070: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
+00017080: 4c61 7965 7222 292c 0a20 2020 2020 2020  Layer"),.       
+00017090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000170a0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+000170b0: 6e69 745f 7479 7065 3d5f 7661 6c69 645f  nit_type=_valid_
+000170c0: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
+000170d0: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
+000170e0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+000170f0: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
 00017100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017110: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-00017120: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00017130: 4c61 7965 7222 292c 0a20 2020 2020 2020  Layer"),.       
-00017140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017150: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-00017160: 6c5f 636f 6e66 6967 3d5f 7661 6c69 645f  l_config=_valid_
-00017170: 7479 7065 5f63 6865 636b 7328 5b4f 7050  type_checks([OpP
-00017180: 6172 616c 6c65 6c43 6f6e 6669 672c 204d  arallelConfig, M
-00017190: 6f45 5061 7261 6c6c 656c 436f 6e66 6967  oEParallelConfig
-000171a0: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-000171b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000171c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017120: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017140: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
+00017150: 456e 636f 6465 724c 6179 6572 2229 2c0a  EncoderLayer"),.
+00017160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017180: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+00017190: 5f76 616c 6964 5f74 7970 655f 6368 6563  _valid_type_chec
+000171a0: 6b73 285b 4f70 5061 7261 6c6c 656c 436f  ks([OpParallelCo
+000171b0: 6e66 6967 2c20 4d6f 4550 6172 616c 6c65  nfig, MoEParalle
+000171c0: 6c43 6f6e 6669 675d 2c0a 2020 2020 2020  lConfig],.      
 000171d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000171e0: 2020 2020 2020 2254 7261 6e73 666f 726d        "Transform
-000171f0: 6572 456e 636f 6465 724c 6179 6572 2229  erEncoderLayer")
-00017200: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00017210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017220: 2020 7573 655f 7061 7374 3d56 616c 6964    use_past=Valid
-00017230: 6174 6f72 2e63 6865 636b 5f62 6f6f 6c2c  ator.check_bool,
-00017240: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017260: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
-00017270: 7469 6f6e 3d56 616c 6964 6174 6f72 2e63  tion=Validator.c
-00017280: 6865 636b 5f62 6f6f 6c29 0a20 2020 2064  heck_bool).    d
-00017290: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
-000172a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000172b0: 2020 2062 6174 6368 5f73 697a 652c 0a20     batch_size,. 
-000172c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000172d0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-000172e0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000172f0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-00017300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017310: 6e75 6d5f 6865 6164 732c 0a20 2020 2020  num_heads,.     
-00017320: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
-00017330: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
-00017340: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-00017350: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
-00017360: 302e 312c 0a20 2020 2020 2020 2020 2020  0.1,.           
-00017370: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
-00017380: 706f 7574 5f72 6174 653d 302e 312c 0a20  pout_rate=0.1,. 
-00017390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000173a0: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-000173b0: 6573 6964 7561 6c3d 4661 6c73 652c 0a20  esidual=False,. 
-000173c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000173d0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-000173e0: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
-000173f0: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-00017400: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
-00017410: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
-00017420: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
-00017430: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00017440: 7261 6d5f 696e 6974 5f74 7970 653d 6d73  ram_init_type=ms
-00017450: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
-00017460: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-00017470: 6964 6465 6e5f 6163 743d 2767 656c 7527  idden_act='gelu'
-00017480: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00017490: 2020 2075 7365 5f70 6173 743d 4661 6c73     use_past=Fals
-000174a0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000174b0: 2020 2020 6d6f 655f 636f 6e66 6967 3d64      moe_config=d
-000174c0: 6566 6175 6c74 5f6d 6f65 5f63 6f6e 6669  efault_moe_confi
-000174d0: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
-000174e0: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-000174f0: 6669 673d 6465 6661 756c 745f 6470 6d70  fig=default_dpmp
-00017500: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
-00017510: 2020 2020 2020 2020 2020 7573 655f 666c            use_fl
-00017520: 6173 685f 6174 7465 6e74 696f 6e3d 4661  ash_attention=Fa
-00017530: 6c73 6529 3a0a 2020 2020 2020 2020 7375  lse):.        su
-00017540: 7065 7228 5472 616e 7366 6f72 6d65 7245  per(TransformerE
-00017550: 6e63 6f64 6572 4c61 7965 722c 2073 656c  ncoderLayer, sel
-00017560: 6629 2e5f 5f69 6e69 745f 5f28 290a 2020  f).__init__().  
-00017570: 2020 2020 2020 6966 2062 6174 6368 5f73        if batch_s
-00017580: 697a 6520 6f72 2075 7365 5f70 6173 743a  ize or use_past:
-00017590: 0a20 2020 2020 2020 2020 2020 2056 616c  .            Val
-000175a0: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-000175b0: 6974 6976 655f 696e 7428 6261 7463 685f  itive_int(batch_
-000175c0: 7369 7a65 290a 2020 2020 2020 2020 7365  size).        se
-000175d0: 6c66 2e62 6174 6368 5f73 697a 6520 3d20  lf.batch_size = 
-000175e0: 6261 7463 685f 7369 7a65 0a20 2020 2020  batch_size.     
-000175f0: 2020 2069 6620 5f67 6574 5f70 6172 616c     if _get_paral
-00017600: 6c65 6c5f 6d6f 6465 2829 2069 6e20 2850  lel_mode() in (P
-00017610: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
-00017620: 5f50 4152 414c 4c45 4c2c 293a 0a20 2020  _PARALLEL,):.   
-00017630: 2020 2020 2020 2020 205f 6368 6563 6b5f           _check_
-00017640: 636f 6e66 6967 2870 6172 616c 6c65 6c5f  config(parallel_
-00017650: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
-00017660: 2020 2020 6966 206e 756d 5f68 6561 6473      if num_heads
-00017670: 2025 2070 6172 616c 6c65 6c5f 636f 6e66   % parallel_conf
-00017680: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-00017690: 6c20 213d 2030 3a0a 2020 2020 2020 2020  l != 0:.        
-000176a0: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
-000176b0: 6c75 6545 7272 6f72 280a 2020 2020 2020  lueError(.      
-000176c0: 2020 2020 2020 2020 2020 2020 2020 2246                "F
-000176d0: 6f72 2027 5472 616e 7366 6f72 6d65 7245  or 'TransformerE
-000176e0: 6e63 6f64 6572 4c61 7965 7227 2c20 7468  ncoderLayer', th
-000176f0: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
-00017700: 2027 6e75 6d5f 6865 6164 7327 206d 7573   'num_heads' mus
-00017710: 7420 6265 2064 6976 6973 6962 6c65 6420  t be divisibled 
-00017720: 6279 2074 6865 2022 0a20 2020 2020 2020  by the ".       
-00017730: 2020 2020 2020 2020 2020 2020 2022 2770               "'p
-00017740: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00017750: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
-00017760: 6275 7420 676f 7420 7468 6520 6e75 6d5f  but got the num_
-00017770: 6865 6164 7320 6973 207b 7d20 616e 6420  heads is {} and 
-00017780: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00017790: 2020 2020 2020 2270 6172 616c 6c65 6c5f        "parallel_
-000177a0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-000177b0: 616c 6c65 6c20 6973 207b 7d2e 222e 666f  allel is {}.".fo
-000177c0: 726d 6174 286e 756d 5f68 6561 6473 2c20  rmat(num_heads, 
-000177d0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-000177e0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
-000177f0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00017800: 6869 6464 656e 5f73 697a 6520 2520 7061  hidden_size % pa
-00017810: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-00017820: 6465 6c5f 7061 7261 6c6c 656c 2021 3d20  del_parallel != 
-00017830: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-00017840: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00017850: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
-00017860: 2020 2020 2020 2020 2022 466f 7220 2754           "For 'T
-00017870: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00017880: 724c 6179 6572 272c 2074 6865 2063 6c61  rLayer', the cla
-00017890: 7373 2076 6172 6961 626c 6520 2768 6964  ss variable 'hid
-000178a0: 6465 6e5f 7369 7a65 2720 6d75 7374 2062  den_size' must b
-000178b0: 6520 6469 7669 7369 626c 6564 2062 7920  e divisibled by 
-000178c0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-000178d0: 2020 2020 2020 2274 6865 2027 7061 7261        "the 'para
-000178e0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-000178f0: 6c5f 7061 7261 6c6c 656c 272c 2062 7574  l_parallel', but
-00017900: 2067 6f74 2074 6865 2068 6964 6465 6e5f   got the hidden_
-00017910: 7369 7a65 2069 7320 7b7d 2061 6e64 2070  size is {} and p
-00017920: 6172 616c 6c65 6c5f 636f 6e66 6967 2e22  arallel_config."
-00017930: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017940: 2020 2020 2022 206d 6f64 656c 5f70 6172       " model_par
-00017950: 616c 6c65 6c20 6973 207b 7d2e 222e 666f  allel is {}.".fo
-00017960: 726d 6174 2868 6964 6465 6e5f 7369 7a65  rmat(hidden_size
-00017970: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
-00017980: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00017990: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i
-000179a0: 6620 6666 6e5f 6869 6464 656e 5f73 697a  f ffn_hidden_siz
-000179b0: 6520 2520 7061 7261 6c6c 656c 5f63 6f6e  e % parallel_con
-000179c0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-000179d0: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
-000179e0: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-000179f0: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
-00017a00: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00017a10: 466f 7220 2754 7261 6e73 666f 726d 6572  For 'Transformer
-00017a20: 456e 636f 6465 724c 6179 6572 272c 2074  EncoderLayer', t
-00017a30: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
-00017a40: 6520 2766 666e 5f68 6964 6465 6e5f 7369  e 'ffn_hidden_si
-00017a50: 7a65 2720 6d75 7374 2062 6520 6469 7669  ze' must be divi
-00017a60: 7369 626c 6564 2022 0a20 2020 2020 2020  sibled ".       
-00017a70: 2020 2020 2020 2020 2020 2020 2022 6279               "by
-00017a80: 2074 6865 2027 7061 7261 6c6c 656c 5f63   the 'parallel_c
-00017a90: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00017aa0: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
-00017ab0: 6865 2066 666e 5f68 6964 6465 6e5f 7369  he ffn_hidden_si
-00017ac0: 7a65 2069 7320 7b7d 2022 0a20 2020 2020  ze is {} ".     
-00017ad0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00017ae0: 616e 6420 7061 7261 6c6c 656c 5f63 6f6e  and parallel_con
-00017af0: 6669 672e 206d 6f64 656c 5f70 6172 616c  fig. model_paral
-00017b00: 6c65 6c20 6973 207b 7d2e 220a 2020 2020  lel is {}.".    
-00017b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017b20: 2e66 6f72 6d61 7428 6666 6e5f 6869 6464  .format(ffn_hidd
-00017b30: 656e 5f73 697a 652c 2070 6172 616c 6c65  en_size, paralle
-00017b40: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00017b50: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
-00017b60: 2020 2020 2020 5f63 6865 636b 5f6d 6f65        _check_moe
-00017b70: 5f63 6f6e 6669 6728 6d6f 655f 636f 6e66  _config(moe_conf
-00017b80: 6967 2c20 7061 7261 6c6c 656c 5f63 6f6e  ig, parallel_con
-00017b90: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
-00017ba0: 2073 656c 662e 7573 655f 6d6f 6520 3d20   self.use_moe = 
-00017bb0: 286d 6f65 5f63 6f6e 6669 672e 6578 7065  (moe_config.expe
-00017bc0: 7274 5f6e 756d 203e 2031 290a 2020 2020  rt_num > 1).    
-00017bd0: 2020 2020 2020 2020 7365 6c66 2e75 7365          self.use
-00017be0: 5f70 6173 7420 3d20 7573 655f 7061 7374  _past = use_past
-00017bf0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00017c00: 662e 7365 715f 6c65 6e67 7468 203d 2073  f.seq_length = s
-00017c10: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
-00017c20: 2020 2020 2020 7365 6c66 2e68 6964 6465        self.hidde
-00017c30: 6e5f 7369 7a65 203d 2068 6964 6465 6e5f  n_size = hidden_
-00017c40: 7369 7a65 0a20 2020 2020 2020 2020 2020  size.           
-00017c50: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
-00017c60: 203d 204c 6179 6572 4e6f 726d 2828 6869   = LayerNorm((hi
-00017c70: 6464 656e 5f73 697a 652c 2929 2e74 6f5f  dden_size,)).to_
-00017c80: 666c 6f61 7428 6c61 7965 726e 6f72 6d5f  float(layernorm_
-00017c90: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
-00017ca0: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
-00017cb0: 6179 6572 6e6f 726d 3220 3d20 4c61 7965  ayernorm2 = Laye
-00017cc0: 724e 6f72 6d28 2868 6964 6465 6e5f 7369  rNorm((hidden_si
-00017cd0: 7a65 2c29 292e 746f 5f66 6c6f 6174 286c  ze,)).to_float(l
-00017ce0: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-00017cf0: 5f74 7970 6529 0a0a 2020 2020 2020 2020  _type)..        
-00017d00: 2020 2020 6174 7465 6e74 696f 6e5f 7061      attention_pa
-00017d10: 7261 6c6c 656c 5f63 6f6e 6669 6720 3d20  rallel_config = 
-00017d20: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00017d30: 6470 6d70 2069 6620 7365 6c66 2e75 7365  dpmp if self.use
-00017d40: 5f6d 6f65 2065 6c73 6520 7061 7261 6c6c  _moe else parall
-00017d50: 656c 5f63 6f6e 6669 670a 2020 2020 2020  el_config.      
-00017d60: 2020 2020 2020 7365 6c66 2e61 7474 656e        self.atten
-00017d70: 7469 6f6e 203d 204d 756c 7469 4865 6164  tion = MultiHead
-00017d80: 4174 7465 6e74 696f 6e28 6261 7463 685f  Attention(batch_
-00017d90: 7369 7a65 3d62 6174 6368 5f73 697a 652c  size=batch_size,
-00017da0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017dd0: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
-00017de0: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
-00017df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017e10: 2020 2020 2020 2020 2020 2020 7467 745f              tgt_
-00017e20: 7365 715f 6c65 6e67 7468 3d73 6571 5f6c  seq_length=seq_l
-00017e30: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
-00017e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017e60: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
-00017e70: 7a65 3d68 6964 6465 6e5f 7369 7a65 2c0a  ze=hidden_size,.
-00017e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017eb0: 6e75 6d5f 6865 6164 733d 6e75 6d5f 6865  num_heads=num_he
-00017ec0: 6164 732c 0a20 2020 2020 2020 2020 2020  ads,.           
-00017ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017ef0: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
-00017f00: 6f75 745f 7261 7465 3d68 6964 6465 6e5f  out_rate=hidden_
-00017f10: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-00017f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017f40: 2020 2020 2020 2020 2020 2020 2020 6174                at
-00017f50: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
-00017f60: 7261 7465 3d61 7474 656e 7469 6f6e 5f64  rate=attention_d
-00017f70: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
-00017f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017fa0: 2020 2020 2020 2020 2020 2020 2073 6f66               sof
-00017fb0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
-00017fc0: 653d 736f 6674 6d61 785f 636f 6d70 7574  e=softmax_comput
-00017fd0: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
-00017fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018000: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
-00018010: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
-00018020: 6974 5f74 7970 652c 0a20 2020 2020 2020  it_type,.       
-00018030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018050: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-00018060: 743d 7573 655f 7061 7374 2c0a 2020 2020  t=use_past,.    
+000171e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000171f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017200: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
+00017210: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
+00017220: 4c61 7965 7222 292c 0a20 2020 2020 2020  Layer"),.       
+00017230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017240: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
+00017250: 743d 5661 6c69 6461 746f 722e 6368 6563  t=Validator.chec
+00017260: 6b5f 626f 6f6c 2c0a 2020 2020 2020 2020  k_bool,.        
+00017270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017280: 2020 2020 2020 2020 7573 655f 666c 6173          use_flas
+00017290: 685f 6174 7465 6e74 696f 6e3d 5661 6c69  h_attention=Vali
+000172a0: 6461 746f 722e 6368 6563 6b5f 626f 6f6c  dator.check_bool
+000172b0: 290a 2020 2020 6465 6620 5f5f 696e 6974  ).    def __init
+000172c0: 5f5f 2873 656c 662c 0a20 2020 2020 2020  __(self,.       
+000172d0: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+000172e0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+000172f0: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
+00017300: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00017310: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+00017320: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00017330: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
+00017340: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00017350: 2020 2073 6571 5f6c 656e 6774 682c 0a20     seq_length,. 
+00017360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017370: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+00017380: 745f 7261 7465 3d30 2e31 2c0a 2020 2020  t_rate=0.1,.    
+00017390: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+000173a0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+000173b0: 3d30 2e31 2c0a 2020 2020 2020 2020 2020  =0.1,.          
+000173c0: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
+000173d0: 726e 6f72 6d5f 7265 7369 6475 616c 3d46  rnorm_residual=F
+000173e0: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
+000173f0: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
+00017400: 5f63 6f6d 7075 7465 5f74 7970 653d 6d73  _compute_type=ms
+00017410: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+00017420: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00017430: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+00017440: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
+00017450: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+00017460: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+00017470: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
+00017480: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
+00017490: 2020 2020 2020 6869 6464 656e 5f61 6374        hidden_act
+000174a0: 3d27 6765 6c75 272c 0a20 2020 2020 2020  ='gelu',.       
+000174b0: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
+000174c0: 7374 3d46 616c 7365 2c0a 2020 2020 2020  st=False,.      
+000174d0: 2020 2020 2020 2020 2020 206d 6f65 5f63             moe_c
+000174e0: 6f6e 6669 673d 6465 6661 756c 745f 6d6f  onfig=default_mo
+000174f0: 655f 636f 6e66 6967 2c0a 2020 2020 2020  e_config,.      
+00017500: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+00017510: 6c65 6c5f 636f 6e66 6967 3d64 6566 6175  lel_config=defau
+00017520: 6c74 5f64 706d 705f 636f 6e66 6967 2c0a  lt_dpmp_config,.
+00017530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017540: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
+00017550: 7469 6f6e 3d46 616c 7365 293a 0a20 2020  tion=False):.   
+00017560: 2020 2020 2073 7570 6572 2854 7261 6e73       super(Trans
+00017570: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
+00017580: 6572 2c20 7365 6c66 292e 5f5f 696e 6974  er, self).__init
+00017590: 5f5f 2829 0a20 2020 2020 2020 2069 6620  __().        if 
+000175a0: 6261 7463 685f 7369 7a65 206f 7220 7573  batch_size or us
+000175b0: 655f 7061 7374 3a0a 2020 2020 2020 2020  e_past:.        
+000175c0: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
+000175d0: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+000175e0: 2862 6174 6368 5f73 697a 6529 0a20 2020  (batch_size).   
+000175f0: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
+00017600: 7369 7a65 203d 2062 6174 6368 5f73 697a  size = batch_siz
+00017610: 650a 2020 2020 2020 2020 6966 205f 6765  e.        if _ge
+00017620: 745f 7061 7261 6c6c 656c 5f6d 6f64 6528  t_parallel_mode(
+00017630: 2920 696e 2028 5061 7261 6c6c 656c 4d6f  ) in (ParallelMo
+00017640: 6465 2e41 5554 4f5f 5041 5241 4c4c 454c  de.AUTO_PARALLEL
+00017650: 2c29 3a0a 2020 2020 2020 2020 2020 2020  ,):.            
+00017660: 5f63 6865 636b 5f63 6f6e 6669 6728 7061  _check_config(pa
+00017670: 7261 6c6c 656c 5f63 6f6e 6669 6729 0a20  rallel_config). 
+00017680: 2020 2020 2020 2020 2020 2069 6620 6e75             if nu
+00017690: 6d5f 6865 6164 7320 2520 7061 7261 6c6c  m_heads % parall
+000176a0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+000176b0: 7061 7261 6c6c 656c 2021 3d20 303a 0a20  parallel != 0:. 
+000176c0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+000176d0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+000176e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000176f0: 2020 2020 2022 466f 7220 2754 7261 6e73       "For 'Trans
+00017700: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
+00017710: 6572 272c 2074 6865 2063 6c61 7373 2076  er', the class v
+00017720: 6172 6961 626c 6520 276e 756d 5f68 6561  ariable 'num_hea
+00017730: 6473 2720 6d75 7374 2062 6520 6469 7669  ds' must be divi
+00017740: 7369 626c 6564 2062 7920 7468 6520 220a  sibled by the ".
+00017750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017760: 2020 2020 2227 7061 7261 6c6c 656c 5f63      "'parallel_c
+00017770: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00017780: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
+00017790: 6865 206e 756d 5f68 6561 6473 2069 7320  he num_heads is 
+000177a0: 7b7d 2061 6e64 2022 0a20 2020 2020 2020  {} and ".       
+000177b0: 2020 2020 2020 2020 2020 2020 2022 7061               "pa
+000177c0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+000177d0: 6465 6c5f 7061 7261 6c6c 656c 2069 7320  del_parallel is 
+000177e0: 7b7d 2e22 2e66 6f72 6d61 7428 6e75 6d5f  {}.".format(num_
+000177f0: 6865 6164 732c 2070 6172 616c 6c65 6c5f  heads, parallel_
+00017800: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00017810: 616c 6c65 6c29 290a 2020 2020 2020 2020  allel)).        
+00017820: 2020 2020 6966 2068 6964 6465 6e5f 7369      if hidden_si
+00017830: 7a65 2025 2070 6172 616c 6c65 6c5f 636f  ze % parallel_co
+00017840: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00017850: 6c65 6c20 213d 2030 3a0a 2020 2020 2020  lel != 0:.      
+00017860: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+00017870: 5661 6c75 6545 7272 6f72 280a 2020 2020  ValueError(.    
+00017880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017890: 2246 6f72 2027 5472 616e 7366 6f72 6d65  "For 'Transforme
+000178a0: 7245 6e63 6f64 6572 4c61 7965 7227 2c20  rEncoderLayer', 
+000178b0: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
+000178c0: 6c65 2027 6869 6464 656e 5f73 697a 6527  le 'hidden_size'
+000178d0: 206d 7573 7420 6265 2064 6976 6973 6962   must be divisib
+000178e0: 6c65 6420 6279 2022 0a20 2020 2020 2020  led by ".       
+000178f0: 2020 2020 2020 2020 2020 2020 2022 7468               "th
+00017900: 6520 2770 6172 616c 6c65 6c5f 636f 6e66  e 'parallel_conf
+00017910: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00017920: 6c27 2c20 6275 7420 676f 7420 7468 6520  l', but got the 
+00017930: 6869 6464 656e 5f73 697a 6520 6973 207b  hidden_size is {
+00017940: 7d20 616e 6420 7061 7261 6c6c 656c 5f63  } and parallel_c
+00017950: 6f6e 6669 672e 220a 2020 2020 2020 2020  onfig.".        
+00017960: 2020 2020 2020 2020 2020 2020 2220 6d6f              " mo
+00017970: 6465 6c5f 7061 7261 6c6c 656c 2069 7320  del_parallel is 
+00017980: 7b7d 2e22 2e66 6f72 6d61 7428 6869 6464  {}.".format(hidd
+00017990: 656e 5f73 697a 652c 2070 6172 616c 6c65  en_size, paralle
+000179a0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+000179b0: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
+000179c0: 2020 2020 2020 6966 2066 666e 5f68 6964        if ffn_hid
+000179d0: 6465 6e5f 7369 7a65 2025 2070 6172 616c  den_size % paral
+000179e0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+000179f0: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
+00017a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017a10: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00017a20: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00017a30: 2020 2020 2020 2246 6f72 2027 5472 616e        "For 'Tran
+00017a40: 7366 6f72 6d65 7245 6e63 6f64 6572 4c61  sformerEncoderLa
+00017a50: 7965 7227 2c20 7468 6520 636c 6173 7320  yer', the class 
+00017a60: 7661 7269 6162 6c65 2027 6666 6e5f 6869  variable 'ffn_hi
+00017a70: 6464 656e 5f73 697a 6527 206d 7573 7420  dden_size' must 
+00017a80: 6265 2064 6976 6973 6962 6c65 6420 220a  be divisibled ".
+00017a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017aa0: 2020 2020 2262 7920 7468 6520 2770 6172      "by the 'par
+00017ab0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+00017ac0: 656c 5f70 6172 616c 6c65 6c27 2c20 6275  el_parallel', bu
+00017ad0: 7420 676f 7420 7468 6520 6666 6e5f 6869  t got the ffn_hi
+00017ae0: 6464 656e 5f73 697a 6520 6973 207b 7d20  dden_size is {} 
+00017af0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00017b00: 2020 2020 2020 2261 6e64 2070 6172 616c        "and paral
+00017b10: 6c65 6c5f 636f 6e66 6967 2e20 6d6f 6465  lel_config. mode
+00017b20: 6c5f 7061 7261 6c6c 656c 2069 7320 7b7d  l_parallel is {}
+00017b30: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
+00017b40: 2020 2020 2020 202e 666f 726d 6174 2866         .format(f
+00017b50: 666e 5f68 6964 6465 6e5f 7369 7a65 2c20  fn_hidden_size, 
+00017b60: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00017b70: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
+00017b80: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+00017b90: 6563 6b5f 6d6f 655f 636f 6e66 6967 286d  eck_moe_config(m
+00017ba0: 6f65 5f63 6f6e 6669 672c 2070 6172 616c  oe_config, paral
+00017bb0: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+00017bc0: 2020 2020 2020 2020 7365 6c66 2e75 7365          self.use
+00017bd0: 5f6d 6f65 203d 2028 6d6f 655f 636f 6e66  _moe = (moe_conf
+00017be0: 6967 2e65 7870 6572 745f 6e75 6d20 3e20  ig.expert_num > 
+00017bf0: 3129 0a20 2020 2020 2020 2020 2020 2073  1).            s
+00017c00: 656c 662e 7573 655f 7061 7374 203d 2075  elf.use_past = u
+00017c10: 7365 5f70 6173 740a 2020 2020 2020 2020  se_past.        
+00017c20: 2020 2020 7365 6c66 2e73 6571 5f6c 656e      self.seq_len
+00017c30: 6774 6820 3d20 7365 715f 6c65 6e67 7468  gth = seq_length
+00017c40: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00017c50: 662e 6869 6464 656e 5f73 697a 6520 3d20  f.hidden_size = 
+00017c60: 6869 6464 656e 5f73 697a 650a 2020 2020  hidden_size.    
+00017c70: 2020 2020 2020 2020 7365 6c66 2e6c 6179          self.lay
+00017c80: 6572 6e6f 726d 3120 3d20 4c61 7965 724e  ernorm1 = LayerN
+00017c90: 6f72 6d28 2868 6964 6465 6e5f 7369 7a65  orm((hidden_size
+00017ca0: 2c29 292e 746f 5f66 6c6f 6174 286c 6179  ,)).to_float(lay
+00017cb0: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
+00017cc0: 7970 6529 0a20 2020 2020 2020 2020 2020  ype).           
+00017cd0: 2073 656c 662e 6c61 7965 726e 6f72 6d32   self.layernorm2
+00017ce0: 203d 204c 6179 6572 4e6f 726d 2828 6869   = LayerNorm((hi
+00017cf0: 6464 656e 5f73 697a 652c 2929 2e74 6f5f  dden_size,)).to_
+00017d00: 666c 6f61 7428 6c61 7965 726e 6f72 6d5f  float(layernorm_
+00017d10: 636f 6d70 7574 655f 7479 7065 290a 0a20  compute_type).. 
+00017d20: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00017d30: 7469 6f6e 5f70 6172 616c 6c65 6c5f 636f  tion_parallel_co
+00017d40: 6e66 6967 203d 2070 6172 616c 6c65 6c5f  nfig = parallel_
+00017d50: 636f 6e66 6967 2e64 706d 7020 6966 2073  config.dpmp if s
+00017d60: 656c 662e 7573 655f 6d6f 6520 656c 7365  elf.use_moe else
+00017d70: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00017d80: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00017d90: 662e 6174 7465 6e74 696f 6e20 3d20 4d75  f.attention = Mu
+00017da0: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+00017db0: 2862 6174 6368 5f73 697a 653d 6261 7463  (batch_size=batc
+00017dc0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
+00017dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017df0: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
+00017e00: 6c65 6e67 7468 3d73 6571 5f6c 656e 6774  length=seq_lengt
+00017e10: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
+00017e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017e40: 2020 2074 6774 5f73 6571 5f6c 656e 6774     tgt_seq_lengt
+00017e50: 683d 7365 715f 6c65 6e67 7468 2c0a 2020  h=seq_length,.  
+00017e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017e80: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+00017e90: 6464 656e 5f73 697a 653d 6869 6464 656e  dden_size=hidden
+00017ea0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+00017eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017ed0: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
+00017ee0: 3d6e 756d 5f68 6561 6473 2c0a 2020 2020  =num_heads,.    
+00017ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017f10: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00017f20: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
+00017f30: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00017f40: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
+00017f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017f70: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
+00017f80: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
+00017f90: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+00017fa0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
+00017fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017fd0: 2020 2020 736f 6674 6d61 785f 636f 6d70      softmax_comp
+00017fe0: 7574 655f 7479 7065 3d73 6f66 746d 6178  ute_type=softmax
+00017ff0: 5f63 6f6d 7075 7465 5f74 7970 652c 0a20  _compute_type,. 
+00018000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018020: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00018030: 6172 616d 5f69 6e69 745f 7479 7065 3d70  aram_init_type=p
+00018040: 6172 616d 5f69 6e69 745f 7479 7065 2c0a  aram_init_type,.
+00018050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018060: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00018070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018090: 2020 2020 2020 2020 2020 2020 7061 7261              para
-000180a0: 6c6c 656c 5f63 6f6e 6669 673d 6174 7465  llel_config=atte
-000180b0: 6e74 696f 6e5f 7061 7261 6c6c 656c 5f63  ntion_parallel_c
-000180c0: 6f6e 6669 672c 0a20 2020 2020 2020 2020  onfig,.         
-000180d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000180e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000180f0: 2020 2020 2020 2075 7365 5f66 6c61 7368         use_flash
-00018100: 5f61 7474 656e 7469 6f6e 3d75 7365 5f66  _attention=use_f
-00018110: 6c61 7368 5f61 7474 656e 7469 6f6e 290a  lash_attention).
-00018120: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-00018130: 656c 662e 7573 655f 6d6f 653a 0a20 2020  elf.use_moe:.   
-00018140: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00018150: 662e 6f75 7470 7574 203d 204d 6f45 2868  f.output = MoE(h
-00018160: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
-00018170: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00018180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018190: 2020 2020 2020 2020 2020 6472 6f70 6f75            dropou
-000181a0: 745f 7261 7465 3d68 6964 6465 6e5f 6472  t_rate=hidden_dr
-000181b0: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
-000181c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000181d0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000181e0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-000181f0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-00018200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018220: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00018230: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00018240: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00018250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018260: 2020 2020 6869 6464 656e 5f61 6374 3d68      hidden_act=h
-00018270: 6964 6465 6e5f 6163 742c 0a20 2020 2020  idden_act,.     
-00018280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018290: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-000182a0: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-000182b0: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-000182c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000182d0: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-000182e0: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
-000182f0: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
-00018300: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00018310: 2020 2020 2020 2020 2023 2046 6565 6420           # Feed 
-00018320: 466f 7277 6172 6420 4e65 7477 6f72 6b2c  Forward Network,
-00018330: 2046 464e 0a20 2020 2020 2020 2020 2020   FFN.           
-00018340: 2020 2020 2073 656c 662e 6f75 7470 7574       self.output
-00018350: 203d 2046 6565 6446 6f72 7761 7264 2868   = FeedForward(h
-00018360: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
-00018370: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00018380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000183a0: 2020 6472 6f70 6f75 745f 7261 7465 3d68    dropout_rate=h
-000183b0: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-000183c0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-000183d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000183e0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000183f0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-00018400: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-00018410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018430: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-00018440: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-00018450: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
-00018460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018480: 2020 2020 6869 6464 656e 5f61 6374 3d68      hidden_act=h
-00018490: 6964 6465 6e5f 6163 742c 0a20 2020 2020  idden_act,.     
-000184a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000184b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000184c0: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-000184d0: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
-000184e0: 6e66 6967 290a 2020 2020 2020 2020 2020  nfig).          
-000184f0: 2020 7365 6c66 2e70 6f73 745f 6c61 7965    self.post_laye
-00018500: 726e 6f72 6d5f 7265 7369 6475 616c 203d  rnorm_residual =
-00018510: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
-00018520: 7265 7369 6475 616c 0a20 2020 2020 2020  residual.       
-00018530: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
-00018540: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
-00018550: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00018560: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
-00018570: 292c 2028 7061 7261 6c6c 656c 5f63 6f6e  ), (parallel_con
-00018580: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00018590: 6c2c 2031 2929 290a 2020 2020 2020 2020  l, 1))).        
-000185a0: 2020 2020 7365 6c66 2e61 6464 5f33 6420      self.add_3d 
-000185b0: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
-000185c0: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-000185d0: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-000185e0: 2031 2c20 3129 2c20 2870 6172 616c 6c65   1, 1), (paralle
+00018080: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
+00018090: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000180a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000180b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000180c0: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+000180d0: 6967 3d61 7474 656e 7469 6f6e 5f70 6172  ig=attention_par
+000180e0: 616c 6c65 6c5f 636f 6e66 6967 2c0a 2020  allel_config,.  
+000180f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018110: 2020 2020 2020 2020 2020 2020 2020 7573                us
+00018120: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+00018130: 6e3d 7573 655f 666c 6173 685f 6174 7465  n=use_flash_atte
+00018140: 6e74 696f 6e29 0a20 2020 2020 2020 2020  ntion).         
+00018150: 2020 2069 6620 7365 6c66 2e75 7365 5f6d     if self.use_m
+00018160: 6f65 3a0a 2020 2020 2020 2020 2020 2020  oe:.            
+00018170: 2020 2020 7365 6c66 2e6f 7574 7075 7420      self.output 
+00018180: 3d20 4d6f 4528 6869 6464 656e 5f73 697a  = MoE(hidden_siz
+00018190: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
+000181a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000181b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000181c0: 2064 726f 706f 7574 5f72 6174 653d 6869   dropout_rate=hi
+000181d0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+000181e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000181f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018200: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+00018210: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+00018220: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00018230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018240: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+00018250: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
+00018260: 6974 5f74 7970 652c 0a20 2020 2020 2020  it_type,.       
+00018270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018280: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+00018290: 6e5f 6163 743d 6869 6464 656e 5f61 6374  n_act=hidden_act
+000182a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000182b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000182c0: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+000182d0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+000182e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000182f0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00018300: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
+00018310: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+00018320: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00018330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018340: 2320 4665 6564 2046 6f72 7761 7264 204e  # Feed Forward N
+00018350: 6574 776f 726b 2c20 4646 4e0a 2020 2020  etwork, FFN.    
+00018360: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00018370: 2e6f 7574 7075 7420 3d20 4665 6564 466f  .output = FeedFo
+00018380: 7277 6172 6428 6869 6464 656e 5f73 697a  rward(hidden_siz
+00018390: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
+000183a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000183b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000183c0: 2020 2020 2020 2020 2064 726f 706f 7574           dropout
+000183d0: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
+000183e0: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+000183f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018410: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+00018420: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+00018430: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00018440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018460: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+00018470: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
+00018480: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00018490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000184a0: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+000184b0: 6e5f 6163 743d 6869 6464 656e 5f61 6374  n_act=hidden_act
+000184c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000184d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000184e0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+000184f0: 6c6c 656c 5f63 6f6e 6669 673d 7061 7261  llel_config=para
+00018500: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+00018510: 2020 2020 2020 2020 2073 656c 662e 706f           self.po
+00018520: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00018530: 6964 7561 6c20 3d20 706f 7374 5f6c 6179  idual = post_lay
+00018540: 6572 6e6f 726d 5f72 6573 6964 7561 6c0a  ernorm_residual.
+00018550: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00018560: 2e61 6464 203d 2050 2e41 6464 2829 2e73  .add = P.Add().s
+00018570: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+00018580: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00018590: 6c6c 656c 2c20 3129 2c20 2870 6172 616c  llel, 1), (paral
+000185a0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+000185b0: 7061 7261 6c6c 656c 2c20 3129 2929 0a20  parallel, 1))). 
+000185c0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000185d0: 6164 645f 3364 203d 2050 2e41 6464 2829  add_3d = P.Add()
+000185e0: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
 000185f0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00018600: 7261 6c6c 656c 2c20 312c 2031 2929 290a  rallel, 1, 1))).
-00018610: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00018620: 2e64 7479 7065 203d 206d 7374 7970 652e  .dtype = mstype.
-00018630: 666c 6f61 7431 360a 2020 2020 2020 2020  float16.        
-00018640: 2020 2020 7365 6c66 2e6b 6579 5f70 6173      self.key_pas
-00018650: 7420 3d20 4e6f 6e65 0a20 2020 2020 2020  t = None.       
-00018660: 2020 2020 2073 656c 662e 7661 6c75 655f       self.value_
-00018670: 7061 7374 203d 204e 6f6e 650a 0a20 2020  past = None..   
-00018680: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
-00018690: 2e75 7365 5f70 6173 743a 0a20 2020 2020  .use_past:.     
-000186a0: 2020 2020 2020 2020 2020 2023 206f 7065             # ope
-000186b0: 7261 746f 7220 7573 6564 2066 6f72 2073  rator used for s
-000186c0: 7461 7465 2072 6575 7365 0a20 2020 2020  tate reuse.     
-000186d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000186e0: 7265 6475 6365 7375 6d20 3d20 502e 5265  reducesum = P.Re
-000186f0: 6475 6365 5375 6d28 292e 7368 6172 6428  duceSum().shard(
-00018700: 2828 312c 2031 2c20 312c 2031 292c 2929  ((1, 1, 1, 1),))
-00018710: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00018720: 2073 656c 662e 6e6f 745f 6571 7561 6c20   self.not_equal 
-00018730: 3d20 502e 4e6f 7445 7175 616c 2829 2e73  = P.NotEqual().s
-00018740: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
-00018750: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
-00018760: 2020 2020 2020 2020 2073 656c 662e 736c           self.sl
-00018770: 6963 6520 3d20 502e 5374 7269 6465 6453  ice = P.StridedS
-00018780: 6c69 6365 2829 2e73 6861 7264 2828 2831  lice().shard(((1
-00018790: 2c20 312c 2031 2c20 3129 2c29 290a 2020  , 1, 1, 1),)).  
-000187a0: 2020 2020 2020 2020 2020 2020 2020 7369                si
-000187b0: 7a65 5f70 6572 5f68 6561 6420 3d20 6869  ze_per_head = hi
-000187c0: 6464 656e 5f73 697a 6520 2f2f 206e 756d  dden_size // num
-000187d0: 5f68 6561 6473 0a20 2020 2020 2020 2020  _heads.         
-000187e0: 2020 2020 2020 2073 656c 662e 6b65 795f         self.key_
-000187f0: 7368 6170 6520 3d20 2862 6174 6368 5f73  shape = (batch_s
-00018800: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
-00018810: 7369 7a65 5f70 6572 5f68 6561 642c 2073  size_per_head, s
-00018820: 6571 5f6c 656e 6774 6829 0a20 2020 2020  eq_length).     
-00018830: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00018840: 7661 6c75 655f 7368 6170 6520 3d20 2862  value_shape = (b
-00018850: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
-00018860: 6561 6473 2c20 7365 715f 6c65 6e67 7468  eads, seq_length
-00018870: 2c20 7369 7a65 5f70 6572 5f68 6561 6429  , size_per_head)
-00018880: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00018890: 2023 2070 6172 616d 6574 6572 7320 7361   # parameters sa
-000188a0: 7669 6e67 206b 6579 2061 6e64 2076 616c  ving key and val
-000188b0: 7565 2073 7461 7465 730a 2020 2020 2020  ue states.      
-000188c0: 2020 2020 2020 2020 2020 7365 6c66 2e6b            self.k
-000188d0: 6579 5f70 6173 7420 3d20 5061 7261 6d65  ey_past = Parame
-000188e0: 7465 7228 5465 6e73 6f72 286e 702e 7a65  ter(Tensor(np.ze
-000188f0: 726f 7328 7368 6170 653d 7365 6c66 2e6b  ros(shape=self.k
-00018900: 6579 5f73 6861 7065 292c 2073 656c 662e  ey_shape), self.
-00018910: 6474 7970 6529 2c20 6e61 6d65 3d22 6b65  dtype), name="ke
-00018920: 795f 7061 7374 2229 0a20 2020 2020 2020  y_past").       
-00018930: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
-00018940: 6c75 655f 7061 7374 203d 2050 6172 616d  lue_past = Param
-00018950: 6574 6572 2854 656e 736f 7228 6e70 2e7a  eter(Tensor(np.z
-00018960: 6572 6f73 2873 6861 7065 3d73 656c 662e  eros(shape=self.
-00018970: 7661 6c75 655f 7368 6170 6529 2c20 7365  value_shape), se
-00018980: 6c66 2e64 7479 7065 292c 206e 616d 653d  lf.dtype), name=
-00018990: 2276 616c 7565 5f70 6173 7422 290a 2020  "value_past").  
-000189a0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-000189b0: 6c66 2e74 696c 6520 3d20 502e 5469 6c65  lf.tile = P.Tile
-000189c0: 2829 2e73 6861 7264 2828 2831 2c20 3129  ().shard(((1, 1)
-000189d0: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
-000189e0: 2020 2020 7365 6c66 2e6d 756c 203d 2050      self.mul = P
-000189f0: 2e4d 756c 2829 2e73 6861 7264 2828 2831  .Mul().shard(((1
-00018a00: 2c20 312c 2031 2c20 3129 2c20 2831 2c29  , 1, 1, 1), (1,)
-00018a10: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-00018a20: 2020 2073 656c 662e 6173 7369 676e 203d     self.assign =
-00018a30: 2050 2e41 7373 6967 6e28 292e 7368 6172   P.Assign().shar
-00018a40: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
-00018a50: 2028 312c 2031 2c20 312c 2031 2929 290a   (1, 1, 1, 1))).
-00018a60: 2020 2020 2020 2020 656c 6966 205f 6765          elif _ge
-00018a70: 745f 7061 7261 6c6c 656c 5f6d 6f64 6528  t_parallel_mode(
-00018a80: 2920 6e6f 7420 696e 2028 5061 7261 6c6c  ) not in (Parall
-00018a90: 656c 4d6f 6465 2e41 5554 4f5f 5041 5241  elMode.AUTO_PARA
-00018aa0: 4c4c 454c 2c29 3a0a 2020 2020 2020 2020  LLEL,):.        
-00018ab0: 2020 2020 5f63 6865 636b 5f63 6f6e 6669      _check_confi
-00018ac0: 6728 7061 7261 6c6c 656c 5f63 6f6e 6669  g(parallel_confi
-00018ad0: 6729 0a20 2020 2020 2020 2020 2020 2069  g).            i
-00018ae0: 6620 6e75 6d5f 6865 6164 7320 2520 7061  f num_heads % pa
-00018af0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-00018b00: 6465 6c5f 7061 7261 6c6c 656c 2021 3d20  del_parallel != 
-00018b10: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-00018b20: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00018b30: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
-00018b40: 2020 2020 2020 2020 2022 466f 7220 2754           "For 'T
-00018b50: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00018b60: 724c 6179 6572 272c 2074 6865 2063 6c61  rLayer', the cla
-00018b70: 7373 2076 6172 6961 626c 6520 276e 756d  ss variable 'num
-00018b80: 5f68 6561 6473 2720 6d75 7374 2062 6520  _heads' must be 
-00018b90: 6469 7669 7369 626c 6564 2062 7920 7468  divisibled by th
-00018ba0: 6520 220a 2020 2020 2020 2020 2020 2020  e ".            
-00018bb0: 2020 2020 2020 2020 2227 7061 7261 6c6c          "'parall
-00018bc0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00018bd0: 7061 7261 6c6c 656c 272c 2062 7574 2067  parallel', but g
-00018be0: 6f74 2074 6865 206e 756d 5f68 6561 6473  ot the num_heads
-00018bf0: 2069 7320 7b7d 2061 6e64 2022 0a20 2020   is {} and ".   
-00018c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018c10: 2022 7061 7261 6c6c 656c 5f63 6f6e 6669   "parallel_confi
-00018c20: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00018c30: 2069 7320 7b7d 2e22 2e66 6f72 6d61 7428   is {}.".format(
-00018c40: 6e75 6d5f 6865 6164 732c 2070 6172 616c  num_heads, paral
-00018c50: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-00018c60: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
-00018c70: 2020 2020 2020 2020 6966 2068 6964 6465          if hidde
-00018c80: 6e5f 7369 7a65 2025 2070 6172 616c 6c65  n_size % paralle
-00018c90: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00018ca0: 6172 616c 6c65 6c20 213d 2030 3a0a 2020  arallel != 0:.  
-00018cb0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-00018cc0: 6973 6520 5661 6c75 6545 7272 6f72 280a  ise ValueError(.
-00018cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018ce0: 2020 2020 2246 6f72 2027 5472 616e 7366      "For 'Transf
-00018cf0: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
-00018d00: 7227 2c20 7468 6520 636c 6173 7320 7661  r', the class va
-00018d10: 7269 6162 6c65 2027 6869 6464 656e 5f73  riable 'hidden_s
-00018d20: 697a 6527 206d 7573 7420 6265 2064 6976  ize' must be div
-00018d30: 6973 6962 6c65 6420 6279 2022 0a20 2020  isibled by ".   
-00018d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018d50: 2022 7468 6520 2770 6172 616c 6c65 6c5f   "the 'parallel_
-00018d60: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-00018d70: 616c 6c65 6c27 2c20 6275 7420 676f 7420  allel', but got 
-00018d80: 7468 6520 6869 6464 656e 5f73 697a 6520  the hidden_size 
-00018d90: 6973 207b 7d20 616e 6420 7061 7261 6c6c  is {} and parall
-00018da0: 656c 5f63 6f6e 6669 672e 220a 2020 2020  el_config.".    
-00018db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018dc0: 2220 6d6f 6465 6c5f 7061 7261 6c6c 656c  " model_parallel
-00018dd0: 2069 7320 7b7d 2e22 2e66 6f72 6d61 7428   is {}.".format(
-00018de0: 6869 6464 656e 5f73 697a 652c 2070 6172  hidden_size, par
-00018df0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00018e00: 656c 5f70 6172 616c 6c65 6c29 290a 2020  el_parallel)).  
-00018e10: 2020 2020 2020 2020 2020 6966 2066 666e            if ffn
-00018e20: 5f68 6964 6465 6e5f 7369 7a65 2025 2070  _hidden_size % p
-00018e30: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00018e40: 6f64 656c 5f70 6172 616c 6c65 6c20 213d  odel_parallel !=
-00018e50: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-00018e60: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00018e70: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
-00018e80: 2020 2020 2020 2020 2020 2246 6f72 2027            "For '
-00018e90: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
-00018ea0: 6572 4c61 7965 7227 2c20 7468 6520 636c  erLayer', the cl
-00018eb0: 6173 7320 7661 7269 6162 6c65 2027 6666  ass variable 'ff
-00018ec0: 6e5f 6869 6464 656e 5f73 697a 6527 206d  n_hidden_size' m
-00018ed0: 7573 7420 6265 2064 6976 6973 6962 6c65  ust be divisible
-00018ee0: 6420 220a 2020 2020 2020 2020 2020 2020  d ".            
-00018ef0: 2020 2020 2020 2020 2262 7920 7468 6520          "by the 
-00018f00: 2770 6172 616c 6c65 6c5f 636f 6e66 6967  'parallel_config
-00018f10: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c27  .model_parallel'
-00018f20: 2c20 6275 7420 676f 7420 7468 6520 6666  , but got the ff
-00018f30: 6e5f 6869 6464 656e 5f73 697a 6520 6973  n_hidden_size is
-00018f40: 207b 7d20 220a 2020 2020 2020 2020 2020   {} ".          
-00018f50: 2020 2020 2020 2020 2020 2261 6e64 2070            "and p
-00018f60: 6172 616c 6c65 6c5f 636f 6e66 6967 2e20  arallel_config. 
-00018f70: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2069  model_parallel i
-00018f80: 7320 7b7d 2e22 0a20 2020 2020 2020 2020  s {}.".         
-00018f90: 2020 2020 2020 2020 2020 202e 666f 726d             .form
-00018fa0: 6174 2866 666e 5f68 6964 6465 6e5f 7369  at(ffn_hidden_si
-00018fb0: 7a65 2c20 7061 7261 6c6c 656c 5f63 6f6e  ze, parallel_con
-00018fc0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-00018fd0: 656c 2929 0a20 2020 2020 2020 2020 2020  el)).           
-00018fe0: 205f 6368 6563 6b5f 6d6f 655f 636f 6e66   _check_moe_conf
-00018ff0: 6967 286d 6f65 5f63 6f6e 6669 672c 2070  ig(moe_config, p
-00019000: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
-00019010: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00019020: 2e75 7365 5f6d 6f65 203d 2028 6d6f 655f  .use_moe = (moe_
-00019030: 636f 6e66 6967 2e65 7870 6572 745f 6e75  config.expert_nu
-00019040: 6d20 3e20 3129 0a20 2020 2020 2020 2020  m > 1).         
-00019050: 2020 2073 656c 662e 7573 655f 7061 7374     self.use_past
-00019060: 203d 2075 7365 5f70 6173 740a 2020 2020   = use_past.    
-00019070: 2020 2020 2020 2020 7365 6c66 2e73 6571          self.seq
-00019080: 5f6c 656e 6774 6820 3d20 7365 715f 6c65  _length = seq_le
-00019090: 6e67 7468 0a20 2020 2020 2020 2020 2020  ngth.           
-000190a0: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-000190b0: 6520 3d20 6869 6464 656e 5f73 697a 650a  e = hidden_size.
-000190c0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000190d0: 2e6c 6179 6572 6e6f 726d 3120 3d20 4c61  .layernorm1 = La
-000190e0: 7965 724e 6f72 6d28 2868 6964 6465 6e5f  yerNorm((hidden_
-000190f0: 7369 7a65 2c29 292e 746f 5f66 6c6f 6174  size,)).to_float
-00019100: 286c 6179 6572 6e6f 726d 5f63 6f6d 7075  (layernorm_compu
-00019110: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
-00019120: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
-00019130: 6f72 6d31 2e73 6861 7264 2828 2870 6172  orm1.shard(((par
-00019140: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00019150: 615f 7061 7261 6c6c 656c 2c20 3129 2c29  a_parallel, 1),)
-00019160: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00019170: 6c66 2e6c 6179 6572 6e6f 726d 3220 3d20  lf.layernorm2 = 
-00019180: 4c61 7965 724e 6f72 6d28 2868 6964 6465  LayerNorm((hidde
-00019190: 6e5f 7369 7a65 2c29 292e 746f 5f66 6c6f  n_size,)).to_flo
-000191a0: 6174 286c 6179 6572 6e6f 726d 5f63 6f6d  at(layernorm_com
-000191b0: 7075 7465 5f74 7970 6529 0a20 2020 2020  pute_type).     
-000191c0: 2020 2020 2020 2073 656c 662e 6c61 7965         self.laye
-000191d0: 726e 6f72 6d32 2e73 6861 7264 2828 2870  rnorm2.shard(((p
-000191e0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-000191f0: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
-00019200: 2c29 290a 0a20 2020 2020 2020 2020 2020  ,))..           
-00019210: 2061 7474 656e 7469 6f6e 5f70 6172 616c   attention_paral
-00019220: 6c65 6c5f 636f 6e66 6967 203d 2070 6172  lel_config = par
-00019230: 616c 6c65 6c5f 636f 6e66 6967 2e64 706d  allel_config.dpm
-00019240: 7020 6966 2073 656c 662e 7573 655f 6d6f  p if self.use_mo
-00019250: 6520 656c 7365 2070 6172 616c 6c65 6c5f  e else parallel_
-00019260: 636f 6e66 6967 0a20 2020 2020 2020 2020  config.         
-00019270: 2020 2073 656c 662e 6174 7465 6e74 696f     self.attentio
-00019280: 6e20 3d20 4d75 6c74 6948 6561 6441 7474  n = MultiHeadAtt
-00019290: 656e 7469 6f6e 2862 6174 6368 5f73 697a  ention(batch_siz
-000192a0: 653d 6261 7463 685f 7369 7a65 2c0a 2020  e=batch_size,.  
-000192b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192d0: 2020 2020 2020 2020 2020 2020 2020 7372                sr
-000192e0: 635f 7365 715f 6c65 6e67 7468 3d73 6571  c_seq_length=seq
-000192f0: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-00019300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019320: 2020 2020 2020 2020 2074 6774 5f73 6571           tgt_seq
-00019330: 5f6c 656e 6774 683d 7365 715f 6c65 6e67  _length=seq_leng
-00019340: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
-00019350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019370: 2020 2020 6869 6464 656e 5f73 697a 653d      hidden_size=
-00019380: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-00019390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000193a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000193b0: 2020 2020 2020 2020 2020 2020 206e 756d               num
-000193c0: 5f68 6561 6473 3d6e 756d 5f68 6561 6473  _heads=num_heads
-000193d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000193e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000193f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019400: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
-00019410: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
-00019420: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
-00019430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019450: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-00019460: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00019470: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
-00019480: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-00019490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000194a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000194b0: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
-000194c0: 785f 636f 6d70 7574 655f 7479 7065 3d73  x_compute_type=s
-000194d0: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-000194e0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-000194f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019510: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
-00019520: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
-00019530: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-00019540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019560: 2020 2020 2020 7573 655f 7061 7374 3d75        use_past=u
-00019570: 7365 5f70 6173 742c 0a20 2020 2020 2020  se_past,.       
-00019580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000195a0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-000195b0: 6c5f 636f 6e66 6967 3d61 7474 656e 7469  l_config=attenti
-000195c0: 6f6e 5f70 6172 616c 6c65 6c5f 636f 6e66  on_parallel_conf
-000195d0: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-000195e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000195f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019600: 2020 2020 7573 655f 666c 6173 685f 6174      use_flash_at
-00019610: 7465 6e74 696f 6e3d 7573 655f 666c 6173  tention=use_flas
-00019620: 685f 6174 7465 6e74 696f 6e29 0a20 2020  h_attention).   
-00019630: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
-00019640: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-00019650: 2020 2020 2020 2020 2020 7365 6c66 2e6f            self.o
-00019660: 7574 7075 7420 3d20 4d6f 4528 6869 6464  utput = MoE(hidd
-00019670: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
-00019680: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00019690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000196a0: 2020 2020 2020 2064 726f 706f 7574 5f72         dropout_r
-000196b0: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
-000196c0: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
-000196d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000196e0: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-000196f0: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
-00019700: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00019710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019720: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00019730: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-00019740: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
-00019750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019770: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-00019780: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-00019790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000197a0: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
-000197b0: 6e66 6967 3d6d 6f65 5f63 6f6e 6669 672c  nfig=moe_config,
-000197c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000197d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000197e0: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
-000197f0: 6967 3d70 6172 616c 6c65 6c5f 636f 6e66  ig=parallel_conf
-00019800: 6967 290a 2020 2020 2020 2020 2020 2020  ig).            
-00019810: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00019820: 2020 2020 2020 2320 4665 6564 2046 6f72        # Feed For
-00019830: 7761 7264 204e 6574 776f 726b 2c20 4646  ward Network, FF
-00019840: 4e0a 2020 2020 2020 2020 2020 2020 2020  N.              
-00019850: 2020 7365 6c66 2e6f 7574 7075 7420 3d20    self.output = 
-00019860: 4665 6564 466f 7277 6172 6428 6869 6464  FeedForward(hidd
-00019870: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
-00019880: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00019890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000198a0: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-000198b0: 726f 706f 7574 5f72 6174 653d 6869 6464  ropout_rate=hidd
-000198c0: 656e 5f64 726f 706f 7574 5f72 6174 652c  en_dropout_rate,
-000198d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000198e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000198f0: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-00019900: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
-00019910: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00019920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019940: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00019950: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
-00019960: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
-00019970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019990: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-000199a0: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-000199b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000199c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000199d0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
-000199e0: 673d 7061 7261 6c6c 656c 5f63 6f6e 6669  g=parallel_confi
-000199f0: 6729 0a20 2020 2020 2020 2020 2020 2073  g).            s
-00019a00: 656c 662e 706f 7374 5f6c 6179 6572 6e6f  elf.post_layerno
-00019a10: 726d 5f72 6573 6964 7561 6c20 3d20 706f  rm_residual = po
-00019a20: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
-00019a30: 6964 7561 6c0a 2020 2020 2020 2020 2020  idual.          
-00019a40: 2020 7365 6c66 2e61 6464 203d 2050 2e41    self.add = P.A
-00019a50: 6464 2829 2e73 6861 7264 2828 2870 6172  dd().shard(((par
-00019a60: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00019a70: 615f 7061 7261 6c6c 656c 2c20 3129 2c20  a_parallel, 1), 
-00019a80: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00019a90: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00019aa0: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
-00019ab0: 2073 656c 662e 6164 645f 3364 203d 2050   self.add_3d = P
-00019ac0: 2e41 6464 2829 2e73 6861 7264 2828 2870  .Add().shard(((p
-00019ad0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00019ae0: 6174 615f 7061 7261 6c6c 656c 2c20 312c  ata_parallel, 1,
-00019af0: 2031 292c 2028 7061 7261 6c6c 656c 5f63   1), (parallel_c
+00018600: 7261 6c6c 656c 2c20 312c 2031 292c 2028  rallel, 1, 1), (
+00018610: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00018620: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
+00018630: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
+00018640: 2020 2073 656c 662e 6474 7970 6520 3d20     self.dtype = 
+00018650: 6d73 7479 7065 2e66 6c6f 6174 3136 0a20  mstype.float16. 
+00018660: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00018670: 6b65 795f 7061 7374 203d 204e 6f6e 650a  key_past = None.
+00018680: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00018690: 2e76 616c 7565 5f70 6173 7420 3d20 4e6f  .value_past = No
+000186a0: 6e65 0a0a 2020 2020 2020 2020 2020 2020  ne..            
+000186b0: 6966 2073 656c 662e 7573 655f 7061 7374  if self.use_past
+000186c0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000186d0: 2020 2320 6f70 6572 6174 6f72 2075 7365    # operator use
+000186e0: 6420 666f 7220 7374 6174 6520 7265 7573  d for state reus
+000186f0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00018700: 2020 7365 6c66 2e72 6564 7563 6573 756d    self.reducesum
+00018710: 203d 2050 2e52 6564 7563 6553 756d 2829   = P.ReduceSum()
+00018720: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
+00018730: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+00018740: 2020 2020 2020 2020 7365 6c66 2e6e 6f74          self.not
+00018750: 5f65 7175 616c 203d 2050 2e4e 6f74 4571  _equal = P.NotEq
+00018760: 7561 6c28 292e 7368 6172 6428 2828 312c  ual().shard(((1,
+00018770: 2031 2c20 312c 2031 292c 2028 2929 290a   1, 1, 1), ())).
+00018780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018790: 7365 6c66 2e73 6c69 6365 203d 2050 2e53  self.slice = P.S
+000187a0: 7472 6964 6564 536c 6963 6528 292e 7368  tridedSlice().sh
+000187b0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+000187c0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+000187d0: 2020 2020 2073 697a 655f 7065 725f 6865       size_per_he
+000187e0: 6164 203d 2068 6964 6465 6e5f 7369 7a65  ad = hidden_size
+000187f0: 202f 2f20 6e75 6d5f 6865 6164 730a 2020   // num_heads.  
+00018800: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00018810: 6c66 2e6b 6579 5f73 6861 7065 203d 2028  lf.key_shape = (
+00018820: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
+00018830: 6865 6164 732c 2073 697a 655f 7065 725f  heads, size_per_
+00018840: 6865 6164 2c20 7365 715f 6c65 6e67 7468  head, seq_length
+00018850: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00018860: 2020 7365 6c66 2e76 616c 7565 5f73 6861    self.value_sha
+00018870: 7065 203d 2028 6261 7463 685f 7369 7a65  pe = (batch_size
+00018880: 2c20 6e75 6d5f 6865 6164 732c 2073 6571  , num_heads, seq
+00018890: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
+000188a0: 725f 6865 6164 290a 2020 2020 2020 2020  r_head).        
+000188b0: 2020 2020 2020 2020 2320 7061 7261 6d65          # parame
+000188c0: 7465 7273 2073 6176 696e 6720 6b65 7920  ters saving key 
+000188d0: 616e 6420 7661 6c75 6520 7374 6174 6573  and value states
+000188e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000188f0: 2073 656c 662e 6b65 795f 7061 7374 203d   self.key_past =
+00018900: 2050 6172 616d 6574 6572 2854 656e 736f   Parameter(Tenso
+00018910: 7228 6e70 2e7a 6572 6f73 2873 6861 7065  r(np.zeros(shape
+00018920: 3d73 656c 662e 6b65 795f 7368 6170 6529  =self.key_shape)
+00018930: 2c20 7365 6c66 2e64 7479 7065 292c 206e  , self.dtype), n
+00018940: 616d 653d 226b 6579 5f70 6173 7422 290a  ame="key_past").
+00018950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018960: 7365 6c66 2e76 616c 7565 5f70 6173 7420  self.value_past 
+00018970: 3d20 5061 7261 6d65 7465 7228 5465 6e73  = Parameter(Tens
+00018980: 6f72 286e 702e 7a65 726f 7328 7368 6170  or(np.zeros(shap
+00018990: 653d 7365 6c66 2e76 616c 7565 5f73 6861  e=self.value_sha
+000189a0: 7065 292c 2073 656c 662e 6474 7970 6529  pe), self.dtype)
+000189b0: 2c20 6e61 6d65 3d22 7661 6c75 655f 7061  , name="value_pa
+000189c0: 7374 2229 0a20 2020 2020 2020 2020 2020  st").           
+000189d0: 2020 2020 2073 656c 662e 7469 6c65 203d       self.tile =
+000189e0: 2050 2e54 696c 6528 292e 7368 6172 6428   P.Tile().shard(
+000189f0: 2828 312c 2031 292c 2929 0a20 2020 2020  ((1, 1),)).     
+00018a00: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00018a10: 6d75 6c20 3d20 502e 4d75 6c28 292e 7368  mul = P.Mul().sh
+00018a20: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+00018a30: 292c 2028 312c 2929 290a 2020 2020 2020  ), (1,))).      
+00018a40: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+00018a50: 7373 6967 6e20 3d20 502e 4173 7369 676e  ssign = P.Assign
+00018a60: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+00018a70: 2031 2c20 3129 2c20 2831 2c20 312c 2031   1, 1), (1, 1, 1
+00018a80: 2c20 3129 2929 0a20 2020 2020 2020 2065  , 1))).        e
+00018a90: 6c69 6620 5f67 6574 5f70 6172 616c 6c65  lif _get_paralle
+00018aa0: 6c5f 6d6f 6465 2829 206e 6f74 2069 6e20  l_mode() not in 
+00018ab0: 2850 6172 616c 6c65 6c4d 6f64 652e 4155  (ParallelMode.AU
+00018ac0: 544f 5f50 4152 414c 4c45 4c2c 293a 0a20  TO_PARALLEL,):. 
+00018ad0: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
+00018ae0: 6b5f 636f 6e66 6967 2870 6172 616c 6c65  k_config(paralle
+00018af0: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
+00018b00: 2020 2020 2020 6966 206e 756d 5f68 6561        if num_hea
+00018b10: 6473 2025 2070 6172 616c 6c65 6c5f 636f  ds % parallel_co
+00018b20: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00018b30: 6c65 6c20 213d 2030 3a0a 2020 2020 2020  lel != 0:.      
+00018b40: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+00018b50: 5661 6c75 6545 7272 6f72 280a 2020 2020  ValueError(.    
+00018b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018b70: 2246 6f72 2027 5472 616e 7366 6f72 6d65  "For 'Transforme
+00018b80: 7245 6e63 6f64 6572 4c61 7965 7227 2c20  rEncoderLayer', 
+00018b90: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
+00018ba0: 6c65 2027 6e75 6d5f 6865 6164 7327 206d  le 'num_heads' m
+00018bb0: 7573 7420 6265 2064 6976 6973 6962 6c65  ust be divisible
+00018bc0: 6420 6279 2074 6865 2022 0a20 2020 2020  d by the ".     
+00018bd0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00018be0: 2770 6172 616c 6c65 6c5f 636f 6e66 6967  'parallel_config
+00018bf0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c27  .model_parallel'
+00018c00: 2c20 6275 7420 676f 7420 7468 6520 6e75  , but got the nu
+00018c10: 6d5f 6865 6164 7320 6973 207b 7d20 616e  m_heads is {} an
+00018c20: 6420 220a 2020 2020 2020 2020 2020 2020  d ".            
+00018c30: 2020 2020 2020 2020 2270 6172 616c 6c65          "paralle
+00018c40: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+00018c50: 6172 616c 6c65 6c20 6973 207b 7d2e 222e  arallel is {}.".
+00018c60: 666f 726d 6174 286e 756d 5f68 6561 6473  format(num_heads
+00018c70: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+00018c80: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00018c90: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i
+00018ca0: 6620 6869 6464 656e 5f73 697a 6520 2520  f hidden_size % 
+00018cb0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00018cc0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2021  model_parallel !
+00018cd0: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+00018ce0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+00018cf0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+00018d00: 2020 2020 2020 2020 2020 2022 466f 7220             "For 
+00018d10: 2754 7261 6e73 666f 726d 6572 456e 636f  'TransformerEnco
+00018d20: 6465 724c 6179 6572 272c 2074 6865 2063  derLayer', the c
+00018d30: 6c61 7373 2076 6172 6961 626c 6520 2768  lass variable 'h
+00018d40: 6964 6465 6e5f 7369 7a65 2720 6d75 7374  idden_size' must
+00018d50: 2062 6520 6469 7669 7369 626c 6564 2062   be divisibled b
+00018d60: 7920 220a 2020 2020 2020 2020 2020 2020  y ".            
+00018d70: 2020 2020 2020 2020 2274 6865 2027 7061          "the 'pa
+00018d80: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00018d90: 6465 6c5f 7061 7261 6c6c 656c 272c 2062  del_parallel', b
+00018da0: 7574 2067 6f74 2074 6865 2068 6964 6465  ut got the hidde
+00018db0: 6e5f 7369 7a65 2069 7320 7b7d 2061 6e64  n_size is {} and
+00018dc0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00018dd0: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
+00018de0: 2020 2020 2020 2022 206d 6f64 656c 5f70         " model_p
+00018df0: 6172 616c 6c65 6c20 6973 207b 7d2e 222e  arallel is {}.".
+00018e00: 666f 726d 6174 2868 6964 6465 6e5f 7369  format(hidden_si
+00018e10: 7a65 2c20 7061 7261 6c6c 656c 5f63 6f6e  ze, parallel_con
+00018e20: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+00018e30: 656c 2929 0a20 2020 2020 2020 2020 2020  el)).           
+00018e40: 2069 6620 6666 6e5f 6869 6464 656e 5f73   if ffn_hidden_s
+00018e50: 697a 6520 2520 7061 7261 6c6c 656c 5f63  ize % parallel_c
+00018e60: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00018e70: 6c6c 656c 2021 3d20 303a 0a20 2020 2020  llel != 0:.     
+00018e80: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+00018e90: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+00018ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018eb0: 2022 466f 7220 2754 7261 6e73 666f 726d   "For 'Transform
+00018ec0: 6572 456e 636f 6465 724c 6179 6572 272c  erEncoderLayer',
+00018ed0: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
+00018ee0: 626c 6520 2766 666e 5f68 6964 6465 6e5f  ble 'ffn_hidden_
+00018ef0: 7369 7a65 2720 6d75 7374 2062 6520 6469  size' must be di
+00018f00: 7669 7369 626c 6564 2022 0a20 2020 2020  visibled ".     
+00018f10: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00018f20: 6279 2074 6865 2027 7061 7261 6c6c 656c  by the 'parallel
+00018f30: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+00018f40: 7261 6c6c 656c 272c 2062 7574 2067 6f74  rallel', but got
+00018f50: 2074 6865 2066 666e 5f68 6964 6465 6e5f   the ffn_hidden_
+00018f60: 7369 7a65 2069 7320 7b7d 2022 0a20 2020  size is {} ".   
+00018f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018f80: 2022 616e 6420 7061 7261 6c6c 656c 5f63   "and parallel_c
+00018f90: 6f6e 6669 672e 206d 6f64 656c 5f70 6172  onfig. model_par
+00018fa0: 616c 6c65 6c20 6973 207b 7d2e 220a 2020  allel is {}.".  
+00018fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018fc0: 2020 2e66 6f72 6d61 7428 6666 6e5f 6869    .format(ffn_hi
+00018fd0: 6464 656e 5f73 697a 652c 2070 6172 616c  dden_size, paral
+00018fe0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+00018ff0: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
+00019000: 2020 2020 2020 2020 5f63 6865 636b 5f6d          _check_m
+00019010: 6f65 5f63 6f6e 6669 6728 6d6f 655f 636f  oe_config(moe_co
+00019020: 6e66 6967 2c20 7061 7261 6c6c 656c 5f63  nfig, parallel_c
+00019030: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+00019040: 2020 2073 656c 662e 7573 655f 6d6f 6520     self.use_moe 
+00019050: 3d20 286d 6f65 5f63 6f6e 6669 672e 6578  = (moe_config.ex
+00019060: 7065 7274 5f6e 756d 203e 2031 290a 2020  pert_num > 1).  
+00019070: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
+00019080: 7365 5f70 6173 7420 3d20 7573 655f 7061  se_past = use_pa
+00019090: 7374 0a20 2020 2020 2020 2020 2020 2073  st.            s
+000190a0: 656c 662e 7365 715f 6c65 6e67 7468 203d  elf.seq_length =
+000190b0: 2073 6571 5f6c 656e 6774 680a 2020 2020   seq_length.    
+000190c0: 2020 2020 2020 2020 7365 6c66 2e68 6964          self.hid
+000190d0: 6465 6e5f 7369 7a65 203d 2068 6964 6465  den_size = hidde
+000190e0: 6e5f 7369 7a65 0a20 2020 2020 2020 2020  n_size.         
+000190f0: 2020 2073 656c 662e 6c61 7965 726e 6f72     self.layernor
+00019100: 6d31 203d 204c 6179 6572 4e6f 726d 2828  m1 = LayerNorm((
+00019110: 6869 6464 656e 5f73 697a 652c 2929 2e74  hidden_size,)).t
+00019120: 6f5f 666c 6f61 7428 6c61 7965 726e 6f72  o_float(layernor
+00019130: 6d5f 636f 6d70 7574 655f 7479 7065 290a  m_compute_type).
+00019140: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00019150: 2e6c 6179 6572 6e6f 726d 312e 7368 6172  .layernorm1.shar
+00019160: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
+00019170: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00019180: 6c2c 2031 292c 2929 0a20 2020 2020 2020  l, 1),)).       
+00019190: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
+000191a0: 6f72 6d32 203d 204c 6179 6572 4e6f 726d  orm2 = LayerNorm
+000191b0: 2828 6869 6464 656e 5f73 697a 652c 2929  ((hidden_size,))
+000191c0: 2e74 6f5f 666c 6f61 7428 6c61 7965 726e  .to_float(layern
+000191d0: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
+000191e0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+000191f0: 6c66 2e6c 6179 6572 6e6f 726d 322e 7368  lf.layernorm2.sh
+00019200: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
+00019210: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00019220: 6c65 6c2c 2031 292c 2929 0a0a 2020 2020  lel, 1),))..    
+00019230: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+00019240: 6e5f 7061 7261 6c6c 656c 5f63 6f6e 6669  n_parallel_confi
+00019250: 6720 3d20 7061 7261 6c6c 656c 5f63 6f6e  g = parallel_con
+00019260: 6669 672e 6470 6d70 2069 6620 7365 6c66  fig.dpmp if self
+00019270: 2e75 7365 5f6d 6f65 2065 6c73 6520 7061  .use_moe else pa
+00019280: 7261 6c6c 656c 5f63 6f6e 6669 670a 2020  rallel_config.  
+00019290: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+000192a0: 7474 656e 7469 6f6e 203d 204d 756c 7469  ttention = Multi
+000192b0: 4865 6164 4174 7465 6e74 696f 6e28 6261  HeadAttention(ba
+000192c0: 7463 685f 7369 7a65 3d62 6174 6368 5f73  tch_size=batch_s
+000192d0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+000192e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000192f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019300: 2020 2020 2073 7263 5f73 6571 5f6c 656e       src_seq_len
+00019310: 6774 683d 7365 715f 6c65 6e67 7468 2c0a  gth=seq_length,.
+00019320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019350: 7467 745f 7365 715f 6c65 6e67 7468 3d73  tgt_seq_length=s
+00019360: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
+00019370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019390: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+000193a0: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+000193b0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+000193c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000193d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000193e0: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
+000193f0: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
+00019400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019410: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019420: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00019430: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
+00019440: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+00019450: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00019460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019480: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+00019490: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
+000194a0: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
+000194b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000194c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000194d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000194e0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+000194f0: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
+00019500: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
+00019510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019530: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00019540: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
+00019550: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
+00019560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019580: 2020 2020 2020 2020 2020 2020 2075 7365               use
+00019590: 5f70 6173 743d 7573 655f 7061 7374 2c0a  _past=use_past,.
+000195a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000195b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000195c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000195d0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+000195e0: 6174 7465 6e74 696f 6e5f 7061 7261 6c6c  attention_parall
+000195f0: 656c 5f63 6f6e 6669 672c 0a20 2020 2020  el_config,.     
+00019600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019620: 2020 2020 2020 2020 2020 2075 7365 5f66             use_f
+00019630: 6c61 7368 5f61 7474 656e 7469 6f6e 3d75  lash_attention=u
+00019640: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
+00019650: 6f6e 290a 2020 2020 2020 2020 2020 2020  on).            
+00019660: 6966 2073 656c 662e 7573 655f 6d6f 653a  if self.use_moe:
+00019670: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00019680: 2073 656c 662e 6f75 7470 7574 203d 204d   self.output = M
+00019690: 6f45 2868 6964 6465 6e5f 7369 7a65 3d68  oE(hidden_size=h
+000196a0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+000196b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000196c0: 2020 2020 2020 2020 2020 2020 2020 6472                dr
+000196d0: 6f70 6f75 745f 7261 7465 3d68 6964 6465  opout_rate=hidde
+000196e0: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+000196f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019710: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+00019720: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
+00019730: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00019740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019750: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+00019760: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
+00019770: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+00019780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019790: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+000197a0: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+000197b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000197c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000197d0: 206d 6f65 5f63 6f6e 6669 673d 6d6f 655f   moe_config=moe_
+000197e0: 636f 6e66 6967 2c0a 2020 2020 2020 2020  config,.        
+000197f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019800: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+00019810: 656c 5f63 6f6e 6669 673d 7061 7261 6c6c  el_config=parall
+00019820: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
+00019830: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00019840: 2020 2020 2020 2020 2020 2020 2023 2046               # F
+00019850: 6565 6420 466f 7277 6172 6420 4e65 7477  eed Forward Netw
+00019860: 6f72 6b2c 2046 464e 0a20 2020 2020 2020  ork, FFN.       
+00019870: 2020 2020 2020 2020 2073 656c 662e 6f75           self.ou
+00019880: 7470 7574 203d 2046 6565 6446 6f72 7761  tput = FeedForwa
+00019890: 7264 2868 6964 6465 6e5f 7369 7a65 3d68  rd(hidden_size=h
+000198a0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+000198b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000198c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000198d0: 2020 2020 2020 6472 6f70 6f75 745f 7261        dropout_ra
+000198e0: 7465 3d68 6964 6465 6e5f 6472 6f70 6f75  te=hidden_dropou
+000198f0: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
+00019900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019920: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+00019930: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
+00019940: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00019950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019960: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00019970: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00019980: 616d 5f69 6e69 745f 7479 7065 2c0a 2020  am_init_type,.  
+00019990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000199a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000199b0: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+000199c0: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+000199d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000199e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000199f0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
+00019a00: 6c5f 636f 6e66 6967 3d70 6172 616c 6c65  l_config=paralle
+00019a10: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
+00019a20: 2020 2020 2020 7365 6c66 2e70 6f73 745f        self.post_
+00019a30: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+00019a40: 616c 203d 2070 6f73 745f 6c61 7965 726e  al = post_layern
+00019a50: 6f72 6d5f 7265 7369 6475 616c 0a20 2020  orm_residual.   
+00019a60: 2020 2020 2020 2020 2073 656c 662e 6164           self.ad
+00019a70: 6420 3d20 502e 4164 6428 292e 7368 6172  d = P.Add().shar
+00019a80: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
+00019a90: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00019aa0: 6c2c 2031 292c 2028 7061 7261 6c6c 656c  l, 1), (parallel
+00019ab0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00019ac0: 616c 6c65 6c2c 2031 2929 290a 2020 2020  allel, 1))).    
+00019ad0: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+00019ae0: 5f33 6420 3d20 502e 4164 6428 292e 7368  _3d = P.Add().sh
+00019af0: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
 00019b00: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-00019b10: 6c65 6c2c 2031 2c20 3129 2929 0a20 2020  lel, 1, 1))).   
-00019b20: 2020 2020 2020 2020 2073 656c 662e 6474           self.dt
-00019b30: 7970 6520 3d20 6d73 7479 7065 2e66 6c6f  ype = mstype.flo
-00019b40: 6174 3136 0a20 2020 2020 2020 2020 2020  at16.           
-00019b50: 2073 656c 662e 6b65 795f 7061 7374 203d   self.key_past =
-00019b60: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
-00019b70: 2020 7365 6c66 2e76 616c 7565 5f70 6173    self.value_pas
-00019b80: 7420 3d20 4e6f 6e65 0a0a 2020 2020 2020  t = None..      
-00019b90: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
-00019ba0: 655f 7061 7374 3a0a 2020 2020 2020 2020  e_past:.        
-00019bb0: 2020 2020 2020 2020 2320 6f70 6572 6174          # operat
-00019bc0: 6f72 2075 7365 6420 666f 7220 7374 6174  or used for stat
-00019bd0: 6520 7265 7573 650a 2020 2020 2020 2020  e reuse.        
-00019be0: 2020 2020 2020 2020 7365 6c66 2e72 6564          self.red
-00019bf0: 7563 6573 756d 203d 2050 2e52 6564 7563  ucesum = P.Reduc
-00019c00: 6553 756d 2829 2e73 6861 7264 2828 2831  eSum().shard(((1
-00019c10: 2c20 312c 2031 2c20 3129 2c29 290a 2020  , 1, 1, 1),)).  
-00019c20: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00019c30: 6c66 2e6e 6f74 5f65 7175 616c 203d 2050  lf.not_equal = P
-00019c40: 2e4e 6f74 4571 7561 6c28 292e 7368 6172  .NotEqual().shar
-00019c50: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
-00019c60: 2028 2929 290a 2020 2020 2020 2020 2020   ())).          
-00019c70: 2020 2020 2020 7365 6c66 2e73 6c69 6365        self.slice
-00019c80: 203d 2050 2e53 7472 6964 6564 536c 6963   = P.StridedSlic
-00019c90: 6528 292e 7368 6172 6428 2828 312c 2031  e().shard(((1, 1
-00019ca0: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-00019cb0: 2020 2020 2020 2020 2020 2073 697a 655f             size_
-00019cc0: 7065 725f 6865 6164 203d 2068 6964 6465  per_head = hidde
-00019cd0: 6e5f 7369 7a65 202f 2f20 6e75 6d5f 6865  n_size // num_he
-00019ce0: 6164 730a 2020 2020 2020 2020 2020 2020  ads.            
-00019cf0: 2020 2020 7365 6c66 2e6b 6579 5f73 6861      self.key_sha
-00019d00: 7065 203d 2028 6261 7463 685f 7369 7a65  pe = (batch_size
-00019d10: 2c20 6e75 6d5f 6865 6164 732c 2073 697a  , num_heads, siz
-00019d20: 655f 7065 725f 6865 6164 2c20 7365 715f  e_per_head, seq_
-00019d30: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
-00019d40: 2020 2020 2020 2020 7365 6c66 2e76 616c          self.val
-00019d50: 7565 5f73 6861 7065 203d 2028 6261 7463  ue_shape = (batc
-00019d60: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-00019d70: 732c 2073 6571 5f6c 656e 6774 682c 2073  s, seq_length, s
-00019d80: 697a 655f 7065 725f 6865 6164 290a 2020  ize_per_head).  
-00019d90: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-00019da0: 7061 7261 6d65 7465 7273 2073 6176 696e  parameters savin
-00019db0: 6720 6b65 7920 616e 6420 7661 6c75 6520  g key and value 
-00019dc0: 7374 6174 6573 0a20 2020 2020 2020 2020  states.         
-00019dd0: 2020 2020 2020 2073 656c 662e 6b65 795f         self.key_
-00019de0: 7061 7374 203d 2050 6172 616d 6574 6572  past = Parameter
-00019df0: 2854 656e 736f 7228 6e70 2e7a 6572 6f73  (Tensor(np.zeros
-00019e00: 2873 6861 7065 3d73 656c 662e 6b65 795f  (shape=self.key_
-00019e10: 7368 6170 6529 2c20 7365 6c66 2e64 7479  shape), self.dty
-00019e20: 7065 292c 206e 616d 653d 226b 6579 5f70  pe), name="key_p
-00019e30: 6173 7422 290a 2020 2020 2020 2020 2020  ast").          
-00019e40: 2020 2020 2020 7365 6c66 2e76 616c 7565        self.value
-00019e50: 5f70 6173 7420 3d20 5061 7261 6d65 7465  _past = Paramete
-00019e60: 7228 5465 6e73 6f72 286e 702e 7a65 726f  r(Tensor(np.zero
-00019e70: 7328 7368 6170 653d 7365 6c66 2e76 616c  s(shape=self.val
-00019e80: 7565 5f73 6861 7065 292c 2073 656c 662e  ue_shape), self.
-00019e90: 6474 7970 6529 2c20 6e61 6d65 3d22 7661  dtype), name="va
-00019ea0: 6c75 655f 7061 7374 2229 0a20 2020 2020  lue_past").     
-00019eb0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00019ec0: 7469 6c65 203d 2050 2e54 696c 6528 292e  tile = P.Tile().
-00019ed0: 7368 6172 6428 2828 312c 2031 292c 2929  shard(((1, 1),))
-00019ee0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00019ef0: 2073 656c 662e 6d75 6c20 3d20 502e 4d75   self.mul = P.Mu
-00019f00: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
-00019f10: 2c20 312c 2031 292c 2028 312c 2929 290a  , 1, 1), (1,))).
-00019f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019f30: 7365 6c66 2e61 7373 6967 6e20 3d20 502e  self.assign = P.
-00019f40: 4173 7369 676e 2829 2e73 6861 7264 2828  Assign().shard((
-00019f50: 2831 2c20 312c 2031 2c20 3129 2c20 2831  (1, 1, 1, 1), (1
-00019f60: 2c20 312c 2031 2c20 3129 2929 0a0a 2020  , 1, 1, 1)))..  
-00019f70: 2020 2020 2020 2020 2020 6966 2070 6172            if par
-00019f80: 616c 6c65 6c5f 636f 6e66 6967 2e75 7365  allel_config.use
-00019f90: 5f73 6571 5f70 6172 616c 6c65 6c3a 0a20  _seq_parallel:. 
-00019fa0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00019fb0: 656c 662e 6164 642e 7368 6172 6428 2828  elf.add.shard(((
-00019fc0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00019fd0: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
-00019fe0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00019ff0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
-0001a000: 3129 2c0a 2020 2020 2020 2020 2020 2020  1),.            
-0001a010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a020: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
-0001a030: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-0001a040: 656c 202a 2070 6172 616c 6c65 6c5f 636f  el * parallel_co
-0001a050: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-0001a060: 6c65 6c2c 2031 2929 290a 2020 2020 2020  lel, 1))).      
-0001a070: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
-0001a080: 6179 6572 6e6f 726d 312e 7368 6172 6428  ayernorm1.shard(
-0001a090: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-0001a0a0: 672e 6461 7461 5f70 6172 616c 6c65 6c20  g.data_parallel 
-0001a0b0: 2a20 7061 7261 6c6c 656c 5f63 6f6e 6669  * parallel_confi
-0001a0c0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-0001a0d0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-0001a0e0: 2020 2020 2020 2020 7365 6c66 2e6c 6179          self.lay
-0001a0f0: 6572 6e6f 726d 322e 7368 6172 6428 2828  ernorm2.shard(((
-0001a100: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0001a110: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
-0001a120: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0001a130: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
-0001a140: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
-0001a150: 2020 2020 2020 6966 2070 6172 616c 6c65        if paralle
-0001a160: 6c5f 636f 6e66 6967 2e72 6563 6f6d 7075  l_config.recompu
-0001a170: 7465 2e73 656c 6563 745f 7265 636f 6d70  te.select_recomp
-0001a180: 7574 653a 0a20 2020 2020 2020 2020 2020  ute:.           
-0001a190: 2020 2020 2020 2020 2023 20e6 ada4 e5a4           # .....
-0001a1a0: 84e4 bc9a e6b6 88e8 8097 e8be 83e5 a4a7  ................
-0001a1b0: e586 85e5 ad98 efbc 8ce5 bc80 e590 afe5  ................
-0001a1c0: 908e e4bc 9ae6 8d9f e5a4 b1e4 b880 e983  ................
-0001a1d0: a8e5 8886 e8ae a1e7 ae97 e680 a7e8 83bd  ................
-0001a1e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001a1f0: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
-0001a200: 6f72 6d32 2e6c 6179 6572 5f6e 6f72 6d2e  orm2.layer_norm.
-0001a210: 7265 636f 6d70 7574 6528 290a 2020 2020  recompute().    
-0001a220: 2020 2020 2020 2020 2020 2020 6966 206e              if n
-0001a230: 6f74 2073 656c 662e 7573 655f 6d6f 653a  ot self.use_moe:
-0001a240: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001a250: 2020 2020 2073 656c 662e 6f75 7470 7574       self.output
-0001a260: 2e70 726f 6a65 6374 696f 6e2e 7368 6172  .projection.shar
-0001a270: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
-0001a280: 2020 2020 2020 2020 2020 2073 7472 6174             strat
-0001a290: 6567 795f 6269 6173 3d28 2870 6172 616c  egy_bias=((paral
-0001a2a0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-0001a2b0: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
-0001a2c0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0001a2d0: 5f70 6172 616c 6c65 6c2c 2031 292c 2028  _parallel, 1), (
-0001a2e0: 312c 2929 2c0a 2020 2020 2020 2020 2020  1,)),.          
-0001a2f0: 2020 2020 2020 2020 2020 2020 2020 7374                st
-0001a300: 7261 7465 6779 5f6d 6174 6d75 6c3d 2828  rategy_matmul=((
-0001a310: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0001a320: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
-0001a330: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001a340: 6f64 656c 5f70 6172 616c 6c65 6c29 2c0a  odel_parallel),.
-0001a350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a370: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
-0001a380: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0001a390: 7061 7261 6c6c 656c 2c20 3129 292c 0a20  parallel, 1)),. 
-0001a3a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a3b0: 2020 2020 2020 206f 7574 5f73 7472 6174         out_strat
-0001a3c0: 6567 795f 6d61 746d 756c 3d28 2870 6172  egy_matmul=((par
-0001a3d0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-0001a3e0: 615f 7061 7261 6c6c 656c 202a 2070 6172  a_parallel * par
-0001a3f0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001a400: 656c 5f70 6172 616c 6c65 6c2c 2031 292c  el_parallel, 1),
-0001a410: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-0001a420: 2020 2020 2020 2073 656c 662e 6f75 7470         self.outp
-0001a430: 7574 2e64 726f 706f 7574 2e64 726f 706f  ut.dropout.dropo
-0001a440: 7574 2e73 6861 7264 280a 2020 2020 2020  ut.shard(.      
-0001a450: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a460: 2020 2828 7061 7261 6c6c 656c 5f63 6f6e    ((parallel_con
-0001a470: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-0001a480: 6c20 2a20 7061 7261 6c6c 656c 5f63 6f6e  l * parallel_con
-0001a490: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0001a4a0: 656c 2c20 3129 2c29 290a 2020 2020 2020  el, 1),)).      
-0001a4b0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-0001a4c0: 2020 2020 7261 6973 6520 5275 6e74 696d      raise Runtim
-0001a4d0: 6545 7272 6f72 2866 2254 6865 207b 7365  eError(f"The {se
-0001a4e0: 6c66 2e63 6c73 5f6e 616d 657d 206f 6e6c  lf.cls_name} onl
-0001a4f0: 7920 7375 7070 6f72 7420 7368 6172 6469  y support shardi
-0001a500: 6e67 2070 726f 7061 6761 7469 6f6e 206f  ng propagation o
-0001a510: 7220 220a 2020 2020 2020 2020 2020 2020  r ".            
-0001a520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a530: 2020 2066 2273 656d 692d 6175 746f 2070     f"semi-auto p
-0001a540: 6172 616c 6c65 6c20 6d6f 6465 206e 6f77  arallel mode now
-0001a550: 2e22 290a 0a20 2020 2064 6566 2063 6f6e  .")..    def con
-0001a560: 7374 7275 6374 2873 656c 662c 2078 2c20  struct(self, x, 
-0001a570: 696e 7075 745f 6d61 736b 3d4e 6f6e 652c  input_mask=None,
-0001a580: 2069 6e69 745f 7265 7365 743d 5472 7565   init_reset=True
-0001a590: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
-0001a5a0: 6e67 7468 3d4e 6f6e 6529 3a0a 2020 2020  ngth=None):.    
-0001a5b0: 2020 2020 2222 2266 6f72 7761 7264 2070      """forward p
-0001a5c0: 726f 6365 7373 2222 220a 2020 2020 2020  rocess""".      
-0001a5d0: 2020 7365 6c66 2e5f 6368 6563 6b5f 696e    self._check_in
-0001a5e0: 7075 7428 782c 2069 6e70 7574 5f6d 6173  put(x, input_mas
-0001a5f0: 6b2c 2069 6e69 745f 7265 7365 742c 2062  k, init_reset, b
-0001a600: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-0001a610: 6829 0a20 2020 2020 2020 2078 5f73 6861  h).        x_sha
-0001a620: 7065 203d 2046 2e73 6861 7065 2878 290a  pe = F.shape(x).
-0001a630: 2020 2020 2020 2020 7820 3d20 462e 7265          x = F.re
-0001a640: 7368 6170 6528 782c 2028 2d31 2c20 785f  shape(x, (-1, x_
-0001a650: 7368 6170 655b 2d31 5d29 290a 2020 2020  shape[-1])).    
-0001a660: 2020 2020 6966 2073 656c 662e 706f 7374      if self.post
-0001a670: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0001a680: 7561 6c3a 0a20 2020 2020 2020 2020 2020  ual:.           
-0001a690: 2069 6e70 7574 5f78 203d 2078 0a20 2020   input_x = x.   
-0001a6a0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0001a6b0: 2020 2020 2020 2069 6e70 7574 5f78 203d         input_x =
-0001a6c0: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
-0001a6d0: 2878 290a 2020 2020 2020 2020 696e 7075  (x).        inpu
-0001a6e0: 745f 7820 3d20 462e 6361 7374 2869 6e70  t_x = F.cast(inp
-0001a6f0: 7574 5f78 2c20 7365 6c66 2e64 7479 7065  ut_x, self.dtype
-0001a700: 290a 0a20 2020 2020 2020 2023 2069 6e64  )..        # ind
-0001a710: 6963 6174 6520 7768 6574 6865 7220 7265  icate whether re
-0001a720: 7365 7420 7361 7665 6420 7374 6174 6573  set saved states
-0001a730: 0a20 2020 2020 2020 206b 6579 5f72 6573  .        key_res
-0001a740: 6574 203d 204e 6f6e 650a 2020 2020 2020  et = None.      
-0001a750: 2020 7661 6c75 655f 7265 7365 7420 3d20    value_reset = 
-0001a760: 4e6f 6e65 0a0a 2020 2020 2020 2020 6966  None..        if
-0001a770: 2073 656c 662e 7573 655f 7061 7374 3a0a   self.use_past:.
-0001a780: 2020 2020 2020 2020 2020 2020 2320 7265              # re
-0001a790: 7365 7420 7374 6174 6573 2c20 696e 6974  set states, init
-0001a7a0: 5f72 6573 6574 2054 7275 6520 666f 7220  _reset True for 
-0001a7b0: 7265 7573 6520 616e 6420 4661 6c73 6520  reuse and False 
-0001a7c0: 666f 7220 7265 7365 740a 2020 2020 2020  for reset.      
-0001a7d0: 2020 2020 2020 7365 6c66 2e61 7373 6967        self.assig
-0001a7e0: 6e28 7365 6c66 2e6b 6579 5f70 6173 742c  n(self.key_past,
-0001a7f0: 2073 656c 662e 6d75 6c28 7365 6c66 2e6b   self.mul(self.k
-0001a800: 6579 5f70 6173 742c 2046 2e63 6173 7428  ey_past, F.cast(
-0001a810: 696e 6974 5f72 6573 6574 2c20 7365 6c66  init_reset, self
-0001a820: 2e64 7479 7065 2929 290a 2020 2020 2020  .dtype))).      
-0001a830: 2020 2020 2020 6b65 795f 7265 7365 7420        key_reset 
-0001a840: 3d20 7365 6c66 2e6b 6579 5f70 6173 740a  = self.key_past.
-0001a850: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001a860: 2e61 7373 6967 6e28 7365 6c66 2e76 616c  .assign(self.val
-0001a870: 7565 5f70 6173 742c 2073 656c 662e 6d75  ue_past, self.mu
-0001a880: 6c28 7365 6c66 2e76 616c 7565 5f70 6173  l(self.value_pas
-0001a890: 742c 2046 2e63 6173 7428 696e 6974 5f72  t, F.cast(init_r
-0001a8a0: 6573 6574 2c20 7365 6c66 2e64 7479 7065  eset, self.dtype
-0001a8b0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-0001a8c0: 7661 6c75 655f 7265 7365 7420 3d20 7365  value_reset = se
-0001a8d0: 6c66 2e76 616c 7565 5f70 6173 740a 2020  lf.value_past.  
-0001a8e0: 2020 2020 2020 2020 2020 2320 6164 6420            # add 
-0001a8f0: 6465 7065 6e64 656e 6379 2066 6f72 2064  dependency for d
-0001a900: 6573 6972 6564 2065 7865 6375 7469 6f6e  esired execution
-0001a910: 206f 7264 6572 0a20 2020 2020 2020 2020   order.         
-0001a920: 2020 2069 6e70 7574 5f78 203d 2046 2e64     input_x = F.d
-0001a930: 6570 656e 6428 696e 7075 745f 782c 206b  epend(input_x, k
-0001a940: 6579 5f72 6573 6574 290a 2020 2020 2020  ey_reset).      
-0001a950: 2020 2020 2020 696e 7075 745f 7820 3d20        input_x = 
-0001a960: 462e 6465 7065 6e64 2869 6e70 7574 5f78  F.depend(input_x
-0001a970: 2c20 7661 6c75 655f 7265 7365 7429 0a0a  , value_reset)..
-0001a980: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0001a990: 6e2c 206c 6179 6572 5f70 7265 7365 6e74  n, layer_present
-0001a9a0: 203d 2073 656c 662e 6174 7465 6e74 696f   = self.attentio
-0001a9b0: 6e28 696e 7075 745f 782c 2069 6e70 7574  n(input_x, input
-0001a9c0: 5f78 2c20 696e 7075 745f 782c 2069 6e70  _x, input_x, inp
-0001a9d0: 7574 5f6d 6173 6b2c 0a20 2020 2020 2020  ut_mask,.       
-0001a9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001aa00: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001aa10: 6b65 795f 7061 7374 2c20 7365 6c66 2e76  key_past, self.v
-0001aa20: 616c 7565 5f70 6173 742c 2062 6174 6368  alue_past, batch
-0001aa30: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
-0001aa40: 2020 2020 2020 2023 2046 6f72 2070 6f73         # For pos
-0001aa50: 742d 6c61 7965 726e 6f72 6d20 7468 6520  t-layernorm the 
-0001aa60: 696e 7075 7473 2066 6f72 2072 6573 6964  inputs for resid
-0001aa70: 7561 6c20 7061 7468 2061 7265 206f 7574  ual path are out
-0001aa80: 7075 7420 6f66 2073 656c 662d 6174 7465  put of self-atte
-0001aa90: 6e74 696f 6e20 616e 6420 6f75 7470 7574  ntion and output
-0001aaa0: 206f 6620 6c61 7965 726e 6f72 6d0a 2020   of layernorm.  
-0001aab0: 2020 2020 2020 6966 2073 656c 662e 706f        if self.po
-0001aac0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
-0001aad0: 6964 7561 6c3a 0a20 2020 2020 2020 2020  idual:.         
-0001aae0: 2020 2078 203d 2073 656c 662e 6164 6428     x = self.add(
-0001aaf0: 696e 7075 745f 782c 2061 7474 656e 7469  input_x, attenti
-0001ab00: 6f6e 290a 2020 2020 2020 2020 2320 466f  on).        # Fo
-0001ab10: 7220 7072 652d 6c61 7965 726e 6f72 6d20  r pre-layernorm 
-0001ab20: 7468 6520 696e 7075 7473 2066 6f72 2072  the inputs for r
-0001ab30: 6573 6964 7561 6c20 7061 7468 2061 7265  esidual path are
-0001ab40: 206f 7574 7075 7420 6f66 2073 656c 662d   output of self-
-0001ab50: 6174 7465 6e74 696f 6e20 616e 6420 696e  attention and in
-0001ab60: 7075 7420 6f66 2074 6869 7320 6c61 7965  put of this laye
-0001ab70: 720a 2020 2020 2020 2020 656c 7365 3a0a  r.        else:.
-0001ab80: 2020 2020 2020 2020 2020 2020 7820 3d20              x = 
-0001ab90: 7365 6c66 2e61 6464 2878 2c20 6174 7465  self.add(x, atte
-0001aba0: 6e74 696f 6e29 0a0a 2020 2020 2020 2020  ntion)..        
-0001abb0: 6f75 7470 7574 5f78 203d 2073 656c 662e  output_x = self.
-0001abc0: 6c61 7965 726e 6f72 6d32 2878 290a 2020  layernorm2(x).  
-0001abd0: 2020 2020 2020 6f75 7470 7574 5f78 203d        output_x =
-0001abe0: 2046 2e63 6173 7428 6f75 7470 7574 5f78   F.cast(output_x
-0001abf0: 2c20 7365 6c66 2e64 7479 7065 290a 2020  , self.dtype).  
-0001ac00: 2020 2020 2020 6175 785f 6c6f 7373 203d        aux_loss =
-0001ac10: 204e 6f6e 650a 2020 2020 2020 2020 6966   None.        if
-0001ac20: 2073 656c 662e 7573 655f 6d6f 653a 0a20   self.use_moe:. 
-0001ac30: 2020 2020 2020 2020 2020 206d 6c70 5f6c             mlp_l
-0001ac40: 6f67 6974 2c20 6175 785f 6c6f 7373 203d  ogit, aux_loss =
-0001ac50: 2073 656c 662e 6f75 7470 7574 286f 7574   self.output(out
-0001ac60: 7075 745f 7829 0a20 2020 2020 2020 2065  put_x).        e
-0001ac70: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-0001ac80: 206d 6c70 5f6c 6f67 6974 203d 2073 656c   mlp_logit = sel
-0001ac90: 662e 6f75 7470 7574 286f 7574 7075 745f  f.output(output_
-0001aca0: 7829 0a0a 2020 2020 2020 2020 7661 6c75  x)..        valu
-0001acb0: 655f 7570 6461 7465 203d 204e 6f6e 650a  e_update = None.
-0001acc0: 2020 2020 2020 2020 6b65 795f 7570 6461          key_upda
-0001acd0: 7465 203d 204e 6f6e 650a 2020 2020 2020  te = None.      
-0001ace0: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
-0001acf0: 7374 3a0a 2020 2020 2020 2020 2020 2020  st:.            
-0001ad00: 2320 6375 7272 656e 7420 6b65 7920 616e  # current key an
-0001ad10: 6420 7661 6c75 650a 2020 2020 2020 2020  d value.        
-0001ad20: 2020 2020 6b65 795f 7072 6573 656e 742c      key_present,
-0001ad30: 2076 616c 7565 5f70 7265 7365 6e74 203d   value_present =
-0001ad40: 206c 6179 6572 5f70 7265 7365 6e74 0a20   layer_present. 
-0001ad50: 2020 2020 2020 2020 2020 2023 2075 7064             # upd
-0001ad60: 6174 6520 6b65 7920 616e 6420 7661 6c75  ate key and valu
-0001ad70: 6520 6361 6c63 756c 6174 6564 2074 6869  e calculated thi
-0001ad80: 7320 7374 6570 0a20 2020 2020 2020 2020  s step.         
-0001ad90: 2020 2073 656c 662e 6173 7369 676e 2873     self.assign(s
-0001ada0: 656c 662e 6b65 795f 7061 7374 2c20 6b65  elf.key_past, ke
-0001adb0: 795f 7072 6573 656e 7429 0a20 2020 2020  y_present).     
-0001adc0: 2020 2020 2020 206b 6579 5f75 7064 6174         key_updat
-0001add0: 6520 3d20 7365 6c66 2e6b 6579 5f70 6173  e = self.key_pas
-0001ade0: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
-0001adf0: 6c66 2e61 7373 6967 6e28 7365 6c66 2e76  lf.assign(self.v
-0001ae00: 616c 7565 5f70 6173 742c 2076 616c 7565  alue_past, value
-0001ae10: 5f70 7265 7365 6e74 290a 2020 2020 2020  _present).      
-0001ae20: 2020 2020 2020 7661 6c75 655f 7570 6461        value_upda
-0001ae30: 7465 203d 2073 656c 662e 7661 6c75 655f  te = self.value_
-0001ae40: 7061 7374 0a20 2020 2020 2020 2020 2020  past.           
-0001ae50: 2023 2061 6464 2064 6570 656e 6465 6e63   # add dependenc
-0001ae60: 7920 666f 7220 6465 7369 7265 6420 6578  y for desired ex
-0001ae70: 6563 7574 696f 6e20 6f72 6465 720a 2020  ecution order.  
-0001ae80: 2020 2020 2020 2020 2020 6b65 795f 7570            key_up
-0001ae90: 6461 7465 203d 2046 2e64 6570 656e 6428  date = F.depend(
-0001aea0: 6b65 795f 7570 6461 7465 2c20 6b65 795f  key_update, key_
-0001aeb0: 7265 7365 7429 0a20 2020 2020 2020 2020  reset).         
-0001aec0: 2020 2076 616c 7565 5f75 7064 6174 6520     value_update 
-0001aed0: 3d20 462e 6465 7065 6e64 2876 616c 7565  = F.depend(value
-0001aee0: 5f75 7064 6174 652c 2076 616c 7565 5f72  _update, value_r
-0001aef0: 6573 6574 290a 0a20 2020 2020 2020 2023  eset)..        #
-0001af00: 2061 6464 2064 6570 656e 6465 6e63 7920   add dependency 
-0001af10: 666f 7220 6465 7369 7265 6420 6578 6563  for desired exec
-0001af20: 7574 696f 6e20 6f72 6465 720a 2020 2020  ution order.    
-0001af30: 2020 2020 6d6c 705f 6c6f 6769 7420 3d20      mlp_logit = 
-0001af40: 462e 6465 7065 6e64 286d 6c70 5f6c 6f67  F.depend(mlp_log
-0001af50: 6974 2c20 7661 6c75 655f 7570 6461 7465  it, value_update
-0001af60: 290a 2020 2020 2020 2020 6d6c 705f 6c6f  ).        mlp_lo
-0001af70: 6769 7420 3d20 462e 6465 7065 6e64 286d  git = F.depend(m
-0001af80: 6c70 5f6c 6f67 6974 2c20 6b65 795f 7570  lp_logit, key_up
-0001af90: 6461 7465 290a 0a20 2020 2020 2020 2023  date)..        #
-0001afa0: 2069 6620 7368 6170 6520 6973 2033 642c   if shape is 3d,
-0001afb0: 2077 6520 7265 7368 6170 6520 7468 6520   we reshape the 
-0001afc0: 696e 7075 7473 206f 6620 7468 6520 6164  inputs of the ad
-0001afd0: 640a 2020 2020 2020 2020 6966 206c 656e  d.        if len
-0001afe0: 2878 5f73 6861 7065 2920 3d3d 2033 3a0a  (x_shape) == 3:.
-0001aff0: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-0001b000: 7574 5f78 203d 2050 2e52 6573 6861 7065  ut_x = P.Reshape
-0001b010: 2829 286f 7574 7075 745f 782c 2078 5f73  ()(output_x, x_s
-0001b020: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
-0001b030: 2020 6d6c 705f 6c6f 6769 7420 3d20 502e    mlp_logit = P.
-0001b040: 5265 7368 6170 6528 2928 6d6c 705f 6c6f  Reshape()(mlp_lo
-0001b050: 6769 742c 2078 5f73 6861 7065 290a 2020  git, x_shape).  
-0001b060: 2020 2020 2020 2020 2020 7820 3d20 502e            x = P.
-0001b070: 5265 7368 6170 6528 2928 782c 2078 5f73  Reshape()(x, x_s
-0001b080: 6861 7065 290a 0a20 2020 2020 2020 2020  hape)..         
-0001b090: 2020 2069 6620 7365 6c66 2e70 6f73 745f     if self.post_
-0001b0a0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0001b0b0: 616c 3a0a 2020 2020 2020 2020 2020 2020  al:.            
-0001b0c0: 2020 2020 6f75 7470 7574 203d 2073 656c      output = sel
-0001b0d0: 662e 6164 645f 3364 286f 7574 7075 745f  f.add_3d(output_
-0001b0e0: 782c 206d 6c70 5f6c 6f67 6974 290a 2020  x, mlp_logit).  
-0001b0f0: 2020 2020 2020 2020 2020 2020 2020 6f75                ou
-0001b100: 7470 7574 203d 2046 2e72 6573 6861 7065  tput = F.reshape
-0001b110: 286f 7574 7075 742c 2028 2d31 2c20 785f  (output, (-1, x_
-0001b120: 7368 6170 655b 2d31 5d29 290a 2020 2020  shape[-1])).    
-0001b130: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-0001b140: 7574 203d 2073 656c 662e 6c61 7965 726e  ut = self.layern
-0001b150: 6f72 6d31 286f 7574 7075 7429 0a20 2020  orm1(output).   
-0001b160: 2020 2020 2020 2020 2020 2020 206f 7574               out
-0001b170: 7075 7420 3d20 462e 7265 7368 6170 6528  put = F.reshape(
-0001b180: 6f75 7470 7574 2c20 785f 7368 6170 6529  output, x_shape)
-0001b190: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
-0001b1a0: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-0001b1b0: 2020 206f 7574 7075 7420 3d20 7365 6c66     output = self
-0001b1c0: 2e61 6464 5f33 6428 782c 206d 6c70 5f6c  .add_3d(x, mlp_l
-0001b1d0: 6f67 6974 290a 2020 2020 2020 2020 656c  ogit).        el
-0001b1e0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-0001b1f0: 6966 2073 656c 662e 706f 7374 5f6c 6179  if self.post_lay
-0001b200: 6572 6e6f 726d 5f72 6573 6964 7561 6c3a  ernorm_residual:
-0001b210: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001b220: 206f 7574 7075 7420 3d20 7365 6c66 2e61   output = self.a
-0001b230: 6464 286f 7574 7075 745f 782c 206d 6c70  dd(output_x, mlp
-0001b240: 5f6c 6f67 6974 290a 2020 2020 2020 2020  _logit).        
-0001b250: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
-0001b260: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
-0001b270: 286f 7574 7075 7429 0a20 2020 2020 2020  (output).       
-0001b280: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0001b290: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-0001b2a0: 7420 3d20 7365 6c66 2e61 6464 2878 2c20  t = self.add(x, 
-0001b2b0: 6d6c 705f 6c6f 6769 7429 0a20 2020 2020  mlp_logit).     
-0001b2c0: 2020 2020 2020 206f 7574 7075 7420 3d20         output = 
-0001b2d0: 462e 7265 7368 6170 6528 6f75 7470 7574  F.reshape(output
-0001b2e0: 2c20 785f 7368 6170 6529 0a0a 2020 2020  , x_shape)..    
-0001b2f0: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
-0001b300: 6d6f 653a 0a20 2020 2020 2020 2020 2020  moe:.           
-0001b310: 2072 6574 7572 6e20 6f75 7470 7574 2c20   return output, 
-0001b320: 6c61 7965 725f 7072 6573 656e 742c 2061  layer_present, a
-0001b330: 7578 5f6c 6f73 730a 2020 2020 2020 2020  ux_loss.        
-0001b340: 7265 7475 726e 206f 7574 7075 742c 206c  return output, l
-0001b350: 6179 6572 5f70 7265 7365 6e74 0a0a 2020  ayer_present..  
-0001b360: 2020 6465 6620 5f63 6865 636b 5f69 6e70    def _check_inp
-0001b370: 7574 2873 656c 662c 2078 2c20 696e 7075  ut(self, x, inpu
-0001b380: 745f 6d61 736b 2c20 696e 6974 5f72 6573  t_mask, init_res
-0001b390: 6574 2c20 6261 7463 685f 7661 6c69 645f  et, batch_valid_
-0001b3a0: 6c65 6e67 7468 293a 0a20 2020 2020 2020  length):.       
-0001b3b0: 2072 2222 2243 6865 636b 2069 6e70 7574   r"""Check input
-0001b3c0: 7322 2222 0a20 2020 2020 2020 205f 6368  s""".        _ch
-0001b3d0: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
-0001b3e0: 462e 6474 7970 6528 7829 2c20 2278 222c  F.dtype(x), "x",
-0001b3f0: 205b 6d73 7479 7065 2e66 6c6f 6174 3332   [mstype.float32
-0001b400: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-0001b410: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-0001b420: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
-0001b430: 6529 0a20 2020 2020 2020 2069 6620 696e  e).        if in
-0001b440: 7075 745f 6d61 736b 2069 7320 6e6f 7420  put_mask is not 
-0001b450: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
-0001b460: 2020 5f63 6865 636b 5f69 6e70 7574 5f64    _check_input_d
-0001b470: 7479 7065 2846 2e64 7479 7065 2869 6e70  type(F.dtype(inp
-0001b480: 7574 5f6d 6173 6b29 2c20 2269 6e70 7574  ut_mask), "input
-0001b490: 5f6d 6173 6b22 2c0a 2020 2020 2020 2020  _mask",.        
-0001b4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b4b0: 2020 2020 2020 205b 6d73 7479 7065 2e66         [mstype.f
-0001b4c0: 6c6f 6174 3332 2c20 6d73 7479 7065 2e66  loat32, mstype.f
-0001b4d0: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
-0001b4e0: 666c 6f61 7431 365d 2c20 7365 6c66 2e63  float16], self.c
-0001b4f0: 6c73 5f6e 616d 6529 0a0a 2020 2020 2020  ls_name)..      
-0001b500: 2020 696e 6974 5f72 6573 6574 5f69 735f    init_reset_is_
-0001b510: 7465 6e73 6f72 203d 2069 7369 6e73 7461  tensor = isinsta
-0001b520: 6e63 6528 696e 6974 5f72 6573 6574 2c20  nce(init_reset, 
-0001b530: 5465 6e73 6f72 290a 2020 2020 2020 2020  Tensor).        
-0001b540: 696e 6974 5f72 6573 6574 5f69 735f 6465  init_reset_is_de
-0001b550: 6661 756c 7420 3d20 696e 6974 5f72 6573  fault = init_res
-0001b560: 6574 2069 7320 5472 7565 0a20 2020 2020  et is True.     
-0001b570: 2020 2062 6174 6368 5f76 616c 6964 5f6c     batch_valid_l
-0001b580: 656e 6774 685f 6973 5f74 656e 736f 7220  ength_is_tensor 
-0001b590: 3d20 6973 696e 7374 616e 6365 2862 6174  = isinstance(bat
-0001b5a0: 6368 5f76 616c 6964 5f6c 656e 6774 682c  ch_valid_length,
-0001b5b0: 2054 656e 736f 7229 0a20 2020 2020 2020   Tensor).       
-0001b5c0: 2062 6174 6368 5f69 735f 6465 6661 756c   batch_is_defaul
-0001b5d0: 7420 3d20 6261 7463 685f 7661 6c69 645f  t = batch_valid_
-0001b5e0: 6c65 6e67 7468 2069 7320 4e6f 6e65 0a20  length is None. 
-0001b5f0: 2020 2020 2020 205f 6368 6563 6b5f 7061         _check_pa
-0001b600: 7374 5f6e 6f6e 655f 696e 7075 745f 6e6f  st_none_input_no
-0001b610: 6e65 2873 656c 662e 7573 655f 7061 7374  ne(self.use_past
-0001b620: 2c20 2269 6e69 745f 7265 7365 7422 2c20  , "init_reset", 
-0001b630: 7365 6c66 2e63 6c73 5f6e 616d 652c 2054  self.cls_name, T
-0001b640: 7275 652c 2069 6e69 745f 7265 7365 745f  rue, init_reset_
-0001b650: 6973 5f74 656e 736f 722c 0a20 2020 2020  is_tensor,.     
-0001b660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b670: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0001b680: 6e69 745f 7265 7365 745f 6973 5f64 6566  nit_reset_is_def
-0001b690: 6175 6c74 290a 2020 2020 2020 2020 5f63  ault).        _c
-0001b6a0: 6865 636b 5f70 6173 745f 6e6f 6e65 5f69  heck_past_none_i
-0001b6b0: 6e70 7574 5f6e 6f6e 6528 7365 6c66 2e75  nput_none(self.u
-0001b6c0: 7365 5f70 6173 742c 2022 6261 7463 685f  se_past, "batch_
-0001b6d0: 7661 6c69 645f 6c65 6e67 7468 222c 2073  valid_length", s
-0001b6e0: 656c 662e 636c 735f 6e61 6d65 2c20 4e6f  elf.cls_name, No
-0001b6f0: 6e65 2c0a 2020 2020 2020 2020 2020 2020  ne,.            
-0001b700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b710: 2020 2020 2020 2020 6261 7463 685f 7661          batch_va
-0001b720: 6c69 645f 6c65 6e67 7468 5f69 735f 7465  lid_length_is_te
-0001b730: 6e73 6f72 2c20 6261 7463 685f 6973 5f64  nsor, batch_is_d
-0001b740: 6566 6175 6c74 290a 0a20 2020 2020 2020  efault)..       
-0001b750: 2069 6620 7365 6c66 2e75 7365 5f70 6173   if self.use_pas
-0001b760: 743a 0a20 2020 2020 2020 2020 2020 205f  t:.            _
-0001b770: 6368 6563 6b5f 696e 7075 745f 6474 7970  check_input_dtyp
-0001b780: 6528 462e 6474 7970 6528 6261 7463 685f  e(F.dtype(batch_
-0001b790: 7661 6c69 645f 6c65 6e67 7468 292c 2022  valid_length), "
-0001b7a0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-0001b7b0: 7468 222c 205b 6d73 7479 7065 2e69 6e74  th", [mstype.int
-0001b7c0: 3332 5d2c 2073 656c 662e 636c 735f 6e61  32], self.cls_na
-0001b7d0: 6d65 290a 2020 2020 2020 2020 7265 7475  me).        retu
-0001b7e0: 726e 2054 7275 650a 0a0a 636c 6173 7320  rn True...class 
-0001b7f0: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
-0001b800: 6572 4c61 7965 7228 4365 6c6c 293a 0a20  erLayer(Cell):. 
-0001b810: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
-0001b820: 5472 616e 7366 6f72 6d65 7220 4465 636f  Transformer Deco
-0001b830: 6465 7220 4c61 7965 722e 2054 6869 7320  der Layer. This 
-0001b840: 6973 2061 6e20 696d 706c 656d 656e 7461  is an implementa
-0001b850: 7469 6f6e 206f 6620 7468 6520 7369 6e67  tion of the sing
-0001b860: 6c65 206c 6179 6572 206f 6620 7468 6520  le layer of the 
-0001b870: 7472 616e 7366 6f72 6d65 720a 2020 2020  transformer.    
-0001b880: 2020 2020 6465 636f 6465 7220 6c61 7965      decoder laye
-0001b890: 722c 2069 6e63 6c75 6469 6e67 2073 656c  r, including sel
-0001b8a0: 662d 6174 7465 6e74 696f 6e2c 2063 726f  f-attention, cro
-0001b8b0: 7373 2061 7474 656e 7469 6f6e 2061 6e64  ss attention and
-0001b8c0: 2066 6565 6477 6172 6420 6c61 7965 722e   feedward layer.
-0001b8d0: 2057 6865 6e20 7468 6520 656e 636f 6465   When the encode
-0001b8e0: 725f 6f75 7470 7574 2069 7320 4e6f 6e65  r_output is None
-0001b8f0: 2c0a 2020 2020 2020 2020 7468 6520 6372  ,.        the cr
-0001b900: 6f73 7320 6174 7465 6e74 696f 6e20 7769  oss attention wi
-0001b910: 6c6c 206e 6f74 2062 6520 6566 6665 6374  ll not be effect
-0001b920: 6976 652e 0a0a 2020 2020 2020 2020 4172  ive...        Ar
-0001b930: 6773 3a0a 2020 2020 2020 2020 2020 2020  gs:.            
-0001b940: 6869 6464 656e 5f73 697a 6528 696e 7429  hidden_size(int)
-0001b950: 3a20 5468 6520 6869 6464 656e 2073 697a  : The hidden siz
-0001b960: 6520 6f66 2074 6865 2069 6e70 7574 2e0a  e of the input..
-0001b970: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
-0001b980: 6869 6464 656e 5f73 697a 6528 696e 7429  hidden_size(int)
-0001b990: 3a20 5468 6520 6869 6464 656e 2073 697a  : The hidden siz
-0001b9a0: 6520 6f66 2062 6f74 746c 656e 6563 6b20  e of bottleneck 
-0001b9b0: 696e 2074 6865 2066 6565 6466 6f72 7761  in the feedforwa
-0001b9c0: 7264 206c 6179 6572 2e0a 2020 2020 2020  rd layer..      
-0001b9d0: 2020 2020 2020 6e75 6d5f 6865 6164 7328        num_heads(
-0001b9e0: 696e 7429 3a20 5468 6520 6e75 6d62 6572  int): The number
-0001b9f0: 206f 6620 7468 6520 6865 6164 732e 0a20   of the heads.. 
-0001ba00: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-0001ba10: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
-0001ba20: 6261 7463 6820 7369 7a65 206f 6620 7468  batch size of th
-0001ba30: 6520 696e 7075 7420 7465 6e73 6f72 2077  e input tensor w
-0001ba40: 6865 6e20 646f 2069 6e63 7265 6e6d 656e  hen do increnmen
-0001ba50: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
-0001ba60: 5368 6f75 6c64 2062 6520 6120 706f 7369  Should be a posi
-0001ba70: 7469 7665 0a20 2020 2020 2020 2020 2020  tive.           
-0001ba80: 2020 2020 2076 616c 7565 2e20 5768 656e       value. When
-0001ba90: 2064 6f20 7472 6169 6e69 6e67 206f 7220   do training or 
-0001baa0: 7072 6564 6963 7469 6f6e 2c20 7468 6520  prediction, the 
-0001bab0: 6172 6775 6d65 6e74 2077 696c 6c20 6e6f  argument will no
-0001bac0: 7420 776f 726b 2061 6e64 2074 6865 2075  t work and the u
-0001bad0: 7365 7220 6361 6e20 6a75 7374 2070 6173  ser can just pas
-0001bae0: 7320 4e6f 6e65 2074 6f0a 2020 2020 2020  s None to.      
-0001baf0: 2020 2020 2020 2020 2020 7468 6520 6172            the ar
-0001bb00: 6775 6d65 6e74 2e0a 2020 2020 2020 2020  gument..        
-0001bb10: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
-0001bb20: 7468 2869 6e74 293a 2054 6865 2069 6e70  th(int): The inp
-0001bb30: 7574 2073 6f75 7263 6520 7365 7175 656e  ut source sequen
-0001bb40: 6365 206c 656e 6774 682e 0a20 2020 2020  ce length..     
-0001bb50: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
-0001bb60: 656e 6774 6828 696e 7429 3a20 5468 6520  ength(int): The 
-0001bb70: 696e 7075 7420 7461 7267 6574 2073 6571  input target seq
-0001bb80: 7565 6e63 6520 6c65 6e67 7468 2e0a 2020  uence length..  
-0001bb90: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-0001bba0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-0001bbb0: 2866 6c6f 6174 293a 2054 6865 2064 726f  (float): The dro
-0001bbc0: 706f 7574 2072 6174 6520 6f66 2074 6865  pout rate of the
-0001bbd0: 2061 7474 656e 7469 6f6e 2073 636f 7265   attention score
-0001bbe0: 732e 2044 6566 6175 6c74 3a30 2e31 2e0a  s. Default:0.1..
-0001bbf0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-0001bc00: 656e 5f64 726f 706f 7574 5f72 6174 6528  en_dropout_rate(
-0001bc10: 666c 6f61 7429 3a20 5468 6520 6472 6f70  float): The drop
-0001bc20: 6f75 7420 7261 7465 206f 6620 7468 6520  out rate of the 
-0001bc30: 6669 6e61 6c20 6f75 7470 7574 206f 6620  final output of 
-0001bc40: 7468 6520 6c61 7965 722e 2044 6566 6175  the layer. Defau
-0001bc50: 6c74 3a30 2e31 2e0a 2020 2020 2020 2020  lt:0.1..        
-0001bc60: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
-0001bc70: 726d 5f72 6573 6964 7561 6c28 626f 6f6c  rm_residual(bool
-0001bc80: 293a 2044 6f20 7265 7369 6475 616c 7320  ): Do residuals 
-0001bc90: 6164 6473 2062 6566 6f72 6520 7468 6520  adds before the 
-0001bca0: 6c61 7965 726e 6f72 6d2e 2044 6566 6175  layernorm. Defau
-0001bcb0: 6c74 2046 616c 7365 2e0a 2020 2020 2020  lt False..      
-0001bcc0: 2020 2020 2020 7573 655f 7061 7374 2862        use_past(b
-0001bcd0: 6f6f 6c29 3a20 5573 6520 7468 6520 7061  ool): Use the pa
-0001bce0: 7374 2073 7461 7465 2074 6f20 636f 6d70  st state to comp
-0001bcf0: 7574 652c 2075 7365 6420 666f 7220 696e  ute, used for in
-0001bd00: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
-0001bd10: 7469 6f6e 2e20 4465 6661 756c 7420 4661  tion. Default Fa
-0001bd20: 6c73 652e 0a20 2020 2020 2020 2020 2020  lse..           
-0001bd30: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
-0001bd40: 7465 5f74 7970 6528 6474 7970 652e 4e75  te_type(dtype.Nu
-0001bd50: 6d62 6572 293a 2054 6865 2063 6f6d 7075  mber): The compu
-0001bd60: 7461 7469 6f6e 2074 7970 6520 6f66 2074  tation type of t
-0001bd70: 6865 206c 6179 6572 6e6f 726d 2e0a 2020  he layernorm..  
-0001bd80: 2020 2020 2020 2020 2020 2020 2020 5368                Sh
-0001bd90: 6f75 6c64 2062 6520 6474 7970 652e 666c  ould be dtype.fl
-0001bda0: 6f61 7433 3220 6f72 2064 7479 7065 2e66  oat32 or dtype.f
-0001bdb0: 6c6f 6174 3136 2e20 4465 6661 756c 7420  loat16. Default 
-0001bdc0: 6474 7970 652e 666c 6f61 7433 322e 0a20  dtype.float32.. 
-0001bdd0: 2020 2020 2020 2020 2020 2073 6f66 746d             softm
-0001bde0: 6178 5f63 6f6d 7075 7465 5f74 7970 6528  ax_compute_type(
-0001bdf0: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-0001be00: 6865 2063 6f6d 7075 7461 7469 6f6e 2074  he computation t
-0001be10: 7970 6520 6f66 2074 6865 2073 6f66 746d  ype of the softm
-0001be20: 6178 2069 6e20 7468 6520 6174 7465 6e74  ax in the attent
-0001be30: 696f 6e2e 0a20 2020 2020 2020 2020 2020  ion..           
-0001be40: 2020 2020 2053 686f 756c 6420 6265 2064       Should be d
-0001be50: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
-0001be60: 6474 7970 652e 666c 6f61 7431 362e 2044  dtype.float16. D
-0001be70: 6566 6175 6c74 206d 7374 7970 652e 666c  efault mstype.fl
-0001be80: 6f61 7433 322e 0a20 2020 2020 2020 2020  oat32..         
-0001be90: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
-0001bea0: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
-0001beb0: 3a20 5468 6520 7061 7261 6d65 7465 7220  : The parameter 
-0001bec0: 696e 6974 6961 6c69 7a61 7469 6f6e 2074  initialization t
-0001bed0: 7970 6520 6f66 2074 6865 206d 6f64 756c  ype of the modul
-0001bee0: 652e 0a20 2020 2020 2020 2020 2020 2020  e..             
-0001bef0: 2020 2053 686f 756c 6420 6265 2064 7479     Should be dty
-0001bf00: 7065 2e66 6c6f 6174 3332 206f 7220 6474  pe.float32 or dt
-0001bf10: 7970 652e 666c 6f61 7431 362e 2044 6566  ype.float16. Def
-0001bf20: 6175 6c74 2064 7479 7065 2e66 6c6f 6174  ault dtype.float
-0001bf30: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-0001bf40: 6869 6464 656e 5f61 6374 2028 7374 722c  hidden_act (str,
-0001bf50: 206e 6e2e 4365 6c6c 293a 2054 6865 2061   nn.Cell): The a
-0001bf60: 6374 6976 6174 696f 6e20 6f66 2074 6865  ctivation of the
-0001bf70: 2069 6e74 6572 6e61 6c20 6665 6564 666f   internal feedfo
-0001bf80: 7277 6172 6420 6c61 7965 722e 2053 7570  rward layer. Sup
-0001bf90: 706f 7274 7320 2772 656c 7527 2c0a 2020  ports 'relu',.  
-0001bfa0: 2020 2020 2020 2020 2020 2020 2020 2772                'r
-0001bfb0: 656c 7536 272c 2027 7461 6e68 272c 2027  elu6', 'tanh', '
-0001bfc0: 6765 6c75 272c 2027 6661 7374 5f67 656c  gelu', 'fast_gel
-0001bfd0: 7527 2c20 2765 6c75 272c 2027 7369 676d  u', 'elu', 'sigm
-0001bfe0: 6f69 6427 2c20 2770 7265 6c75 272c 2027  oid', 'prelu', '
-0001bff0: 6c65 616b 7972 656c 7527 2c20 2768 7377  leakyrelu', 'hsw
-0001c000: 6973 6827 2c0a 2020 2020 2020 2020 2020  ish',.          
-0001c010: 2020 2020 2020 2768 7369 676d 6f69 6427        'hsigmoid'
-0001c020: 2c20 276c 6f67 7369 676d 6f69 6427 2061  , 'logsigmoid' a
-0001c030: 6e64 2073 6f20 6f6e 2e20 5573 6572 2063  nd so on. User c
-0001c040: 616e 2070 726f 7669 6465 2063 7573 746f  an provide custo
-0001c050: 6d20 6163 7469 7669 7469 6f6e 2074 6f20  m activition to 
-0001c060: 7468 6520 6172 6775 6d65 6e74 2e0a 2020  the argument..  
-0001c070: 2020 2020 2020 2020 2020 2020 2020 4966                If
-0001c080: 2075 7365 7220 7761 6e74 7320 746f 2072   user wants to r
-0001c090: 756e 2074 6865 206e 6574 2069 6e20 7468  un the net in th
-0001c0a0: 6520 7061 7261 6c6c 656c 206d 6f64 652c  e parallel mode,
-0001c0b0: 2074 6865 2063 7573 746f 6d20 6163 7469   the custom acti
-0001c0c0: 7661 7469 6f6e 206d 7573 7420 616c 736f  vation must also
-0001c0d0: 2070 726f 7669 6465 0a20 2020 2020 2020   provide.       
-0001c0e0: 2020 2020 2020 2020 2074 6865 2060 6163           the `ac
-0001c0f0: 7469 7661 7469 6f6e 5f73 6861 7264 6020  tivation_shard` 
-0001c100: 6675 6e63 7469 6f6e 2e20 506c 6561 7365  function. Please
-0001c110: 2073 6565 2074 6865 2065 7861 6d70 6c65   see the example
-0001c120: 7320 6f66 2074 6865 0a20 2020 2020 2020  s of the.       
-0001c130: 2020 2020 2020 2020 2063 6c61 7373 3a60           class:`
-0001c140: 6d69 6e64 666f 726d 6572 732e 6d6f 6475  mindformers.modu
-0001c150: 6c65 732e 7472 616e 7366 6f72 6d65 722e  les.transformer.
-0001c160: 4665 6564 466f 7277 6172 6460 2e20 4465  FeedForward`. De
-0001c170: 6661 756c 743a 2067 656c 752e 0a20 2020  fault: gelu..   
-0001c180: 2020 2020 2020 2020 206d 6f65 5f63 6f6e           moe_con
-0001c190: 6669 6728 4d6f 4543 6f6e 6669 6729 3a20  fig(MoEConfig): 
-0001c1a0: 5468 6520 636f 6e66 6967 7572 6174 696f  The configuratio
-0001c1b0: 6e20 6f66 204d 6f45 2028 4d69 7874 7572  n of MoE (Mixtur
-0001c1c0: 6520 6f66 2045 7870 6572 7429 2e20 4465  e of Expert). De
-0001c1d0: 6661 756c 7420 6973 2061 6e20 696e 7374  fault is an inst
-0001c1e0: 616e 6365 206f 6620 4d6f 4543 6f6e 6669  ance of MoEConfi
-0001c1f0: 670a 2020 2020 2020 2020 2020 2020 2020  g.              
-0001c200: 2020 7769 7468 2064 6566 6175 6c74 2076    with default v
-0001c210: 616c 7565 732e 2050 6c65 6173 6520 7365  alues. Please se
-0001c220: 6520 604d 6f45 436f 6e66 6967 602e 0a20  e `MoEConfig`.. 
-0001c230: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0001c240: 6c65 6c5f 636f 6e66 6967 284f 7050 6172  lel_config(OpPar
-0001c250: 616c 6c65 6c43 6f6e 6669 672c 204d 6f45  allelConfig, MoE
-0001c260: 5061 7261 6c6c 656c 436f 6e66 6967 293a  ParallelConfig):
-0001c270: 2054 6865 2070 6172 616c 6c65 6c20 636f   The parallel co
-0001c280: 6e66 6967 7572 652e 2057 6865 6e20 4d6f  nfigure. When Mo
-0001c290: 4520 6973 2061 7070 6c69 6564 2c0a 2020  E is applied,.  
-0001c2a0: 2020 2020 2020 2020 2020 2020 2020 4d6f                Mo
-0001c2b0: 4550 6172 616c 6c65 6c43 6f6e 6669 6720  EParallelConfig 
-0001c2c0: 6973 2065 6666 6563 7469 7665 2c20 6f74  is effective, ot
-0001c2d0: 6865 7277 6973 6520 4f70 5061 7261 6c6c  herwise OpParall
-0001c2e0: 656c 436f 6e66 6967 2069 7320 6566 6665  elConfig is effe
-0001c2f0: 6374 6976 652e 2044 6566 6175 6c74 2060  ctive. Default `
-0001c300: 6465 6661 756c 745f 6470 6d70 5f63 6f6e  default_dpmp_con
-0001c310: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
-0001c320: 2020 2020 2020 616e 2069 6e73 7461 6e63        an instanc
-0001c330: 6520 6f66 2060 4f70 5061 7261 6c6c 656c  e of `OpParallel
-0001c340: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
-0001c350: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
-0001c360: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
-0001c370: 2020 2020 2020 2020 2d20 2a2a 6869 6464          - **hidd
-0001c380: 656e 5f73 7461 7473 2a2a 2028 5465 6e73  en_stats** (Tens
-0001c390: 6f72 2920 2d20 5468 6520 696e 7075 7420  or) - The input 
-0001c3a0: 7465 6e73 6f72 2077 6974 6820 7368 6170  tensor with shap
-0001c3b0: 6520 5b62 6174 6368 5f73 697a 652c 2074  e [batch_size, t
-0001c3c0: 6774 5f73 6571 5f6c 656e 6774 682c 2068  gt_seq_length, h
-0001c3d0: 6964 6465 6e5f 7369 7a65 5d20 6f72 0a20  idden_size] or. 
-0001c3e0: 2020 2020 2020 2020 2020 2020 205b 6261               [ba
-0001c3f0: 7463 685f 7369 7a65 202a 2074 6774 5f73  tch_size * tgt_s
-0001c400: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0001c410: 6e5f 7369 7a65 5d2e 0a20 2020 2020 2020  n_size]..       
-0001c420: 2020 2020 202d 202a 2a64 6563 6f64 6572       - **decoder
-0001c430: 5f6d 6173 6b2a 2a20 2854 656e 736f 7229  _mask** (Tensor)
-0001c440: 202d 2054 6865 2061 7474 656e 7469 6f6e   - The attention
-0001c450: 206d 6173 6b20 666f 7220 6465 636f 6465   mask for decode
-0001c460: 7220 7769 7468 2073 6861 7065 205b 6261  r with shape [ba
-0001c470: 7463 685f 7369 7a65 2c20 7372 635f 7365  tch_size, src_se
-0001c480: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-0001c490: 2020 2020 2020 2020 7365 715f 6c65 6e67          seq_leng
-0001c4a0: 7468 5d20 6f72 204e 6f6e 652e 204e 6f6e  th] or None. Non
-0001c4b0: 6520 6d65 616e 7320 7468 6572 6520 7769  e means there wi
-0001c4c0: 6c6c 2062 6520 6e6f 206d 6173 6b20 696e  ll be no mask in
-0001c4d0: 2073 6f66 746d 6178 2063 6f6d 7075 7461   softmax computa
-0001c4e0: 7469 6f6e 2069 6e20 7365 6c66 2061 7474  tion in self att
-0001c4f0: 656e 7469 6f6e 2e0a 2020 2020 2020 2020  ention..        
-0001c500: 2020 2020 2d20 2a2a 656e 636f 6465 725f      - **encoder_
-0001c510: 6f75 7470 7574 2a2a 2028 5465 6e73 6f72  output** (Tensor
-0001c520: 2920 2d20 5468 6520 6f75 7470 7574 206f  ) - The output o
-0001c530: 6620 7468 6520 656e 636f 6465 7220 7769  f the encoder wi
-0001c540: 7468 2073 6861 7065 205b 6261 7463 685f  th shape [batch_
-0001c550: 7369 7a65 2c20 7365 715f 6c65 6e67 7468  size, seq_length
-0001c560: 2c20 6869 6464 656e 5f73 697a 655d 0a20  , hidden_size]. 
-0001c570: 2020 2020 2020 2020 2020 2020 206f 7220               or 
-0001c580: 5b62 6174 6368 5f73 697a 6520 2a20 7365  [batch_size * se
-0001c590: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-0001c5a0: 5f73 697a 655d 2e0a 2020 2020 2020 2020  _size]..        
-0001c5b0: 2020 2020 2020 4e6f 7465 2074 6869 7320        Note this 
-0001c5c0: 6172 6773 2063 616e 206e 6f74 2062 6520  args can not be 
-0001c5d0: 7061 7373 6564 2062 7920 4e6f 6e65 2077  passed by None w
-0001c5e0: 6865 6e20 7468 6520 6e65 7420 6973 2069  hen the net is i
-0001c5f0: 6e20 6f75 7465 726d 6f73 7420 6c61 7965  n outermost laye
-0001c600: 722e 2044 6566 6175 6c74 204e 6f6e 652e  r. Default None.
-0001c610: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-0001c620: 2a6d 656d 6f72 795f 6d61 736b 2a2a 2028  *memory_mask** (
-0001c630: 5465 6e73 6f72 2920 2d20 5468 6520 6d65  Tensor) - The me
-0001c640: 6d6f 7279 206d 6173 6b20 6f66 2074 6865  mory mask of the
-0001c650: 2063 726f 7373 2061 7474 656e 7469 6f6e   cross attention
-0001c660: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
-0001c670: 6368 2c20 7467 745f 7365 715f 6c65 6e67  ch, tgt_seq_leng
-0001c680: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
-0001c690: 2020 7372 635f 7365 715f 6c65 6e67 7468    src_seq_length
-0001c6a0: 5d20 7768 6572 6520 7467 745f 7365 715f  ] where tgt_seq_
-0001c6b0: 6c65 6e67 7468 2069 7320 7468 6520 6c65  length is the le
-0001c6c0: 6e67 7468 206f 6620 7468 6520 6465 636f  ngth of the deco
-0001c6d0: 6465 722e 2054 6865 2075 7365 7220 6361  der. The user ca
-0001c6e0: 6e20 616c 736f 2070 6173 7320 4e6f 6e65  n also pass None
-0001c6f0: 2e20 4e6f 6e65 0a20 2020 2020 2020 2020  . None.         
-0001c700: 2020 2020 206d 6561 6e73 2074 6865 7265       means there
-0001c710: 2077 696c 6c20 6265 206e 6f20 6d61 736b   will be no mask
-0001c720: 2069 6e20 736f 6674 6d61 7820 636f 6d70   in softmax comp
-0001c730: 7574 6174 696f 6e20 696e 2063 726f 7373  utation in cross
-0001c740: 2061 7474 656e 7469 6f6e 2e20 4465 6661   attention. Defa
-0001c750: 756c 7420 4e6f 6e65 2e0a 2020 2020 2020  ult None..      
-0001c760: 2020 2020 2020 2d20 2a2a 696e 6974 5f72        - **init_r
-0001c770: 6573 6574 2a2a 2028 5465 6e73 6f72 2920  eset** (Tensor) 
-0001c780: 2d20 4120 626f 6f6c 2074 656e 736f 7220  - A bool tensor 
-0001c790: 7769 7468 2073 6861 7065 205b 315d 2c20  with shape [1], 
-0001c7a0: 7573 6564 2074 6f20 636c 6561 7220 7468  used to clear th
-0001c7b0: 6520 7061 7374 206b 6579 2070 6172 616d  e past key param
-0001c7c0: 6574 6572 2061 6e64 0a20 2020 2020 2020  eter and.       
-0001c7d0: 2020 2020 2020 2070 6173 7420 7661 6c75         past valu
-0001c7e0: 6520 7061 7261 6d65 7465 7220 7573 6564  e parameter used
-0001c7f0: 2069 6e20 7468 6520 696e 6372 656d 656e   in the incremen
-0001c800: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
-0001c810: 4f6e 6c79 2076 616c 6964 2077 6865 6e20  Only valid when 
-0001c820: 7573 655f 7061 7374 2069 7320 5472 7565  use_past is True
-0001c830: 2e20 4465 6661 756c 7420 5472 7565 2e0a  . Default True..
-0001c840: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
-0001c850: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-0001c860: 7468 2a2a 2028 5465 6e73 6f72 2920 2d20  th** (Tensor) - 
-0001c870: 496e 7433 3220 7465 6e73 6f72 2077 6974  Int32 tensor wit
-0001c880: 6820 7368 6170 6520 5b62 6174 6368 5f73  h shape [batch_s
-0001c890: 697a 655d 2074 6865 2070 6173 7420 6361  ize] the past ca
-0001c8a0: 6c63 756c 6174 6564 2074 6865 2069 6e64  lculated the ind
-0001c8b0: 6578 2e0a 2020 2020 2020 2020 2020 2020  ex..            
-0001c8c0: 2020 5573 6564 2066 6f72 2069 6e63 7265    Used for incre
-0001c8d0: 6d65 6e74 616c 2070 7265 6469 6374 696f  mental predictio
-0001c8e0: 6e20 7768 656e 2074 6865 2075 7365 5f70  n when the use_p
-0001c8f0: 6173 7420 6973 2054 7275 652e 2044 6566  ast is True. Def
-0001c900: 6175 6c74 204e 6f6e 652e 0a0a 2020 2020  ault None...    
-0001c910: 2020 2020 4f75 7470 7574 733a 0a20 2020      Outputs:.   
-0001c920: 2020 2020 2020 2020 2054 7570 6c65 2c20           Tuple, 
-0001c930: 6120 7475 706c 6520 636f 6e74 6169 6e73  a tuple contains
-0001c940: 2860 6f75 7470 7574 602c 2060 6c61 7965  (`output`, `laye
-0001c950: 725f 7072 6573 656e 7460 290a 0a20 2020  r_present`)..   
-0001c960: 2020 2020 2020 2020 202d 202a 2a6f 7574           - **out
-0001c970: 7075 742a 2a20 2854 656e 736f 7229 202d  put** (Tensor) -
-0001c980: 2054 6865 206f 7574 7075 7420 6c6f 6769   The output logi
-0001c990: 7420 6f66 2074 6869 7320 6c61 7965 722e  t of this layer.
-0001c9a0: 2054 6865 2073 6861 7065 2069 7320 5b62   The shape is [b
-0001c9b0: 6174 6368 2c20 7365 715f 6c65 6e67 7468  atch, seq_length
-0001c9c0: 2c20 6869 6464 656e 5f73 697a 655d 206f  , hidden_size] o
-0001c9d0: 720a 2020 2020 2020 2020 2020 2020 2020  r.              
-0001c9e0: 5b62 6174 6368 202a 2073 6571 5f6c 656e  [batch * seq_len
-0001c9f0: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0001ca00: 5d2e 0a20 2020 2020 2020 2020 2020 202d  ]..            -
-0001ca10: 202a 2a6c 6179 6572 5f70 7265 7365 6e74   **layer_present
-0001ca20: 2a2a 2028 5475 706c 6529 202d 2041 2074  ** (Tuple) - A t
-0001ca30: 7570 6c65 2c20 7768 6572 6520 6561 6368  uple, where each
-0001ca40: 2074 7570 6c65 2069 7320 7468 6520 7465   tuple is the te
-0001ca50: 6e73 6f72 206f 6620 7468 6520 7072 6f6a  nsor of the proj
-0001ca60: 6563 7465 6420 6b65 7920 616e 6420 7661  ected key and va
-0001ca70: 6c75 650a 2020 2020 2020 2020 2020 2020  lue.            
-0001ca80: 2020 7665 6374 6f72 2069 6e20 7365 6c66    vector in self
-0001ca90: 2061 7474 656e 7469 6f6e 2077 6974 6820   attention with 
-0001caa0: 7368 6170 6520 2828 6261 7463 685f 7369  shape ((batch_si
-0001cab0: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
-0001cac0: 697a 655f 7065 725f 6865 6164 2c20 7467  ize_per_head, tg
-0001cad0: 745f 7365 715f 6c65 6e67 7468 292c 0a20  t_seq_length),. 
-0001cae0: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
-0001caf0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
-0001cb00: 6164 732c 2074 6774 5f73 6571 5f6c 656e  ads, tgt_seq_len
-0001cb10: 6774 682c 2073 697a 655f 7065 725f 6865  gth, size_per_he
-0001cb20: 6164 292c 2061 6e64 206f 6620 7468 6520  ad), and of the 
-0001cb30: 7072 6f6a 6563 7465 6420 6b65 7920 616e  projected key an
-0001cb40: 6420 7661 6c75 6520 7665 6374 6f72 0a20  d value vector. 
-0001cb50: 2020 2020 2020 2020 2020 2020 2069 6e20               in 
-0001cb60: 6372 6f73 7320 6174 7465 6e74 696f 6e20  cross attention 
-0001cb70: 7769 7468 2073 6861 7065 2020 2862 6174  with shape  (bat
-0001cb80: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
-0001cb90: 6473 2c20 7369 7a65 5f70 6572 5f68 6561  ds, size_per_hea
-0001cba0: 642c 2073 7263 5f73 6571 5f6c 656e 6774  d, src_seq_lengt
-0001cbb0: 6829 2c0a 2020 2020 2020 2020 2020 2020  h),.            
-0001cbc0: 2020 2862 6174 6368 5f73 697a 652c 206e    (batch_size, n
-0001cbd0: 756d 5f68 6561 6473 2c20 7372 635f 7365  um_heads, src_se
-0001cbe0: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-0001cbf0: 6572 5f68 6561 6429 292e 0a0a 2020 2020  er_head))...    
-0001cc00: 2020 2020 5375 7070 6f72 7465 6420 506c      Supported Pl
-0001cc10: 6174 666f 726d 733a 0a20 2020 2020 2020  atforms:.       
-0001cc20: 2020 2020 2060 6041 7363 656e 6460 6020       ``Ascend`` 
-0001cc30: 6060 4750 5560 600a 0a20 2020 2020 2020  ``GPU``..       
-0001cc40: 2045 7861 6d70 6c65 733a 0a20 2020 2020   Examples:.     
-0001cc50: 2020 2020 2020 203e 3e3e 2069 6d70 6f72         >>> impor
-0001cc60: 7420 6e75 6d70 7920 6173 206e 700a 2020  t numpy as np.  
-0001cc70: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
-0001cc80: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
-0001cc90: 6f72 7420 6474 7970 6520 6173 206d 7374  ort dtype as mst
-0001cca0: 7970 650a 2020 2020 2020 2020 2020 2020  ype.            
-0001ccb0: 3e3e 3e20 6672 6f6d 206d 696e 6466 6f72  >>> from mindfor
-0001ccc0: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
-0001ccd0: 6e73 666f 726d 6572 2069 6d70 6f72 7420  nsformer import 
-0001cce0: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
-0001ccf0: 6572 4c61 7965 720a 2020 2020 2020 2020  erLayer.        
-0001cd00: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
-0001cd10: 6473 706f 7265 2069 6d70 6f72 7420 5465  dspore import Te
-0001cd20: 6e73 6f72 0a20 2020 2020 2020 2020 2020  nsor.           
-0001cd30: 203e 3e3e 206d 6f64 656c 203d 2054 7261   >>> model = Tra
-0001cd40: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
-0001cd50: 6179 6572 2862 6174 6368 5f73 697a 653d  ayer(batch_size=
-0001cd60: 322c 2068 6964 6465 6e5f 7369 7a65 3d36  2, hidden_size=6
-0001cd70: 342c 2066 666e 5f68 6964 6465 6e5f 7369  4, ffn_hidden_si
-0001cd80: 7a65 3d36 342c 206e 756d 5f68 6561 6473  ze=64, num_heads
-0001cd90: 3d32 2c0a 2020 2020 2020 2020 2020 2020  =2,.            
-0001cda0: 2e2e 2e20 2020 2020 2020 2020 2020 2020  ...             
-0001cdb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cdc0: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
-0001cdd0: 7468 3d32 302c 2074 6774 5f73 6571 5f6c  th=20, tgt_seq_l
-0001cde0: 656e 6774 683d 3130 290a 2020 2020 2020  ength=10).      
-0001cdf0: 2020 2020 2020 3e3e 3e20 656e 636f 6465        >>> encode
-0001ce00: 725f 696e 7075 745f 7661 6c75 6520 3d20  r_input_value = 
-0001ce10: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
-0001ce20: 322c 2032 302c 2036 3429 292c 206d 7374  2, 20, 64)), mst
-0001ce30: 7970 652e 666c 6f61 7433 3229 0a20 2020  ype.float32).   
-0001ce40: 2020 2020 2020 2020 203e 3e3e 2064 6563           >>> dec
-0001ce50: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
-0001ce60: 203d 2054 656e 736f 7228 6e70 2e6f 6e65   = Tensor(np.one
-0001ce70: 7328 2832 2c20 3130 2c20 3634 2929 2c20  s((2, 10, 64)), 
-0001ce80: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
-0001ce90: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-0001cea0: 6465 636f 6465 725f 696e 7075 745f 6d61  decoder_input_ma
-0001ceb0: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
-0001cec0: 6e65 7328 2832 2c20 3130 2c20 3130 2929  nes((2, 10, 10))
-0001ced0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-0001cee0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-0001cef0: 3e20 6d65 6d6f 7279 5f6d 6173 6b20 3d20  > memory_mask = 
-0001cf00: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
-0001cf10: 322c 2031 302c 2032 3029 292c 206d 7374  2, 10, 20)), mst
-0001cf20: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
-0001cf30: 2020 2020 2020 2020 203e 3e3e 206f 7574           >>> out
-0001cf40: 7075 742c 2070 6173 7420 3d20 6d6f 6465  put, past = mode
-0001cf50: 6c28 6465 636f 6465 725f 696e 7075 745f  l(decoder_input_
-0001cf60: 7661 6c75 652c 2064 6563 6f64 6572 5f69  value, decoder_i
-0001cf70: 6e70 7574 5f6d 6173 6b2c 2065 6e63 6f64  nput_mask, encod
-0001cf80: 6572 5f69 6e70 7574 5f76 616c 7565 2c20  er_input_value, 
-0001cf90: 6d65 6d6f 7279 5f6d 6173 6b29 0a20 2020  memory_mask).   
-0001cfa0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-0001cfb0: 6e74 286f 7574 7075 742e 7368 6170 6529  nt(output.shape)
-0001cfc0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-0001cfd0: 2031 302c 2036 3429 0a20 2020 2020 2020   10, 64).       
-0001cfe0: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-0001cff0: 6173 745b 305d 2e73 6861 7065 290a 2020  ast[0].shape).  
-0001d000: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-0001d010: 2033 322c 2031 3029 0a20 2020 2020 2020   32, 10).       
-0001d020: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-0001d030: 6173 745b 315d 2e73 6861 7065 290a 2020  ast[1].shape).  
-0001d040: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-0001d050: 2031 302c 2033 3229 0a20 2020 2020 2020   10, 32).       
-0001d060: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-0001d070: 6173 745b 325d 2e73 6861 7065 290a 2020  ast[2].shape).  
-0001d080: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-0001d090: 2033 322c 2032 3029 0a20 2020 2020 2020   32, 20).       
-0001d0a0: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-0001d0b0: 6173 745b 335d 2e73 6861 7065 290a 2020  ast[3].shape).  
-0001d0c0: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-0001d0d0: 2032 302c 2033 3229 0a20 2020 2022 2222   20, 32).    """
-0001d0e0: 0a0a 2020 2020 405f 4c6f 6741 6374 696f  ..    @_LogActio
-0001d0f0: 6e4f 6e63 6528 6d5f 6c6f 6767 6572 3d6c  nOnce(m_logger=l
-0001d100: 6f67 6765 722c 206b 6579 3d27 5472 616e  ogger, key='Tran
-0001d110: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
-0001d120: 7965 7227 2c0a 2020 2020 2020 2020 2020  yer',.          
-0001d130: 2020 2020 2020 2020 2020 6e6f 5f77 6172            no_war
-0001d140: 6e69 6e67 3d5f 6765 745f 7061 7261 6c6c  ning=_get_parall
-0001d150: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
-0001d160: 7261 6c6c 656c 4d6f 6465 2e53 5441 4e44  rallelMode.STAND
-0001d170: 5f41 4c4f 4e45 2c29 290a 2020 2020 405f  _ALONE,)).    @_
-0001d180: 6172 6773 5f74 7970 655f 7661 6c69 6461  args_type_valida
-0001d190: 746f 725f 6368 6563 6b28 6869 6464 656e  tor_check(hidden
-0001d1a0: 5f73 697a 653d 5661 6c69 6461 746f 722e  _size=Validator.
-0001d1b0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
-0001d1c0: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-0001d1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d1e0: 2020 2020 6e75 6d5f 6865 6164 733d 5661      num_heads=Va
-0001d1f0: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
-0001d200: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
-0001d210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d220: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
-0001d230: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
-0001d240: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-0001d250: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-0001d260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d270: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-0001d280: 715f 6c65 6e67 7468 3d56 616c 6964 6174  q_length=Validat
-0001d290: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
-0001d2a0: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
-0001d2b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d2c0: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
-0001d2d0: 656e 6774 683d 5661 6c69 6461 746f 722e  ength=Validator.
-0001d2e0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
-0001d2f0: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-0001d300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d310: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-0001d320: 6f70 6f75 745f 7261 7465 3d56 616c 6964  opout_rate=Valid
-0001d330: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
-0001d340: 6567 6174 6976 655f 666c 6f61 742c 0a20  egative_float,. 
-0001d350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d360: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-0001d370: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-0001d380: 7465 3d56 616c 6964 6174 6f72 2e63 6865  te=Validator.che
-0001d390: 636b 5f6e 6f6e 5f6e 6567 6174 6976 655f  ck_non_negative_
-0001d3a0: 666c 6f61 742c 0a20 2020 2020 2020 2020  float,.         
-0001d3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d3c0: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
-0001d3d0: 726e 6f72 6d5f 7265 7369 6475 616c 3d56  rnorm_residual=V
-0001d3e0: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
-0001d3f0: 6f6f 6c2c 0a20 2020 2020 2020 2020 2020  ool,.           
-0001d400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d410: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
-0001d420: 6f6d 7075 7465 5f74 7970 653d 5f76 616c  ompute_type=_val
-0001d430: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
-0001d440: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-0001d450: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019b10: 6c65 6c2c 2031 2c20 3129 2c20 2870 6172  lel, 1, 1), (par
+00019b20: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00019b30: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
+00019b40: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+00019b50: 7365 6c66 2e64 7479 7065 203d 206d 7374  self.dtype = mst
+00019b60: 7970 652e 666c 6f61 7431 360a 2020 2020  ype.float16.    
+00019b70: 2020 2020 2020 2020 7365 6c66 2e6b 6579          self.key
+00019b80: 5f70 6173 7420 3d20 4e6f 6e65 0a20 2020  _past = None.   
+00019b90: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
+00019ba0: 6c75 655f 7061 7374 203d 204e 6f6e 650a  lue_past = None.
+00019bb0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00019bc0: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
+00019bd0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00019be0: 206f 7065 7261 746f 7220 7573 6564 2066   operator used f
+00019bf0: 6f72 2073 7461 7465 2072 6575 7365 0a20  or state reuse. 
+00019c00: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00019c10: 656c 662e 7265 6475 6365 7375 6d20 3d20  elf.reducesum = 
+00019c20: 502e 5265 6475 6365 5375 6d28 292e 7368  P.ReduceSum().sh
+00019c30: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+00019c40: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00019c50: 2020 2020 2073 656c 662e 6e6f 745f 6571       self.not_eq
+00019c60: 7561 6c20 3d20 502e 4e6f 7445 7175 616c  ual = P.NotEqual
+00019c70: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+00019c80: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
+00019c90: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00019ca0: 662e 736c 6963 6520 3d20 502e 5374 7269  f.slice = P.Stri
+00019cb0: 6465 6453 6c69 6365 2829 2e73 6861 7264  dedSlice().shard
+00019cc0: 2828 2831 2c20 312c 2031 2c20 3129 2c29  (((1, 1, 1, 1),)
+00019cd0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00019ce0: 2020 7369 7a65 5f70 6572 5f68 6561 6420    size_per_head 
+00019cf0: 3d20 6869 6464 656e 5f73 697a 6520 2f2f  = hidden_size //
+00019d00: 206e 756d 5f68 6561 6473 0a20 2020 2020   num_heads.     
+00019d10: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00019d20: 6b65 795f 7368 6170 6520 3d20 2862 6174  key_shape = (bat
+00019d30: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
+00019d40: 6473 2c20 7369 7a65 5f70 6572 5f68 6561  ds, size_per_hea
+00019d50: 642c 2073 6571 5f6c 656e 6774 6829 0a20  d, seq_length). 
+00019d60: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00019d70: 656c 662e 7661 6c75 655f 7368 6170 6520  elf.value_shape 
+00019d80: 3d20 2862 6174 6368 5f73 697a 652c 206e  = (batch_size, n
+00019d90: 756d 5f68 6561 6473 2c20 7365 715f 6c65  um_heads, seq_le
+00019da0: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
+00019db0: 6561 6429 0a20 2020 2020 2020 2020 2020  ead).           
+00019dc0: 2020 2020 2023 2070 6172 616d 6574 6572       # parameter
+00019dd0: 7320 7361 7669 6e67 206b 6579 2061 6e64  s saving key and
+00019de0: 2076 616c 7565 2073 7461 7465 730a 2020   value states.  
+00019df0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00019e00: 6c66 2e6b 6579 5f70 6173 7420 3d20 5061  lf.key_past = Pa
+00019e10: 7261 6d65 7465 7228 5465 6e73 6f72 286e  rameter(Tensor(n
+00019e20: 702e 7a65 726f 7328 7368 6170 653d 7365  p.zeros(shape=se
+00019e30: 6c66 2e6b 6579 5f73 6861 7065 292c 2073  lf.key_shape), s
+00019e40: 656c 662e 6474 7970 6529 2c20 6e61 6d65  elf.dtype), name
+00019e50: 3d22 6b65 795f 7061 7374 2229 0a20 2020  ="key_past").   
+00019e60: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00019e70: 662e 7661 6c75 655f 7061 7374 203d 2050  f.value_past = P
+00019e80: 6172 616d 6574 6572 2854 656e 736f 7228  arameter(Tensor(
+00019e90: 6e70 2e7a 6572 6f73 2873 6861 7065 3d73  np.zeros(shape=s
+00019ea0: 656c 662e 7661 6c75 655f 7368 6170 6529  elf.value_shape)
+00019eb0: 2c20 7365 6c66 2e64 7479 7065 292c 206e  , self.dtype), n
+00019ec0: 616d 653d 2276 616c 7565 5f70 6173 7422  ame="value_past"
+00019ed0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00019ee0: 2020 7365 6c66 2e74 696c 6520 3d20 502e    self.tile = P.
+00019ef0: 5469 6c65 2829 2e73 6861 7264 2828 2831  Tile().shard(((1
+00019f00: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+00019f10: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
+00019f20: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
+00019f30: 2828 2831 2c20 312c 2031 2c20 3129 2c20  (((1, 1, 1, 1), 
+00019f40: 2831 2c29 2929 0a20 2020 2020 2020 2020  (1,))).         
+00019f50: 2020 2020 2020 2073 656c 662e 6173 7369         self.assi
+00019f60: 676e 203d 2050 2e41 7373 6967 6e28 292e  gn = P.Assign().
+00019f70: 7368 6172 6428 2828 312c 2031 2c20 312c  shard(((1, 1, 1,
+00019f80: 2031 292c 2028 312c 2031 2c20 312c 2031   1), (1, 1, 1, 1
+00019f90: 2929 290a 0a20 2020 2020 2020 2020 2020  )))..           
+00019fa0: 2069 6620 7061 7261 6c6c 656c 5f63 6f6e   if parallel_con
+00019fb0: 6669 672e 7573 655f 7365 715f 7061 7261  fig.use_seq_para
+00019fc0: 6c6c 656c 3a0a 2020 2020 2020 2020 2020  llel:.          
+00019fd0: 2020 2020 2020 7365 6c66 2e61 6464 2e73        self.add.s
+00019fe0: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+00019ff0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+0001a000: 6c6c 656c 202a 2070 6172 616c 6c65 6c5f  llel * parallel_
+0001a010: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0001a020: 616c 6c65 6c2c 2031 292c 0a20 2020 2020  allel, 1),.     
+0001a030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a040: 2020 2020 2020 2020 2020 2028 7061 7261             (para
+0001a050: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+0001a060: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
+0001a070: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+0001a080: 6c5f 7061 7261 6c6c 656c 2c20 3129 2929  l_parallel, 1)))
+0001a090: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001a0a0: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
+0001a0b0: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
+0001a0c0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+0001a0d0: 7261 6c6c 656c 202a 2070 6172 616c 6c65  rallel * paralle
+0001a0e0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0001a0f0: 6172 616c 6c65 6c2c 2031 292c 2929 0a20  arallel, 1),)). 
+0001a100: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0001a110: 656c 662e 6c61 7965 726e 6f72 6d32 2e73  elf.layernorm2.s
+0001a120: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+0001a130: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+0001a140: 6c6c 656c 202a 2070 6172 616c 6c65 6c5f  llel * parallel_
+0001a150: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0001a160: 616c 6c65 6c2c 2031 292c 2929 0a20 2020  allel, 1),)).   
+0001a170: 2020 2020 2020 2020 2020 2020 2069 6620               if 
+0001a180: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001a190: 7265 636f 6d70 7574 652e 7365 6c65 6374  recompute.select
+0001a1a0: 5f72 6563 6f6d 7075 7465 3a0a 2020 2020  _recompute:.    
+0001a1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a1c0: 2320 e6ad a4e5 a484 e4bc 9ae6 b688 e880  # ..............
+0001a1d0: 97e8 be83 e5a4 a7e5 8685 e5ad 98ef bc8c  ................
+0001a1e0: e5bc 80e5 90af e590 8ee4 bc9a e68d 9fe5  ................
+0001a1f0: a4b1 e4b8 80e9 83a8 e588 86e8 aea1 e7ae  ................
+0001a200: 97e6 80a7 e883 bd0a 2020 2020 2020 2020  ........        
+0001a210: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001a220: 2e6c 6179 6572 6e6f 726d 322e 6c61 7965  .layernorm2.laye
+0001a230: 725f 6e6f 726d 2e72 6563 6f6d 7075 7465  r_norm.recompute
+0001a240: 2829 0a20 2020 2020 2020 2020 2020 2020  ().             
+0001a250: 2020 2069 6620 6e6f 7420 7365 6c66 2e75     if not self.u
+0001a260: 7365 5f6d 6f65 3a0a 2020 2020 2020 2020  se_moe:.        
+0001a270: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001a280: 2e6f 7574 7075 742e 7072 6f6a 6563 7469  .output.projecti
+0001a290: 6f6e 2e73 6861 7264 280a 2020 2020 2020  on.shard(.      
+0001a2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a2b0: 2020 7374 7261 7465 6779 5f62 6961 733d    strategy_bias=
+0001a2c0: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+0001a2d0: 672e 6461 7461 5f70 6172 616c 6c65 6c20  g.data_parallel 
+0001a2e0: 2a20 7061 7261 6c6c 656c 5f63 6f6e 6669  * parallel_confi
+0001a2f0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+0001a300: 2c20 3129 2c20 2831 2c29 292c 0a20 2020  , 1), (1,)),.   
+0001a310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a320: 2020 2020 2073 7472 6174 6567 795f 6d61       strategy_ma
+0001a330: 746d 756c 3d28 2870 6172 616c 6c65 6c5f  tmul=((parallel_
+0001a340: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+0001a350: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+0001a360: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001a370: 6c6c 656c 292c 0a20 2020 2020 2020 2020  llel),.         
+0001a380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a3a0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+0001a3b0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+0001a3c0: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
+0001a3d0: 2020 2020 2020 2020 2020 2020 2020 6f75                ou
+0001a3e0: 745f 7374 7261 7465 6779 5f6d 6174 6d75  t_strategy_matmu
+0001a3f0: 6c3d 2828 7061 7261 6c6c 656c 5f63 6f6e  l=((parallel_con
+0001a400: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+0001a410: 6c20 2a20 7061 7261 6c6c 656c 5f63 6f6e  l * parallel_con
+0001a420: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001a430: 656c 2c20 3129 2c29 290a 2020 2020 2020  el, 1),)).      
+0001a440: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001a450: 6c66 2e6f 7574 7075 742e 6472 6f70 6f75  lf.output.dropou
+0001a460: 742e 6472 6f70 6f75 742e 7368 6172 6428  t.dropout.shard(
+0001a470: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001a480: 2020 2020 2020 2020 2028 2870 6172 616c           ((paral
+0001a490: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+0001a4a0: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
+0001a4b0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0001a4c0: 5f70 6172 616c 6c65 6c2c 2031 292c 2929  _parallel, 1),))
+0001a4d0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
+0001a4e0: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+0001a4f0: 2052 756e 7469 6d65 4572 726f 7228 6622   RuntimeError(f"
+0001a500: 5468 6520 7b73 656c 662e 636c 735f 6e61  The {self.cls_na
+0001a510: 6d65 7d20 6f6e 6c79 2073 7570 706f 7274  me} only support
+0001a520: 2073 6861 7264 696e 6720 7072 6f70 6167   sharding propag
+0001a530: 6174 696f 6e20 6f72 2022 0a20 2020 2020  ation or ".     
+0001a540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a550: 2020 2020 2020 2020 2020 6622 7365 6d69            f"semi
+0001a560: 2d61 7574 6f20 7061 7261 6c6c 656c 206d  -auto parallel m
+0001a570: 6f64 6520 6e6f 772e 2229 0a0a 2020 2020  ode now.")..    
+0001a580: 6465 6620 636f 6e73 7472 7563 7428 7365  def construct(se
+0001a590: 6c66 2c20 782c 2069 6e70 7574 5f6d 6173  lf, x, input_mas
+0001a5a0: 6b3d 4e6f 6e65 2c20 696e 6974 5f72 6573  k=None, init_res
+0001a5b0: 6574 3d54 7275 652c 2062 6174 6368 5f76  et=True, batch_v
+0001a5c0: 616c 6964 5f6c 656e 6774 683d 4e6f 6e65  alid_length=None
+0001a5d0: 293a 0a20 2020 2020 2020 2022 2222 666f  ):.        """fo
+0001a5e0: 7277 6172 6420 7072 6f63 6573 7322 2222  rward process"""
+0001a5f0: 0a20 2020 2020 2020 2073 656c 662e 5f63  .        self._c
+0001a600: 6865 636b 5f69 6e70 7574 2878 2c20 696e  heck_input(x, in
+0001a610: 7075 745f 6d61 736b 2c20 696e 6974 5f72  put_mask, init_r
+0001a620: 6573 6574 2c20 6261 7463 685f 7661 6c69  eset, batch_vali
+0001a630: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
+0001a640: 2020 785f 7368 6170 6520 3d20 462e 7368    x_shape = F.sh
+0001a650: 6170 6528 7829 0a20 2020 2020 2020 2078  ape(x).        x
+0001a660: 203d 2046 2e72 6573 6861 7065 2878 2c20   = F.reshape(x, 
+0001a670: 282d 312c 2078 5f73 6861 7065 5b2d 315d  (-1, x_shape[-1]
+0001a680: 2929 0a20 2020 2020 2020 2069 6620 7365  )).        if se
+0001a690: 6c66 2e70 6f73 745f 6c61 7965 726e 6f72  lf.post_layernor
+0001a6a0: 6d5f 7265 7369 6475 616c 3a0a 2020 2020  m_residual:.    
+0001a6b0: 2020 2020 2020 2020 696e 7075 745f 7820          input_x 
+0001a6c0: 3d20 780a 2020 2020 2020 2020 656c 7365  = x.        else
+0001a6d0: 3a0a 2020 2020 2020 2020 2020 2020 696e  :.            in
+0001a6e0: 7075 745f 7820 3d20 7365 6c66 2e6c 6179  put_x = self.lay
+0001a6f0: 6572 6e6f 726d 3128 7829 0a20 2020 2020  ernorm1(x).     
+0001a700: 2020 2069 6e70 7574 5f78 203d 2046 2e63     input_x = F.c
+0001a710: 6173 7428 696e 7075 745f 782c 2073 656c  ast(input_x, sel
+0001a720: 662e 6474 7970 6529 0a0a 2020 2020 2020  f.dtype)..      
+0001a730: 2020 2320 696e 6469 6361 7465 2077 6865    # indicate whe
+0001a740: 7468 6572 2072 6573 6574 2073 6176 6564  ther reset saved
+0001a750: 2073 7461 7465 730a 2020 2020 2020 2020   states.        
+0001a760: 6b65 795f 7265 7365 7420 3d20 4e6f 6e65  key_reset = None
+0001a770: 0a20 2020 2020 2020 2076 616c 7565 5f72  .        value_r
+0001a780: 6573 6574 203d 204e 6f6e 650a 0a20 2020  eset = None..   
+0001a790: 2020 2020 2069 6620 7365 6c66 2e75 7365       if self.use
+0001a7a0: 5f70 6173 743a 0a20 2020 2020 2020 2020  _past:.         
+0001a7b0: 2020 2023 2072 6573 6574 2073 7461 7465     # reset state
+0001a7c0: 732c 2069 6e69 745f 7265 7365 7420 5472  s, init_reset Tr
+0001a7d0: 7565 2066 6f72 2072 6575 7365 2061 6e64  ue for reuse and
+0001a7e0: 2046 616c 7365 2066 6f72 2072 6573 6574   False for reset
+0001a7f0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0001a800: 662e 6173 7369 676e 2873 656c 662e 6b65  f.assign(self.ke
+0001a810: 795f 7061 7374 2c20 7365 6c66 2e6d 756c  y_past, self.mul
+0001a820: 2873 656c 662e 6b65 795f 7061 7374 2c20  (self.key_past, 
+0001a830: 462e 6361 7374 2869 6e69 745f 7265 7365  F.cast(init_rese
+0001a840: 742c 2073 656c 662e 6474 7970 6529 2929  t, self.dtype)))
+0001a850: 0a20 2020 2020 2020 2020 2020 206b 6579  .            key
+0001a860: 5f72 6573 6574 203d 2073 656c 662e 6b65  _reset = self.ke
+0001a870: 795f 7061 7374 0a20 2020 2020 2020 2020  y_past.         
+0001a880: 2020 2073 656c 662e 6173 7369 676e 2873     self.assign(s
+0001a890: 656c 662e 7661 6c75 655f 7061 7374 2c20  elf.value_past, 
+0001a8a0: 7365 6c66 2e6d 756c 2873 656c 662e 7661  self.mul(self.va
+0001a8b0: 6c75 655f 7061 7374 2c20 462e 6361 7374  lue_past, F.cast
+0001a8c0: 2869 6e69 745f 7265 7365 742c 2073 656c  (init_reset, sel
+0001a8d0: 662e 6474 7970 6529 2929 0a20 2020 2020  f.dtype))).     
+0001a8e0: 2020 2020 2020 2076 616c 7565 5f72 6573         value_res
+0001a8f0: 6574 203d 2073 656c 662e 7661 6c75 655f  et = self.value_
+0001a900: 7061 7374 0a20 2020 2020 2020 2020 2020  past.           
+0001a910: 2023 2061 6464 2064 6570 656e 6465 6e63   # add dependenc
+0001a920: 7920 666f 7220 6465 7369 7265 6420 6578  y for desired ex
+0001a930: 6563 7574 696f 6e20 6f72 6465 720a 2020  ecution order.  
+0001a940: 2020 2020 2020 2020 2020 696e 7075 745f            input_
+0001a950: 7820 3d20 462e 6465 7065 6e64 2869 6e70  x = F.depend(inp
+0001a960: 7574 5f78 2c20 6b65 795f 7265 7365 7429  ut_x, key_reset)
+0001a970: 0a20 2020 2020 2020 2020 2020 2069 6e70  .            inp
+0001a980: 7574 5f78 203d 2046 2e64 6570 656e 6428  ut_x = F.depend(
+0001a990: 696e 7075 745f 782c 2076 616c 7565 5f72  input_x, value_r
+0001a9a0: 6573 6574 290a 0a20 2020 2020 2020 2061  eset)..        a
+0001a9b0: 7474 656e 7469 6f6e 2c20 6c61 7965 725f  ttention, layer_
+0001a9c0: 7072 6573 656e 7420 3d20 7365 6c66 2e61  present = self.a
+0001a9d0: 7474 656e 7469 6f6e 2869 6e70 7574 5f78  ttention(input_x
+0001a9e0: 2c20 696e 7075 745f 782c 2069 6e70 7574  , input_x, input
+0001a9f0: 5f78 2c20 696e 7075 745f 6d61 736b 2c0a  _x, input_mask,.
+0001aa00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aa10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aa20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aa30: 2020 7365 6c66 2e6b 6579 5f70 6173 742c    self.key_past,
+0001aa40: 2073 656c 662e 7661 6c75 655f 7061 7374   self.value_past
+0001aa50: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+0001aa60: 6e67 7468 290a 2020 2020 2020 2020 2320  ngth).        # 
+0001aa70: 466f 7220 706f 7374 2d6c 6179 6572 6e6f  For post-layerno
+0001aa80: 726d 2074 6865 2069 6e70 7574 7320 666f  rm the inputs fo
+0001aa90: 7220 7265 7369 6475 616c 2070 6174 6820  r residual path 
+0001aaa0: 6172 6520 6f75 7470 7574 206f 6620 7365  are output of se
+0001aab0: 6c66 2d61 7474 656e 7469 6f6e 2061 6e64  lf-attention and
+0001aac0: 206f 7574 7075 7420 6f66 206c 6179 6572   output of layer
+0001aad0: 6e6f 726d 0a20 2020 2020 2020 2069 6620  norm.        if 
+0001aae0: 7365 6c66 2e70 6f73 745f 6c61 7965 726e  self.post_layern
+0001aaf0: 6f72 6d5f 7265 7369 6475 616c 3a0a 2020  orm_residual:.  
+0001ab00: 2020 2020 2020 2020 2020 7820 3d20 7365            x = se
+0001ab10: 6c66 2e61 6464 2869 6e70 7574 5f78 2c20  lf.add(input_x, 
+0001ab20: 6174 7465 6e74 696f 6e29 0a20 2020 2020  attention).     
+0001ab30: 2020 2023 2046 6f72 2070 7265 2d6c 6179     # For pre-lay
+0001ab40: 6572 6e6f 726d 2074 6865 2069 6e70 7574  ernorm the input
+0001ab50: 7320 666f 7220 7265 7369 6475 616c 2070  s for residual p
+0001ab60: 6174 6820 6172 6520 6f75 7470 7574 206f  ath are output o
+0001ab70: 6620 7365 6c66 2d61 7474 656e 7469 6f6e  f self-attention
+0001ab80: 2061 6e64 2069 6e70 7574 206f 6620 7468   and input of th
+0001ab90: 6973 206c 6179 6572 0a20 2020 2020 2020  is layer.       
+0001aba0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+0001abb0: 2020 2078 203d 2073 656c 662e 6164 6428     x = self.add(
+0001abc0: 782c 2061 7474 656e 7469 6f6e 290a 0a20  x, attention).. 
+0001abd0: 2020 2020 2020 206f 7574 7075 745f 7820         output_x 
+0001abe0: 3d20 7365 6c66 2e6c 6179 6572 6e6f 726d  = self.layernorm
+0001abf0: 3228 7829 0a20 2020 2020 2020 206f 7574  2(x).        out
+0001ac00: 7075 745f 7820 3d20 462e 6361 7374 286f  put_x = F.cast(o
+0001ac10: 7574 7075 745f 782c 2073 656c 662e 6474  utput_x, self.dt
+0001ac20: 7970 6529 0a20 2020 2020 2020 2061 7578  ype).        aux
+0001ac30: 5f6c 6f73 7320 3d20 4e6f 6e65 0a20 2020  _loss = None.   
+0001ac40: 2020 2020 2069 6620 7365 6c66 2e75 7365       if self.use
+0001ac50: 5f6d 6f65 3a0a 2020 2020 2020 2020 2020  _moe:.          
+0001ac60: 2020 6d6c 705f 6c6f 6769 742c 2061 7578    mlp_logit, aux
+0001ac70: 5f6c 6f73 7320 3d20 7365 6c66 2e6f 7574  _loss = self.out
+0001ac80: 7075 7428 6f75 7470 7574 5f78 290a 2020  put(output_x).  
+0001ac90: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+0001aca0: 2020 2020 2020 2020 6d6c 705f 6c6f 6769          mlp_logi
+0001acb0: 7420 3d20 7365 6c66 2e6f 7574 7075 7428  t = self.output(
+0001acc0: 6f75 7470 7574 5f78 290a 0a20 2020 2020  output_x)..     
+0001acd0: 2020 2076 616c 7565 5f75 7064 6174 6520     value_update 
+0001ace0: 3d20 4e6f 6e65 0a20 2020 2020 2020 206b  = None.        k
+0001acf0: 6579 5f75 7064 6174 6520 3d20 4e6f 6e65  ey_update = None
+0001ad00: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+0001ad10: 2e75 7365 5f70 6173 743a 0a20 2020 2020  .use_past:.     
+0001ad20: 2020 2020 2020 2023 2063 7572 7265 6e74         # current
+0001ad30: 206b 6579 2061 6e64 2076 616c 7565 0a20   key and value. 
+0001ad40: 2020 2020 2020 2020 2020 206b 6579 5f70             key_p
+0001ad50: 7265 7365 6e74 2c20 7661 6c75 655f 7072  resent, value_pr
+0001ad60: 6573 656e 7420 3d20 6c61 7965 725f 7072  esent = layer_pr
+0001ad70: 6573 656e 740a 2020 2020 2020 2020 2020  esent.          
+0001ad80: 2020 2320 7570 6461 7465 206b 6579 2061    # update key a
+0001ad90: 6e64 2076 616c 7565 2063 616c 6375 6c61  nd value calcula
+0001ada0: 7465 6420 7468 6973 2073 7465 700a 2020  ted this step.  
+0001adb0: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+0001adc0: 7373 6967 6e28 7365 6c66 2e6b 6579 5f70  ssign(self.key_p
+0001add0: 6173 742c 206b 6579 5f70 7265 7365 6e74  ast, key_present
+0001ade0: 290a 2020 2020 2020 2020 2020 2020 6b65  ).            ke
+0001adf0: 795f 7570 6461 7465 203d 2073 656c 662e  y_update = self.
+0001ae00: 6b65 795f 7061 7374 0a20 2020 2020 2020  key_past.       
+0001ae10: 2020 2020 2073 656c 662e 6173 7369 676e       self.assign
+0001ae20: 2873 656c 662e 7661 6c75 655f 7061 7374  (self.value_past
+0001ae30: 2c20 7661 6c75 655f 7072 6573 656e 7429  , value_present)
+0001ae40: 0a20 2020 2020 2020 2020 2020 2076 616c  .            val
+0001ae50: 7565 5f75 7064 6174 6520 3d20 7365 6c66  ue_update = self
+0001ae60: 2e76 616c 7565 5f70 6173 740a 2020 2020  .value_past.    
+0001ae70: 2020 2020 2020 2020 2320 6164 6420 6465          # add de
+0001ae80: 7065 6e64 656e 6379 2066 6f72 2064 6573  pendency for des
+0001ae90: 6972 6564 2065 7865 6375 7469 6f6e 206f  ired execution o
+0001aea0: 7264 6572 0a20 2020 2020 2020 2020 2020  rder.           
+0001aeb0: 206b 6579 5f75 7064 6174 6520 3d20 462e   key_update = F.
+0001aec0: 6465 7065 6e64 286b 6579 5f75 7064 6174  depend(key_updat
+0001aed0: 652c 206b 6579 5f72 6573 6574 290a 2020  e, key_reset).  
+0001aee0: 2020 2020 2020 2020 2020 7661 6c75 655f            value_
+0001aef0: 7570 6461 7465 203d 2046 2e64 6570 656e  update = F.depen
+0001af00: 6428 7661 6c75 655f 7570 6461 7465 2c20  d(value_update, 
+0001af10: 7661 6c75 655f 7265 7365 7429 0a0a 2020  value_reset)..  
+0001af20: 2020 2020 2020 2320 6164 6420 6465 7065        # add depe
+0001af30: 6e64 656e 6379 2066 6f72 2064 6573 6972  ndency for desir
+0001af40: 6564 2065 7865 6375 7469 6f6e 206f 7264  ed execution ord
+0001af50: 6572 0a20 2020 2020 2020 206d 6c70 5f6c  er.        mlp_l
+0001af60: 6f67 6974 203d 2046 2e64 6570 656e 6428  ogit = F.depend(
+0001af70: 6d6c 705f 6c6f 6769 742c 2076 616c 7565  mlp_logit, value
+0001af80: 5f75 7064 6174 6529 0a20 2020 2020 2020  _update).       
+0001af90: 206d 6c70 5f6c 6f67 6974 203d 2046 2e64   mlp_logit = F.d
+0001afa0: 6570 656e 6428 6d6c 705f 6c6f 6769 742c  epend(mlp_logit,
+0001afb0: 206b 6579 5f75 7064 6174 6529 0a0a 2020   key_update)..  
+0001afc0: 2020 2020 2020 2320 6966 2073 6861 7065        # if shape
+0001afd0: 2069 7320 3364 2c20 7765 2072 6573 6861   is 3d, we resha
+0001afe0: 7065 2074 6865 2069 6e70 7574 7320 6f66  pe the inputs of
+0001aff0: 2074 6865 2061 6464 0a20 2020 2020 2020   the add.       
+0001b000: 2069 6620 6c65 6e28 785f 7368 6170 6529   if len(x_shape)
+0001b010: 203d 3d20 333a 0a20 2020 2020 2020 2020   == 3:.         
+0001b020: 2020 206f 7574 7075 745f 7820 3d20 502e     output_x = P.
+0001b030: 5265 7368 6170 6528 2928 6f75 7470 7574  Reshape()(output
+0001b040: 5f78 2c20 785f 7368 6170 6529 0a20 2020  _x, x_shape).   
+0001b050: 2020 2020 2020 2020 206d 6c70 5f6c 6f67           mlp_log
+0001b060: 6974 203d 2050 2e52 6573 6861 7065 2829  it = P.Reshape()
+0001b070: 286d 6c70 5f6c 6f67 6974 2c20 785f 7368  (mlp_logit, x_sh
+0001b080: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0001b090: 2078 203d 2050 2e52 6573 6861 7065 2829   x = P.Reshape()
+0001b0a0: 2878 2c20 785f 7368 6170 6529 0a0a 2020  (x, x_shape)..  
+0001b0b0: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+0001b0c0: 662e 706f 7374 5f6c 6179 6572 6e6f 726d  f.post_layernorm
+0001b0d0: 5f72 6573 6964 7561 6c3a 0a20 2020 2020  _residual:.     
+0001b0e0: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
+0001b0f0: 7420 3d20 7365 6c66 2e61 6464 5f33 6428  t = self.add_3d(
+0001b100: 6f75 7470 7574 5f78 2c20 6d6c 705f 6c6f  output_x, mlp_lo
+0001b110: 6769 7429 0a20 2020 2020 2020 2020 2020  git).           
+0001b120: 2020 2020 206f 7574 7075 7420 3d20 462e       output = F.
+0001b130: 7265 7368 6170 6528 6f75 7470 7574 2c20  reshape(output, 
+0001b140: 282d 312c 2078 5f73 6861 7065 5b2d 315d  (-1, x_shape[-1]
+0001b150: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+0001b160: 2020 206f 7574 7075 7420 3d20 7365 6c66     output = self
+0001b170: 2e6c 6179 6572 6e6f 726d 3128 6f75 7470  .layernorm1(outp
+0001b180: 7574 290a 2020 2020 2020 2020 2020 2020  ut).            
+0001b190: 2020 2020 6f75 7470 7574 203d 2046 2e72      output = F.r
+0001b1a0: 6573 6861 7065 286f 7574 7075 742c 2078  eshape(output, x
+0001b1b0: 5f73 6861 7065 290a 2020 2020 2020 2020  _shape).        
+0001b1c0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0001b1d0: 2020 2020 2020 2020 2020 6f75 7470 7574            output
+0001b1e0: 203d 2073 656c 662e 6164 645f 3364 2878   = self.add_3d(x
+0001b1f0: 2c20 6d6c 705f 6c6f 6769 7429 0a20 2020  , mlp_logit).   
+0001b200: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+0001b210: 2020 2020 2020 2069 6620 7365 6c66 2e70         if self.p
+0001b220: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+0001b230: 7369 6475 616c 3a0a 2020 2020 2020 2020  sidual:.        
+0001b240: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
+0001b250: 2073 656c 662e 6164 6428 6f75 7470 7574   self.add(output
+0001b260: 5f78 2c20 6d6c 705f 6c6f 6769 7429 0a20  _x, mlp_logit). 
+0001b270: 2020 2020 2020 2020 2020 2020 2020 206f                 o
+0001b280: 7574 7075 7420 3d20 7365 6c66 2e6c 6179  utput = self.lay
+0001b290: 6572 6e6f 726d 3128 6f75 7470 7574 290a  ernorm1(output).
+0001b2a0: 2020 2020 2020 2020 2020 2020 656c 7365              else
+0001b2b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001b2c0: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
+0001b2d0: 6164 6428 782c 206d 6c70 5f6c 6f67 6974  add(x, mlp_logit
+0001b2e0: 290a 2020 2020 2020 2020 2020 2020 6f75  ).            ou
+0001b2f0: 7470 7574 203d 2046 2e72 6573 6861 7065  tput = F.reshape
+0001b300: 286f 7574 7075 742c 2078 5f73 6861 7065  (output, x_shape
+0001b310: 290a 0a20 2020 2020 2020 2069 6620 7365  )..        if se
+0001b320: 6c66 2e75 7365 5f6d 6f65 3a0a 2020 2020  lf.use_moe:.    
+0001b330: 2020 2020 2020 2020 7265 7475 726e 206f          return o
+0001b340: 7574 7075 742c 206c 6179 6572 5f70 7265  utput, layer_pre
+0001b350: 7365 6e74 2c20 6175 785f 6c6f 7373 0a20  sent, aux_loss. 
+0001b360: 2020 2020 2020 2072 6574 7572 6e20 6f75         return ou
+0001b370: 7470 7574 2c20 6c61 7965 725f 7072 6573  tput, layer_pres
+0001b380: 656e 740a 0a20 2020 2064 6566 205f 6368  ent..    def _ch
+0001b390: 6563 6b5f 696e 7075 7428 7365 6c66 2c20  eck_input(self, 
+0001b3a0: 782c 2069 6e70 7574 5f6d 6173 6b2c 2069  x, input_mask, i
+0001b3b0: 6e69 745f 7265 7365 742c 2062 6174 6368  nit_reset, batch
+0001b3c0: 5f76 616c 6964 5f6c 656e 6774 6829 3a0a  _valid_length):.
+0001b3d0: 2020 2020 2020 2020 7222 2222 4368 6563          r"""Chec
+0001b3e0: 6b20 696e 7075 7473 2222 220a 2020 2020  k inputs""".    
+0001b3f0: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
+0001b400: 5f64 7479 7065 2846 2e64 7479 7065 2878  _dtype(F.dtype(x
+0001b410: 292c 2022 7822 2c20 5b6d 7374 7970 652e  ), "x", [mstype.
+0001b420: 666c 6f61 7433 322c 206d 7374 7970 652e  float32, mstype.
+0001b430: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+0001b440: 6266 6c6f 6174 3136 5d2c 2073 656c 662e  bfloat16], self.
+0001b450: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+0001b460: 2020 6966 2069 6e70 7574 5f6d 6173 6b20    if input_mask 
+0001b470: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   
+0001b480: 2020 2020 2020 2020 205f 6368 6563 6b5f           _check_
+0001b490: 696e 7075 745f 6474 7970 6528 462e 6474  input_dtype(F.dt
+0001b4a0: 7970 6528 696e 7075 745f 6d61 736b 292c  ype(input_mask),
+0001b4b0: 2022 696e 7075 745f 6d61 736b 222c 0a20   "input_mask",. 
+0001b4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b4d0: 2020 2020 2020 2020 2020 2020 2020 5b6d                [m
+0001b4e0: 7374 7970 652e 666c 6f61 7433 322c 206d  stype.float32, m
+0001b4f0: 7374 7970 652e 666c 6f61 7431 362c 206d  stype.float16, m
+0001b500: 7374 7970 652e 6266 6c6f 6174 3136 5d2c  stype.bfloat16],
+0001b510: 2073 656c 662e 636c 735f 6e61 6d65 290a   self.cls_name).
+0001b520: 0a20 2020 2020 2020 2069 6e69 745f 7265  .        init_re
+0001b530: 7365 745f 6973 5f74 656e 736f 7220 3d20  set_is_tensor = 
+0001b540: 6973 696e 7374 616e 6365 2869 6e69 745f  isinstance(init_
+0001b550: 7265 7365 742c 2054 656e 736f 7229 0a20  reset, Tensor). 
+0001b560: 2020 2020 2020 2069 6e69 745f 7265 7365         init_rese
+0001b570: 745f 6973 5f64 6566 6175 6c74 203d 2069  t_is_default = i
+0001b580: 6e69 745f 7265 7365 7420 6973 2054 7275  nit_reset is Tru
+0001b590: 650a 2020 2020 2020 2020 6261 7463 685f  e.        batch_
+0001b5a0: 7661 6c69 645f 6c65 6e67 7468 5f69 735f  valid_length_is_
+0001b5b0: 7465 6e73 6f72 203d 2069 7369 6e73 7461  tensor = isinsta
+0001b5c0: 6e63 6528 6261 7463 685f 7661 6c69 645f  nce(batch_valid_
+0001b5d0: 6c65 6e67 7468 2c20 5465 6e73 6f72 290a  length, Tensor).
+0001b5e0: 2020 2020 2020 2020 6261 7463 685f 6973          batch_is
+0001b5f0: 5f64 6566 6175 6c74 203d 2062 6174 6368  _default = batch
+0001b600: 5f76 616c 6964 5f6c 656e 6774 6820 6973  _valid_length is
+0001b610: 204e 6f6e 650a 2020 2020 2020 2020 5f63   None.        _c
+0001b620: 6865 636b 5f70 6173 745f 6e6f 6e65 5f69  heck_past_none_i
+0001b630: 6e70 7574 5f6e 6f6e 6528 7365 6c66 2e75  nput_none(self.u
+0001b640: 7365 5f70 6173 742c 2022 696e 6974 5f72  se_past, "init_r
+0001b650: 6573 6574 222c 2073 656c 662e 636c 735f  eset", self.cls_
+0001b660: 6e61 6d65 2c20 5472 7565 2c20 696e 6974  name, True, init
+0001b670: 5f72 6573 6574 5f69 735f 7465 6e73 6f72  _reset_is_tensor
+0001b680: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001b690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b6a0: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
+0001b6b0: 5f69 735f 6465 6661 756c 7429 0a20 2020  _is_default).   
+0001b6c0: 2020 2020 205f 6368 6563 6b5f 7061 7374       _check_past
+0001b6d0: 5f6e 6f6e 655f 696e 7075 745f 6e6f 6e65  _none_input_none
+0001b6e0: 2873 656c 662e 7573 655f 7061 7374 2c20  (self.use_past, 
+0001b6f0: 2262 6174 6368 5f76 616c 6964 5f6c 656e  "batch_valid_len
+0001b700: 6774 6822 2c20 7365 6c66 2e63 6c73 5f6e  gth", self.cls_n
+0001b710: 616d 652c 204e 6f6e 652c 0a20 2020 2020  ame, None,.     
+0001b720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b730: 2020 2020 2020 2020 2020 2020 2020 2062                 b
+0001b740: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
+0001b750: 685f 6973 5f74 656e 736f 722c 2062 6174  h_is_tensor, bat
+0001b760: 6368 5f69 735f 6465 6661 756c 7429 0a0a  ch_is_default)..
+0001b770: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+0001b780: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
+0001b790: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
+0001b7a0: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
+0001b7b0: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
+0001b7c0: 6774 6829 2c20 2262 6174 6368 5f76 616c  gth), "batch_val
+0001b7d0: 6964 5f6c 656e 6774 6822 2c20 5b6d 7374  id_length", [mst
+0001b7e0: 7970 652e 696e 7433 325d 2c20 7365 6c66  ype.int32], self
+0001b7f0: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
+0001b800: 2020 2072 6574 7572 6e20 5472 7565 0a0a     return True..
+0001b810: 0a63 6c61 7373 2054 7261 6e73 666f 726d  .class Transform
+0001b820: 6572 4465 636f 6465 724c 6179 6572 2843  erDecoderLayer(C
+0001b830: 656c 6c29 3a0a 2020 2020 7222 2222 0a20  ell):.    r""". 
+0001b840: 2020 2020 2020 2054 7261 6e73 666f 726d         Transform
+0001b850: 6572 2044 6563 6f64 6572 204c 6179 6572  er Decoder Layer
+0001b860: 2e20 5468 6973 2069 7320 616e 2069 6d70  . This is an imp
+0001b870: 6c65 6d65 6e74 6174 696f 6e20 6f66 2074  lementation of t
+0001b880: 6865 2073 696e 676c 6520 6c61 7965 7220  he single layer 
+0001b890: 6f66 2074 6865 2074 7261 6e73 666f 726d  of the transform
+0001b8a0: 6572 0a20 2020 2020 2020 2064 6563 6f64  er.        decod
+0001b8b0: 6572 206c 6179 6572 2c20 696e 636c 7564  er layer, includ
+0001b8c0: 696e 6720 7365 6c66 2d61 7474 656e 7469  ing self-attenti
+0001b8d0: 6f6e 2c20 6372 6f73 7320 6174 7465 6e74  on, cross attent
+0001b8e0: 696f 6e20 616e 6420 6665 6564 7761 7264  ion and feedward
+0001b8f0: 206c 6179 6572 2e20 5768 656e 2074 6865   layer. When the
+0001b900: 2065 6e63 6f64 6572 5f6f 7574 7075 7420   encoder_output 
+0001b910: 6973 204e 6f6e 652c 0a20 2020 2020 2020  is None,.       
+0001b920: 2074 6865 2063 726f 7373 2061 7474 656e   the cross atten
+0001b930: 7469 6f6e 2077 696c 6c20 6e6f 7420 6265  tion will not be
+0001b940: 2065 6666 6563 7469 7665 2e0a 0a20 2020   effective...   
+0001b950: 2020 2020 2041 7267 733a 0a20 2020 2020       Args:.     
+0001b960: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
+0001b970: 7a65 2869 6e74 293a 2054 6865 2068 6964  ze(int): The hid
+0001b980: 6465 6e20 7369 7a65 206f 6620 7468 6520  den size of the 
+0001b990: 696e 7075 742e 0a20 2020 2020 2020 2020  input..         
+0001b9a0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0001b9b0: 7a65 2869 6e74 293a 2054 6865 2068 6964  ze(int): The hid
+0001b9c0: 6465 6e20 7369 7a65 206f 6620 626f 7474  den size of bott
+0001b9d0: 6c65 6e65 636b 2069 6e20 7468 6520 6665  leneck in the fe
+0001b9e0: 6564 666f 7277 6172 6420 6c61 7965 722e  edforward layer.
+0001b9f0: 0a20 2020 2020 2020 2020 2020 206e 756d  .            num
+0001ba00: 5f68 6561 6473 2869 6e74 293a 2054 6865  _heads(int): The
+0001ba10: 206e 756d 6265 7220 6f66 2074 6865 2068   number of the h
+0001ba20: 6561 6473 2e0a 2020 2020 2020 2020 2020  eads..          
+0001ba30: 2020 6261 7463 685f 7369 7a65 2869 6e74    batch_size(int
+0001ba40: 293a 2054 6865 2062 6174 6368 2073 697a  ): The batch siz
+0001ba50: 6520 6f66 2074 6865 2069 6e70 7574 2074  e of the input t
+0001ba60: 656e 736f 7220 7768 656e 2064 6f20 696e  ensor when do in
+0001ba70: 6372 656e 6d65 6e74 616c 2070 7265 6469  crenmental predi
+0001ba80: 6374 696f 6e2e 2053 686f 756c 6420 6265  ction. Should be
+0001ba90: 2061 2070 6f73 6974 6976 650a 2020 2020   a positive.    
+0001baa0: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
+0001bab0: 652e 2057 6865 6e20 646f 2074 7261 696e  e. When do train
+0001bac0: 696e 6720 6f72 2070 7265 6469 6374 696f  ing or predictio
+0001bad0: 6e2c 2074 6865 2061 7267 756d 656e 7420  n, the argument 
+0001bae0: 7769 6c6c 206e 6f74 2077 6f72 6b20 616e  will not work an
+0001baf0: 6420 7468 6520 7573 6572 2063 616e 206a  d the user can j
+0001bb00: 7573 7420 7061 7373 204e 6f6e 6520 746f  ust pass None to
+0001bb10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001bb20: 2074 6865 2061 7267 756d 656e 742e 0a20   the argument.. 
+0001bb30: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
+0001bb40: 6571 5f6c 656e 6774 6828 696e 7429 3a20  eq_length(int): 
+0001bb50: 5468 6520 696e 7075 7420 736f 7572 6365  The input source
+0001bb60: 2073 6571 7565 6e63 6520 6c65 6e67 7468   sequence length
+0001bb70: 2e0a 2020 2020 2020 2020 2020 2020 7467  ..            tg
+0001bb80: 745f 7365 715f 6c65 6e67 7468 2869 6e74  t_seq_length(int
+0001bb90: 293a 2054 6865 2069 6e70 7574 2074 6172  ): The input tar
+0001bba0: 6765 7420 7365 7175 656e 6365 206c 656e  get sequence len
+0001bbb0: 6774 682e 0a20 2020 2020 2020 2020 2020  gth..           
+0001bbc0: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
+0001bbd0: 7574 5f72 6174 6528 666c 6f61 7429 3a20  ut_rate(float): 
+0001bbe0: 5468 6520 6472 6f70 6f75 7420 7261 7465  The dropout rate
+0001bbf0: 206f 6620 7468 6520 6174 7465 6e74 696f   of the attentio
+0001bc00: 6e20 7363 6f72 6573 2e20 4465 6661 756c  n scores. Defaul
+0001bc10: 743a 302e 312e 0a20 2020 2020 2020 2020  t:0.1..         
+0001bc20: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
+0001bc30: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
+0001bc40: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
+0001bc50: 6f66 2074 6865 2066 696e 616c 206f 7574  of the final out
+0001bc60: 7075 7420 6f66 2074 6865 206c 6179 6572  put of the layer
+0001bc70: 2e20 4465 6661 756c 743a 302e 312e 0a20  . Default:0.1.. 
+0001bc80: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
+0001bc90: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+0001bca0: 616c 2862 6f6f 6c29 3a20 446f 2072 6573  al(bool): Do res
+0001bcb0: 6964 7561 6c73 2061 6464 7320 6265 666f  iduals adds befo
+0001bcc0: 7265 2074 6865 206c 6179 6572 6e6f 726d  re the layernorm
+0001bcd0: 2e20 4465 6661 756c 7420 4661 6c73 652e  . Default False.
+0001bce0: 0a20 2020 2020 2020 2020 2020 2075 7365  .            use
+0001bcf0: 5f70 6173 7428 626f 6f6c 293a 2055 7365  _past(bool): Use
+0001bd00: 2074 6865 2070 6173 7420 7374 6174 6520   the past state 
+0001bd10: 746f 2063 6f6d 7075 7465 2c20 7573 6564  to compute, used
+0001bd20: 2066 6f72 2069 6e63 7265 6d65 6e74 616c   for incremental
+0001bd30: 2070 7265 6469 6374 696f 6e2e 2044 6566   prediction. Def
+0001bd40: 6175 6c74 2046 616c 7365 2e0a 2020 2020  ault False..    
+0001bd50: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
+0001bd60: 6d5f 636f 6d70 7574 655f 7479 7065 2864  m_compute_type(d
+0001bd70: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
+0001bd80: 6520 636f 6d70 7574 6174 696f 6e20 7479  e computation ty
+0001bd90: 7065 206f 6620 7468 6520 6c61 7965 726e  pe of the layern
+0001bda0: 6f72 6d2e 0a20 2020 2020 2020 2020 2020  orm..           
+0001bdb0: 2020 2020 2053 686f 756c 6420 6265 2064       Should be d
+0001bdc0: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
+0001bdd0: 6474 7970 652e 666c 6f61 7431 362e 2044  dtype.float16. D
+0001bde0: 6566 6175 6c74 2064 7479 7065 2e66 6c6f  efault dtype.flo
+0001bdf0: 6174 3332 2e0a 2020 2020 2020 2020 2020  at32..          
+0001be00: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
+0001be10: 655f 7479 7065 2864 7479 7065 2e4e 756d  e_type(dtype.Num
+0001be20: 6265 7229 3a20 5468 6520 636f 6d70 7574  ber): The comput
+0001be30: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
+0001be40: 6520 736f 6674 6d61 7820 696e 2074 6865  e softmax in the
+0001be50: 2061 7474 656e 7469 6f6e 2e0a 2020 2020   attention..    
+0001be60: 2020 2020 2020 2020 2020 2020 5368 6f75              Shou
+0001be70: 6c64 2062 6520 6474 7970 652e 666c 6f61  ld be dtype.floa
+0001be80: 7433 3220 6f72 2064 7479 7065 2e66 6c6f  t32 or dtype.flo
+0001be90: 6174 3136 2e20 4465 6661 756c 7420 6d73  at16. Default ms
+0001bea0: 7479 7065 2e66 6c6f 6174 3332 2e0a 2020  type.float32..  
+0001beb0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
+0001bec0: 696e 6974 5f74 7970 6528 6474 7970 652e  init_type(dtype.
+0001bed0: 4e75 6d62 6572 293a 2054 6865 2070 6172  Number): The par
+0001bee0: 616d 6574 6572 2069 6e69 7469 616c 697a  ameter initializ
+0001bef0: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
+0001bf00: 6520 6d6f 6475 6c65 2e0a 2020 2020 2020  e module..      
+0001bf10: 2020 2020 2020 2020 2020 5368 6f75 6c64            Should
+0001bf20: 2062 6520 6474 7970 652e 666c 6f61 7433   be dtype.float3
+0001bf30: 3220 6f72 2064 7479 7065 2e66 6c6f 6174  2 or dtype.float
+0001bf40: 3136 2e20 4465 6661 756c 7420 6474 7970  16. Default dtyp
+0001bf50: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+0001bf60: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
+0001bf70: 7420 2873 7472 2c20 6e6e 2e43 656c 6c29  t (str, nn.Cell)
+0001bf80: 3a20 5468 6520 6163 7469 7661 7469 6f6e  : The activation
+0001bf90: 206f 6620 7468 6520 696e 7465 726e 616c   of the internal
+0001bfa0: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
+0001bfb0: 6572 2e20 5375 7070 6f72 7473 2027 7265  er. Supports 're
+0001bfc0: 6c75 272c 0a20 2020 2020 2020 2020 2020  lu',.           
+0001bfd0: 2020 2020 2027 7265 6c75 3627 2c20 2774       'relu6', 't
+0001bfe0: 616e 6827 2c20 2767 656c 7527 2c20 2766  anh', 'gelu', 'f
+0001bff0: 6173 745f 6765 6c75 272c 2027 656c 7527  ast_gelu', 'elu'
+0001c000: 2c20 2773 6967 6d6f 6964 272c 2027 7072  , 'sigmoid', 'pr
+0001c010: 656c 7527 2c20 276c 6561 6b79 7265 6c75  elu', 'leakyrelu
+0001c020: 272c 2027 6873 7769 7368 272c 0a20 2020  ', 'hswish',.   
+0001c030: 2020 2020 2020 2020 2020 2020 2027 6873               'hs
+0001c040: 6967 6d6f 6964 272c 2027 6c6f 6773 6967  igmoid', 'logsig
+0001c050: 6d6f 6964 2720 616e 6420 736f 206f 6e2e  moid' and so on.
+0001c060: 2055 7365 7220 6361 6e20 7072 6f76 6964   User can provid
+0001c070: 6520 6375 7374 6f6d 2061 6374 6976 6974  e custom activit
+0001c080: 696f 6e20 746f 2074 6865 2061 7267 756d  ion to the argum
+0001c090: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
+0001c0a0: 2020 2020 2049 6620 7573 6572 2077 616e       If user wan
+0001c0b0: 7473 2074 6f20 7275 6e20 7468 6520 6e65  ts to run the ne
+0001c0c0: 7420 696e 2074 6865 2070 6172 616c 6c65  t in the paralle
+0001c0d0: 6c20 6d6f 6465 2c20 7468 6520 6375 7374  l mode, the cust
+0001c0e0: 6f6d 2061 6374 6976 6174 696f 6e20 6d75  om activation mu
+0001c0f0: 7374 2061 6c73 6f20 7072 6f76 6964 650a  st also provide.
+0001c100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c110: 7468 6520 6061 6374 6976 6174 696f 6e5f  the `activation_
+0001c120: 7368 6172 6460 2066 756e 6374 696f 6e2e  shard` function.
+0001c130: 2050 6c65 6173 6520 7365 6520 7468 6520   Please see the 
+0001c140: 6578 616d 706c 6573 206f 6620 7468 650a  examples of the.
+0001c150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c160: 636c 6173 733a 606d 696e 6466 6f72 6d65  class:`mindforme
+0001c170: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+0001c180: 666f 726d 6572 2e46 6565 6446 6f72 7761  former.FeedForwa
+0001c190: 7264 602e 2044 6566 6175 6c74 3a20 6765  rd`. Default: ge
+0001c1a0: 6c75 2e0a 2020 2020 2020 2020 2020 2020  lu..            
+0001c1b0: 6d6f 655f 636f 6e66 6967 284d 6f45 436f  moe_config(MoECo
+0001c1c0: 6e66 6967 293a 2054 6865 2063 6f6e 6669  nfig): The confi
+0001c1d0: 6775 7261 7469 6f6e 206f 6620 4d6f 4520  guration of MoE 
+0001c1e0: 284d 6978 7475 7265 206f 6620 4578 7065  (Mixture of Expe
+0001c1f0: 7274 292e 2044 6566 6175 6c74 2069 7320  rt). Default is 
+0001c200: 616e 2069 6e73 7461 6e63 6520 6f66 204d  an instance of M
+0001c210: 6f45 436f 6e66 6967 0a20 2020 2020 2020  oEConfig.       
+0001c220: 2020 2020 2020 2020 2077 6974 6820 6465           with de
+0001c230: 6661 756c 7420 7661 6c75 6573 2e20 506c  fault values. Pl
+0001c240: 6561 7365 2073 6565 2060 4d6f 4543 6f6e  ease see `MoECon
+0001c250: 6669 6760 2e0a 2020 2020 2020 2020 2020  fig`..          
+0001c260: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0001c270: 6728 4f70 5061 7261 6c6c 656c 436f 6e66  g(OpParallelConf
+0001c280: 6967 2c20 4d6f 4550 6172 616c 6c65 6c43  ig, MoEParallelC
+0001c290: 6f6e 6669 6729 3a20 5468 6520 7061 7261  onfig): The para
+0001c2a0: 6c6c 656c 2063 6f6e 6669 6775 7265 2e20  llel configure. 
+0001c2b0: 5768 656e 204d 6f45 2069 7320 6170 706c  When MoE is appl
+0001c2c0: 6965 642c 0a20 2020 2020 2020 2020 2020  ied,.           
+0001c2d0: 2020 2020 204d 6f45 5061 7261 6c6c 656c       MoEParallel
+0001c2e0: 436f 6e66 6967 2069 7320 6566 6665 6374  Config is effect
+0001c2f0: 6976 652c 206f 7468 6572 7769 7365 204f  ive, otherwise O
+0001c300: 7050 6172 616c 6c65 6c43 6f6e 6669 6720  pParallelConfig 
+0001c310: 6973 2065 6666 6563 7469 7665 2e20 4465  is effective. De
+0001c320: 6661 756c 7420 6064 6566 6175 6c74 5f64  fault `default_d
+0001c330: 706d 705f 636f 6e66 6967 602c 0a20 2020  pmp_config`,.   
+0001c340: 2020 2020 2020 2020 2020 2020 2061 6e20               an 
+0001c350: 696e 7374 616e 6365 206f 6620 604f 7050  instance of `OpP
+0001c360: 6172 616c 6c65 6c43 6f6e 6669 6760 2077  arallelConfig` w
+0001c370: 6974 6820 6465 6661 756c 7420 6172 6773  ith default args
+0001c380: 2e0a 0a20 2020 2020 2020 2049 6e70 7574  ...        Input
+0001c390: 733a 0a20 2020 2020 2020 2020 2020 202d  s:.            -
+0001c3a0: 202a 2a68 6964 6465 6e5f 7374 6174 732a   **hidden_stats*
+0001c3b0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+0001c3c0: 2069 6e70 7574 2074 656e 736f 7220 7769   input tensor wi
+0001c3d0: 7468 2073 6861 7065 205b 6261 7463 685f  th shape [batch_
+0001c3e0: 7369 7a65 2c20 7467 745f 7365 715f 6c65  size, tgt_seq_le
+0001c3f0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+0001c400: 655d 206f 720a 2020 2020 2020 2020 2020  e] or.          
+0001c410: 2020 2020 5b62 6174 6368 5f73 697a 6520      [batch_size 
+0001c420: 2a20 7467 745f 7365 715f 6c65 6e67 7468  * tgt_seq_length
+0001c430: 2c20 6869 6464 656e 5f73 697a 655d 2e0a  , hidden_size]..
+0001c440: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+0001c450: 6465 636f 6465 725f 6d61 736b 2a2a 2028  decoder_mask** (
+0001c460: 5465 6e73 6f72 2920 2d20 5468 6520 6174  Tensor) - The at
+0001c470: 7465 6e74 696f 6e20 6d61 736b 2066 6f72  tention mask for
+0001c480: 2064 6563 6f64 6572 2077 6974 6820 7368   decoder with sh
+0001c490: 6170 6520 5b62 6174 6368 5f73 697a 652c  ape [batch_size,
+0001c4a0: 2073 7263 5f73 6571 5f6c 656e 6774 682c   src_seq_length,
+0001c4b0: 0a20 2020 2020 2020 2020 2020 2020 2073  .              s
+0001c4c0: 6571 5f6c 656e 6774 685d 206f 7220 4e6f  eq_length] or No
+0001c4d0: 6e65 2e20 4e6f 6e65 206d 6561 6e73 2074  ne. None means t
+0001c4e0: 6865 7265 2077 696c 6c20 6265 206e 6f20  here will be no 
+0001c4f0: 6d61 736b 2069 6e20 736f 6674 6d61 7820  mask in softmax 
+0001c500: 636f 6d70 7574 6174 696f 6e20 696e 2073  computation in s
+0001c510: 656c 6620 6174 7465 6e74 696f 6e2e 0a20  elf attention.. 
+0001c520: 2020 2020 2020 2020 2020 202d 202a 2a65             - **e
+0001c530: 6e63 6f64 6572 5f6f 7574 7075 742a 2a20  ncoder_output** 
+0001c540: 2854 656e 736f 7229 202d 2054 6865 206f  (Tensor) - The o
+0001c550: 7574 7075 7420 6f66 2074 6865 2065 6e63  utput of the enc
+0001c560: 6f64 6572 2077 6974 6820 7368 6170 6520  oder with shape 
+0001c570: 5b62 6174 6368 5f73 697a 652c 2073 6571  [batch_size, seq
+0001c580: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
+0001c590: 7369 7a65 5d0a 2020 2020 2020 2020 2020  size].          
+0001c5a0: 2020 2020 6f72 205b 6261 7463 685f 7369      or [batch_si
+0001c5b0: 7a65 202a 2073 6571 5f6c 656e 6774 682c  ze * seq_length,
+0001c5c0: 2068 6964 6465 6e5f 7369 7a65 5d2e 0a20   hidden_size].. 
+0001c5d0: 2020 2020 2020 2020 2020 2020 204e 6f74               Not
+0001c5e0: 6520 7468 6973 2061 7267 7320 6361 6e20  e this args can 
+0001c5f0: 6e6f 7420 6265 2070 6173 7365 6420 6279  not be passed by
+0001c600: 204e 6f6e 6520 7768 656e 2074 6865 206e   None when the n
+0001c610: 6574 2069 7320 696e 206f 7574 6572 6d6f  et is in outermo
+0001c620: 7374 206c 6179 6572 2e20 4465 6661 756c  st layer. Defaul
+0001c630: 7420 4e6f 6e65 2e0a 2020 2020 2020 2020  t None..        
+0001c640: 2020 2020 2d20 2a2a 6d65 6d6f 7279 5f6d      - **memory_m
+0001c650: 6173 6b2a 2a20 2854 656e 736f 7229 202d  ask** (Tensor) -
+0001c660: 2054 6865 206d 656d 6f72 7920 6d61 736b   The memory mask
+0001c670: 206f 6620 7468 6520 6372 6f73 7320 6174   of the cross at
+0001c680: 7465 6e74 696f 6e20 7769 7468 2073 6861  tention with sha
+0001c690: 7065 205b 6261 7463 682c 2074 6774 5f73  pe [batch, tgt_s
+0001c6a0: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
+0001c6b0: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
+0001c6c0: 5f6c 656e 6774 685d 2077 6865 7265 2074  _length] where t
+0001c6d0: 6774 5f73 6571 5f6c 656e 6774 6820 6973  gt_seq_length is
+0001c6e0: 2074 6865 206c 656e 6774 6820 6f66 2074   the length of t
+0001c6f0: 6865 2064 6563 6f64 6572 2e20 5468 6520  he decoder. The 
+0001c700: 7573 6572 2063 616e 2061 6c73 6f20 7061  user can also pa
+0001c710: 7373 204e 6f6e 652e 204e 6f6e 650a 2020  ss None. None.  
+0001c720: 2020 2020 2020 2020 2020 2020 6d65 616e              mean
+0001c730: 7320 7468 6572 6520 7769 6c6c 2062 6520  s there will be 
+0001c740: 6e6f 206d 6173 6b20 696e 2073 6f66 746d  no mask in softm
+0001c750: 6178 2063 6f6d 7075 7461 7469 6f6e 2069  ax computation i
+0001c760: 6e20 6372 6f73 7320 6174 7465 6e74 696f  n cross attentio
+0001c770: 6e2e 2044 6566 6175 6c74 204e 6f6e 652e  n. Default None.
+0001c780: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+0001c790: 2a69 6e69 745f 7265 7365 742a 2a20 2854  *init_reset** (T
+0001c7a0: 656e 736f 7229 202d 2041 2062 6f6f 6c20  ensor) - A bool 
+0001c7b0: 7465 6e73 6f72 2077 6974 6820 7368 6170  tensor with shap
+0001c7c0: 6520 5b31 5d2c 2075 7365 6420 746f 2063  e [1], used to c
+0001c7d0: 6c65 6172 2074 6865 2070 6173 7420 6b65  lear the past ke
+0001c7e0: 7920 7061 7261 6d65 7465 7220 616e 640a  y parameter and.
+0001c7f0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0001c800: 7374 2076 616c 7565 2070 6172 616d 6574  st value paramet
+0001c810: 6572 2075 7365 6420 696e 2074 6865 2069  er used in the i
+0001c820: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
+0001c830: 6374 696f 6e2e 204f 6e6c 7920 7661 6c69  ction. Only vali
+0001c840: 6420 7768 656e 2075 7365 5f70 6173 7420  d when use_past 
+0001c850: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
+0001c860: 2054 7275 652e 0a20 2020 2020 2020 2020   True..         
+0001c870: 2020 202d 202a 2a62 6174 6368 5f76 616c     - **batch_val
+0001c880: 6964 5f6c 656e 6774 682a 2a20 2854 656e  id_length** (Ten
+0001c890: 736f 7229 202d 2049 6e74 3332 2074 656e  sor) - Int32 ten
+0001c8a0: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
+0001c8b0: 6261 7463 685f 7369 7a65 5d20 7468 6520  batch_size] the 
+0001c8c0: 7061 7374 2063 616c 6375 6c61 7465 6420  past calculated 
+0001c8d0: 7468 6520 696e 6465 782e 0a20 2020 2020  the index..     
+0001c8e0: 2020 2020 2020 2020 2055 7365 6420 666f           Used fo
+0001c8f0: 7220 696e 6372 656d 656e 7461 6c20 7072  r incremental pr
+0001c900: 6564 6963 7469 6f6e 2077 6865 6e20 7468  ediction when th
+0001c910: 6520 7573 655f 7061 7374 2069 7320 5472  e use_past is Tr
+0001c920: 7565 2e20 4465 6661 756c 7420 4e6f 6e65  ue. Default None
+0001c930: 2e0a 0a20 2020 2020 2020 204f 7574 7075  ...        Outpu
+0001c940: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
+0001c950: 5475 706c 652c 2061 2074 7570 6c65 2063  Tuple, a tuple c
+0001c960: 6f6e 7461 696e 7328 606f 7574 7075 7460  ontains(`output`
+0001c970: 2c20 606c 6179 6572 5f70 7265 7365 6e74  , `layer_present
+0001c980: 6029 0a0a 2020 2020 2020 2020 2020 2020  `)..            
+0001c990: 2d20 2a2a 6f75 7470 7574 2a2a 2028 5465  - **output** (Te
+0001c9a0: 6e73 6f72 2920 2d20 5468 6520 6f75 7470  nsor) - The outp
+0001c9b0: 7574 206c 6f67 6974 206f 6620 7468 6973  ut logit of this
+0001c9c0: 206c 6179 6572 2e20 5468 6520 7368 6170   layer. The shap
+0001c9d0: 6520 6973 205b 6261 7463 682c 2073 6571  e is [batch, seq
+0001c9e0: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
+0001c9f0: 7369 7a65 5d20 6f72 0a20 2020 2020 2020  size] or.       
+0001ca00: 2020 2020 2020 205b 6261 7463 6820 2a20         [batch * 
+0001ca10: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+0001ca20: 656e 5f73 697a 655d 2e0a 2020 2020 2020  en_size]..      
+0001ca30: 2020 2020 2020 2d20 2a2a 6c61 7965 725f        - **layer_
+0001ca40: 7072 6573 656e 742a 2a20 2854 7570 6c65  present** (Tuple
+0001ca50: 2920 2d20 4120 7475 706c 652c 2077 6865  ) - A tuple, whe
+0001ca60: 7265 2065 6163 6820 7475 706c 6520 6973  re each tuple is
+0001ca70: 2074 6865 2074 656e 736f 7220 6f66 2074   the tensor of t
+0001ca80: 6865 2070 726f 6a65 6374 6564 206b 6579  he projected key
+0001ca90: 2061 6e64 2076 616c 7565 0a20 2020 2020   and value.     
+0001caa0: 2020 2020 2020 2020 2076 6563 746f 7220           vector 
+0001cab0: 696e 2073 656c 6620 6174 7465 6e74 696f  in self attentio
+0001cac0: 6e20 7769 7468 2073 6861 7065 2028 2862  n with shape ((b
+0001cad0: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+0001cae0: 6561 6473 2c20 7369 7a65 5f70 6572 5f68  eads, size_per_h
+0001caf0: 6561 642c 2074 6774 5f73 6571 5f6c 656e  ead, tgt_seq_len
+0001cb00: 6774 6829 2c0a 2020 2020 2020 2020 2020  gth),.          
+0001cb10: 2020 2020 2862 6174 6368 5f73 697a 652c      (batch_size,
+0001cb20: 206e 756d 5f68 6561 6473 2c20 7467 745f   num_heads, tgt_
+0001cb30: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
+0001cb40: 5f70 6572 5f68 6561 6429 2c20 616e 6420  _per_head), and 
+0001cb50: 6f66 2074 6865 2070 726f 6a65 6374 6564  of the projected
+0001cb60: 206b 6579 2061 6e64 2076 616c 7565 2076   key and value v
+0001cb70: 6563 746f 720a 2020 2020 2020 2020 2020  ector.          
+0001cb80: 2020 2020 696e 2063 726f 7373 2061 7474      in cross att
+0001cb90: 656e 7469 6f6e 2077 6974 6820 7368 6170  ention with shap
+0001cba0: 6520 2028 6261 7463 685f 7369 7a65 2c20  e  (batch_size, 
+0001cbb0: 6e75 6d5f 6865 6164 732c 2073 697a 655f  num_heads, size_
+0001cbc0: 7065 725f 6865 6164 2c20 7372 635f 7365  per_head, src_se
+0001cbd0: 715f 6c65 6e67 7468 292c 0a20 2020 2020  q_length),.     
+0001cbe0: 2020 2020 2020 2020 2028 6261 7463 685f           (batch_
+0001cbf0: 7369 7a65 2c20 6e75 6d5f 6865 6164 732c  size, num_heads,
+0001cc00: 2073 7263 5f73 6571 5f6c 656e 6774 682c   src_seq_length,
+0001cc10: 2073 697a 655f 7065 725f 6865 6164 2929   size_per_head))
+0001cc20: 2e0a 0a20 2020 2020 2020 2053 7570 706f  ...        Suppo
+0001cc30: 7274 6564 2050 6c61 7466 6f72 6d73 3a0a  rted Platforms:.
+0001cc40: 2020 2020 2020 2020 2020 2020 6060 4173              ``As
+0001cc50: 6365 6e64 6060 2060 6047 5055 6060 0a0a  cend`` ``GPU``..
+0001cc60: 2020 2020 2020 2020 4578 616d 706c 6573          Examples
+0001cc70: 3a0a 2020 2020 2020 2020 2020 2020 3e3e  :.            >>
+0001cc80: 3e20 696d 706f 7274 206e 756d 7079 2061  > import numpy a
+0001cc90: 7320 6e70 0a20 2020 2020 2020 2020 2020  s np.           
+0001cca0: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
+0001ccb0: 6f72 6520 696d 706f 7274 2064 7479 7065  ore import dtype
+0001ccc0: 2061 7320 6d73 7479 7065 0a20 2020 2020   as mstype.     
+0001ccd0: 2020 2020 2020 203e 3e3e 2066 726f 6d20         >>> from 
+0001cce0: 6d69 6e64 666f 726d 6572 732e 6d6f 6475  mindformers.modu
+0001ccf0: 6c65 732e 7472 616e 7366 6f72 6d65 7220  les.transformer 
+0001cd00: 696d 706f 7274 2054 7261 6e73 666f 726d  import Transform
+0001cd10: 6572 4465 636f 6465 724c 6179 6572 0a20  erDecoderLayer. 
+0001cd20: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
+0001cd30: 726f 6d20 6d69 6e64 7370 6f72 6520 696d  rom mindspore im
+0001cd40: 706f 7274 2054 656e 736f 720a 2020 2020  port Tensor.    
+0001cd50: 2020 2020 2020 2020 3e3e 3e20 6d6f 6465          >>> mode
+0001cd60: 6c20 3d20 5472 616e 7366 6f72 6d65 7244  l = TransformerD
+0001cd70: 6563 6f64 6572 4c61 7965 7228 6261 7463  ecoderLayer(batc
+0001cd80: 685f 7369 7a65 3d32 2c20 6869 6464 656e  h_size=2, hidden
+0001cd90: 5f73 697a 653d 3634 2c20 6666 6e5f 6869  _size=64, ffn_hi
+0001cda0: 6464 656e 5f73 697a 653d 3634 2c20 6e75  dden_size=64, nu
+0001cdb0: 6d5f 6865 6164 733d 322c 0a20 2020 2020  m_heads=2,.     
+0001cdc0: 2020 2020 2020 202e 2e2e 2020 2020 2020         ...      
+0001cdd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cde0: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
+0001cdf0: 6571 5f6c 656e 6774 683d 3230 2c20 7467  eq_length=20, tg
+0001ce00: 745f 7365 715f 6c65 6e67 7468 3d31 3029  t_seq_length=10)
+0001ce10: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0001ce20: 2065 6e63 6f64 6572 5f69 6e70 7574 5f76   encoder_input_v
+0001ce30: 616c 7565 203d 2054 656e 736f 7228 6e70  alue = Tensor(np
+0001ce40: 2e6f 6e65 7328 2832 2c20 3230 2c20 3634  .ones((2, 20, 64
+0001ce50: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+0001ce60: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
+0001ce70: 3e3e 3e20 6465 636f 6465 725f 696e 7075  >>> decoder_inpu
+0001ce80: 745f 7661 6c75 6520 3d20 5465 6e73 6f72  t_value = Tensor
+0001ce90: 286e 702e 6f6e 6573 2828 322c 2031 302c  (np.ones((2, 10,
+0001cea0: 2036 3429 292c 206d 7374 7970 652e 666c   64)), mstype.fl
+0001ceb0: 6f61 7433 3229 0a20 2020 2020 2020 2020  oat32).         
+0001cec0: 2020 203e 3e3e 2064 6563 6f64 6572 5f69     >>> decoder_i
+0001ced0: 6e70 7574 5f6d 6173 6b20 3d20 5465 6e73  nput_mask = Tens
+0001cee0: 6f72 286e 702e 6f6e 6573 2828 322c 2031  or(np.ones((2, 1
+0001cef0: 302c 2031 3029 292c 206d 7374 7970 652e  0, 10)), mstype.
+0001cf00: 666c 6f61 7431 3629 0a20 2020 2020 2020  float16).       
+0001cf10: 2020 2020 203e 3e3e 206d 656d 6f72 795f       >>> memory_
+0001cf20: 6d61 736b 203d 2054 656e 736f 7228 6e70  mask = Tensor(np
+0001cf30: 2e6f 6e65 7328 2832 2c20 3130 2c20 3230  .ones((2, 10, 20
+0001cf40: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+0001cf50: 3136 290a 2020 2020 2020 2020 2020 2020  16).            
+0001cf60: 3e3e 3e20 6f75 7470 7574 2c20 7061 7374  >>> output, past
+0001cf70: 203d 206d 6f64 656c 2864 6563 6f64 6572   = model(decoder
+0001cf80: 5f69 6e70 7574 5f76 616c 7565 2c20 6465  _input_value, de
+0001cf90: 636f 6465 725f 696e 7075 745f 6d61 736b  coder_input_mask
+0001cfa0: 2c20 656e 636f 6465 725f 696e 7075 745f  , encoder_input_
+0001cfb0: 7661 6c75 652c 206d 656d 6f72 795f 6d61  value, memory_ma
+0001cfc0: 736b 290a 2020 2020 2020 2020 2020 2020  sk).            
+0001cfd0: 3e3e 3e20 7072 696e 7428 6f75 7470 7574  >>> print(output
+0001cfe0: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+0001cff0: 2020 2020 2832 2c20 3130 2c20 3634 290a      (2, 10, 64).
+0001d000: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0001d010: 7072 696e 7428 7061 7374 5b30 5d2e 7368  print(past[0].sh
+0001d020: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0001d030: 2028 322c 2032 2c20 3332 2c20 3130 290a   (2, 2, 32, 10).
+0001d040: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0001d050: 7072 696e 7428 7061 7374 5b31 5d2e 7368  print(past[1].sh
+0001d060: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0001d070: 2028 322c 2032 2c20 3130 2c20 3332 290a   (2, 2, 10, 32).
+0001d080: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0001d090: 7072 696e 7428 7061 7374 5b32 5d2e 7368  print(past[2].sh
+0001d0a0: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0001d0b0: 2028 322c 2032 2c20 3332 2c20 3230 290a   (2, 2, 32, 20).
+0001d0c0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0001d0d0: 7072 696e 7428 7061 7374 5b33 5d2e 7368  print(past[3].sh
+0001d0e0: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0001d0f0: 2028 322c 2032 2c20 3230 2c20 3332 290a   (2, 2, 20, 32).
+0001d100: 2020 2020 2222 220a 0a20 2020 2040 5f4c      """..    @_L
+0001d110: 6f67 4163 7469 6f6e 4f6e 6365 286d 5f6c  ogActionOnce(m_l
+0001d120: 6f67 6765 723d 6c6f 6767 6572 2c20 6b65  ogger=logger, ke
+0001d130: 793d 2754 7261 6e73 666f 726d 6572 4465  y='TransformerDe
+0001d140: 636f 6465 724c 6179 6572 272c 0a20 2020  coderLayer',.   
+0001d150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d160: 206e 6f5f 7761 726e 696e 673d 5f67 6574   no_warning=_get
+0001d170: 5f70 6172 616c 6c65 6c5f 6d6f 6465 2829  _parallel_mode()
+0001d180: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
+0001d190: 652e 5354 414e 445f 414c 4f4e 452c 2929  e.STAND_ALONE,))
+0001d1a0: 0a20 2020 2040 5f61 7267 735f 7479 7065  .    @_args_type
+0001d1b0: 5f76 616c 6964 6174 6f72 5f63 6865 636b  _validator_check
+0001d1c0: 2868 6964 6465 6e5f 7369 7a65 3d56 616c  (hidden_size=Val
+0001d1d0: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
+0001d1e0: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
+0001d1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d200: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+0001d210: 6561 6473 3d56 616c 6964 6174 6f72 2e63  eads=Validator.c
+0001d220: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+0001d230: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+0001d240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d250: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0001d260: 7a65 3d56 616c 6964 6174 6f72 2e63 6865  ze=Validator.che
+0001d270: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+0001d280: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001d290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d2a0: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
+0001d2b0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+0001d2c0: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
+0001d2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d2e0: 2020 2020 2020 2020 2020 2020 2020 7467                tg
+0001d2f0: 745f 7365 715f 6c65 6e67 7468 3d56 616c  t_seq_length=Val
+0001d300: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
+0001d310: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
+0001d320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d330: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+0001d340: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+0001d350: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
+0001d360: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f66  k_non_negative_f
+0001d370: 6c6f 6174 2c0a 2020 2020 2020 2020 2020  loat,.          
+0001d380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d390: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+0001d3a0: 706f 7574 5f72 6174 653d 5661 6c69 6461  pout_rate=Valida
+0001d3b0: 746f 722e 6368 6563 6b5f 6e6f 6e5f 6e65  tor.check_non_ne
+0001d3c0: 6761 7469 7665 5f66 6c6f 6174 2c0a 2020  gative_float,.  
+0001d3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d3e0: 2020 2020 2020 2020 2020 2020 2020 706f                po
+0001d3f0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+0001d400: 6964 7561 6c3d 5661 6c69 6461 746f 722e  idual=Validator.
+0001d410: 6368 6563 6b5f 626f 6f6c 2c0a 2020 2020  check_bool,.    
+0001d420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d430: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
+0001d440: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
+0001d450: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
+0001d460: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
+0001d470: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
 0001d480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d490: 2020 2020 2020 2020 2020 2020 206d 7374               mst
-0001d4a0: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
-0001d4b0: 7970 652e 6266 6c6f 6174 3136 5d2c 0a20  ype.bfloat16],. 
-0001d4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d4c0: 2020 2020 6d73 7479 7065 2e66 6c6f 6174      mstype.float
+0001d4d0: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
+0001d4e0: 7431 365d 2c0a 2020 2020 2020 2020 2020  t16],.          
 0001d4f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d500: 2020 2020 2020 2020 2020 2254 7261 6e73            "Trans
-0001d510: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
-0001d520: 6572 2229 2c0a 2020 2020 2020 2020 2020  er"),.          
-0001d530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d540: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
-0001d550: 6d70 7574 655f 7479 7065 3d5f 7661 6c69  mpute_type=_vali
-0001d560: 645f 7661 6c75 655f 6368 6563 6b73 285b  d_value_checks([
-0001d570: 6d73 7479 7065 2e66 6c6f 6174 3332 2c0a  mstype.float32,.
-0001d580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d530: 2022 5472 616e 7366 6f72 6d65 7244 6563   "TransformerDec
+0001d540: 6f64 6572 4c61 7965 7222 292c 0a20 2020  oderLayer"),.   
+0001d550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d560: 2020 2020 2020 2020 2020 2020 2073 6f66               sof
+0001d570: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0001d580: 653d 5f76 616c 6964 5f76 616c 7565 5f63  e=_valid_value_c
+0001d590: 6865 636b 7328 5b6d 7374 7970 652e 666c  hecks([mstype.fl
+0001d5a0: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
 0001d5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d5c0: 2020 2020 2020 2020 2020 6d73 7479 7065            mstype
-0001d5d0: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
-0001d5e0: 2e62 666c 6f61 7431 365d 2c0a 2020 2020  .bfloat16],.    
-0001d5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d5f0: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
+0001d600: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
+0001d610: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
 0001d620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d630: 2020 2020 2022 5472 616e 7366 6f72 6d65       "Transforme
-0001d640: 7244 6563 6f64 6572 4c61 7965 7222 292c  rDecoderLayer"),
-0001d650: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d670: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-0001d680: 3d5f 7661 6c69 645f 7661 6c75 655f 6368  =_valid_value_ch
-0001d690: 6563 6b73 285b 6d73 7479 7065 2e66 6c6f  ecks([mstype.flo
-0001d6a0: 6174 3332 2c20 6d73 7479 7065 2e66 6c6f  at32, mstype.flo
-0001d6b0: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
-0001d6c0: 6f61 7431 365d 2c0a 2020 2020 2020 2020  oat16],.        
-0001d6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d650: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
+0001d660: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
+0001d670: 6179 6572 2229 2c0a 2020 2020 2020 2020  ayer"),.        
+0001d680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d690: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+0001d6a0: 6974 5f74 7970 653d 5f76 616c 6964 5f76  it_type=_valid_v
+0001d6b0: 616c 7565 5f63 6865 636b 7328 5b6d 7374  alue_checks([mst
+0001d6c0: 7970 652e 666c 6f61 7433 322c 206d 7374  ype.float32, mst
+0001d6d0: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
+0001d6e0: 7970 652e 6266 6c6f 6174 3136 5d2c 0a20  ype.bfloat16],. 
 0001d6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d700: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
-0001d710: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
-0001d720: 6179 6572 2229 2c0a 2020 2020 2020 2020  ayer"),.        
-0001d730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d740: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-0001d750: 5f63 6f6e 6669 673d 5f76 616c 6964 5f74  _config=_valid_t
-0001d760: 7970 655f 6368 6563 6b73 285b 4f70 5061  ype_checks([OpPa
-0001d770: 7261 6c6c 656c 436f 6e66 6967 2c20 4d6f  rallelConfig, Mo
-0001d780: 4550 6172 616c 6c65 6c43 6f6e 6669 675d  EParallelConfig]
-0001d790: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001d7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d730: 2020 2022 5472 616e 7366 6f72 6d65 7244     "TransformerD
+0001d740: 6563 6f64 6572 4c61 7965 7222 292c 0a20  ecoderLayer"),. 
+0001d750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d760: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0001d770: 6172 616c 6c65 6c5f 636f 6e66 6967 3d5f  arallel_config=_
+0001d780: 7661 6c69 645f 7479 7065 5f63 6865 636b  valid_type_check
+0001d790: 7328 5b4f 7050 6172 616c 6c65 6c43 6f6e  s([OpParallelCon
+0001d7a0: 6669 672c 204d 6f45 5061 7261 6c6c 656c  fig, MoEParallel
+0001d7b0: 436f 6e66 6967 5d2c 0a20 2020 2020 2020  Config],.       
 0001d7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d7d0: 2020 2020 2022 5472 616e 7366 6f72 6d65       "Transforme
-0001d7e0: 7244 6563 6f64 6572 4c61 7965 7222 292c  rDecoderLayer"),
-0001d7f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d800: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d810: 2075 7365 5f70 6173 743d 5661 6c69 6461   use_past=Valida
-0001d820: 746f 722e 6368 6563 6b5f 626f 6f6c 290a  tor.check_bool).
-0001d830: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
-0001d840: 2873 656c 662c 2068 6964 6465 6e5f 7369  (self, hidden_si
-0001d850: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-0001d860: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
-0001d870: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-0001d880: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
-0001d890: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001d8a0: 2020 2062 6174 6368 5f73 697a 652c 0a20     batch_size,. 
-0001d8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d8c0: 7372 635f 7365 715f 6c65 6e67 7468 2c0a  src_seq_length,.
-0001d8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d8e0: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
-0001d8f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d900: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
-0001d910: 6f75 745f 7261 7465 3d30 2e31 2c0a 2020  out_rate=0.1,.  
-0001d920: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-0001d930: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-0001d940: 7465 3d30 2e31 2c0a 2020 2020 2020 2020  te=0.1,.        
-0001d950: 2020 2020 2020 2020 2070 6f73 745f 6c61           post_la
-0001d960: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-0001d970: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-0001d980: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-0001d990: 743d 4661 6c73 652c 0a20 2020 2020 2020  t=False,.       
-0001d9a0: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
-0001d9b0: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-0001d9c0: 3d6d 7374 7970 652e 666c 6f61 7433 322c  =mstype.float32,
-0001d9d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d9e0: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
-0001d9f0: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
-0001da00: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-0001da10: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
-0001da20: 6974 5f74 7970 653d 6d73 7479 7065 2e66  it_type=mstype.f
-0001da30: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
-0001da40: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-0001da50: 6163 743d 2767 656c 7527 2c0a 2020 2020  act='gelu',.    
-0001da60: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-0001da70: 5f63 6f6e 6669 673d 6465 6661 756c 745f  _config=default_
-0001da80: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
-0001da90: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0001daa0: 616c 6c65 6c5f 636f 6e66 6967 3d64 6566  allel_config=def
-0001dab0: 6175 6c74 5f64 706d 705f 636f 6e66 6967  ault_dpmp_config
-0001dac0: 293a 0a20 2020 2020 2020 2073 7570 6572  ):.        super
-0001dad0: 2854 7261 6e73 666f 726d 6572 4465 636f  (TransformerDeco
-0001dae0: 6465 724c 6179 6572 2c20 7365 6c66 292e  derLayer, self).
-0001daf0: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
-0001db00: 2020 205f 6368 6563 6b5f 6d6f 655f 636f     _check_moe_co
-0001db10: 6e66 6967 286d 6f65 5f63 6f6e 6669 672c  nfig(moe_config,
-0001db20: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0001db30: 290a 2020 2020 2020 2020 7365 6c66 2e75  ).        self.u
-0001db40: 7365 5f6d 6f65 203d 2028 6d6f 655f 636f  se_moe = (moe_co
-0001db50: 6e66 6967 2e65 7870 6572 745f 6e75 6d20  nfig.expert_num 
-0001db60: 3e20 3129 0a20 2020 2020 2020 2063 6f6e  > 1).        con
-0001db70: 6669 675f 746f 5f61 7474 656e 7469 6f6e  fig_to_attention
-0001db80: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
-0001db90: 6967 2e64 706d 7020 6966 2073 656c 662e  ig.dpmp if self.
-0001dba0: 7573 655f 6d6f 6520 656c 7365 2070 6172  use_moe else par
-0001dbb0: 616c 6c65 6c5f 636f 6e66 6967 0a20 2020  allel_config.   
-0001dbc0: 2020 2020 2069 6620 6261 7463 685f 7369       if batch_si
-0001dbd0: 7a65 206f 7220 7573 655f 7061 7374 3a0a  ze or use_past:.
-0001dbe0: 2020 2020 2020 2020 2020 2020 5661 6c69              Vali
-0001dbf0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-0001dc00: 7469 7665 5f69 6e74 2862 6174 6368 5f73  tive_int(batch_s
-0001dc10: 697a 6529 0a20 2020 2020 2020 2069 6620  ize).        if 
-0001dc20: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
-0001dc30: 6465 2829 2069 6e20 2850 6172 616c 6c65  de() in (Paralle
-0001dc40: 6c4d 6f64 652e 4155 544f 5f50 4152 414c  lMode.AUTO_PARAL
-0001dc50: 4c45 4c2c 293a 0a20 2020 2020 2020 2020  LEL,):.         
-0001dc60: 2020 205f 6368 6563 6b5f 636f 6e66 6967     _check_config
-0001dc70: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0001dc80: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
-0001dc90: 206e 756d 5f68 6561 6473 2025 2070 6172   num_heads % par
-0001dca0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001dcb0: 656c 5f70 6172 616c 6c65 6c20 213d 2030  el_parallel != 0
-0001dcc0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001dcd0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0001dce0: 6f72 2822 466f 7220 2754 7261 6e73 666f  or("For 'Transfo
-0001dcf0: 726d 6572 4465 636f 6465 724c 6179 6572  rmerDecoderLayer
-0001dd00: 272c 2074 6865 2063 6c61 7373 2076 6172  ', the class var
-0001dd10: 6961 626c 6520 276e 756d 5f68 6561 6473  iable 'num_heads
-0001dd20: 2720 6d75 7374 2062 6520 6469 7669 7369  ' must be divisi
-0001dd30: 626c 6564 2062 7920 220a 2020 2020 2020  bled by ".      
-0001dd40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dd50: 2020 2020 2020 2020 2020 2022 2770 6172             "'par
-0001dd60: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001dd70: 656c 5f70 6172 616c 6c65 6c27 2c20 6275  el_parallel', bu
-0001dd80: 7420 676f 7420 7468 6520 6e75 6d5f 6865  t got the num_he
-0001dd90: 6164 7320 6973 207b 7d20 616e 6420 220a  ads is {} and ".
-0001dda0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ddb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ddc0: 2022 7061 7261 6c6c 656c 5f63 6f6e 6669   "parallel_confi
-0001ddd0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-0001dde0: 2069 7320 7b7d 2e22 2e66 6f72 6d61 7428   is {}.".format(
-0001ddf0: 6e75 6d5f 6865 6164 732c 0a20 2020 2020  num_heads,.     
-0001de00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001de10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001de20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d7f0: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
+0001d800: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
+0001d810: 6179 6572 2229 2c0a 2020 2020 2020 2020  ayer"),.        
+0001d820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d830: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
+0001d840: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+0001d850: 5f62 6f6f 6c29 0a20 2020 2064 6566 205f  _bool).    def _
+0001d860: 5f69 6e69 745f 5f28 7365 6c66 2c20 6869  _init__(self, hi
+0001d870: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0001d880: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
+0001d890: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
+0001d8a0: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
+0001d8b0: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
+0001d8c0: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+0001d8d0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+0001d8e0: 2020 2020 2020 2073 7263 5f73 6571 5f6c         src_seq_l
+0001d8f0: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+0001d900: 2020 2020 2020 2020 7467 745f 7365 715f          tgt_seq_
+0001d910: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+0001d920: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
+0001d930: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
+0001d940: 302e 312c 0a20 2020 2020 2020 2020 2020  0.1,.           
+0001d950: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+0001d960: 706f 7574 5f72 6174 653d 302e 312c 0a20  pout_rate=0.1,. 
+0001d970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d980: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+0001d990: 6573 6964 7561 6c3d 4661 6c73 652c 0a20  esidual=False,. 
+0001d9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d9b0: 7573 655f 7061 7374 3d46 616c 7365 2c0a  use_past=False,.
+0001d9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d9d0: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
+0001d9e0: 7465 5f74 7970 653d 6d73 7479 7065 2e66  te_type=mstype.f
+0001d9f0: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
+0001da00: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+0001da10: 5f63 6f6d 7075 7465 5f74 7970 653d 6d73  _compute_type=ms
+0001da20: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+0001da30: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0001da40: 6172 616d 5f69 6e69 745f 7479 7065 3d6d  aram_init_type=m
+0001da50: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
+0001da60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001da70: 6869 6464 656e 5f61 6374 3d27 6765 6c75  hidden_act='gelu
+0001da80: 272c 0a20 2020 2020 2020 2020 2020 2020  ',.             
+0001da90: 2020 2020 6d6f 655f 636f 6e66 6967 3d64      moe_config=d
+0001daa0: 6566 6175 6c74 5f6d 6f65 5f63 6f6e 6669  efault_moe_confi
+0001dab0: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
+0001dac0: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
+0001dad0: 6669 673d 6465 6661 756c 745f 6470 6d70  fig=default_dpmp
+0001dae0: 5f63 6f6e 6669 6729 3a0a 2020 2020 2020  _config):.      
+0001daf0: 2020 7375 7065 7228 5472 616e 7366 6f72    super(Transfor
+0001db00: 6d65 7244 6563 6f64 6572 4c61 7965 722c  merDecoderLayer,
+0001db10: 2073 656c 6629 2e5f 5f69 6e69 745f 5f28   self).__init__(
+0001db20: 290a 2020 2020 2020 2020 5f63 6865 636b  ).        _check
+0001db30: 5f6d 6f65 5f63 6f6e 6669 6728 6d6f 655f  _moe_config(moe_
+0001db40: 636f 6e66 6967 2c20 7061 7261 6c6c 656c  config, parallel
+0001db50: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
+0001db60: 2073 656c 662e 7573 655f 6d6f 6520 3d20   self.use_moe = 
+0001db70: 286d 6f65 5f63 6f6e 6669 672e 6578 7065  (moe_config.expe
+0001db80: 7274 5f6e 756d 203e 2031 290a 2020 2020  rt_num > 1).    
+0001db90: 2020 2020 636f 6e66 6967 5f74 6f5f 6174      config_to_at
+0001dba0: 7465 6e74 696f 6e20 3d20 7061 7261 6c6c  tention = parall
+0001dbb0: 656c 5f63 6f6e 6669 672e 6470 6d70 2069  el_config.dpmp i
+0001dbc0: 6620 7365 6c66 2e75 7365 5f6d 6f65 2065  f self.use_moe e
+0001dbd0: 6c73 6520 7061 7261 6c6c 656c 5f63 6f6e  lse parallel_con
+0001dbe0: 6669 670a 2020 2020 2020 2020 6966 2062  fig.        if b
+0001dbf0: 6174 6368 5f73 697a 6520 6f72 2075 7365  atch_size or use
+0001dc00: 5f70 6173 743a 0a20 2020 2020 2020 2020  _past:.         
+0001dc10: 2020 2056 616c 6964 6174 6f72 2e63 6865     Validator.che
+0001dc20: 636b 5f70 6f73 6974 6976 655f 696e 7428  ck_positive_int(
+0001dc30: 6261 7463 685f 7369 7a65 290a 2020 2020  batch_size).    
+0001dc40: 2020 2020 6966 205f 6765 745f 7061 7261      if _get_para
+0001dc50: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
+0001dc60: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
+0001dc70: 4f5f 5041 5241 4c4c 454c 2c29 3a0a 2020  O_PARALLEL,):.  
+0001dc80: 2020 2020 2020 2020 2020 5f63 6865 636b            _check
+0001dc90: 5f63 6f6e 6669 6728 7061 7261 6c6c 656c  _config(parallel
+0001dca0: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
+0001dcb0: 2020 2020 2069 6620 6e75 6d5f 6865 6164       if num_head
+0001dcc0: 7320 2520 7061 7261 6c6c 656c 5f63 6f6e  s % parallel_con
+0001dcd0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001dce0: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
+0001dcf0: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
+0001dd00: 616c 7565 4572 726f 7228 2246 6f72 2027  alueError("For '
+0001dd10: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
+0001dd20: 6572 4c61 7965 7227 2c20 7468 6520 636c  erLayer', the cl
+0001dd30: 6173 7320 7661 7269 6162 6c65 2027 6e75  ass variable 'nu
+0001dd40: 6d5f 6865 6164 7327 206d 7573 7420 6265  m_heads' must be
+0001dd50: 2064 6976 6973 6962 6c65 6420 6279 2022   divisibled by "
+0001dd60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001dd70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dd80: 2020 2227 7061 7261 6c6c 656c 5f63 6f6e    "'parallel_con
+0001dd90: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001dda0: 656c 272c 2062 7574 2067 6f74 2074 6865  el', but got the
+0001ddb0: 206e 756d 5f68 6561 6473 2069 7320 7b7d   num_heads is {}
+0001ddc0: 2061 6e64 2022 0a20 2020 2020 2020 2020   and ".         
+0001ddd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dde0: 2020 2020 2020 2020 2270 6172 616c 6c65          "paralle
+0001ddf0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0001de00: 6172 616c 6c65 6c20 6973 207b 7d2e 222e  arallel is {}.".
+0001de10: 666f 726d 6174 286e 756d 5f68 6561 6473  format(num_heads
+0001de20: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 0001de30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001de40: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0001de50: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0001de60: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
-0001de70: 2020 2020 2020 2020 6966 2068 6964 6465          if hidde
-0001de80: 6e5f 7369 7a65 2025 2070 6172 616c 6c65  n_size % paralle
-0001de90: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001dea0: 6172 616c 6c65 6c20 213d 2030 3a0a 2020  arallel != 0:.  
-0001deb0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-0001dec0: 6973 6520 5661 6c75 6545 7272 6f72 280a  ise ValueError(.
-0001ded0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dee0: 2020 2020 2246 6f72 2027 5472 616e 7366      "For 'Transf
-0001def0: 6f72 6d65 7244 6563 6f64 6572 4c61 7965  ormerDecoderLaye
-0001df00: 7227 2c20 7468 6520 636c 6173 7320 7661  r', the class va
-0001df10: 7269 6162 6c65 2027 6869 6464 656e 5f73  riable 'hidden_s
-0001df20: 697a 6527 206d 7573 7420 6265 2064 6976  ize' must be div
-0001df30: 6973 6962 6c65 6420 6279 2022 0a20 2020  isibled by ".   
-0001df40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001df50: 2022 2770 6172 616c 6c65 6c5f 636f 6e66   "'parallel_conf
-0001df60: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-0001df70: 6c27 2c20 6275 7420 676f 7420 7468 6520  l', but got the 
-0001df80: 6869 6464 656e 5f73 697a 6520 6973 207b  hidden_size is {
-0001df90: 7d20 616e 6420 220a 2020 2020 2020 2020  } and ".        
-0001dfa0: 2020 2020 2020 2020 2020 2020 2270 6172              "par
-0001dfb0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001dfc0: 656c 5f70 6172 616c 6c65 6c20 6973 207b  el_parallel is {
-0001dfd0: 7d2e 220a 2020 2020 2020 2020 2020 2020  }.".            
-0001dfe0: 2020 2020 2020 2020 2e66 6f72 6d61 7428          .format(
-0001dff0: 6869 6464 656e 5f73 697a 652c 2070 6172  hidden_size, par
-0001e000: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001e010: 656c 5f70 6172 616c 6c65 6c29 290a 2020  el_parallel)).  
-0001e020: 2020 2020 2020 2020 2020 6966 2066 666e            if ffn
-0001e030: 5f68 6964 6465 6e5f 7369 7a65 2025 2070  _hidden_size % p
-0001e040: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001e050: 6f64 656c 5f70 6172 616c 6c65 6c20 213d  odel_parallel !=
-0001e060: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-0001e070: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-0001e080: 7272 6f72 2822 466f 7220 2754 7261 6e73  rror("For 'Trans
-0001e090: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
-0001e0a0: 6572 272c 2074 6865 2063 6c61 7373 2076  er', the class v
-0001e0b0: 6172 6961 626c 6520 2766 666e 5f68 6964  ariable 'ffn_hid
-0001e0c0: 6465 6e5f 7369 7a65 2720 6d75 7374 2062  den_size' must b
-0001e0d0: 6520 220a 2020 2020 2020 2020 2020 2020  e ".            
-0001e0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e0f0: 2020 2020 2022 6469 7669 7369 626c 6564       "divisibled
-0001e100: 2062 7920 2770 6172 616c 6c65 6c5f 636f   by 'parallel_co
-0001e110: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-0001e120: 6c65 6c27 2c20 6275 7420 676f 7420 7468  lel', but got th
-0001e130: 6520 6666 6e5f 6869 6464 656e 5f73 697a  e ffn_hidden_siz
-0001e140: 6520 6973 207b 7d20 220a 2020 2020 2020  e is {} ".      
-0001e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e160: 2020 2020 2020 2020 2020 2022 616e 6420             "and 
-0001e170: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0001e180: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2069  model_parallel i
-0001e190: 7320 7b7d 2e22 0a20 2020 2020 2020 2020  s {}.".         
-0001e1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e1b0: 2020 2020 2020 2020 2e66 6f72 6d61 7428          .format(
-0001e1c0: 6666 6e5f 6869 6464 656e 5f73 697a 652c  ffn_hidden_size,
-0001e1d0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0001e1e0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c29  .model_parallel)
-0001e1f0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
-0001e200: 2075 7365 5f70 6173 743a 0a20 2020 2020   use_past:.     
-0001e210: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-0001e220: 2056 616c 7565 4572 726f 7228 6622 5468   ValueError(f"Th
-0001e230: 6520 7b73 656c 662e 636c 735f 6e61 6d65  e {self.cls_name
-0001e240: 7d20 646f 6573 206e 6f74 2073 7570 706f  } does not suppo
-0001e250: 7274 2075 7365 5f70 6173 743d 5472 7565  rt use_past=True
-0001e260: 2e22 290a 2020 2020 2020 2020 2020 2020  .").            
-0001e270: 7365 6c66 2e62 6174 6368 5f73 697a 6520  self.batch_size 
-0001e280: 3d20 6261 7463 685f 7369 7a65 0a20 2020  = batch_size.   
-0001e290: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
-0001e2a0: 655f 7061 7374 203d 2075 7365 5f70 6173  e_past = use_pas
-0001e2b0: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
-0001e2c0: 6c66 2e73 6f66 746d 6178 5f63 6f6d 7075  lf.softmax_compu
-0001e2d0: 7465 5f74 7970 6520 3d20 736f 6674 6d61  te_type = softma
-0001e2e0: 785f 636f 6d70 7574 655f 7479 7065 0a0a  x_compute_type..
-0001e2f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001e300: 2e73 7263 5f73 6571 5f6c 656e 6774 6820  .src_seq_length 
-0001e310: 3d20 7372 635f 7365 715f 6c65 6e67 7468  = src_seq_length
-0001e320: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001e330: 662e 7467 745f 7365 715f 6c65 6e67 7468  f.tgt_seq_length
-0001e340: 203d 2074 6774 5f73 6571 5f6c 656e 6774   = tgt_seq_lengt
-0001e350: 680a 2020 2020 2020 2020 2020 2020 7365  h.            se
-0001e360: 6c66 2e75 7365 5f70 6173 7420 3d20 7573  lf.use_past = us
-0001e370: 655f 7061 7374 0a20 2020 2020 2020 2020  e_past.         
-0001e380: 2020 2073 656c 662e 6869 6464 656e 5f73     self.hidden_s
-0001e390: 697a 6520 3d20 6869 6464 656e 5f73 697a  ize = hidden_siz
-0001e3a0: 650a 0a20 2020 2020 2020 2020 2020 2073  e..            s
-0001e3b0: 656c 662e 6c61 7965 726e 6f72 6d31 203d  elf.layernorm1 =
-0001e3c0: 204c 6179 6572 4e6f 726d 2828 6869 6464   LayerNorm((hidd
-0001e3d0: 656e 5f73 697a 652c 2929 2e74 6f5f 666c  en_size,)).to_fl
-0001e3e0: 6f61 7428 6c61 7965 726e 6f72 6d5f 636f  oat(layernorm_co
-0001e3f0: 6d70 7574 655f 7479 7065 290a 2020 2020  mpute_type).    
-0001e400: 2020 2020 2020 2020 7365 6c66 2e6c 6179          self.lay
-0001e410: 6572 6e6f 726d 3220 3d20 4c61 7965 724e  ernorm2 = LayerN
-0001e420: 6f72 6d28 2868 6964 6465 6e5f 7369 7a65  orm((hidden_size
-0001e430: 2c29 292e 746f 5f66 6c6f 6174 286c 6179  ,)).to_float(lay
-0001e440: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-0001e450: 7970 6529 0a20 2020 2020 2020 2020 2020  ype).           
-0001e460: 2073 656c 662e 6174 7465 6e74 696f 6e20   self.attention 
-0001e470: 3d20 4d75 6c74 6948 6561 6441 7474 656e  = MultiHeadAtten
-0001e480: 7469 6f6e 2868 6964 6465 6e5f 7369 7a65  tion(hidden_size
-0001e490: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
-0001e4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e4c0: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
-0001e4d0: 6d5f 6865 6164 733d 6e75 6d5f 6865 6164  m_heads=num_head
-0001e4e0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
-0001e4f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e510: 2020 2062 6174 6368 5f73 697a 653d 6261     batch_size=ba
-0001e520: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
-0001e530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e550: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-0001e560: 715f 6c65 6e67 7468 3d74 6774 5f73 6571  q_length=tgt_seq
-0001e570: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-0001e580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e5a0: 2020 2020 2020 2020 2074 6774 5f73 6571           tgt_seq
-0001e5b0: 5f6c 656e 6774 683d 7467 745f 7365 715f  _length=tgt_seq_
-0001e5c0: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
-0001e5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e5f0: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
-0001e600: 726f 706f 7574 5f72 6174 653d 6869 6464  ropout_rate=hidd
-0001e610: 656e 5f64 726f 706f 7574 5f72 6174 652c  en_dropout_rate,
-0001e620: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001e630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e650: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
-0001e660: 7574 5f72 6174 653d 6174 7465 6e74 696f  ut_rate=attentio
-0001e670: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
-0001e680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e6b0: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
-0001e6c0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0001e6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e6f0: 2020 2073 6f66 746d 6178 5f63 6f6d 7075     softmax_compu
-0001e700: 7465 5f74 7970 653d 736f 6674 6d61 785f  te_type=softmax_
-0001e710: 636f 6d70 7574 655f 7479 7065 2c0a 2020  compute_type,.  
-0001e720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e740: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0001e750: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-0001e760: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
-0001e770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e790: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0001e7a0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d63  arallel_config=c
-0001e7b0: 6f6e 6669 675f 746f 5f61 7474 656e 7469  onfig_to_attenti
-0001e7c0: 6f6e 290a 0a20 2020 2020 2020 2020 2020  on)..           
-0001e7d0: 2023 2043 726f 7373 2061 7474 656e 7469   # Cross attenti
-0001e7e0: 6f6e 2077 6974 6820 7468 6520 6f75 7470  on with the outp
-0001e7f0: 7574 206f 6620 656e 636f 6465 7220 6173  ut of encoder as
-0001e800: 206d 656d 6f72 7920 7465 6e73 6f72 0a20   memory tensor. 
-0001e810: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001e820: 6372 6f73 735f 6174 7465 6e74 696f 6e20  cross_attention 
-0001e830: 3d20 4d75 6c74 6948 6561 6441 7474 656e  = MultiHeadAtten
-0001e840: 7469 6f6e 2868 6964 6465 6e5f 7369 7a65  tion(hidden_size
-0001e850: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
-0001e860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e890: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
-0001e8a0: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
-0001e8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e8d0: 2020 2020 2020 2020 2020 2020 2020 2062                 b
-0001e8e0: 6174 6368 5f73 697a 653d 6261 7463 685f  atch_size=batch_
-0001e8f0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-0001e900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e920: 2020 2020 2020 2020 2020 2020 7372 635f              src_
-0001e930: 7365 715f 6c65 6e67 7468 3d74 6774 5f73  seq_length=tgt_s
-0001e940: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-0001e950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e980: 2074 6774 5f73 6571 5f6c 656e 6774 683d   tgt_seq_length=
-0001e990: 7372 635f 7365 715f 6c65 6e67 7468 2c0a  src_seq_length,.
-0001e9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e9d0: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
-0001e9e0: 706f 7574 5f72 6174 653d 6869 6464 656e  pout_rate=hidden
-0001e9f0: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
-0001ea00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ea10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ea20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ea30: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-0001ea40: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
-0001ea50: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-0001ea60: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-0001ea70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ea80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ea90: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
-0001eaa0: 785f 636f 6d70 7574 655f 7479 7065 3d73  x_compute_type=s
-0001eab0: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-0001eac0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-0001ead0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eaf0: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-0001eb00: 6173 743d 7573 655f 7061 7374 2c0a 2020  ast=use_past,.  
+0001de40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001de50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001de60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001de70: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0001de80: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+0001de90: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i
+0001dea0: 6620 6869 6464 656e 5f73 697a 6520 2520  f hidden_size % 
+0001deb0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001dec0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2021  model_parallel !
+0001ded0: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+0001dee0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0001def0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+0001df00: 2020 2020 2020 2020 2020 2022 466f 7220             "For 
+0001df10: 2754 7261 6e73 666f 726d 6572 4465 636f  'TransformerDeco
+0001df20: 6465 724c 6179 6572 272c 2074 6865 2063  derLayer', the c
+0001df30: 6c61 7373 2076 6172 6961 626c 6520 2768  lass variable 'h
+0001df40: 6964 6465 6e5f 7369 7a65 2720 6d75 7374  idden_size' must
+0001df50: 2062 6520 6469 7669 7369 626c 6564 2062   be divisibled b
+0001df60: 7920 220a 2020 2020 2020 2020 2020 2020  y ".            
+0001df70: 2020 2020 2020 2020 2227 7061 7261 6c6c          "'parall
+0001df80: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+0001df90: 7061 7261 6c6c 656c 272c 2062 7574 2067  parallel', but g
+0001dfa0: 6f74 2074 6865 2068 6964 6465 6e5f 7369  ot the hidden_si
+0001dfb0: 7a65 2069 7320 7b7d 2061 6e64 2022 0a20  ze is {} and ". 
+0001dfc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dfd0: 2020 2022 7061 7261 6c6c 656c 5f63 6f6e     "parallel_con
+0001dfe0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001dff0: 656c 2069 7320 7b7d 2e22 0a20 2020 2020  el is {}.".     
+0001e000: 2020 2020 2020 2020 2020 2020 2020 202e                 .
+0001e010: 666f 726d 6174 2868 6964 6465 6e5f 7369  format(hidden_si
+0001e020: 7a65 2c20 7061 7261 6c6c 656c 5f63 6f6e  ze, parallel_con
+0001e030: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001e040: 656c 2929 0a20 2020 2020 2020 2020 2020  el)).           
+0001e050: 2069 6620 6666 6e5f 6869 6464 656e 5f73   if ffn_hidden_s
+0001e060: 697a 6520 2520 7061 7261 6c6c 656c 5f63  ize % parallel_c
+0001e070: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001e080: 6c6c 656c 2021 3d20 303a 0a20 2020 2020  llel != 0:.     
+0001e090: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+0001e0a0: 2056 616c 7565 4572 726f 7228 2246 6f72   ValueError("For
+0001e0b0: 2027 5472 616e 7366 6f72 6d65 7244 6563   'TransformerDec
+0001e0c0: 6f64 6572 4c61 7965 7227 2c20 7468 6520  oderLayer', the 
+0001e0d0: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
+0001e0e0: 6666 6e5f 6869 6464 656e 5f73 697a 6527  ffn_hidden_size'
+0001e0f0: 206d 7573 7420 6265 2022 0a20 2020 2020   must be ".     
+0001e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e110: 2020 2020 2020 2020 2020 2020 2264 6976              "div
+0001e120: 6973 6962 6c65 6420 6279 2027 7061 7261  isibled by 'para
+0001e130: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+0001e140: 6c5f 7061 7261 6c6c 656c 272c 2062 7574  l_parallel', but
+0001e150: 2067 6f74 2074 6865 2066 666e 5f68 6964   got the ffn_hid
+0001e160: 6465 6e5f 7369 7a65 2069 7320 7b7d 2022  den_size is {} "
+0001e170: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001e180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e190: 2020 2261 6e64 2070 6172 616c 6c65 6c5f    "and parallel_
+0001e1a0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0001e1b0: 616c 6c65 6c20 6973 207b 7d2e 220a 2020  allel is {}.".  
+0001e1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e1d0: 2020 2020 2020 2020 2020 2020 2020 202e                 .
+0001e1e0: 666f 726d 6174 2866 666e 5f68 6964 6465  format(ffn_hidde
+0001e1f0: 6e5f 7369 7a65 2c20 7061 7261 6c6c 656c  n_size, parallel
+0001e200: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+0001e210: 7261 6c6c 656c 2929 0a20 2020 2020 2020  rallel)).       
+0001e220: 2020 2020 2069 6620 7573 655f 7061 7374       if use_past
+0001e230: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001e240: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0001e250: 6f72 2866 2254 6865 207b 7365 6c66 2e63  or(f"The {self.c
+0001e260: 6c73 5f6e 616d 657d 2064 6f65 7320 6e6f  ls_name} does no
+0001e270: 7420 7375 7070 6f72 7420 7573 655f 7061  t support use_pa
+0001e280: 7374 3d54 7275 652e 2229 0a20 2020 2020  st=True.").     
+0001e290: 2020 2020 2020 2073 656c 662e 6261 7463         self.batc
+0001e2a0: 685f 7369 7a65 203d 2062 6174 6368 5f73  h_size = batch_s
+0001e2b0: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
+0001e2c0: 7365 6c66 2e75 7365 5f70 6173 7420 3d20  self.use_past = 
+0001e2d0: 7573 655f 7061 7374 0a20 2020 2020 2020  use_past.       
+0001e2e0: 2020 2020 2073 656c 662e 736f 6674 6d61       self.softma
+0001e2f0: 785f 636f 6d70 7574 655f 7479 7065 203d  x_compute_type =
+0001e300: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+0001e310: 5f74 7970 650a 0a20 2020 2020 2020 2020  _type..         
+0001e320: 2020 2073 656c 662e 7372 635f 7365 715f     self.src_seq_
+0001e330: 6c65 6e67 7468 203d 2073 7263 5f73 6571  length = src_seq
+0001e340: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
+0001e350: 2020 2020 7365 6c66 2e74 6774 5f73 6571      self.tgt_seq
+0001e360: 5f6c 656e 6774 6820 3d20 7467 745f 7365  _length = tgt_se
+0001e370: 715f 6c65 6e67 7468 0a20 2020 2020 2020  q_length.       
+0001e380: 2020 2020 2073 656c 662e 7573 655f 7061       self.use_pa
+0001e390: 7374 203d 2075 7365 5f70 6173 740a 2020  st = use_past.  
+0001e3a0: 2020 2020 2020 2020 2020 7365 6c66 2e68            self.h
+0001e3b0: 6964 6465 6e5f 7369 7a65 203d 2068 6964  idden_size = hid
+0001e3c0: 6465 6e5f 7369 7a65 0a0a 2020 2020 2020  den_size..      
+0001e3d0: 2020 2020 2020 7365 6c66 2e6c 6179 6572        self.layer
+0001e3e0: 6e6f 726d 3120 3d20 4c61 7965 724e 6f72  norm1 = LayerNor
+0001e3f0: 6d28 2868 6964 6465 6e5f 7369 7a65 2c29  m((hidden_size,)
+0001e400: 292e 746f 5f66 6c6f 6174 286c 6179 6572  ).to_float(layer
+0001e410: 6e6f 726d 5f63 6f6d 7075 7465 5f74 7970  norm_compute_typ
+0001e420: 6529 0a20 2020 2020 2020 2020 2020 2073  e).            s
+0001e430: 656c 662e 6c61 7965 726e 6f72 6d32 203d  elf.layernorm2 =
+0001e440: 204c 6179 6572 4e6f 726d 2828 6869 6464   LayerNorm((hidd
+0001e450: 656e 5f73 697a 652c 2929 2e74 6f5f 666c  en_size,)).to_fl
+0001e460: 6f61 7428 6c61 7965 726e 6f72 6d5f 636f  oat(layernorm_co
+0001e470: 6d70 7574 655f 7479 7065 290a 2020 2020  mpute_type).    
+0001e480: 2020 2020 2020 2020 7365 6c66 2e61 7474          self.att
+0001e490: 656e 7469 6f6e 203d 204d 756c 7469 4865  ention = MultiHe
+0001e4a0: 6164 4174 7465 6e74 696f 6e28 6869 6464  adAttention(hidd
+0001e4b0: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
+0001e4c0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+0001e4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e4f0: 2020 2020 206e 756d 5f68 6561 6473 3d6e       num_heads=n
+0001e500: 756d 5f68 6561 6473 2c0a 2020 2020 2020  um_heads,.      
+0001e510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e530: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+0001e540: 7369 7a65 3d62 6174 6368 5f73 697a 652c  size=batch_size,
+0001e550: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001e560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e580: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
+0001e590: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
+0001e5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e5d0: 7467 745f 7365 715f 6c65 6e67 7468 3d74  tgt_seq_length=t
+0001e5e0: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
+0001e5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e610: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0001e620: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+0001e630: 7465 3d68 6964 6465 6e5f 6472 6f70 6f75  te=hidden_dropou
+0001e640: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
+0001e650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e670: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+0001e680: 6e5f 6472 6f70 6f75 745f 7261 7465 3d61  n_dropout_rate=a
+0001e690: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
+0001e6a0: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
+0001e6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e6c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e6d0: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+0001e6e0: 7573 655f 7061 7374 2c0a 2020 2020 2020  use_past,.      
+0001e6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e710: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
+0001e720: 785f 636f 6d70 7574 655f 7479 7065 3d73  x_compute_type=s
+0001e730: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+0001e740: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+0001e750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e770: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+0001e780: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
+0001e790: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+0001e7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e7c0: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+0001e7d0: 6f6e 6669 673d 636f 6e66 6967 5f74 6f5f  onfig=config_to_
+0001e7e0: 6174 7465 6e74 696f 6e29 0a0a 2020 2020  attention)..    
+0001e7f0: 2020 2020 2020 2020 2320 4372 6f73 7320          # Cross 
+0001e800: 6174 7465 6e74 696f 6e20 7769 7468 2074  attention with t
+0001e810: 6865 206f 7574 7075 7420 6f66 2065 6e63  he output of enc
+0001e820: 6f64 6572 2061 7320 6d65 6d6f 7279 2074  oder as memory t
+0001e830: 656e 736f 720a 2020 2020 2020 2020 2020  ensor.          
+0001e840: 2020 7365 6c66 2e63 726f 7373 5f61 7474    self.cross_att
+0001e850: 656e 7469 6f6e 203d 204d 756c 7469 4865  ention = MultiHe
+0001e860: 6164 4174 7465 6e74 696f 6e28 6869 6464  adAttention(hidd
+0001e870: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
+0001e880: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+0001e890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e8b0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+0001e8c0: 6561 6473 3d6e 756d 5f68 6561 6473 2c0a  eads=num_heads,.
+0001e8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e900: 2020 2020 2020 6261 7463 685f 7369 7a65        batch_size
+0001e910: 3d62 6174 6368 5f73 697a 652c 0a20 2020  =batch_size,.   
+0001e920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e950: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
+0001e960: 683d 7467 745f 7365 715f 6c65 6e67 7468  h=tgt_seq_length
+0001e970: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001e980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e9a0: 2020 2020 2020 2020 7467 745f 7365 715f          tgt_seq_
+0001e9b0: 6c65 6e67 7468 3d73 7263 5f73 6571 5f6c  length=src_seq_l
+0001e9c0: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+0001e9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e9f0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+0001ea00: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+0001ea10: 3d68 6964 6465 6e5f 6472 6f70 6f75 745f  =hidden_dropout_
+0001ea20: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
+0001ea30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ea40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ea50: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+0001ea60: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+0001ea70: 7465 3d61 7474 656e 7469 6f6e 5f64 726f  te=attention_dro
+0001ea80: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+0001ea90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eaa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eac0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+0001ead0: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
+0001eae0: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
+0001eaf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001eb10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eb20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eb40: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-0001eb50: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-0001eb60: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-0001eb70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eb80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eb90: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0001eba0: 6c65 6c5f 636f 6e66 6967 3d63 6f6e 6669  lel_config=confi
-0001ebb0: 675f 746f 5f61 7474 656e 7469 6f6e 290a  g_to_attention).
-0001ebc0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001ebd0: 2e63 726f 7373 5f61 7474 656e 7469 6f6e  .cross_attention
-0001ebe0: 5f6c 6179 6572 6e6f 726d 203d 204c 6179  _layernorm = Lay
-0001ebf0: 6572 4e6f 726d 2828 6869 6464 656e 5f73  erNorm((hidden_s
-0001ec00: 697a 652c 2929 2e74 6f5f 666c 6f61 7428  ize,)).to_float(
-0001ec10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ec20: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
-0001ec30: 7465 5f74 7970 6529 0a0a 2020 2020 2020  te_type)..      
-0001ec40: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
-0001ec50: 655f 6d6f 653a 0a20 2020 2020 2020 2020  e_moe:.         
-0001ec60: 2020 2020 2020 2073 656c 662e 6f75 7470         self.outp
-0001ec70: 7574 203d 204d 6f45 2868 6964 6465 6e5f  ut = MoE(hidden_
-0001ec80: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
-0001ec90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001eca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ecb0: 2020 2020 6472 6f70 6f75 745f 7261 7465      dropout_rate
-0001ecc0: 3d68 6964 6465 6e5f 6472 6f70 6f75 745f  =hidden_dropout_
-0001ecd0: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
-0001ece0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ecf0: 2020 2020 2020 2020 6666 6e5f 6869 6464          ffn_hidd
-0001ed00: 656e 5f73 697a 653d 6666 6e5f 6869 6464  en_size=ffn_hidd
-0001ed10: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-0001ed20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ed30: 2020 2020 2020 2020 2020 2070 6172 616d             param
-0001ed40: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
-0001ed50: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
-0001ed60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ed70: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-0001ed80: 6464 656e 5f61 6374 3d68 6964 6465 6e5f  dden_act=hidden_
-0001ed90: 6163 742c 0a20 2020 2020 2020 2020 2020  act,.           
-0001eda0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001edb0: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
-0001edc0: 673d 6d6f 655f 636f 6e66 6967 2c0a 2020  g=moe_config,.  
-0001edd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ede0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001edf0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-0001ee00: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-0001ee10: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
-0001ee20: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-0001ee30: 2020 2023 2046 6565 6420 466f 7277 6172     # Feed Forwar
-0001ee40: 6420 4e65 7477 6f72 6b2c 2046 464e 0a20  d Network, FFN. 
-0001ee50: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001ee60: 656c 662e 6f75 7470 7574 203d 2046 6565  elf.output = Fee
-0001ee70: 6446 6f72 7761 7264 2868 6964 6465 6e5f  dForward(hidden_
-0001ee80: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
-0001ee90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001eea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eeb0: 2020 2020 2020 2020 2020 2020 6472 6f70              drop
-0001eec0: 6f75 745f 7261 7465 3d68 6964 6465 6e5f  out_rate=hidden_
-0001eed0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-0001eee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef00: 2020 2020 2020 2020 6666 6e5f 6869 6464          ffn_hidd
-0001ef10: 656e 5f73 697a 653d 6666 6e5f 6869 6464  en_size=ffn_hidd
-0001ef20: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-0001ef30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef50: 2020 2068 6964 6465 6e5f 6163 743d 6869     hidden_act=hi
-0001ef60: 6464 656e 5f61 6374 2c0a 2020 2020 2020  dden_act,.      
-0001ef70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef90: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-0001efa0: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-0001efb0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-0001efc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001efd0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0001efe0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d70  arallel_config=p
-0001eff0: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
-0001f000: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001f010: 2e70 6f73 745f 6c61 7965 726e 6f72 6d5f  .post_layernorm_
-0001f020: 7265 7369 6475 616c 203d 2070 6f73 745f  residual = post_
-0001f030: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0001f040: 616c 0a20 2020 2020 2020 2020 2020 2073  al.            s
-0001f050: 656c 662e 6164 6420 3d20 502e 4164 6428  elf.add = P.Add(
-0001f060: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0001f070: 6c66 2e61 6464 5f33 6420 3d20 502e 4164  lf.add_3d = P.Ad
-0001f080: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
-0001f090: 7365 6c66 2e64 7479 7065 203d 206d 7374  self.dtype = mst
-0001f0a0: 7970 652e 666c 6f61 7431 360a 2020 2020  ype.float16.    
-0001f0b0: 2020 2020 2020 2020 7365 6c66 2e6b 6579          self.key
-0001f0c0: 5f70 6173 7420 3d20 4e6f 6e65 0a20 2020  _past = None.   
-0001f0d0: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
-0001f0e0: 6c75 655f 7061 7374 203d 204e 6f6e 650a  lue_past = None.
-0001f0f0: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-0001f100: 656c 662e 7573 655f 7061 7374 3a0a 2020  elf.use_past:.  
-0001f110: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-0001f120: 6f70 6572 6174 6f72 2075 7365 6420 666f  operator used fo
-0001f130: 7220 7374 6174 6520 7265 7573 650a 2020  r state reuse.  
-0001f140: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001f150: 6c66 2e72 6564 7563 6573 756d 203d 2050  lf.reducesum = P
-0001f160: 2e52 6564 7563 6553 756d 2829 2e73 6861  .ReduceSum().sha
-0001f170: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
-0001f180: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
-0001f190: 2020 2020 7365 6c66 2e6e 6f74 5f65 7175      self.not_equ
-0001f1a0: 616c 203d 2050 2e4e 6f74 4571 7561 6c28  al = P.NotEqual(
-0001f1b0: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-0001f1c0: 312c 2031 292c 2028 2929 290a 2020 2020  1, 1), ())).    
-0001f1d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001f1e0: 2e73 6c69 6365 203d 2050 2e53 7472 6964  .slice = P.Strid
-0001f1f0: 6564 536c 6963 6528 292e 7368 6172 6428  edSlice().shard(
-0001f200: 2828 312c 2031 2c20 312c 2031 292c 2929  ((1, 1, 1, 1),))
-0001f210: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001f220: 2073 697a 655f 7065 725f 6865 6164 203d   size_per_head =
-0001f230: 2068 6964 6465 6e5f 7369 7a65 202f 2f20   hidden_size // 
-0001f240: 6e75 6d5f 6865 6164 730a 2020 2020 2020  num_heads.      
-0001f250: 2020 2020 2020 2020 2020 7365 6c66 2e6b            self.k
-0001f260: 6579 5f73 6861 7065 203d 2028 6261 7463  ey_shape = (batc
-0001f270: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-0001f280: 732c 2073 697a 655f 7065 725f 6865 6164  s, size_per_head
-0001f290: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
-0001f2a0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001f2b0: 2020 7365 6c66 2e76 616c 7565 5f73 6861    self.value_sha
-0001f2c0: 7065 203d 2028 6261 7463 685f 7369 7a65  pe = (batch_size
-0001f2d0: 2c20 6e75 6d5f 6865 6164 732c 2074 6774  , num_heads, tgt
-0001f2e0: 5f73 6571 5f6c 656e 6774 682c 2073 697a  _seq_length, siz
-0001f2f0: 655f 7065 725f 6865 6164 290a 2020 2020  e_per_head).    
-0001f300: 2020 2020 2020 2020 2020 2020 2320 7061              # pa
-0001f310: 7261 6d65 7465 7273 2073 6176 696e 6720  rameters saving 
-0001f320: 6b65 7920 616e 6420 7661 6c75 6520 7374  key and value st
-0001f330: 6174 6573 0a20 2020 2020 2020 2020 2020  ates.           
-0001f340: 2020 2020 2073 656c 662e 6b65 795f 7061       self.key_pa
-0001f350: 7374 203d 2050 6172 616d 6574 6572 2854  st = Parameter(T
-0001f360: 656e 736f 7228 6e70 2e7a 6572 6f73 2873  ensor(np.zeros(s
-0001f370: 6861 7065 3d73 656c 662e 6b65 795f 7368  hape=self.key_sh
-0001f380: 6170 6529 2c20 7365 6c66 2e64 7479 7065  ape), self.dtype
-0001f390: 292c 206e 616d 653d 226b 6579 5f70 6173  ), name="key_pas
-0001f3a0: 7422 290a 2020 2020 2020 2020 2020 2020  t").            
-0001f3b0: 2020 2020 7365 6c66 2e76 616c 7565 5f70      self.value_p
-0001f3c0: 6173 7420 3d20 5061 7261 6d65 7465 7228  ast = Parameter(
-0001f3d0: 5465 6e73 6f72 286e 702e 7a65 726f 7328  Tensor(np.zeros(
-0001f3e0: 7368 6170 653d 7365 6c66 2e76 616c 7565  shape=self.value
-0001f3f0: 5f73 6861 7065 292c 2073 656c 662e 6474  _shape), self.dt
-0001f400: 7970 6529 2c20 6e61 6d65 3d22 7661 6c75  ype), name="valu
-0001f410: 655f 7061 7374 2229 0a20 2020 2020 2020  e_past").       
-0001f420: 2020 2020 2020 2020 2073 656c 662e 7469           self.ti
-0001f430: 6c65 203d 2050 2e54 696c 6528 292e 7368  le = P.Tile().sh
-0001f440: 6172 6428 2828 312c 2031 292c 2929 0a20  ard(((1, 1),)). 
-0001f450: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001f460: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
-0001f470: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-0001f480: 312c 2031 292c 2028 312c 2929 290a 2020  1, 1), (1,))).  
-0001f490: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001f4a0: 6c66 2e61 7373 6967 6e20 3d20 502e 4173  lf.assign = P.As
-0001f4b0: 7369 676e 2829 2e73 6861 7264 2828 2831  sign().shard(((1
-0001f4c0: 2c20 312c 2031 2c20 3129 2c20 2831 2c20  , 1, 1, 1), (1, 
-0001f4d0: 312c 2031 2c20 3129 2929 0a20 2020 2020  1, 1, 1))).     
-0001f4e0: 2020 2065 6c69 6620 5f67 6574 5f70 6172     elif _get_par
-0001f4f0: 616c 6c65 6c5f 6d6f 6465 2829 206e 6f74  allel_mode() not
-0001f500: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
-0001f510: 652e 4155 544f 5f50 4152 414c 4c45 4c2c  e.AUTO_PARALLEL,
-0001f520: 293a 0a20 2020 2020 2020 2020 2020 205f  ):.            _
-0001f530: 6368 6563 6b5f 636f 6e66 6967 2870 6172  check_config(par
-0001f540: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-0001f550: 2020 2020 2020 2020 2020 6966 206e 756d            if num
-0001f560: 5f68 6561 6473 2025 2070 6172 616c 6c65  _heads % paralle
-0001f570: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001f580: 6172 616c 6c65 6c20 213d 2030 3a0a 2020  arallel != 0:.  
-0001f590: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-0001f5a0: 6973 6520 5661 6c75 6545 7272 6f72 2822  ise ValueError("
-0001f5b0: 466f 7220 2754 7261 6e73 666f 726d 6572  For 'Transformer
-0001f5c0: 4465 636f 6465 724c 6179 6572 272c 2074  DecoderLayer', t
-0001f5d0: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
-0001f5e0: 6520 276e 756d 5f68 6561 6473 2720 6d75  e 'num_heads' mu
-0001f5f0: 7374 2062 6520 6469 7669 7369 626c 6564  st be divisibled
-0001f600: 2062 7920 220a 2020 2020 2020 2020 2020   by ".          
-0001f610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f620: 2020 2020 2020 2022 2770 6172 616c 6c65         "'paralle
-0001f630: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001f640: 6172 616c 6c65 6c27 2c20 6275 7420 676f  arallel', but go
-0001f650: 7420 7468 6520 6e75 6d5f 6865 6164 7320  t the num_heads 
-0001f660: 6973 207b 7d20 616e 6420 220a 2020 2020  is {} and ".    
-0001f670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f680: 2020 2020 2020 2020 2020 2020 2022 7061               "pa
-0001f690: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0001f6a0: 6465 6c5f 7061 7261 6c6c 656c 2069 7320  del_parallel is 
-0001f6b0: 7b7d 2e22 2e66 6f72 6d61 7428 6e75 6d5f  {}.".format(num_
-0001f6c0: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
-0001f6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eb20: 2020 7573 655f 7061 7374 3d75 7365 5f70    use_past=use_p
+0001eb30: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+0001eb40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eb50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eb60: 2020 2020 2020 2020 2020 2070 6172 616d             param
+0001eb70: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
+0001eb80: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
+0001eb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ebb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ebc0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0001ebd0: 673d 636f 6e66 6967 5f74 6f5f 6174 7465  g=config_to_atte
+0001ebe0: 6e74 696f 6e29 0a20 2020 2020 2020 2020  ntion).         
+0001ebf0: 2020 2073 656c 662e 6372 6f73 735f 6174     self.cross_at
+0001ec00: 7465 6e74 696f 6e5f 6c61 7965 726e 6f72  tention_layernor
+0001ec10: 6d20 3d20 4c61 7965 724e 6f72 6d28 2868  m = LayerNorm((h
+0001ec20: 6964 6465 6e5f 7369 7a65 2c29 292e 746f  idden_size,)).to
+0001ec30: 5f66 6c6f 6174 280a 2020 2020 2020 2020  _float(.        
+0001ec40: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
+0001ec50: 6d5f 636f 6d70 7574 655f 7479 7065 290a  m_compute_type).
+0001ec60: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0001ec70: 7365 6c66 2e75 7365 5f6d 6f65 3a0a 2020  self.use_moe:.  
+0001ec80: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001ec90: 6c66 2e6f 7574 7075 7420 3d20 4d6f 4528  lf.output = MoE(
+0001eca0: 6869 6464 656e 5f73 697a 653d 6869 6464  hidden_size=hidd
+0001ecb0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+0001ecc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ecd0: 2020 2020 2020 2020 2020 2064 726f 706f             dropo
+0001ece0: 7574 5f72 6174 653d 6869 6464 656e 5f64  ut_rate=hidden_d
+0001ecf0: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
+0001ed00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ed10: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+0001ed20: 666e 5f68 6964 6465 6e5f 7369 7a65 3d66  fn_hidden_size=f
+0001ed30: 666e 5f68 6964 6465 6e5f 7369 7a65 2c0a  fn_hidden_size,.
+0001ed40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ed50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ed60: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+0001ed70: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
+0001ed80: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0001ed90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eda0: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
+0001edb0: 6869 6464 656e 5f61 6374 2c0a 2020 2020  hidden_act,.    
+0001edc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001edd0: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+0001ede0: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
+0001edf0: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
+0001ee00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ee10: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+0001ee20: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
+0001ee30: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+0001ee40: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0001ee50: 2020 2020 2020 2020 2020 2320 4665 6564            # Feed
+0001ee60: 2046 6f72 7761 7264 204e 6574 776f 726b   Forward Network
+0001ee70: 2c20 4646 4e0a 2020 2020 2020 2020 2020  , FFN.          
+0001ee80: 2020 2020 2020 7365 6c66 2e6f 7574 7075        self.outpu
+0001ee90: 7420 3d20 4665 6564 466f 7277 6172 6428  t = FeedForward(
+0001eea0: 6869 6464 656e 5f73 697a 653d 6869 6464  hidden_size=hidd
+0001eeb0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+0001eec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eee0: 2020 2064 726f 706f 7574 5f72 6174 653d     dropout_rate=
+0001eef0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+0001ef00: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
+0001ef10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ef20: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+0001ef30: 666e 5f68 6964 6465 6e5f 7369 7a65 3d66  fn_hidden_size=f
+0001ef40: 666e 5f68 6964 6465 6e5f 7369 7a65 2c0a  fn_hidden_size,.
+0001ef50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ef60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ef70: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0001ef80: 5f61 6374 3d68 6964 6465 6e5f 6163 742c  _act=hidden_act,
+0001ef90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001efa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001efb0: 2020 2020 2020 2020 2020 2070 6172 616d             param
+0001efc0: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
+0001efd0: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
+0001efe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f000: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+0001f010: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
+0001f020: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+0001f030: 2020 2073 656c 662e 706f 7374 5f6c 6179     self.post_lay
+0001f040: 6572 6e6f 726d 5f72 6573 6964 7561 6c20  ernorm_residual 
+0001f050: 3d20 706f 7374 5f6c 6179 6572 6e6f 726d  = post_layernorm
+0001f060: 5f72 6573 6964 7561 6c0a 2020 2020 2020  _residual.      
+0001f070: 2020 2020 2020 7365 6c66 2e61 6464 203d        self.add =
+0001f080: 2050 2e41 6464 2829 0a20 2020 2020 2020   P.Add().       
+0001f090: 2020 2020 2073 656c 662e 6164 645f 3364       self.add_3d
+0001f0a0: 203d 2050 2e41 6464 2829 0a20 2020 2020   = P.Add().     
+0001f0b0: 2020 2020 2020 2073 656c 662e 6474 7970         self.dtyp
+0001f0c0: 6520 3d20 6d73 7479 7065 2e66 6c6f 6174  e = mstype.float
+0001f0d0: 3136 0a20 2020 2020 2020 2020 2020 2073  16.            s
+0001f0e0: 656c 662e 6b65 795f 7061 7374 203d 204e  elf.key_past = N
+0001f0f0: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
+0001f100: 7365 6c66 2e76 616c 7565 5f70 6173 7420  self.value_past 
+0001f110: 3d20 4e6f 6e65 0a20 2020 2020 2020 2020  = None.         
+0001f120: 2020 2069 6620 7365 6c66 2e75 7365 5f70     if self.use_p
+0001f130: 6173 743a 0a20 2020 2020 2020 2020 2020  ast:.           
+0001f140: 2020 2020 2023 206f 7065 7261 746f 7220       # operator 
+0001f150: 7573 6564 2066 6f72 2073 7461 7465 2072  used for state r
+0001f160: 6575 7365 0a20 2020 2020 2020 2020 2020  euse.           
+0001f170: 2020 2020 2073 656c 662e 7265 6475 6365       self.reduce
+0001f180: 7375 6d20 3d20 502e 5265 6475 6365 5375  sum = P.ReduceSu
+0001f190: 6d28 292e 7368 6172 6428 2828 312c 2031  m().shard(((1, 1
+0001f1a0: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
+0001f1b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001f1c0: 6e6f 745f 6571 7561 6c20 3d20 502e 4e6f  not_equal = P.No
+0001f1d0: 7445 7175 616c 2829 2e73 6861 7264 2828  tEqual().shard((
+0001f1e0: 2831 2c20 312c 2031 2c20 3129 2c20 2829  (1, 1, 1, 1), ()
+0001f1f0: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+0001f200: 2020 2073 656c 662e 736c 6963 6520 3d20     self.slice = 
+0001f210: 502e 5374 7269 6465 6453 6c69 6365 2829  P.StridedSlice()
+0001f220: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
+0001f230: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0001f240: 2020 2020 2020 2020 7369 7a65 5f70 6572          size_per
+0001f250: 5f68 6561 6420 3d20 6869 6464 656e 5f73  _head = hidden_s
+0001f260: 697a 6520 2f2f 206e 756d 5f68 6561 6473  ize // num_heads
+0001f270: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001f280: 2073 656c 662e 6b65 795f 7368 6170 6520   self.key_shape 
+0001f290: 3d20 2862 6174 6368 5f73 697a 652c 206e  = (batch_size, n
+0001f2a0: 756d 5f68 6561 6473 2c20 7369 7a65 5f70  um_heads, size_p
+0001f2b0: 6572 5f68 6561 642c 2074 6774 5f73 6571  er_head, tgt_seq
+0001f2c0: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
+0001f2d0: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
+0001f2e0: 6c75 655f 7368 6170 6520 3d20 2862 6174  lue_shape = (bat
+0001f2f0: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
+0001f300: 6473 2c20 7467 745f 7365 715f 6c65 6e67  ds, tgt_seq_leng
+0001f310: 7468 2c20 7369 7a65 5f70 6572 5f68 6561  th, size_per_hea
+0001f320: 6429 0a20 2020 2020 2020 2020 2020 2020  d).             
+0001f330: 2020 2023 2070 6172 616d 6574 6572 7320     # parameters 
+0001f340: 7361 7669 6e67 206b 6579 2061 6e64 2076  saving key and v
+0001f350: 616c 7565 2073 7461 7465 730a 2020 2020  alue states.    
+0001f360: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001f370: 2e6b 6579 5f70 6173 7420 3d20 5061 7261  .key_past = Para
+0001f380: 6d65 7465 7228 5465 6e73 6f72 286e 702e  meter(Tensor(np.
+0001f390: 7a65 726f 7328 7368 6170 653d 7365 6c66  zeros(shape=self
+0001f3a0: 2e6b 6579 5f73 6861 7065 292c 2073 656c  .key_shape), sel
+0001f3b0: 662e 6474 7970 6529 2c20 6e61 6d65 3d22  f.dtype), name="
+0001f3c0: 6b65 795f 7061 7374 2229 0a20 2020 2020  key_past").     
+0001f3d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001f3e0: 7661 6c75 655f 7061 7374 203d 2050 6172  value_past = Par
+0001f3f0: 616d 6574 6572 2854 656e 736f 7228 6e70  ameter(Tensor(np
+0001f400: 2e7a 6572 6f73 2873 6861 7065 3d73 656c  .zeros(shape=sel
+0001f410: 662e 7661 6c75 655f 7368 6170 6529 2c20  f.value_shape), 
+0001f420: 7365 6c66 2e64 7479 7065 292c 206e 616d  self.dtype), nam
+0001f430: 653d 2276 616c 7565 5f70 6173 7422 290a  e="value_past").
+0001f440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f450: 7365 6c66 2e74 696c 6520 3d20 502e 5469  self.tile = P.Ti
+0001f460: 6c65 2829 2e73 6861 7264 2828 2831 2c20  le().shard(((1, 
+0001f470: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+0001f480: 2020 2020 2020 7365 6c66 2e6d 756c 203d        self.mul =
+0001f490: 2050 2e4d 756c 2829 2e73 6861 7264 2828   P.Mul().shard((
+0001f4a0: 2831 2c20 312c 2031 2c20 3129 2c20 2831  (1, 1, 1, 1), (1
+0001f4b0: 2c29 2929 0a20 2020 2020 2020 2020 2020  ,))).           
+0001f4c0: 2020 2020 2073 656c 662e 6173 7369 676e       self.assign
+0001f4d0: 203d 2050 2e41 7373 6967 6e28 292e 7368   = P.Assign().sh
+0001f4e0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+0001f4f0: 292c 2028 312c 2031 2c20 312c 2031 2929  ), (1, 1, 1, 1))
+0001f500: 290a 2020 2020 2020 2020 656c 6966 205f  ).        elif _
+0001f510: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
+0001f520: 6528 2920 6e6f 7420 696e 2028 5061 7261  e() not in (Para
+0001f530: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
+0001f540: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
+0001f550: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
+0001f560: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
+0001f570: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
+0001f580: 2069 6620 6e75 6d5f 6865 6164 7320 2520   if num_heads % 
+0001f590: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001f5a0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2021  model_parallel !
+0001f5b0: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+0001f5c0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0001f5d0: 4572 726f 7228 2246 6f72 2027 5472 616e  Error("For 'Tran
+0001f5e0: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
+0001f5f0: 7965 7227 2c20 7468 6520 636c 6173 7320  yer', the class 
+0001f600: 7661 7269 6162 6c65 2027 6e75 6d5f 6865  variable 'num_he
+0001f610: 6164 7327 206d 7573 7420 6265 2064 6976  ads' must be div
+0001f620: 6973 6962 6c65 6420 6279 2022 0a20 2020  isibled by ".   
+0001f630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f640: 2020 2020 2020 2020 2020 2020 2020 2227                "'
+0001f650: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001f660: 6d6f 6465 6c5f 7061 7261 6c6c 656c 272c  model_parallel',
+0001f670: 2062 7574 2067 6f74 2074 6865 206e 756d   but got the num
+0001f680: 5f68 6561 6473 2069 7320 7b7d 2061 6e64  _heads is {} and
+0001f690: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
+0001f6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f6b0: 2020 2020 2270 6172 616c 6c65 6c5f 636f      "parallel_co
+0001f6c0: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+0001f6d0: 6c65 6c20 6973 207b 7d2e 222e 666f 726d  lel is {}.".form
+0001f6e0: 6174 286e 756d 5f68 6561 6473 2c0a 2020  at(num_heads,.  
 0001f6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001f700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f710: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-0001f720: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-0001f730: 616c 6c65 6c29 290a 2020 2020 2020 2020  allel)).        
-0001f740: 2020 2020 6966 2068 6964 6465 6e5f 7369      if hidden_si
-0001f750: 7a65 2025 2070 6172 616c 6c65 6c5f 636f  ze % parallel_co
-0001f760: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-0001f770: 6c65 6c20 213d 2030 3a0a 2020 2020 2020  lel != 0:.      
-0001f780: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-0001f790: 5661 6c75 6545 7272 6f72 280a 2020 2020  ValueError(.    
-0001f7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f7b0: 2246 6f72 2027 5472 616e 7366 6f72 6d65  "For 'Transforme
-0001f7c0: 7244 6563 6f64 6572 4c61 7965 7227 2c20  rDecoderLayer', 
-0001f7d0: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
-0001f7e0: 6c65 2027 6869 6464 656e 5f73 697a 6527  le 'hidden_size'
-0001f7f0: 206d 7573 7420 6265 2064 6976 6973 6962   must be divisib
-0001f800: 6c65 6420 6279 2022 0a20 2020 2020 2020  led by ".       
-0001f810: 2020 2020 2020 2020 2020 2020 2022 2770               "'p
-0001f820: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001f830: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
-0001f840: 6275 7420 676f 7420 7468 6520 6869 6464  but got the hidd
-0001f850: 656e 5f73 697a 6520 6973 207b 7d20 616e  en_size is {} an
-0001f860: 6420 220a 2020 2020 2020 2020 2020 2020  d ".            
-0001f870: 2020 2020 2020 2020 2270 6172 616c 6c65          "paralle
-0001f880: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001f890: 6172 616c 6c65 6c20 6973 207b 7d2e 220a  arallel is {}.".
-0001f8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f8b0: 2020 2020 2e66 6f72 6d61 7428 6869 6464      .format(hidd
-0001f8c0: 656e 5f73 697a 652c 2070 6172 616c 6c65  en_size, paralle
-0001f8d0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001f8e0: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
-0001f8f0: 2020 2020 2020 6966 2066 666e 5f68 6964        if ffn_hid
-0001f900: 6465 6e5f 7369 7a65 2025 2070 6172 616c  den_size % paral
-0001f910: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-0001f920: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
-0001f930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f940: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0001f950: 2822 466f 7220 2754 7261 6e73 666f 726d  ("For 'Transform
-0001f960: 6572 4465 636f 6465 724c 6179 6572 272c  erDecoderLayer',
-0001f970: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
-0001f980: 626c 6520 2766 666e 5f68 6964 6465 6e5f  ble 'ffn_hidden_
-0001f990: 7369 7a65 2720 6d75 7374 2062 6520 220a  size' must be ".
-0001f9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f9c0: 2022 6469 7669 7369 626c 6564 2062 7920   "divisibled by 
-0001f9d0: 2770 6172 616c 6c65 6c5f 636f 6e66 6967  'parallel_config
-0001f9e0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c27  .model_parallel'
-0001f9f0: 2c20 6275 7420 676f 7420 7468 6520 6666  , but got the ff
-0001fa00: 6e5f 6869 6464 656e 5f73 697a 6520 6973  n_hidden_size is
-0001fa10: 207b 7d20 220a 2020 2020 2020 2020 2020   {} ".          
-0001fa20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fa30: 2020 2020 2020 2022 616e 6420 7061 7261         "and para
-0001fa40: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-0001fa50: 6c5f 7061 7261 6c6c 656c 2069 7320 7b7d  l_parallel is {}
-0001fa60: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
-0001fa70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fa80: 2020 2020 2e66 6f72 6d61 7428 6666 6e5f      .format(ffn_
-0001fa90: 6869 6464 656e 5f73 697a 652c 2070 6172  hidden_size, par
-0001faa0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0001fab0: 656c 5f70 6172 616c 6c65 6c29 290a 2020  el_parallel)).  
-0001fac0: 2020 2020 2020 2020 2020 6966 2075 7365            if use
-0001fad0: 5f70 6173 743a 0a20 2020 2020 2020 2020  _past:.         
-0001fae0: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
-0001faf0: 7565 4572 726f 7228 6622 5468 6520 7b73  ueError(f"The {s
-0001fb00: 656c 662e 636c 735f 6e61 6d65 7d20 646f  elf.cls_name} do
-0001fb10: 6573 206e 6f74 2073 7570 706f 7274 2075  es not support u
-0001fb20: 7365 5f70 6173 743d 5472 7565 2e22 290a  se_past=True.").
-0001fb30: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001fb40: 2e62 6174 6368 5f73 697a 6520 3d20 6261  .batch_size = ba
-0001fb50: 7463 685f 7369 7a65 0a20 2020 2020 2020  tch_size.       
-0001fb60: 2020 2020 2073 656c 662e 7573 655f 7061       self.use_pa
-0001fb70: 7374 203d 2075 7365 5f70 6173 740a 2020  st = use_past.  
-0001fb80: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-0001fb90: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-0001fba0: 7970 6520 3d20 736f 6674 6d61 785f 636f  ype = softmax_co
-0001fbb0: 6d70 7574 655f 7479 7065 0a0a 2020 2020  mpute_type..    
-0001fbc0: 2020 2020 2020 2020 7365 6c66 2e73 7263          self.src
-0001fbd0: 5f73 6571 5f6c 656e 6774 6820 3d20 7372  _seq_length = sr
-0001fbe0: 635f 7365 715f 6c65 6e67 7468 0a20 2020  c_seq_length.   
-0001fbf0: 2020 2020 2020 2020 2073 656c 662e 7467           self.tg
-0001fc00: 745f 7365 715f 6c65 6e67 7468 203d 2074  t_seq_length = t
-0001fc10: 6774 5f73 6571 5f6c 656e 6774 680a 2020  gt_seq_length.  
-0001fc20: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
-0001fc30: 7365 5f70 6173 7420 3d20 7573 655f 7061  se_past = use_pa
-0001fc40: 7374 0a20 2020 2020 2020 2020 2020 2073  st.            s
-0001fc50: 656c 662e 6869 6464 656e 5f73 697a 6520  elf.hidden_size 
-0001fc60: 3d20 6869 6464 656e 5f73 697a 650a 0a20  = hidden_size.. 
-0001fc70: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001fc80: 6c61 7965 726e 6f72 6d31 203d 204c 6179  layernorm1 = Lay
-0001fc90: 6572 4e6f 726d 2828 6869 6464 656e 5f73  erNorm((hidden_s
-0001fca0: 697a 652c 2929 2e74 6f5f 666c 6f61 7428  ize,)).to_float(
-0001fcb0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-0001fcc0: 655f 7479 7065 290a 2020 2020 2020 2020  e_type).        
-0001fcd0: 2020 2020 7365 6c66 2e6c 6179 6572 6e6f      self.layerno
-0001fce0: 726d 312e 7368 6172 6428 2828 7061 7261  rm1.shard(((para
-0001fcf0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-0001fd00: 5f70 6172 616c 6c65 6c2c 2031 292c 2929  _parallel, 1),))
-0001fd10: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001fd20: 662e 6c61 7965 726e 6f72 6d32 203d 204c  f.layernorm2 = L
-0001fd30: 6179 6572 4e6f 726d 2828 6869 6464 656e  ayerNorm((hidden
-0001fd40: 5f73 697a 652c 2929 2e74 6f5f 666c 6f61  _size,)).to_floa
-0001fd50: 7428 6c61 7965 726e 6f72 6d5f 636f 6d70  t(layernorm_comp
-0001fd60: 7574 655f 7479 7065 290a 2020 2020 2020  ute_type).      
-0001fd70: 2020 2020 2020 7365 6c66 2e6c 6179 6572        self.layer
-0001fd80: 6e6f 726d 322e 7368 6172 6428 2828 7061  norm2.shard(((pa
-0001fd90: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-0001fda0: 7461 5f70 6172 616c 6c65 6c2c 2031 292c  ta_parallel, 1),
-0001fdb0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0001fdc0: 656c 662e 6174 7465 6e74 696f 6e20 3d20  elf.attention = 
-0001fdd0: 4d75 6c74 6948 6561 6441 7474 656e 7469  MultiHeadAttenti
-0001fde0: 6f6e 2868 6964 6465 6e5f 7369 7a65 3d68  on(hidden_size=h
-0001fdf0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-0001fe00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe20: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
-0001fe30: 6865 6164 733d 6e75 6d5f 6865 6164 732c  heads=num_heads,
-0001fe40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001fe50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe70: 2062 6174 6368 5f73 697a 653d 6261 7463   batch_size=batc
-0001fe80: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
-0001fe90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001feb0: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
-0001fec0: 6c65 6e67 7468 3d74 6774 5f73 6571 5f6c  length=tgt_seq_l
-0001fed0: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
-0001fee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff00: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
-0001ff10: 656e 6774 683d 7467 745f 7365 715f 6c65  ength=tgt_seq_le
-0001ff20: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
-0001ff30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff50: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
-0001ff60: 706f 7574 5f72 6174 653d 6869 6464 656e  pout_rate=hidden
-0001ff70: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
-0001ff80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ffa0: 2020 2020 2020 2020 2020 2020 2020 2061                 a
-0001ffb0: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
-0001ffc0: 5f72 6174 653d 6174 7465 6e74 696f 6e5f  _rate=attention_
-0001ffd0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-0001ffe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020000: 2020 2020 2020 2020 2020 2020 2020 7573                us
-00020010: 655f 7061 7374 3d75 7365 5f70 6173 742c  e_past=use_past,
-00020020: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020050: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00020060: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
-00020070: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
-00020080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200a0: 2020 2020 2020 2020 2020 2020 7061 7261              para
-000200b0: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-000200c0: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
-000200d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200f0: 2020 2020 2020 2020 2020 2020 2070 6172               par
-00020100: 616c 6c65 6c5f 636f 6e66 6967 3d63 6f6e  allel_config=con
-00020110: 6669 675f 746f 5f61 7474 656e 7469 6f6e  fig_to_attention
-00020120: 290a 0a20 2020 2020 2020 2020 2020 2023  )..            #
-00020130: 2043 726f 7373 2061 7474 656e 7469 6f6e   Cross attention
-00020140: 2077 6974 6820 7468 6520 6f75 7470 7574   with the output
-00020150: 206f 6620 656e 636f 6465 7220 6173 206d   of encoder as m
-00020160: 656d 6f72 7920 7465 6e73 6f72 0a20 2020  emory tensor.   
-00020170: 2020 2020 2020 2020 2073 656c 662e 6372           self.cr
-00020180: 6f73 735f 6174 7465 6e74 696f 6e20 3d20  oss_attention = 
-00020190: 4d75 6c74 6948 6561 6441 7474 656e 7469  MultiHeadAttenti
-000201a0: 6f6e 2868 6964 6465 6e5f 7369 7a65 3d68  on(hidden_size=h
-000201b0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-000201c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000201d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000201e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000201f0: 2020 6e75 6d5f 6865 6164 733d 6e75 6d5f    num_heads=num_
-00020200: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
-00020210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020220: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020230: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-00020240: 6368 5f73 697a 653d 6261 7463 685f 7369  ch_size=batch_si
-00020250: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00020260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020280: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-00020290: 715f 6c65 6e67 7468 3d74 6774 5f73 6571  q_length=tgt_seq
-000202a0: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-000202b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000202c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000202d0: 2020 2020 2020 2020 2020 2020 2020 2074                 t
-000202e0: 6774 5f73 6571 5f6c 656e 6774 683d 7372  gt_seq_length=sr
-000202f0: 635f 7365 715f 6c65 6e67 7468 2c0a 2020  c_seq_length,.  
-00020300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020330: 2020 2020 6869 6464 656e 5f64 726f 706f      hidden_dropo
-00020340: 7574 5f72 6174 653d 6869 6464 656e 5f64  ut_rate=hidden_d
-00020350: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
-00020360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020390: 2020 2061 7474 656e 7469 6f6e 5f64 726f     attention_dro
-000203a0: 706f 7574 5f72 6174 653d 6174 7465 6e74  pout_rate=attent
-000203b0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-000203c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000203d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000203e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000203f0: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
-00020400: 636f 6d70 7574 655f 7479 7065 3d73 6f66  compute_type=sof
-00020410: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
-00020420: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00020430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020450: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-00020460: 743d 7573 655f 7061 7374 2c0a 2020 2020  t=use_past,.    
+0001f710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f730: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0001f740: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+0001f750: 6465 6c5f 7061 7261 6c6c 656c 2929 0a20  del_parallel)). 
+0001f760: 2020 2020 2020 2020 2020 2069 6620 6869             if hi
+0001f770: 6464 656e 5f73 697a 6520 2520 7061 7261  dden_size % para
+0001f780: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+0001f790: 6c5f 7061 7261 6c6c 656c 2021 3d20 303a  l_parallel != 0:
+0001f7a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001f7b0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+0001f7c0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+0001f7d0: 2020 2020 2020 2022 466f 7220 2754 7261         "For 'Tra
+0001f7e0: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
+0001f7f0: 6179 6572 272c 2074 6865 2063 6c61 7373  ayer', the class
+0001f800: 2076 6172 6961 626c 6520 2768 6964 6465   variable 'hidde
+0001f810: 6e5f 7369 7a65 2720 6d75 7374 2062 6520  n_size' must be 
+0001f820: 6469 7669 7369 626c 6564 2062 7920 220a  divisibled by ".
+0001f830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f840: 2020 2020 2227 7061 7261 6c6c 656c 5f63      "'parallel_c
+0001f850: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001f860: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
+0001f870: 6865 2068 6964 6465 6e5f 7369 7a65 2069  he hidden_size i
+0001f880: 7320 7b7d 2061 6e64 2022 0a20 2020 2020  s {} and ".     
+0001f890: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0001f8a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001f8b0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2069  model_parallel i
+0001f8c0: 7320 7b7d 2e22 0a20 2020 2020 2020 2020  s {}.".         
+0001f8d0: 2020 2020 2020 2020 2020 202e 666f 726d             .form
+0001f8e0: 6174 2868 6964 6465 6e5f 7369 7a65 2c20  at(hidden_size, 
+0001f8f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001f900: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
+0001f910: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0001f920: 6666 6e5f 6869 6464 656e 5f73 697a 6520  ffn_hidden_size 
+0001f930: 2520 7061 7261 6c6c 656c 5f63 6f6e 6669  % parallel_confi
+0001f940: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+0001f950: 2021 3d20 303a 0a20 2020 2020 2020 2020   != 0:.         
+0001f960: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+0001f970: 7565 4572 726f 7228 2246 6f72 2027 5472  ueError("For 'Tr
+0001f980: 616e 7366 6f72 6d65 7244 6563 6f64 6572  ansformerDecoder
+0001f990: 4c61 7965 7227 2c20 7468 6520 636c 6173  Layer', the clas
+0001f9a0: 7320 7661 7269 6162 6c65 2027 6666 6e5f  s variable 'ffn_
+0001f9b0: 6869 6464 656e 5f73 697a 6527 206d 7573  hidden_size' mus
+0001f9c0: 7420 6265 2022 0a20 2020 2020 2020 2020  t be ".         
+0001f9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f9e0: 2020 2020 2020 2020 2264 6976 6973 6962          "divisib
+0001f9f0: 6c65 6420 6279 2027 7061 7261 6c6c 656c  led by 'parallel
+0001fa00: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+0001fa10: 7261 6c6c 656c 272c 2062 7574 2067 6f74  rallel', but got
+0001fa20: 2074 6865 2066 666e 5f68 6964 6465 6e5f   the ffn_hidden_
+0001fa30: 7369 7a65 2069 7320 7b7d 2022 0a20 2020  size is {} ".   
+0001fa40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fa50: 2020 2020 2020 2020 2020 2020 2020 2261                "a
+0001fa60: 6e64 2070 6172 616c 6c65 6c5f 636f 6e66  nd parallel_conf
+0001fa70: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+0001fa80: 6c20 6973 207b 7d2e 220a 2020 2020 2020  l is {}.".      
+0001fa90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001faa0: 2020 2020 2020 2020 2020 202e 666f 726d             .form
+0001fab0: 6174 2866 666e 5f68 6964 6465 6e5f 7369  at(ffn_hidden_si
+0001fac0: 7a65 2c20 7061 7261 6c6c 656c 5f63 6f6e  ze, parallel_con
+0001fad0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001fae0: 656c 2929 0a20 2020 2020 2020 2020 2020  el)).           
+0001faf0: 2069 6620 7573 655f 7061 7374 3a0a 2020   if use_past:.  
+0001fb00: 2020 2020 2020 2020 2020 2020 2020 7261                ra
+0001fb10: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
+0001fb20: 2254 6865 207b 7365 6c66 2e63 6c73 5f6e  "The {self.cls_n
+0001fb30: 616d 657d 2064 6f65 7320 6e6f 7420 7375  ame} does not su
+0001fb40: 7070 6f72 7420 7573 655f 7061 7374 3d54  pport use_past=T
+0001fb50: 7275 652e 2229 0a20 2020 2020 2020 2020  rue.").         
+0001fb60: 2020 2073 656c 662e 6261 7463 685f 7369     self.batch_si
+0001fb70: 7a65 203d 2062 6174 6368 5f73 697a 650a  ze = batch_size.
+0001fb80: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001fb90: 2e75 7365 5f70 6173 7420 3d20 7573 655f  .use_past = use_
+0001fba0: 7061 7374 0a20 2020 2020 2020 2020 2020  past.           
+0001fbb0: 2073 656c 662e 736f 6674 6d61 785f 636f   self.softmax_co
+0001fbc0: 6d70 7574 655f 7479 7065 203d 2073 6f66  mpute_type = sof
+0001fbd0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0001fbe0: 650a 0a20 2020 2020 2020 2020 2020 2073  e..            s
+0001fbf0: 656c 662e 7372 635f 7365 715f 6c65 6e67  elf.src_seq_leng
+0001fc00: 7468 203d 2073 7263 5f73 6571 5f6c 656e  th = src_seq_len
+0001fc10: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
+0001fc20: 7365 6c66 2e74 6774 5f73 6571 5f6c 656e  self.tgt_seq_len
+0001fc30: 6774 6820 3d20 7467 745f 7365 715f 6c65  gth = tgt_seq_le
+0001fc40: 6e67 7468 0a20 2020 2020 2020 2020 2020  ngth.           
+0001fc50: 2073 656c 662e 7573 655f 7061 7374 203d   self.use_past =
+0001fc60: 2075 7365 5f70 6173 740a 2020 2020 2020   use_past.      
+0001fc70: 2020 2020 2020 7365 6c66 2e68 6964 6465        self.hidde
+0001fc80: 6e5f 7369 7a65 203d 2068 6964 6465 6e5f  n_size = hidden_
+0001fc90: 7369 7a65 0a0a 2020 2020 2020 2020 2020  size..          
+0001fca0: 2020 7365 6c66 2e6c 6179 6572 6e6f 726d    self.layernorm
+0001fcb0: 3120 3d20 4c61 7965 724e 6f72 6d28 2868  1 = LayerNorm((h
+0001fcc0: 6964 6465 6e5f 7369 7a65 2c29 292e 746f  idden_size,)).to
+0001fcd0: 5f66 6c6f 6174 286c 6179 6572 6e6f 726d  _float(layernorm
+0001fce0: 5f63 6f6d 7075 7465 5f74 7970 6529 0a20  _compute_type). 
+0001fcf0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001fd00: 6c61 7965 726e 6f72 6d31 2e73 6861 7264  layernorm1.shard
+0001fd10: 2828 2870 6172 616c 6c65 6c5f 636f 6e66  (((parallel_conf
+0001fd20: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+0001fd30: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0001fd40: 2020 2020 7365 6c66 2e6c 6179 6572 6e6f      self.layerno
+0001fd50: 726d 3220 3d20 4c61 7965 724e 6f72 6d28  rm2 = LayerNorm(
+0001fd60: 2868 6964 6465 6e5f 7369 7a65 2c29 292e  (hidden_size,)).
+0001fd70: 746f 5f66 6c6f 6174 286c 6179 6572 6e6f  to_float(layerno
+0001fd80: 726d 5f63 6f6d 7075 7465 5f74 7970 6529  rm_compute_type)
+0001fd90: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0001fda0: 662e 6c61 7965 726e 6f72 6d32 2e73 6861  f.layernorm2.sha
+0001fdb0: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
+0001fdc0: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+0001fdd0: 656c 2c20 3129 2c29 290a 2020 2020 2020  el, 1),)).      
+0001fde0: 2020 2020 2020 7365 6c66 2e61 7474 656e        self.atten
+0001fdf0: 7469 6f6e 203d 204d 756c 7469 4865 6164  tion = MultiHead
+0001fe00: 4174 7465 6e74 696f 6e28 6869 6464 656e  Attention(hidden
+0001fe10: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+0001fe20: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0001fe30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fe40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fe50: 2020 206e 756d 5f68 6561 6473 3d6e 756d     num_heads=num
+0001fe60: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
+0001fe70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fe80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fe90: 2020 2020 2020 2020 6261 7463 685f 7369          batch_si
+0001fea0: 7a65 3d62 6174 6368 5f73 697a 652c 0a20  ze=batch_size,. 
+0001feb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fed0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0001fee0: 7263 5f73 6571 5f6c 656e 6774 683d 7467  rc_seq_length=tg
+0001fef0: 745f 7365 715f 6c65 6e67 7468 2c0a 2020  t_seq_length,.  
+0001ff00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff20: 2020 2020 2020 2020 2020 2020 2020 7467                tg
+0001ff30: 745f 7365 715f 6c65 6e67 7468 3d74 6774  t_seq_length=tgt
+0001ff40: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
+0001ff50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff70: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+0001ff80: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+0001ff90: 3d68 6964 6465 6e5f 6472 6f70 6f75 745f  =hidden_dropout_
+0001ffa0: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
+0001ffb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ffc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ffd0: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
+0001ffe0: 6472 6f70 6f75 745f 7261 7465 3d61 7474  dropout_rate=att
+0001fff0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+00020000: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
+00020010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020030: 2020 2020 2075 7365 5f70 6173 743d 7573       use_past=us
+00020040: 655f 7061 7374 2c0a 2020 2020 2020 2020  e_past,.        
+00020050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020070: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+00020080: 636f 6d70 7574 655f 7479 7065 3d73 6f66  compute_type=sof
+00020090: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+000200a0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000200b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000200c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000200d0: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+000200e0: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
+000200f0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+00020100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020120: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
+00020130: 6669 673d 636f 6e66 6967 5f74 6f5f 6174  fig=config_to_at
+00020140: 7465 6e74 696f 6e29 0a0a 2020 2020 2020  tention)..      
+00020150: 2020 2020 2020 2320 4372 6f73 7320 6174        # Cross at
+00020160: 7465 6e74 696f 6e20 7769 7468 2074 6865  tention with the
+00020170: 206f 7574 7075 7420 6f66 2065 6e63 6f64   output of encod
+00020180: 6572 2061 7320 6d65 6d6f 7279 2074 656e  er as memory ten
+00020190: 736f 720a 2020 2020 2020 2020 2020 2020  sor.            
+000201a0: 7365 6c66 2e63 726f 7373 5f61 7474 656e  self.cross_atten
+000201b0: 7469 6f6e 203d 204d 756c 7469 4865 6164  tion = MultiHead
+000201c0: 4174 7465 6e74 696f 6e28 6869 6464 656e  Attention(hidden
+000201d0: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+000201e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000201f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020210: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+00020220: 6473 3d6e 756d 5f68 6561 6473 2c0a 2020  ds=num_heads,.  
+00020230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020260: 2020 2020 6261 7463 685f 7369 7a65 3d62      batch_size=b
+00020270: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
+00020280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202b0: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
+000202c0: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
+000202d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020300: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
+00020310: 6e67 7468 3d73 7263 5f73 6571 5f6c 656e  ngth=src_seq_len
+00020320: 6774 682c 0a20 2020 2020 2020 2020 2020  gth,.           
+00020330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020350: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+00020360: 6e5f 6472 6f70 6f75 745f 7261 7465 3d68  n_dropout_rate=h
+00020370: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+00020380: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
+00020390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203b0: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+000203c0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+000203d0: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
+000203e0: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+000203f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020410: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00020420: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+00020430: 7970 653d 736f 6674 6d61 785f 636f 6d70  ype=softmax_comp
+00020440: 7574 655f 7479 7065 2c0a 2020 2020 2020  ute_type,.      
+00020450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020460: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00020470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000204a0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-000204b0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-000204c0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000204d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000204e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000204f0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-00020500: 6c5f 636f 6e66 6967 3d63 6f6e 6669 675f  l_config=config_
-00020510: 746f 5f61 7474 656e 7469 6f6e 290a 2020  to_attention).  
-00020520: 2020 2020 2020 2020 2020 7365 6c66 2e63            self.c
-00020530: 726f 7373 5f61 7474 656e 7469 6f6e 5f6c  ross_attention_l
-00020540: 6179 6572 6e6f 726d 203d 204c 6179 6572  ayernorm = Layer
-00020550: 4e6f 726d 2828 6869 6464 656e 5f73 697a  Norm((hidden_siz
-00020560: 652c 2929 2e74 6f5f 666c 6f61 7428 0a20  e,)).to_float(. 
-00020570: 2020 2020 2020 2020 2020 2020 2020 206c                 l
-00020580: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-00020590: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
-000205a0: 2020 2073 656c 662e 6372 6f73 735f 6174     self.cross_at
-000205b0: 7465 6e74 696f 6e5f 6c61 7965 726e 6f72  tention_layernor
-000205c0: 6d2e 7368 6172 6428 2828 7061 7261 6c6c  m.shard(((parall
-000205d0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000205e0: 6172 616c 6c65 6c2c 2031 292c 2929 0a0a  arallel, 1),))..
-000205f0: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-00020600: 656c 662e 7573 655f 6d6f 653a 0a20 2020  elf.use_moe:.   
-00020610: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00020620: 662e 6f75 7470 7574 203d 204d 6f45 2868  f.output = MoE(h
-00020630: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
-00020640: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00020650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020660: 2020 2020 2020 2020 2020 6472 6f70 6f75            dropou
-00020670: 745f 7261 7465 3d68 6964 6465 6e5f 6472  t_rate=hidden_dr
-00020680: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
-00020690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000206a0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000206b0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-000206c0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-000206d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000206e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000206f0: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00020700: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00020710: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00020720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020730: 2020 2020 6869 6464 656e 5f61 6374 3d68      hidden_act=h
-00020740: 6964 6465 6e5f 6163 742c 0a20 2020 2020  idden_act,.     
-00020750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020760: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-00020770: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-00020780: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-00020790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000207a0: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-000207b0: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
-000207c0: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
-000207d0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-000207e0: 2020 2020 2020 2020 2023 2046 6565 6420           # Feed 
-000207f0: 466f 7277 6172 6420 4e65 7477 6f72 6b2c  Forward Network,
-00020800: 2046 464e 0a20 2020 2020 2020 2020 2020   FFN.           
-00020810: 2020 2020 2073 656c 662e 6f75 7470 7574       self.output
-00020820: 203d 2046 6565 6446 6f72 7761 7264 2868   = FeedForward(h
-00020830: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
-00020840: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00020850: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020870: 2020 6472 6f70 6f75 745f 7261 7465 3d68    dropout_rate=h
-00020880: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-00020890: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-000208a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000208b0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000208c0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-000208d0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-000208e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000208f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020900: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00020910: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+00020480: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
+00020490: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000204a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000204b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000204c0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+000204d0: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
+000204e0: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
+000204f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020520: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+00020530: 636f 6e66 6967 5f74 6f5f 6174 7465 6e74  config_to_attent
+00020540: 696f 6e29 0a20 2020 2020 2020 2020 2020  ion).           
+00020550: 2073 656c 662e 6372 6f73 735f 6174 7465   self.cross_atte
+00020560: 6e74 696f 6e5f 6c61 7965 726e 6f72 6d20  ntion_layernorm 
+00020570: 3d20 4c61 7965 724e 6f72 6d28 2868 6964  = LayerNorm((hid
+00020580: 6465 6e5f 7369 7a65 2c29 292e 746f 5f66  den_size,)).to_f
+00020590: 6c6f 6174 280a 2020 2020 2020 2020 2020  loat(.          
+000205a0: 2020 2020 2020 6c61 7965 726e 6f72 6d5f        layernorm_
+000205b0: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
+000205c0: 2020 2020 2020 2020 2020 7365 6c66 2e63            self.c
+000205d0: 726f 7373 5f61 7474 656e 7469 6f6e 5f6c  ross_attention_l
+000205e0: 6179 6572 6e6f 726d 2e73 6861 7264 2828  ayernorm.shard((
+000205f0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00020600: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+00020610: 3129 2c29 290a 0a20 2020 2020 2020 2020  1),))..         
+00020620: 2020 2069 6620 7365 6c66 2e75 7365 5f6d     if self.use_m
+00020630: 6f65 3a0a 2020 2020 2020 2020 2020 2020  oe:.            
+00020640: 2020 2020 7365 6c66 2e6f 7574 7075 7420      self.output 
+00020650: 3d20 4d6f 4528 6869 6464 656e 5f73 697a  = MoE(hidden_siz
+00020660: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
+00020670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020690: 2064 726f 706f 7574 5f72 6174 653d 6869   dropout_rate=hi
+000206a0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+000206b0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000206c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000206d0: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+000206e0: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+000206f0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00020700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020710: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+00020720: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
+00020730: 6974 5f74 7970 652c 0a20 2020 2020 2020  it_type,.       
+00020740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020750: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+00020760: 6e5f 6163 743d 6869 6464 656e 5f61 6374  n_act=hidden_act
+00020770: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00020780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020790: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+000207a0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+000207b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000207c0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+000207d0: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
+000207e0: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+000207f0: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00020800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020810: 2320 4665 6564 2046 6f72 7761 7264 204e  # Feed Forward N
+00020820: 6574 776f 726b 2c20 4646 4e0a 2020 2020  etwork, FFN.    
+00020830: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00020840: 2e6f 7574 7075 7420 3d20 4665 6564 466f  .output = FeedFo
+00020850: 7277 6172 6428 6869 6464 656e 5f73 697a  rward(hidden_siz
+00020860: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
+00020870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020890: 2020 2020 2020 2020 2064 726f 706f 7574           dropout
+000208a0: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
+000208b0: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+000208c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000208d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000208e0: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+000208f0: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+00020900: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00020910: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00020920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020940: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-00020950: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-00020960: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-00020970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020990: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-000209a0: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
-000209b0: 6e66 6967 290a 2020 2020 2020 2020 2020  nfig).          
-000209c0: 2020 7365 6c66 2e70 6f73 745f 6c61 7965    self.post_laye
-000209d0: 726e 6f72 6d5f 7265 7369 6475 616c 203d  rnorm_residual =
-000209e0: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
-000209f0: 7265 7369 6475 616c 0a20 2020 2020 2020  residual.       
-00020a00: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
-00020a10: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
-00020a20: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00020a30: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
-00020a40: 292c 2028 7061 7261 6c6c 656c 5f63 6f6e  ), (parallel_con
-00020a50: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00020a60: 6c2c 2031 2929 290a 2020 2020 2020 2020  l, 1))).        
-00020a70: 2020 2020 7365 6c66 2e61 6464 5f33 6420      self.add_3d 
-00020a80: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
-00020a90: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00020aa0: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00020ab0: 2031 2c20 3129 2c20 2870 6172 616c 6c65   1, 1), (paralle
+00020930: 6869 6464 656e 5f61 6374 3d68 6964 6465  hidden_act=hidde
+00020940: 6e5f 6163 742c 0a20 2020 2020 2020 2020  n_act,.         
+00020950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020970: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+00020980: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+00020990: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000209a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000209b0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+000209c0: 6c6c 656c 5f63 6f6e 6669 673d 7061 7261  llel_config=para
+000209d0: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+000209e0: 2020 2020 2020 2020 2073 656c 662e 706f           self.po
+000209f0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00020a00: 6964 7561 6c20 3d20 706f 7374 5f6c 6179  idual = post_lay
+00020a10: 6572 6e6f 726d 5f72 6573 6964 7561 6c0a  ernorm_residual.
+00020a20: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00020a30: 2e61 6464 203d 2050 2e41 6464 2829 2e73  .add = P.Add().s
+00020a40: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+00020a50: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00020a60: 6c6c 656c 2c20 3129 2c20 2870 6172 616c  llel, 1), (paral
+00020a70: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+00020a80: 7061 7261 6c6c 656c 2c20 3129 2929 0a20  parallel, 1))). 
+00020a90: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00020aa0: 6164 645f 3364 203d 2050 2e41 6464 2829  add_3d = P.Add()
+00020ab0: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
 00020ac0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00020ad0: 7261 6c6c 656c 2c20 312c 2031 2929 290a  rallel, 1, 1))).
-00020ae0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00020af0: 2e64 7479 7065 203d 206d 7374 7970 652e  .dtype = mstype.
-00020b00: 666c 6f61 7431 360a 2020 2020 2020 2020  float16.        
-00020b10: 2020 2020 7365 6c66 2e6b 6579 5f70 6173      self.key_pas
-00020b20: 7420 3d20 4e6f 6e65 0a20 2020 2020 2020  t = None.       
-00020b30: 2020 2020 2073 656c 662e 7661 6c75 655f       self.value_
-00020b40: 7061 7374 203d 204e 6f6e 650a 2020 2020  past = None.    
-00020b50: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00020b60: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
-00020b70: 2020 2020 2020 2020 2020 2320 6f70 6572            # oper
-00020b80: 6174 6f72 2075 7365 6420 666f 7220 7374  ator used for st
-00020b90: 6174 6520 7265 7573 650a 2020 2020 2020  ate reuse.      
-00020ba0: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-00020bb0: 6564 7563 6573 756d 203d 2050 2e52 6564  educesum = P.Red
-00020bc0: 7563 6553 756d 2829 2e73 6861 7264 2828  uceSum().shard((
-00020bd0: 2831 2c20 312c 2031 2c20 3129 2c29 290a  (1, 1, 1, 1),)).
-00020be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020bf0: 7365 6c66 2e6e 6f74 5f65 7175 616c 203d  self.not_equal =
-00020c00: 2050 2e4e 6f74 4571 7561 6c28 292e 7368   P.NotEqual().sh
-00020c10: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
-00020c20: 292c 2028 2929 290a 2020 2020 2020 2020  ), ())).        
-00020c30: 2020 2020 2020 2020 7365 6c66 2e73 6c69          self.sli
-00020c40: 6365 203d 2050 2e53 7472 6964 6564 536c  ce = P.StridedSl
-00020c50: 6963 6528 292e 7368 6172 6428 2828 312c  ice().shard(((1,
-00020c60: 2031 2c20 312c 2031 292c 2929 0a20 2020   1, 1, 1),)).   
-00020c70: 2020 2020 2020 2020 2020 2020 2073 697a               siz
-00020c80: 655f 7065 725f 6865 6164 203d 2068 6964  e_per_head = hid
-00020c90: 6465 6e5f 7369 7a65 202f 2f20 6e75 6d5f  den_size // num_
-00020ca0: 6865 6164 730a 2020 2020 2020 2020 2020  heads.          
-00020cb0: 2020 2020 2020 7365 6c66 2e6b 6579 5f73        self.key_s
-00020cc0: 6861 7065 203d 2028 6261 7463 685f 7369  hape = (batch_si
-00020cd0: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
-00020ce0: 697a 655f 7065 725f 6865 6164 2c20 7467  ize_per_head, tg
-00020cf0: 745f 7365 715f 6c65 6e67 7468 290a 2020  t_seq_length).  
-00020d00: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00020d10: 6c66 2e76 616c 7565 5f73 6861 7065 203d  lf.value_shape =
-00020d20: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
-00020d30: 6d5f 6865 6164 732c 2074 6774 5f73 6571  m_heads, tgt_seq
-00020d40: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
-00020d50: 725f 6865 6164 290a 2020 2020 2020 2020  r_head).        
-00020d60: 2020 2020 2020 2020 2320 7061 7261 6d65          # parame
-00020d70: 7465 7273 2073 6176 696e 6720 6b65 7920  ters saving key 
-00020d80: 616e 6420 7661 6c75 6520 7374 6174 6573  and value states
-00020d90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020da0: 2073 656c 662e 6b65 795f 7061 7374 203d   self.key_past =
-00020db0: 2050 6172 616d 6574 6572 2854 656e 736f   Parameter(Tenso
-00020dc0: 7228 6e70 2e7a 6572 6f73 2873 6861 7065  r(np.zeros(shape
-00020dd0: 3d73 656c 662e 6b65 795f 7368 6170 6529  =self.key_shape)
-00020de0: 2c20 7365 6c66 2e64 7479 7065 292c 206e  , self.dtype), n
-00020df0: 616d 653d 226b 6579 5f70 6173 7422 290a  ame="key_past").
-00020e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020e10: 7365 6c66 2e76 616c 7565 5f70 6173 7420  self.value_past 
-00020e20: 3d20 5061 7261 6d65 7465 7228 5465 6e73  = Parameter(Tens
-00020e30: 6f72 286e 702e 7a65 726f 7328 7368 6170  or(np.zeros(shap
-00020e40: 653d 7365 6c66 2e76 616c 7565 5f73 6861  e=self.value_sha
-00020e50: 7065 292c 2073 656c 662e 6474 7970 6529  pe), self.dtype)
-00020e60: 2c20 6e61 6d65 3d22 7661 6c75 655f 7061  , name="value_pa
-00020e70: 7374 2229 0a20 2020 2020 2020 2020 2020  st").           
-00020e80: 2020 2020 2073 656c 662e 7469 6c65 203d       self.tile =
-00020e90: 2050 2e54 696c 6528 292e 7368 6172 6428   P.Tile().shard(
-00020ea0: 2828 312c 2031 292c 2929 0a20 2020 2020  ((1, 1),)).     
-00020eb0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00020ec0: 6d75 6c20 3d20 502e 4d75 6c28 292e 7368  mul = P.Mul().sh
-00020ed0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
-00020ee0: 292c 2028 312c 2929 290a 2020 2020 2020  ), (1,))).      
-00020ef0: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
-00020f00: 7373 6967 6e20 3d20 502e 4173 7369 676e  ssign = P.Assign
-00020f10: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-00020f20: 2031 2c20 3129 2c20 2831 2c20 312c 2031   1, 1), (1, 1, 1
-00020f30: 2c20 3129 2929 0a0a 2020 2020 2020 2020  , 1)))..        
-00020f40: 2020 2020 6966 2070 6172 616c 6c65 6c5f      if parallel_
-00020f50: 636f 6e66 6967 2e75 7365 5f73 6571 5f70  config.use_seq_p
-00020f60: 6172 616c 6c65 6c3a 0a20 2020 2020 2020  arallel:.       
-00020f70: 2020 2020 2020 2020 2073 656c 662e 6164           self.ad
-00020f80: 642e 7368 6172 6428 2828 7061 7261 6c6c  d.shard(((parall
-00020f90: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-00020fa0: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
-00020fb0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00020fc0: 7061 7261 6c6c 656c 2c20 3129 2c0a 2020  parallel, 1),.  
-00020fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020fe0: 2020 2020 2020 2020 2020 2020 2020 2870                (p
-00020ff0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00021000: 6174 615f 7061 7261 6c6c 656c 202a 2070  ata_parallel * p
-00021010: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00021020: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-00021030: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-00021040: 2020 2020 7365 6c66 2e6c 6179 6572 6e6f      self.layerno
-00021050: 726d 312e 7368 6172 6428 2828 7061 7261  rm1.shard(((para
-00021060: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-00021070: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
-00021080: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00021090: 6c5f 7061 7261 6c6c 656c 2c20 3129 2c29  l_parallel, 1),)
-000210a0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-000210b0: 2020 7365 6c66 2e6c 6179 6572 6e6f 726d    self.layernorm
-000210c0: 322e 7368 6172 6428 2828 7061 7261 6c6c  2.shard(((parall
-000210d0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000210e0: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
-000210f0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00021100: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
-00021110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021120: 6966 2070 6172 616c 6c65 6c5f 636f 6e66  if parallel_conf
-00021130: 6967 2e72 6563 6f6d 7075 7465 2e73 656c  ig.recompute.sel
-00021140: 6563 745f 7265 636f 6d70 7574 653a 0a20  ect_recompute:. 
-00021150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021160: 2020 2023 20e6 ada4 e5a4 84e4 bc9a e6b6     # ...........
-00021170: 88e8 8097 e8be 83e5 a4a7 e586 85e5 ad98  ................
-00021180: efbc 8ce5 bc80 e590 afe5 908e e4bc 9ae6  ................
-00021190: 8d9f e5a4 b1e4 b880 e983 a8e5 8886 e8ae  ................
-000211a0: a1e7 ae97 e680 a7e8 83bd 0a20 2020 2020  ...........     
-000211b0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-000211c0: 656c 662e 6c61 7965 726e 6f72 6d32 2e6c  elf.layernorm2.l
-000211d0: 6179 6572 5f6e 6f72 6d2e 7265 636f 6d70  ayer_norm.recomp
-000211e0: 7574 6528 290a 2020 2020 2020 2020 2020  ute().          
-000211f0: 2020 2020 2020 6966 206e 6f74 2073 656c        if not sel
-00021200: 662e 7573 655f 6d6f 653a 0a20 2020 2020  f.use_moe:.     
-00021210: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00021220: 656c 662e 6f75 7470 7574 2e70 726f 6a65  elf.output.proje
-00021230: 6374 696f 6e2e 7368 6172 6428 0a20 2020  ction.shard(.   
-00021240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021250: 2020 2020 2073 7472 6174 6567 795f 6269       strategy_bi
-00021260: 6173 3d28 2870 6172 616c 6c65 6c5f 636f  as=((parallel_co
-00021270: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-00021280: 656c 202a 2070 6172 616c 6c65 6c5f 636f  el * parallel_co
-00021290: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-000212a0: 6c65 6c2c 2031 292c 2028 312c 2929 2c0a  lel, 1), (1,)),.
-000212b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000212c0: 2020 2020 2020 2020 7374 7261 7465 6779          strategy
-000212d0: 5f6d 6174 6d75 6c3d 2828 7061 7261 6c6c  _matmul=((parall
-000212e0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000212f0: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
-00021300: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00021310: 6172 616c 6c65 6c29 2c0a 2020 2020 2020  arallel),.      
-00021320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021340: 2020 2028 7061 7261 6c6c 656c 5f63 6f6e     (parallel_con
-00021350: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-00021360: 656c 2c20 3129 292c 0a20 2020 2020 2020  el, 1)),.       
-00021370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021380: 206f 7574 5f73 7472 6174 6567 795f 6d61   out_strategy_ma
-00021390: 746d 756c 3d28 2870 6172 616c 6c65 6c5f  tmul=((parallel_
-000213a0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-000213b0: 6c6c 656c 202a 2070 6172 616c 6c65 6c5f  llel * parallel_
-000213c0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-000213d0: 616c 6c65 6c2c 2031 292c 2929 0a20 2020  allel, 1),)).   
-000213e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000213f0: 2073 656c 662e 6f75 7470 7574 2e64 726f   self.output.dro
-00021400: 706f 7574 2e64 726f 706f 7574 2e73 6861  pout.dropout.sha
-00021410: 7264 280a 2020 2020 2020 2020 2020 2020  rd(.            
-00021420: 2020 2020 2020 2020 2020 2020 2828 7061              ((pa
-00021430: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-00021440: 7461 5f70 6172 616c 6c65 6c20 2a20 7061  ta_parallel * pa
-00021450: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-00021460: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
-00021470: 2c29 290a 2020 2020 2020 2020 656c 7365  ,)).        else
-00021480: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-00021490: 6973 6520 5275 6e74 696d 6545 7272 6f72  ise RuntimeError
-000214a0: 2866 2254 6865 207b 7365 6c66 2e63 6c73  (f"The {self.cls
-000214b0: 5f6e 616d 657d 206f 6e6c 7920 7375 7070  _name} only supp
-000214c0: 6f72 7420 7368 6172 6469 6e67 2070 726f  ort sharding pro
-000214d0: 7061 6761 7469 6f6e 206f 7220 220a 2020  pagation or ".  
-000214e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000214f0: 2020 2020 2020 2020 2020 2020 2066 2273               f"s
-00021500: 656d 692d 6175 746f 2070 6172 616c 6c65  emi-auto paralle
-00021510: 6c20 6d6f 6465 206e 6f77 2e22 290a 0a20  l mode now.").. 
-00021520: 2020 2064 6566 2063 6f6e 7374 7275 6374     def construct
-00021530: 2873 656c 662c 2068 6964 6465 6e5f 7374  (self, hidden_st
-00021540: 6174 732c 0a20 2020 2020 2020 2020 2020  ats,.           
-00021550: 2020 2020 2020 2064 6563 6f64 6572 5f6d         decoder_m
-00021560: 6173 6b2c 0a20 2020 2020 2020 2020 2020  ask,.           
-00021570: 2020 2020 2020 2065 6e63 6f64 6572 5f6f         encoder_o
-00021580: 7574 7075 743d 4e6f 6e65 2c0a 2020 2020  utput=None,.    
-00021590: 2020 2020 2020 2020 2020 2020 2020 6d65                me
-000215a0: 6d6f 7279 5f6d 6173 6b3d 4e6f 6e65 2c0a  mory_mask=None,.
-000215b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000215c0: 2020 696e 6974 5f72 6573 6574 3d54 7275    init_reset=Tru
-000215d0: 652c 2062 6174 6368 5f76 616c 6964 5f6c  e, batch_valid_l
-000215e0: 656e 6774 683d 4e6f 6e65 293a 0a20 2020  ength=None):.   
-000215f0: 2020 2020 2022 2222 666f 7277 6172 6420       """forward 
-00021600: 7072 6f63 6573 7322 2222 0a20 2020 2020  process""".     
-00021610: 2020 2073 656c 662e 5f63 6865 636b 5f69     self._check_i
-00021620: 6e70 7574 2868 6964 6465 6e5f 7374 6174  nput(hidden_stat
-00021630: 732c 2064 6563 6f64 6572 5f6d 6173 6b2c  s, decoder_mask,
-00021640: 2065 6e63 6f64 6572 5f6f 7574 7075 742c   encoder_output,
-00021650: 206d 656d 6f72 795f 6d61 736b 2c20 696e   memory_mask, in
-00021660: 6974 5f72 6573 6574 2c20 6261 7463 685f  it_reset, batch_
-00021670: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
-00021680: 2020 2020 2020 2320 7468 6520 7265 7475        # the retu
-00021690: 726e 6564 2073 6861 7065 2069 7320 5b62  rned shape is [b
-000216a0: 732c 2073 6571 5f6c 656e 6774 682c 2065  s, seq_length, e
-000216b0: 6d62 6564 6469 6e67 5f73 697a 655d 206f  mbedding_size] o
-000216c0: 7220 5b62 7320 2a20 7365 715f 6c65 6e67  r [bs * seq_leng
-000216d0: 7468 2c20 656d 6265 6464 696e 675f 7369  th, embedding_si
-000216e0: 7a65 5d0a 2020 2020 2020 2020 6869 6464  ze].        hidd
-000216f0: 656e 5f73 6861 7065 203d 2046 2e73 6861  en_shape = F.sha
-00021700: 7065 2868 6964 6465 6e5f 7374 6174 7329  pe(hidden_stats)
-00021710: 0a20 2020 2020 2020 2068 6964 6465 6e5f  .        hidden_
-00021720: 7374 6174 7320 3d20 462e 7265 7368 6170  stats = F.reshap
-00021730: 6528 6869 6464 656e 5f73 7461 7473 2c20  e(hidden_stats, 
-00021740: 282d 312c 2068 6964 6465 6e5f 7368 6170  (-1, hidden_shap
-00021750: 655b 2d31 5d29 290a 2020 2020 2020 2020  e[-1])).        
-00021760: 696e 7075 745f 7820 3d20 7365 6c66 2e6c  input_x = self.l
-00021770: 6179 6572 6e6f 726d 3128 6869 6464 656e  ayernorm1(hidden
-00021780: 5f73 7461 7473 290a 2020 2020 2020 2020  _stats).        
-00021790: 696e 7075 745f 7820 3d20 462e 6361 7374  input_x = F.cast
-000217a0: 2869 6e70 7574 5f78 2c20 7365 6c66 2e64  (input_x, self.d
-000217b0: 7479 7065 290a 0a20 2020 2020 2020 2023  type)..        #
-000217c0: 2069 6e64 6963 6174 6520 7768 6574 6865   indicate whethe
-000217d0: 7220 7265 7365 7420 7361 7665 6420 7374  r reset saved st
-000217e0: 6174 6573 0a20 2020 2020 2020 206b 6579  ates.        key
-000217f0: 5f72 6573 6574 203d 204e 6f6e 650a 2020  _reset = None.  
-00021800: 2020 2020 2020 7661 6c75 655f 7265 7365        value_rese
-00021810: 7420 3d20 4e6f 6e65 0a20 2020 2020 2020  t = None.       
-00021820: 2069 6620 7365 6c66 2e75 7365 5f70 6173   if self.use_pas
-00021830: 743a 0a20 2020 2020 2020 2020 2020 2023  t:.            #
-00021840: 2072 6573 6574 2073 7461 7465 732c 2069   reset states, i
-00021850: 6e69 745f 7265 7365 7420 5472 7565 2066  nit_reset True f
-00021860: 6f72 2072 6575 7365 2061 6e64 2046 616c  or reuse and Fal
-00021870: 7365 2066 6f72 2072 6573 6574 0a20 2020  se for reset.   
-00021880: 2020 2020 2020 2020 2073 656c 662e 6173           self.as
-00021890: 7369 676e 2873 656c 662e 6b65 795f 7061  sign(self.key_pa
-000218a0: 7374 2c20 7365 6c66 2e6d 756c 2873 656c  st, self.mul(sel
-000218b0: 662e 6b65 795f 7061 7374 2c20 462e 6361  f.key_past, F.ca
-000218c0: 7374 2869 6e69 745f 7265 7365 742c 2073  st(init_reset, s
-000218d0: 656c 662e 6474 7970 6529 2929 0a20 2020  elf.dtype))).   
-000218e0: 2020 2020 2020 2020 206b 6579 5f72 6573           key_res
-000218f0: 6574 203d 2073 656c 662e 6b65 795f 7061  et = self.key_pa
-00021900: 7374 0a20 2020 2020 2020 2020 2020 2073  st.            s
-00021910: 656c 662e 6173 7369 676e 2873 656c 662e  elf.assign(self.
-00021920: 7661 6c75 655f 7061 7374 2c20 7365 6c66  value_past, self
-00021930: 2e6d 756c 2873 656c 662e 7661 6c75 655f  .mul(self.value_
-00021940: 7061 7374 2c20 462e 6361 7374 2869 6e69  past, F.cast(ini
-00021950: 745f 7265 7365 742c 2073 656c 662e 6474  t_reset, self.dt
-00021960: 7970 6529 2929 0a20 2020 2020 2020 2020  ype))).         
-00021970: 2020 2076 616c 7565 5f72 6573 6574 203d     value_reset =
-00021980: 2073 656c 662e 7661 6c75 655f 7061 7374   self.value_past
-00021990: 0a20 2020 2020 2020 2020 2020 2023 2061  .            # a
-000219a0: 6464 2064 6570 656e 6465 6e63 7920 666f  dd dependency fo
-000219b0: 7220 6465 7369 7265 6420 6578 6563 7574  r desired execut
-000219c0: 696f 6e20 6f72 6465 720a 2020 2020 2020  ion order.      
-000219d0: 2020 2020 2020 696e 7075 745f 7820 3d20        input_x = 
-000219e0: 462e 6465 7065 6e64 2869 6e70 7574 5f78  F.depend(input_x
-000219f0: 2c20 6b65 795f 7265 7365 7429 0a20 2020  , key_reset).   
-00021a00: 2020 2020 2020 2020 2069 6e70 7574 5f78           input_x
-00021a10: 203d 2046 2e64 6570 656e 6428 696e 7075   = F.depend(inpu
-00021a20: 745f 782c 2076 616c 7565 5f72 6573 6574  t_x, value_reset
-00021a30: 290a 0a20 2020 2020 2020 2061 7474 656e  )..        atten
-00021a40: 7469 6f6e 2c20 6c61 7965 725f 7072 6573  tion, layer_pres
-00021a50: 656e 7420 3d20 7365 6c66 2e61 7474 656e  ent = self.atten
-00021a60: 7469 6f6e 2869 6e70 7574 5f78 2c20 696e  tion(input_x, in
-00021a70: 7075 745f 782c 2069 6e70 7574 5f78 2c20  put_x, input_x, 
-00021a80: 6465 636f 6465 725f 6d61 736b 2c20 7365  decoder_mask, se
-00021a90: 6c66 2e6b 6579 5f70 6173 742c 0a20 2020  lf.key_past,.   
-00021aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021ac0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00021ad0: 656c 662e 7661 6c75 655f 7061 7374 2c20  elf.value_past, 
-00021ae0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-00021af0: 7468 290a 2020 2020 2020 2020 2320 466f  th).        # Fo
-00021b00: 7220 706f 7374 2d6c 6179 6572 6e6f 726d  r post-layernorm
-00021b10: 2074 6865 2069 6e70 7574 7320 666f 7220   the inputs for 
-00021b20: 7265 7369 6475 616c 2070 6174 6820 6172  residual path ar
-00021b30: 6520 6f75 7470 7574 206f 6620 7365 6c66  e output of self
-00021b40: 2d61 7474 656e 7469 6f6e 2061 6e64 206f  -attention and o
-00021b50: 7574 7075 7420 6f66 206c 6179 6572 6e6f  utput of layerno
-00021b60: 726d 0a20 2020 2020 2020 2069 6620 7365  rm.        if se
-00021b70: 6c66 2e70 6f73 745f 6c61 7965 726e 6f72  lf.post_layernor
-00021b80: 6d5f 7265 7369 6475 616c 3a0a 2020 2020  m_residual:.    
-00021b90: 2020 2020 2020 2020 7820 3d20 7365 6c66          x = self
-00021ba0: 2e61 6464 2869 6e70 7574 5f78 2c20 6174  .add(input_x, at
-00021bb0: 7465 6e74 696f 6e29 0a20 2020 2020 2020  tention).       
-00021bc0: 2023 2046 6f72 2070 7265 2d6c 6179 6572   # For pre-layer
-00021bd0: 6e6f 726d 2074 6865 2069 6e70 7574 7320  norm the inputs 
-00021be0: 666f 7220 7265 7369 6475 616c 2070 6174  for residual pat
-00021bf0: 6820 6172 6520 6f75 7470 7574 206f 6620  h are output of 
-00021c00: 7365 6c66 2d61 7474 656e 7469 6f6e 2061  self-attention a
-00021c10: 6e64 2069 6e70 7574 206f 6620 7468 6973  nd input of this
-00021c20: 206c 6179 6572 0a20 2020 2020 2020 2065   layer.        e
-00021c30: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-00021c40: 2078 203d 2073 656c 662e 6164 6428 6869   x = self.add(hi
-00021c50: 6464 656e 5f73 7461 7473 2c20 6174 7465  dden_stats, atte
-00021c60: 6e74 696f 6e29 0a0a 2020 2020 2020 2020  ntion)..        
-00021c70: 6d69 6464 6c65 5f6f 7574 7075 7420 3d20  middle_output = 
-00021c80: 4e6f 6e65 0a20 2020 2020 2020 2069 6620  None.        if 
-00021c90: 656e 636f 6465 725f 6f75 7470 7574 2069  encoder_output i
-00021ca0: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
-00021cb0: 2020 2020 2020 2020 6d69 6464 6c65 5f6f          middle_o
-00021cc0: 7574 7075 7420 3d20 7365 6c66 2e63 726f  utput = self.cro
-00021cd0: 7373 5f61 7474 656e 7469 6f6e 5f6c 6179  ss_attention_lay
-00021ce0: 6572 6e6f 726d 2878 290a 2020 2020 2020  ernorm(x).      
-00021cf0: 2020 2020 2020 6d69 6464 6c65 5f6f 7574        middle_out
-00021d00: 7075 7420 3d20 462e 6361 7374 286d 6964  put = F.cast(mid
-00021d10: 646c 655f 6f75 7470 7574 2c20 7365 6c66  dle_output, self
-00021d20: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-00021d30: 2020 2020 656e 636f 6465 725f 6f75 7470      encoder_outp
-00021d40: 7574 203d 2046 2e63 6173 7428 656e 636f  ut = F.cast(enco
-00021d50: 6465 725f 6f75 7470 7574 2c20 7365 6c66  der_output, self
-00021d60: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-00021d70: 2020 2020 6372 6f73 735f 6174 746e 5f6f      cross_attn_o
-00021d80: 7574 7075 742c 2063 726f 7373 5f6c 6179  utput, cross_lay
-00021d90: 6572 5f70 7265 7365 6e74 203d 2073 656c  er_present = sel
-00021da0: 662e 6372 6f73 735f 6174 7465 6e74 696f  f.cross_attentio
-00021db0: 6e28 6d69 6464 6c65 5f6f 7574 7075 742c  n(middle_output,
-00021dc0: 2065 6e63 6f64 6572 5f6f 7574 7075 742c   encoder_output,
-00021dd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020ad0: 7261 6c6c 656c 2c20 312c 2031 292c 2028  rallel, 1, 1), (
+00020ae0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00020af0: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
+00020b00: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
+00020b10: 2020 2073 656c 662e 6474 7970 6520 3d20     self.dtype = 
+00020b20: 6d73 7479 7065 2e66 6c6f 6174 3136 0a20  mstype.float16. 
+00020b30: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00020b40: 6b65 795f 7061 7374 203d 204e 6f6e 650a  key_past = None.
+00020b50: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00020b60: 2e76 616c 7565 5f70 6173 7420 3d20 4e6f  .value_past = No
+00020b70: 6e65 0a20 2020 2020 2020 2020 2020 2069  ne.            i
+00020b80: 6620 7365 6c66 2e75 7365 5f70 6173 743a  f self.use_past:
+00020b90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020ba0: 2023 206f 7065 7261 746f 7220 7573 6564   # operator used
+00020bb0: 2066 6f72 2073 7461 7465 2072 6575 7365   for state reuse
+00020bc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020bd0: 2073 656c 662e 7265 6475 6365 7375 6d20   self.reducesum 
+00020be0: 3d20 502e 5265 6475 6365 5375 6d28 292e  = P.ReduceSum().
+00020bf0: 7368 6172 6428 2828 312c 2031 2c20 312c  shard(((1, 1, 1,
+00020c00: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00020c10: 2020 2020 2020 2073 656c 662e 6e6f 745f         self.not_
+00020c20: 6571 7561 6c20 3d20 502e 4e6f 7445 7175  equal = P.NotEqu
+00020c30: 616c 2829 2e73 6861 7264 2828 2831 2c20  al().shard(((1, 
+00020c40: 312c 2031 2c20 3129 2c20 2829 2929 0a20  1, 1, 1), ())). 
+00020c50: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00020c60: 656c 662e 736c 6963 6520 3d20 502e 5374  elf.slice = P.St
+00020c70: 7269 6465 6453 6c69 6365 2829 2e73 6861  ridedSlice().sha
+00020c80: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
+00020c90: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+00020ca0: 2020 2020 7369 7a65 5f70 6572 5f68 6561      size_per_hea
+00020cb0: 6420 3d20 6869 6464 656e 5f73 697a 6520  d = hidden_size 
+00020cc0: 2f2f 206e 756d 5f68 6561 6473 0a20 2020  // num_heads.   
+00020cd0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00020ce0: 662e 6b65 795f 7368 6170 6520 3d20 2862  f.key_shape = (b
+00020cf0: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+00020d00: 6561 6473 2c20 7369 7a65 5f70 6572 5f68  eads, size_per_h
+00020d10: 6561 642c 2074 6774 5f73 6571 5f6c 656e  ead, tgt_seq_len
+00020d20: 6774 6829 0a20 2020 2020 2020 2020 2020  gth).           
+00020d30: 2020 2020 2073 656c 662e 7661 6c75 655f       self.value_
+00020d40: 7368 6170 6520 3d20 2862 6174 6368 5f73  shape = (batch_s
+00020d50: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
+00020d60: 7467 745f 7365 715f 6c65 6e67 7468 2c20  tgt_seq_length, 
+00020d70: 7369 7a65 5f70 6572 5f68 6561 6429 0a20  size_per_head). 
+00020d80: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00020d90: 2070 6172 616d 6574 6572 7320 7361 7669   parameters savi
+00020da0: 6e67 206b 6579 2061 6e64 2076 616c 7565  ng key and value
+00020db0: 2073 7461 7465 730a 2020 2020 2020 2020   states.        
+00020dc0: 2020 2020 2020 2020 7365 6c66 2e6b 6579          self.key
+00020dd0: 5f70 6173 7420 3d20 5061 7261 6d65 7465  _past = Paramete
+00020de0: 7228 5465 6e73 6f72 286e 702e 7a65 726f  r(Tensor(np.zero
+00020df0: 7328 7368 6170 653d 7365 6c66 2e6b 6579  s(shape=self.key
+00020e00: 5f73 6861 7065 292c 2073 656c 662e 6474  _shape), self.dt
+00020e10: 7970 6529 2c20 6e61 6d65 3d22 6b65 795f  ype), name="key_
+00020e20: 7061 7374 2229 0a20 2020 2020 2020 2020  past").         
+00020e30: 2020 2020 2020 2073 656c 662e 7661 6c75         self.valu
+00020e40: 655f 7061 7374 203d 2050 6172 616d 6574  e_past = Paramet
+00020e50: 6572 2854 656e 736f 7228 6e70 2e7a 6572  er(Tensor(np.zer
+00020e60: 6f73 2873 6861 7065 3d73 656c 662e 7661  os(shape=self.va
+00020e70: 6c75 655f 7368 6170 6529 2c20 7365 6c66  lue_shape), self
+00020e80: 2e64 7479 7065 292c 206e 616d 653d 2276  .dtype), name="v
+00020e90: 616c 7565 5f70 6173 7422 290a 2020 2020  alue_past").    
+00020ea0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00020eb0: 2e74 696c 6520 3d20 502e 5469 6c65 2829  .tile = P.Tile()
+00020ec0: 2e73 6861 7264 2828 2831 2c20 3129 2c29  .shard(((1, 1),)
+00020ed0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00020ee0: 2020 7365 6c66 2e6d 756c 203d 2050 2e4d    self.mul = P.M
+00020ef0: 756c 2829 2e73 6861 7264 2828 2831 2c20  ul().shard(((1, 
+00020f00: 312c 2031 2c20 3129 2c20 2831 2c29 2929  1, 1, 1), (1,)))
+00020f10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020f20: 2073 656c 662e 6173 7369 676e 203d 2050   self.assign = P
+00020f30: 2e41 7373 6967 6e28 292e 7368 6172 6428  .Assign().shard(
+00020f40: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
+00020f50: 312c 2031 2c20 312c 2031 2929 290a 0a20  1, 1, 1, 1))).. 
+00020f60: 2020 2020 2020 2020 2020 2069 6620 7061             if pa
+00020f70: 7261 6c6c 656c 5f63 6f6e 6669 672e 7573  rallel_config.us
+00020f80: 655f 7365 715f 7061 7261 6c6c 656c 3a0a  e_seq_parallel:.
+00020f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020fa0: 7365 6c66 2e61 6464 2e73 6861 7264 2828  self.add.shard((
+00020fb0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00020fc0: 2e64 6174 615f 7061 7261 6c6c 656c 202a  .data_parallel *
+00020fd0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00020fe0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+00020ff0: 2031 292c 0a20 2020 2020 2020 2020 2020   1),.           
+00021000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021010: 2020 2020 2028 7061 7261 6c6c 656c 5f63       (parallel_c
+00021020: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00021030: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
+00021040: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00021050: 6c6c 656c 2c20 3129 2929 0a20 2020 2020  llel, 1))).     
+00021060: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00021070: 6c61 7965 726e 6f72 6d31 2e73 6861 7264  layernorm1.shard
+00021080: 2828 2870 6172 616c 6c65 6c5f 636f 6e66  (((parallel_conf
+00021090: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+000210a0: 202a 2070 6172 616c 6c65 6c5f 636f 6e66   * parallel_conf
+000210b0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+000210c0: 6c2c 2031 292c 2929 0a20 2020 2020 2020  l, 1),)).       
+000210d0: 2020 2020 2020 2020 2073 656c 662e 6c61           self.la
+000210e0: 7965 726e 6f72 6d32 2e73 6861 7264 2828  yernorm2.shard((
+000210f0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00021100: 2e64 6174 615f 7061 7261 6c6c 656c 202a  .data_parallel *
+00021110: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00021120: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+00021130: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00021140: 2020 2020 2020 2069 6620 7061 7261 6c6c         if parall
+00021150: 656c 5f63 6f6e 6669 672e 7265 636f 6d70  el_config.recomp
+00021160: 7574 652e 7365 6c65 6374 5f72 6563 6f6d  ute.select_recom
+00021170: 7075 7465 3a0a 2020 2020 2020 2020 2020  pute:.          
+00021180: 2020 2020 2020 2020 2020 2320 e6ad a4e5            # ....
+00021190: a484 e4bc 9ae6 b688 e880 97e8 be83 e5a4  ................
+000211a0: a7e5 8685 e5ad 98ef bc8c e5bc 80e5 90af  ................
+000211b0: e590 8ee4 bc9a e68d 9fe5 a4b1 e4b8 80e9  ................
+000211c0: 83a8 e588 86e8 aea1 e7ae 97e6 80a7 e883  ................
+000211d0: bd0a 2020 2020 2020 2020 2020 2020 2020  ..              
+000211e0: 2020 2020 2020 7365 6c66 2e6c 6179 6572        self.layer
+000211f0: 6e6f 726d 322e 6c61 7965 725f 6e6f 726d  norm2.layer_norm
+00021200: 2e72 6563 6f6d 7075 7465 2829 0a20 2020  .recompute().   
+00021210: 2020 2020 2020 2020 2020 2020 2069 6620               if 
+00021220: 6e6f 7420 7365 6c66 2e75 7365 5f6d 6f65  not self.use_moe
+00021230: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00021240: 2020 2020 2020 7365 6c66 2e6f 7574 7075        self.outpu
+00021250: 742e 7072 6f6a 6563 7469 6f6e 2e73 6861  t.projection.sha
+00021260: 7264 280a 2020 2020 2020 2020 2020 2020  rd(.            
+00021270: 2020 2020 2020 2020 2020 2020 7374 7261              stra
+00021280: 7465 6779 5f62 6961 733d 2828 7061 7261  tegy_bias=((para
+00021290: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+000212a0: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
+000212b0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+000212c0: 6c5f 7061 7261 6c6c 656c 2c20 3129 2c20  l_parallel, 1), 
+000212d0: 2831 2c29 292c 0a20 2020 2020 2020 2020  (1,)),.         
+000212e0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000212f0: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
+00021300: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00021310: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+00021320: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00021330: 6d6f 6465 6c5f 7061 7261 6c6c 656c 292c  model_parallel),
+00021340: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00021350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021360: 2020 2020 2020 2020 2020 2870 6172 616c            (paral
+00021370: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+00021380: 5f70 6172 616c 6c65 6c2c 2031 2929 2c0a  _parallel, 1)),.
+00021390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000213a0: 2020 2020 2020 2020 6f75 745f 7374 7261          out_stra
+000213b0: 7465 6779 5f6d 6174 6d75 6c3d 2828 7061  tegy_matmul=((pa
+000213c0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+000213d0: 7461 5f70 6172 616c 6c65 6c20 2a20 7061  ta_parallel * pa
+000213e0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+000213f0: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
+00021400: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+00021410: 2020 2020 2020 2020 7365 6c66 2e6f 7574          self.out
+00021420: 7075 742e 6472 6f70 6f75 742e 6472 6f70  put.dropout.drop
+00021430: 6f75 742e 7368 6172 6428 0a20 2020 2020  out.shard(.     
+00021440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021450: 2020 2028 2870 6172 616c 6c65 6c5f 636f     ((parallel_co
+00021460: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00021470: 656c 202a 2070 6172 616c 6c65 6c5f 636f  el * parallel_co
+00021480: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00021490: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
+000214a0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+000214b0: 2020 2020 2072 6169 7365 2052 756e 7469       raise Runti
+000214c0: 6d65 4572 726f 7228 6622 5468 6520 7b73  meError(f"The {s
+000214d0: 656c 662e 636c 735f 6e61 6d65 7d20 6f6e  elf.cls_name} on
+000214e0: 6c79 2073 7570 706f 7274 2073 6861 7264  ly support shard
+000214f0: 696e 6720 7072 6f70 6167 6174 696f 6e20  ing propagation 
+00021500: 6f72 2022 0a20 2020 2020 2020 2020 2020  or ".           
+00021510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021520: 2020 2020 6622 7365 6d69 2d61 7574 6f20      f"semi-auto 
+00021530: 7061 7261 6c6c 656c 206d 6f64 6520 6e6f  parallel mode no
+00021540: 772e 2229 0a0a 2020 2020 6465 6620 636f  w.")..    def co
+00021550: 6e73 7472 7563 7428 7365 6c66 2c20 6869  nstruct(self, hi
+00021560: 6464 656e 5f73 7461 7473 2c0a 2020 2020  dden_stats,.    
+00021570: 2020 2020 2020 2020 2020 2020 2020 6465                de
+00021580: 636f 6465 725f 6d61 736b 2c0a 2020 2020  coder_mask,.    
+00021590: 2020 2020 2020 2020 2020 2020 2020 656e                en
+000215a0: 636f 6465 725f 6f75 7470 7574 3d4e 6f6e  coder_output=Non
+000215b0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000215c0: 2020 2020 206d 656d 6f72 795f 6d61 736b       memory_mask
+000215d0: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
+000215e0: 2020 2020 2020 2020 2069 6e69 745f 7265           init_re
+000215f0: 7365 743d 5472 7565 2c20 6261 7463 685f  set=True, batch_
+00021600: 7661 6c69 645f 6c65 6e67 7468 3d4e 6f6e  valid_length=Non
+00021610: 6529 3a0a 2020 2020 2020 2020 2222 2266  e):.        """f
+00021620: 6f72 7761 7264 2070 726f 6365 7373 2222  orward process""
+00021630: 220a 2020 2020 2020 2020 7365 6c66 2e5f  ".        self._
+00021640: 6368 6563 6b5f 696e 7075 7428 6869 6464  check_input(hidd
+00021650: 656e 5f73 7461 7473 2c20 6465 636f 6465  en_stats, decode
+00021660: 725f 6d61 736b 2c20 656e 636f 6465 725f  r_mask, encoder_
+00021670: 6f75 7470 7574 2c20 6d65 6d6f 7279 5f6d  output, memory_m
+00021680: 6173 6b2c 2069 6e69 745f 7265 7365 742c  ask, init_reset,
+00021690: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
+000216a0: 6774 6829 0a20 2020 2020 2020 2023 2074  gth).        # t
+000216b0: 6865 2072 6574 7572 6e65 6420 7368 6170  he returned shap
+000216c0: 6520 6973 205b 6273 2c20 7365 715f 6c65  e is [bs, seq_le
+000216d0: 6e67 7468 2c20 656d 6265 6464 696e 675f  ngth, embedding_
+000216e0: 7369 7a65 5d20 6f72 205b 6273 202a 2073  size] or [bs * s
+000216f0: 6571 5f6c 656e 6774 682c 2065 6d62 6564  eq_length, embed
+00021700: 6469 6e67 5f73 697a 655d 0a20 2020 2020  ding_size].     
+00021710: 2020 2068 6964 6465 6e5f 7368 6170 6520     hidden_shape 
+00021720: 3d20 462e 7368 6170 6528 6869 6464 656e  = F.shape(hidden
+00021730: 5f73 7461 7473 290a 2020 2020 2020 2020  _stats).        
+00021740: 6869 6464 656e 5f73 7461 7473 203d 2046  hidden_stats = F
+00021750: 2e72 6573 6861 7065 2868 6964 6465 6e5f  .reshape(hidden_
+00021760: 7374 6174 732c 2028 2d31 2c20 6869 6464  stats, (-1, hidd
+00021770: 656e 5f73 6861 7065 5b2d 315d 2929 0a20  en_shape[-1])). 
+00021780: 2020 2020 2020 2069 6e70 7574 5f78 203d         input_x =
+00021790: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
+000217a0: 2868 6964 6465 6e5f 7374 6174 7329 0a20  (hidden_stats). 
+000217b0: 2020 2020 2020 2069 6e70 7574 5f78 203d         input_x =
+000217c0: 2046 2e63 6173 7428 696e 7075 745f 782c   F.cast(input_x,
+000217d0: 2073 656c 662e 6474 7970 6529 0a0a 2020   self.dtype)..  
+000217e0: 2020 2020 2020 2320 696e 6469 6361 7465        # indicate
+000217f0: 2077 6865 7468 6572 2072 6573 6574 2073   whether reset s
+00021800: 6176 6564 2073 7461 7465 730a 2020 2020  aved states.    
+00021810: 2020 2020 6b65 795f 7265 7365 7420 3d20      key_reset = 
+00021820: 4e6f 6e65 0a20 2020 2020 2020 2076 616c  None.        val
+00021830: 7565 5f72 6573 6574 203d 204e 6f6e 650a  ue_reset = None.
+00021840: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+00021850: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
+00021860: 2020 2020 2020 2320 7265 7365 7420 7374        # reset st
+00021870: 6174 6573 2c20 696e 6974 5f72 6573 6574  ates, init_reset
+00021880: 2054 7275 6520 666f 7220 7265 7573 6520   True for reuse 
+00021890: 616e 6420 4661 6c73 6520 666f 7220 7265  and False for re
+000218a0: 7365 740a 2020 2020 2020 2020 2020 2020  set.            
+000218b0: 7365 6c66 2e61 7373 6967 6e28 7365 6c66  self.assign(self
+000218c0: 2e6b 6579 5f70 6173 742c 2073 656c 662e  .key_past, self.
+000218d0: 6d75 6c28 7365 6c66 2e6b 6579 5f70 6173  mul(self.key_pas
+000218e0: 742c 2046 2e63 6173 7428 696e 6974 5f72  t, F.cast(init_r
+000218f0: 6573 6574 2c20 7365 6c66 2e64 7479 7065  eset, self.dtype
+00021900: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+00021910: 6b65 795f 7265 7365 7420 3d20 7365 6c66  key_reset = self
+00021920: 2e6b 6579 5f70 6173 740a 2020 2020 2020  .key_past.      
+00021930: 2020 2020 2020 7365 6c66 2e61 7373 6967        self.assig
+00021940: 6e28 7365 6c66 2e76 616c 7565 5f70 6173  n(self.value_pas
+00021950: 742c 2073 656c 662e 6d75 6c28 7365 6c66  t, self.mul(self
+00021960: 2e76 616c 7565 5f70 6173 742c 2046 2e63  .value_past, F.c
+00021970: 6173 7428 696e 6974 5f72 6573 6574 2c20  ast(init_reset, 
+00021980: 7365 6c66 2e64 7479 7065 2929 290a 2020  self.dtype))).  
+00021990: 2020 2020 2020 2020 2020 7661 6c75 655f            value_
+000219a0: 7265 7365 7420 3d20 7365 6c66 2e76 616c  reset = self.val
+000219b0: 7565 5f70 6173 740a 2020 2020 2020 2020  ue_past.        
+000219c0: 2020 2020 2320 6164 6420 6465 7065 6e64      # add depend
+000219d0: 656e 6379 2066 6f72 2064 6573 6972 6564  ency for desired
+000219e0: 2065 7865 6375 7469 6f6e 206f 7264 6572   execution order
+000219f0: 0a20 2020 2020 2020 2020 2020 2069 6e70  .            inp
+00021a00: 7574 5f78 203d 2046 2e64 6570 656e 6428  ut_x = F.depend(
+00021a10: 696e 7075 745f 782c 206b 6579 5f72 6573  input_x, key_res
+00021a20: 6574 290a 2020 2020 2020 2020 2020 2020  et).            
+00021a30: 696e 7075 745f 7820 3d20 462e 6465 7065  input_x = F.depe
+00021a40: 6e64 2869 6e70 7574 5f78 2c20 7661 6c75  nd(input_x, valu
+00021a50: 655f 7265 7365 7429 0a0a 2020 2020 2020  e_reset)..      
+00021a60: 2020 6174 7465 6e74 696f 6e2c 206c 6179    attention, lay
+00021a70: 6572 5f70 7265 7365 6e74 203d 2073 656c  er_present = sel
+00021a80: 662e 6174 7465 6e74 696f 6e28 696e 7075  f.attention(inpu
+00021a90: 745f 782c 2069 6e70 7574 5f78 2c20 696e  t_x, input_x, in
+00021aa0: 7075 745f 782c 2064 6563 6f64 6572 5f6d  put_x, decoder_m
+00021ab0: 6173 6b2c 2073 656c 662e 6b65 795f 7061  ask, self.key_pa
+00021ac0: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
+00021ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021af0: 2020 2020 2020 7365 6c66 2e76 616c 7565        self.value
+00021b00: 5f70 6173 742c 2062 6174 6368 5f76 616c  _past, batch_val
+00021b10: 6964 5f6c 656e 6774 6829 0a20 2020 2020  id_length).     
+00021b20: 2020 2023 2046 6f72 2070 6f73 742d 6c61     # For post-la
+00021b30: 7965 726e 6f72 6d20 7468 6520 696e 7075  yernorm the inpu
+00021b40: 7473 2066 6f72 2072 6573 6964 7561 6c20  ts for residual 
+00021b50: 7061 7468 2061 7265 206f 7574 7075 7420  path are output 
+00021b60: 6f66 2073 656c 662d 6174 7465 6e74 696f  of self-attentio
+00021b70: 6e20 616e 6420 6f75 7470 7574 206f 6620  n and output of 
+00021b80: 6c61 7965 726e 6f72 6d0a 2020 2020 2020  layernorm.      
+00021b90: 2020 6966 2073 656c 662e 706f 7374 5f6c    if self.post_l
+00021ba0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+00021bb0: 6c3a 0a20 2020 2020 2020 2020 2020 2078  l:.            x
+00021bc0: 203d 2073 656c 662e 6164 6428 696e 7075   = self.add(inpu
+00021bd0: 745f 782c 2061 7474 656e 7469 6f6e 290a  t_x, attention).
+00021be0: 2020 2020 2020 2020 2320 466f 7220 7072          # For pr
+00021bf0: 652d 6c61 7965 726e 6f72 6d20 7468 6520  e-layernorm the 
+00021c00: 696e 7075 7473 2066 6f72 2072 6573 6964  inputs for resid
+00021c10: 7561 6c20 7061 7468 2061 7265 206f 7574  ual path are out
+00021c20: 7075 7420 6f66 2073 656c 662d 6174 7465  put of self-atte
+00021c30: 6e74 696f 6e20 616e 6420 696e 7075 7420  ntion and input 
+00021c40: 6f66 2074 6869 7320 6c61 7965 720a 2020  of this layer.  
+00021c50: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+00021c60: 2020 2020 2020 2020 7820 3d20 7365 6c66          x = self
+00021c70: 2e61 6464 2868 6964 6465 6e5f 7374 6174  .add(hidden_stat
+00021c80: 732c 2061 7474 656e 7469 6f6e 290a 0a20  s, attention).. 
+00021c90: 2020 2020 2020 206d 6964 646c 655f 6f75         middle_ou
+00021ca0: 7470 7574 203d 204e 6f6e 650a 2020 2020  tput = None.    
+00021cb0: 2020 2020 6966 2065 6e63 6f64 6572 5f6f      if encoder_o
+00021cc0: 7574 7075 7420 6973 206e 6f74 204e 6f6e  utput is not Non
+00021cd0: 653a 0a20 2020 2020 2020 2020 2020 206d  e:.            m
+00021ce0: 6964 646c 655f 6f75 7470 7574 203d 2073  iddle_output = s
+00021cf0: 656c 662e 6372 6f73 735f 6174 7465 6e74  elf.cross_attent
+00021d00: 696f 6e5f 6c61 7965 726e 6f72 6d28 7829  ion_layernorm(x)
+00021d10: 0a20 2020 2020 2020 2020 2020 206d 6964  .            mid
+00021d20: 646c 655f 6f75 7470 7574 203d 2046 2e63  dle_output = F.c
+00021d30: 6173 7428 6d69 6464 6c65 5f6f 7574 7075  ast(middle_outpu
+00021d40: 742c 2073 656c 662e 6474 7970 6529 0a20  t, self.dtype). 
+00021d50: 2020 2020 2020 2020 2020 2065 6e63 6f64             encod
+00021d60: 6572 5f6f 7574 7075 7420 3d20 462e 6361  er_output = F.ca
+00021d70: 7374 2865 6e63 6f64 6572 5f6f 7574 7075  st(encoder_outpu
+00021d80: 742c 2073 656c 662e 6474 7970 6529 0a20  t, self.dtype). 
+00021d90: 2020 2020 2020 2020 2020 2063 726f 7373             cross
+00021da0: 5f61 7474 6e5f 6f75 7470 7574 2c20 6372  _attn_output, cr
+00021db0: 6f73 735f 6c61 7965 725f 7072 6573 656e  oss_layer_presen
+00021dc0: 7420 3d20 7365 6c66 2e63 726f 7373 5f61  t = self.cross_a
+00021dd0: 7474 656e 7469 6f6e 286d 6964 646c 655f  ttention(middle_
+00021de0: 6f75 7470 7574 2c20 656e 636f 6465 725f  output, encoder_
+00021df0: 6f75 7470 7574 2c0a 2020 2020 2020 2020  output,.        
 00021e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021e10: 2020 2020 2020 2020 2020 2065 6e63 6f64             encod
-00021e20: 6572 5f6f 7574 7075 742c 0a20 2020 2020  er_output,.     
+00021e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e40: 2020 656e 636f 6465 725f 6f75 7470 7574    encoder_output
+00021e50: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00021e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021e70: 2020 2020 206d 656d 6f72 795f 6d61 736b       memory_mask
-00021e80: 2c20 7365 6c66 2e6b 6579 5f70 6173 742c  , self.key_past,
-00021e90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e90: 2020 2020 2020 2020 2020 2020 6d65 6d6f              memo
+00021ea0: 7279 5f6d 6173 6b2c 2073 656c 662e 6b65  ry_mask, self.ke
+00021eb0: 795f 7061 7374 2c0a 2020 2020 2020 2020  y_past,.        
 00021ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021ed0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00021ee0: 7661 6c75 655f 7061 7374 2c20 6261 7463  value_past, batc
-00021ef0: 685f 7661 6c69 645f 6c65 6e67 7468 290a  h_valid_length).
-00021f00: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
-00021f10: 725f 7072 6573 656e 7420 2b3d 2063 726f  r_present += cro
-00021f20: 7373 5f6c 6179 6572 5f70 7265 7365 6e74  ss_layer_present
-00021f30: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00021f40: 7365 6c66 2e70 6f73 745f 6c61 7965 726e  self.post_layern
-00021f50: 6f72 6d5f 7265 7369 6475 616c 3a0a 2020  orm_residual:.  
-00021f60: 2020 2020 2020 2020 2020 2020 2020 7820                x 
-00021f70: 3d20 7365 6c66 2e61 6464 286d 6964 646c  = self.add(middl
-00021f80: 655f 6f75 7470 7574 2c20 6372 6f73 735f  e_output, cross_
-00021f90: 6174 746e 5f6f 7574 7075 7429 0a20 2020  attn_output).   
-00021fa0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-00021fb0: 2020 2020 2020 2020 2020 2020 2020 2078                 x
-00021fc0: 203d 2073 656c 662e 6164 6428 782c 2063   = self.add(x, c
-00021fd0: 726f 7373 5f61 7474 6e5f 6f75 7470 7574  ross_attn_output
-00021fe0: 290a 0a20 2020 2020 2020 206f 7574 7075  )..        outpu
-00021ff0: 745f 7820 3d20 7365 6c66 2e6c 6179 6572  t_x = self.layer
-00022000: 6e6f 726d 3228 7829 0a20 2020 2020 2020  norm2(x).       
-00022010: 206f 7574 7075 745f 7820 3d20 462e 6361   output_x = F.ca
-00022020: 7374 286f 7574 7075 745f 782c 2073 656c  st(output_x, sel
-00022030: 662e 6474 7970 6529 0a20 2020 2020 2020  f.dtype).       
-00022040: 2061 7578 5f6c 6f73 7320 3d20 4e6f 6e65   aux_loss = None
-00022050: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
-00022060: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-00022070: 2020 2020 2020 6d6c 705f 6c6f 6769 742c        mlp_logit,
-00022080: 2061 7578 5f6c 6f73 7320 3d20 7365 6c66   aux_loss = self
-00022090: 2e6f 7574 7075 7428 6f75 7470 7574 5f78  .output(output_x
-000220a0: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
-000220b0: 2020 2020 2020 2020 2020 2020 6d6c 705f              mlp_
-000220c0: 6c6f 6769 7420 3d20 7365 6c66 2e6f 7574  logit = self.out
-000220d0: 7075 7428 6f75 7470 7574 5f78 290a 0a20  put(output_x).. 
-000220e0: 2020 2020 2020 2076 616c 7565 5f75 7064         value_upd
-000220f0: 6174 6520 3d20 4e6f 6e65 0a20 2020 2020  ate = None.     
-00022100: 2020 206b 6579 5f75 7064 6174 6520 3d20     key_update = 
-00022110: 4e6f 6e65 0a20 2020 2020 2020 2069 6620  None.        if 
-00022120: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-00022130: 2020 2020 2020 2020 2020 2023 2063 7572             # cur
-00022140: 7265 6e74 206b 6579 2061 6e64 2076 616c  rent key and val
-00022150: 7565 0a20 2020 2020 2020 2020 2020 206b  ue.            k
-00022160: 6579 5f70 7265 7365 6e74 2c20 7661 6c75  ey_present, valu
-00022170: 655f 7072 6573 656e 7420 3d20 6c61 7965  e_present = laye
-00022180: 725f 7072 6573 656e 740a 2020 2020 2020  r_present.      
-00022190: 2020 2020 2020 2320 7570 6461 7465 206b        # update k
-000221a0: 6579 2061 6e64 2076 616c 7565 2063 616c  ey and value cal
-000221b0: 6375 6c61 7465 6420 7468 6973 2073 7465  culated this ste
-000221c0: 700a 2020 2020 2020 2020 2020 2020 7365  p.            se
-000221d0: 6c66 2e61 7373 6967 6e28 7365 6c66 2e6b  lf.assign(self.k
-000221e0: 6579 5f70 6173 742c 206b 6579 5f70 7265  ey_past, key_pre
-000221f0: 7365 6e74 290a 2020 2020 2020 2020 2020  sent).          
-00022200: 2020 6b65 795f 7570 6461 7465 203d 2073    key_update = s
-00022210: 656c 662e 6b65 795f 7061 7374 0a20 2020  elf.key_past.   
-00022220: 2020 2020 2020 2020 2073 656c 662e 6173           self.as
-00022230: 7369 676e 2873 656c 662e 7661 6c75 655f  sign(self.value_
-00022240: 7061 7374 2c20 7661 6c75 655f 7072 6573  past, value_pres
-00022250: 656e 7429 0a20 2020 2020 2020 2020 2020  ent).           
-00022260: 2076 616c 7565 5f75 7064 6174 6520 3d20   value_update = 
-00022270: 7365 6c66 2e76 616c 7565 5f70 6173 740a  self.value_past.
-00022280: 2020 2020 2020 2020 2020 2020 2320 6164              # ad
-00022290: 6420 6465 7065 6e64 656e 6379 2066 6f72  d dependency for
-000222a0: 2064 6573 6972 6564 2065 7865 6375 7469   desired executi
-000222b0: 6f6e 206f 7264 6572 0a20 2020 2020 2020  on order.       
-000222c0: 2020 2020 206b 6579 5f75 7064 6174 6520       key_update 
-000222d0: 3d20 462e 6465 7065 6e64 286b 6579 5f75  = F.depend(key_u
-000222e0: 7064 6174 652c 206b 6579 5f72 6573 6574  pdate, key_reset
-000222f0: 290a 2020 2020 2020 2020 2020 2020 7661  ).            va
-00022300: 6c75 655f 7570 6461 7465 203d 2046 2e64  lue_update = F.d
-00022310: 6570 656e 6428 7661 6c75 655f 7570 6461  epend(value_upda
-00022320: 7465 2c20 7661 6c75 655f 7265 7365 7429  te, value_reset)
-00022330: 0a0a 2020 2020 2020 2020 2320 6164 6420  ..        # add 
-00022340: 6465 7065 6e64 656e 6379 2066 6f72 2064  dependency for d
-00022350: 6573 6972 6564 2065 7865 6375 7469 6f6e  esired execution
-00022360: 206f 7264 6572 0a20 2020 2020 2020 206d   order.        m
-00022370: 6c70 5f6c 6f67 6974 203d 2046 2e64 6570  lp_logit = F.dep
-00022380: 656e 6428 6d6c 705f 6c6f 6769 742c 2076  end(mlp_logit, v
-00022390: 616c 7565 5f75 7064 6174 6529 0a20 2020  alue_update).   
-000223a0: 2020 2020 206d 6c70 5f6c 6f67 6974 203d       mlp_logit =
-000223b0: 2046 2e64 6570 656e 6428 6d6c 705f 6c6f   F.depend(mlp_lo
-000223c0: 6769 742c 206b 6579 5f75 7064 6174 6529  git, key_update)
-000223d0: 0a0a 2020 2020 2020 2020 2320 6966 2073  ..        # if s
-000223e0: 6861 7065 2069 7320 3364 2c20 7765 2072  hape is 3d, we r
-000223f0: 6573 6861 7065 2074 6865 2069 6e70 7574  eshape the input
-00022400: 7320 6f66 2074 6865 2061 6464 0a20 2020  s of the add.   
-00022410: 2020 2020 2069 6620 6c65 6e28 6869 6464       if len(hidd
-00022420: 656e 5f73 6861 7065 2920 3d3d 2033 3a0a  en_shape) == 3:.
-00022430: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-00022440: 7574 5f78 203d 2050 2e52 6573 6861 7065  ut_x = P.Reshape
-00022450: 2829 286f 7574 7075 745f 782c 2068 6964  ()(output_x, hid
-00022460: 6465 6e5f 7368 6170 6529 0a20 2020 2020  den_shape).     
-00022470: 2020 2020 2020 206d 6c70 5f6c 6f67 6974         mlp_logit
-00022480: 203d 2050 2e52 6573 6861 7065 2829 286d   = P.Reshape()(m
-00022490: 6c70 5f6c 6f67 6974 2c20 6869 6464 656e  lp_logit, hidden
-000224a0: 5f73 6861 7065 290a 2020 2020 2020 2020  _shape).        
-000224b0: 2020 2020 7820 3d20 502e 5265 7368 6170      x = P.Reshap
-000224c0: 6528 2928 782c 2068 6964 6465 6e5f 7368  e()(x, hidden_sh
-000224d0: 6170 6529 0a0a 2020 2020 2020 2020 2020  ape)..          
-000224e0: 2020 6966 2073 656c 662e 706f 7374 5f6c    if self.post_l
-000224f0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-00022500: 6c3a 0a20 2020 2020 2020 2020 2020 2020  l:.             
-00022510: 2020 206f 7574 7075 7420 3d20 7365 6c66     output = self
-00022520: 2e61 6464 5f33 6428 6f75 7470 7574 5f78  .add_3d(output_x
-00022530: 2c20 6d6c 705f 6c6f 6769 7429 0a20 2020  , mlp_logit).   
-00022540: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-00022550: 2020 2020 2020 2020 2020 2020 2020 206f                 o
-00022560: 7574 7075 7420 3d20 7365 6c66 2e61 6464  utput = self.add
-00022570: 5f33 6428 782c 206d 6c70 5f6c 6f67 6974  _3d(x, mlp_logit
-00022580: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
-00022590: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-000225a0: 656c 662e 706f 7374 5f6c 6179 6572 6e6f  elf.post_layerno
-000225b0: 726d 5f72 6573 6964 7561 6c3a 0a20 2020  rm_residual:.   
-000225c0: 2020 2020 2020 2020 2020 2020 206f 7574               out
-000225d0: 7075 7420 3d20 7365 6c66 2e61 6464 286f  put = self.add(o
-000225e0: 7574 7075 745f 782c 206d 6c70 5f6c 6f67  utput_x, mlp_log
-000225f0: 6974 290a 2020 2020 2020 2020 2020 2020  it).            
-00022600: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00022610: 2020 2020 2020 6f75 7470 7574 203d 2073        output = s
-00022620: 656c 662e 6164 6428 782c 206d 6c70 5f6c  elf.add(x, mlp_l
-00022630: 6f67 6974 290a 2020 2020 2020 2020 2020  ogit).          
-00022640: 2020 6f75 7470 7574 203d 2046 2e72 6573    output = F.res
-00022650: 6861 7065 286f 7574 7075 742c 2068 6964  hape(output, hid
-00022660: 6465 6e5f 7368 6170 6529 0a0a 2020 2020  den_shape)..    
-00022670: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
-00022680: 6d6f 653a 0a20 2020 2020 2020 2020 2020  moe:.           
-00022690: 2072 6574 7572 6e20 6f75 7470 7574 2c20   return output, 
-000226a0: 6c61 7965 725f 7072 6573 656e 742c 2061  layer_present, a
-000226b0: 7578 5f6c 6f73 730a 2020 2020 2020 2020  ux_loss.        
-000226c0: 7265 7475 726e 206f 7574 7075 742c 206c  return output, l
-000226d0: 6179 6572 5f70 7265 7365 6e74 0a0a 2020  ayer_present..  
-000226e0: 2020 6465 6620 5f63 6865 636b 5f69 6e70    def _check_inp
-000226f0: 7574 2873 656c 662c 2068 6964 6465 6e5f  ut(self, hidden_
-00022700: 7374 6174 6573 2c20 6174 7465 6e74 696f  states, attentio
-00022710: 6e5f 6d61 736b 2c20 656e 636f 6465 725f  n_mask, encoder_
-00022720: 6f75 7470 7574 2c20 6d65 6d6f 7279 5f6d  output, memory_m
-00022730: 6173 6b2c 2069 6e69 745f 7265 7365 742c  ask, init_reset,
-00022740: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-00022750: 6774 6829 3a0a 2020 2020 2020 2020 7222  gth):.        r"
-00022760: 2222 4368 6563 6b20 696e 7075 7473 2222  ""Check inputs""
-00022770: 220a 2020 2020 2020 2020 5f63 6865 636b  ".        _check
-00022780: 5f69 6e70 7574 5f64 7479 7065 2846 2e64  _input_dtype(F.d
-00022790: 7479 7065 2868 6964 6465 6e5f 7374 6174  type(hidden_stat
-000227a0: 6573 292c 2022 6869 6464 656e 5f73 7461  es), "hidden_sta
-000227b0: 7465 7322 2c0a 2020 2020 2020 2020 2020  tes",.          
-000227c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000227d0: 205b 6d73 7479 7065 2e66 6c6f 6174 3332   [mstype.float32
-000227e0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-000227f0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-00022800: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
-00022810: 6529 0a20 2020 2020 2020 2069 6620 6174  e).        if at
-00022820: 7465 6e74 696f 6e5f 6d61 736b 2069 7320  tention_mask is 
-00022830: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
-00022840: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
-00022850: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
-00022860: 2861 7474 656e 7469 6f6e 5f6d 6173 6b29  (attention_mask)
-00022870: 2c20 2261 7474 656e 7469 6f6e 5f6d 6173  , "attention_mas
-00022880: 6b22 2c0a 2020 2020 2020 2020 2020 2020  k",.            
-00022890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000228a0: 2020 205b 6d73 7479 7065 2e66 6c6f 6174     [mstype.float
-000228b0: 3332 2c20 6d73 7479 7065 2e66 6c6f 6174  32, mstype.float
-000228c0: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
-000228d0: 7431 365d 2c0a 2020 2020 2020 2020 2020  t16],.          
-000228e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000228f0: 2020 2020 2073 656c 662e 636c 735f 6e61       self.cls_na
-00022900: 6d65 290a 2020 2020 2020 2020 6966 2065  me).        if e
-00022910: 6e63 6f64 6572 5f6f 7574 7075 7420 6973  ncoder_output is
-00022920: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
-00022930: 2020 2020 2020 205f 6368 6563 6b5f 696e         _check_in
-00022940: 7075 745f 6474 7970 6528 462e 6474 7970  put_dtype(F.dtyp
-00022950: 6528 656e 636f 6465 725f 6f75 7470 7574  e(encoder_output
-00022960: 292c 2022 656e 636f 6465 725f 6f75 7470  ), "encoder_outp
-00022970: 7574 222c 0a20 2020 2020 2020 2020 2020  ut",.           
-00022980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022990: 2020 2020 5b6d 7374 7970 652e 666c 6f61      [mstype.floa
-000229a0: 7433 322c 206d 7374 7970 652e 666c 6f61  t32, mstype.floa
-000229b0: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
-000229c0: 6174 3136 5d2c 2073 656c 662e 636c 735f  at16], self.cls_
-000229d0: 6e61 6d65 290a 2020 2020 2020 2020 6966  name).        if
-000229e0: 206d 656d 6f72 795f 6d61 736b 2069 7320   memory_mask is 
-000229f0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
-00022a00: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
-00022a10: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
-00022a20: 286d 656d 6f72 795f 6d61 736b 292c 2022  (memory_mask), "
-00022a30: 6d65 6d6f 7279 5f6d 6173 6b22 2c0a 2020  memory_mask",.  
-00022a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022a50: 2020 2020 2020 2020 2020 2020 205b 6d73               [ms
-00022a60: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
-00022a70: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-00022a80: 7479 7065 2e62 666c 6f61 7431 365d 2c20  type.bfloat16], 
-00022a90: 7365 6c66 2e63 6c73 5f6e 616d 6529 0a0a  self.cls_name)..
-00022aa0: 2020 2020 2020 2020 696e 6974 5f72 6573          init_res
-00022ab0: 6574 5f69 735f 7465 6e73 6f72 203d 2069  et_is_tensor = i
-00022ac0: 7369 6e73 7461 6e63 6528 696e 6974 5f72  sinstance(init_r
-00022ad0: 6573 6574 2c20 5465 6e73 6f72 290a 2020  eset, Tensor).  
-00022ae0: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
-00022af0: 5f69 735f 6465 6661 756c 7420 3d20 696e  _is_default = in
-00022b00: 6974 5f72 6573 6574 2069 7320 5472 7565  it_reset is True
-00022b10: 0a20 2020 2020 2020 2062 6174 6368 5f76  .        batch_v
-00022b20: 616c 6964 5f6c 656e 6774 685f 6973 5f74  alid_length_is_t
-00022b30: 656e 736f 7220 3d20 6973 696e 7374 616e  ensor = isinstan
-00022b40: 6365 2862 6174 6368 5f76 616c 6964 5f6c  ce(batch_valid_l
-00022b50: 656e 6774 682c 2054 656e 736f 7229 0a20  ength, Tensor). 
-00022b60: 2020 2020 2020 2062 6174 6368 5f69 735f         batch_is_
-00022b70: 6465 6661 756c 7420 3d20 6261 7463 685f  default = batch_
-00022b80: 7661 6c69 645f 6c65 6e67 7468 2069 7320  valid_length is 
-00022b90: 4e6f 6e65 0a20 2020 2020 2020 205f 6368  None.        _ch
-00022ba0: 6563 6b5f 7061 7374 5f6e 6f6e 655f 696e  eck_past_none_in
-00022bb0: 7075 745f 6e6f 6e65 2873 656c 662e 7573  put_none(self.us
-00022bc0: 655f 7061 7374 2c20 2269 6e69 745f 7265  e_past, "init_re
-00022bd0: 7365 7422 2c20 7365 6c66 2e63 6c73 5f6e  set", self.cls_n
-00022be0: 616d 652c 2054 7275 652c 2069 6e69 745f  ame, True, init_
-00022bf0: 7265 7365 745f 6973 5f74 656e 736f 722c  reset_is_tensor,
-00022c00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00022c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022c20: 2020 2020 2069 6e69 745f 7265 7365 745f       init_reset_
-00022c30: 6973 5f64 6566 6175 6c74 290a 2020 2020  is_default).    
-00022c40: 2020 2020 5f63 6865 636b 5f70 6173 745f      _check_past_
-00022c50: 6e6f 6e65 5f69 6e70 7574 5f6e 6f6e 6528  none_input_none(
-00022c60: 7365 6c66 2e75 7365 5f70 6173 742c 2022  self.use_past, "
-00022c70: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-00022c80: 7468 222c 2073 656c 662e 636c 735f 6e61  th", self.cls_na
-00022c90: 6d65 2c20 4e6f 6e65 2c0a 2020 2020 2020  me, None,.      
-00022ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022cb0: 2020 2020 2020 2020 2020 2020 2020 6261                ba
-00022cc0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-00022cd0: 5f69 735f 7465 6e73 6f72 2c20 6261 7463  _is_tensor, batc
-00022ce0: 685f 6973 5f64 6566 6175 6c74 290a 0a20  h_is_default).. 
-00022cf0: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
-00022d00: 7365 5f70 6173 743a 0a20 2020 2020 2020  se_past:.       
-00022d10: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
-00022d20: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
-00022d30: 696e 6974 5f72 6573 6574 292c 2022 696e  init_reset), "in
-00022d40: 6974 5f72 6573 6574 222c 205b 6d73 7479  it_reset", [msty
-00022d50: 7065 2e62 6f6f 6c5f 5d2c 2073 656c 662e  pe.bool_], self.
-00022d60: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
-00022d70: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
-00022d80: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
-00022d90: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
-00022da0: 6774 6829 2c20 2262 6174 6368 5f76 616c  gth), "batch_val
-00022db0: 6964 5f6c 656e 6774 6822 2c20 5b6d 7374  id_length", [mst
-00022dc0: 7970 652e 696e 7433 325d 2c20 7365 6c66  ype.int32], self
-00022dd0: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
-00022de0: 2020 2072 6574 7572 6e20 5472 7565 0a0a     return True..
-00022df0: 0a64 6566 205f 6765 745f 6c61 6d62 6461  .def _get_lambda
-00022e00: 5f66 756e 6328 746f 7461 6c5f 6c61 7965  _func(total_laye
-00022e10: 723d 4e6f 6e65 293a 0a20 2020 2072 2222  r=None):.    r""
-00022e20: 220a 2020 2020 4120 7772 6170 7065 7220  ".    A wrapper 
-00022e30: 6675 6e63 7469 6f6e 206f 6620 7370 6563  function of spec
-00022e40: 6966 7969 6e67 2070 6970 656c 696e 6520  ifying pipeline 
-00022e50: 7374 6167 6520 616e 6420 6772 6164 6965  stage and gradie
-00022e60: 6e74 2061 6767 7265 6761 7469 6f6e 2066  nt aggregation f
-00022e70: 7573 696f 6e2e 2049 6620 7468 6520 746f  usion. If the to
-00022e80: 7461 6c20 6c61 7965 720a 2020 2020 6973  tal layer.    is
-00022e90: 206e 6f74 204e 6f6e 652c 2066 6f72 2065   not None, for e
-00022ea0: 7861 6d70 6c65 2c20 7365 7420 696e 2074  xample, set in t
-00022eb0: 6865 2074 7261 6e73 666f 726d 6572 206d  he transformer m
-00022ec0: 6f64 656c 2c20 7468 6520 7069 7065 6c69  odel, the pipeli
-00022ed0: 6e65 2073 7461 6765 2073 6574 7469 6e67  ne stage setting
-00022ee0: 2066 756e 6374 696f 6e20 7769 6c6c 2062   function will b
-00022ef0: 650a 2020 2020 6028 6c61 7965 725f 6964  e.    `(layer_id
-00022f00: 202b 2030 2920 2f2f 2028 746f 7461 6c5f   + 0) // (total_
-00022f10: 6c61 7965 7273 202f 2070 6172 616c 6c65  layers / paralle
-00022f20: 6c5f 636f 6e66 6967 2e70 6970 656c 696e  l_config.pipelin
-00022f30: 655f 7374 6167 6529 6020 666f 7220 7468  e_stage)` for th
-00022f40: 6520 656e 636f 6465 7220 616e 642c 0a20  e encoder and,. 
-00022f50: 2020 2060 286c 6179 6572 5f69 6420 2b20     `(layer_id + 
-00022f60: 6f66 6673 6574 2920 2f2f 0a20 2020 2028  offset) //.    (
-00022f70: 746f 7461 6c5f 6c61 7965 7273 202f 2070  total_layers / p
-00022f80: 6172 616c 6c65 6c5f 636f 6e66 6967 2e70  arallel_config.p
-00022f90: 6970 656c 696e 655f 7374 6167 6529 6020  ipeline_stage)` 
-00022fa0: 666f 7220 7468 6520 6465 636f 6465 722c  for the decoder,
-00022fb0: 2077 6865 7265 2060 6f66 6673 6574 6020   where `offset` 
-00022fc0: 6973 2074 6865 206c 6179 6572 7320 696e  is the layers in
-00022fd0: 2074 6865 2065 6e63 6f64 6572 2e0a 2020   the encoder..  
-00022fe0: 2020 2222 220a 0a20 2020 2064 6566 205f    """..    def _
-00022ff0: 7365 745f 7061 7261 6c6c 656c 5f63 6f6e  set_parallel_con
-00023000: 6669 6775 7265 5f66 6f72 5f6c 6179 6572  figure_for_layer
-00023010: 286e 6574 776f 726b 2c20 6c61 7965 725f  (network, layer_
-00023020: 6964 2c20 6f66 6673 6574 2c20 7061 7261  id, offset, para
-00023030: 6c6c 656c 5f63 6f6e 6669 672c 206c 6179  llel_config, lay
-00023040: 6572 7329 3a0a 2020 2020 2020 2020 7222  ers):.        r"
-00023050: 2222 0a20 2020 2020 2020 2044 6566 6175  "".        Defau
-00023060: 6c74 2073 6574 7469 6e67 2066 6f72 2074  lt setting for t
-00023070: 6865 2070 6970 656c 696e 6520 6973 3a20  he pipeline is: 
-00023080: 6028 6c61 7965 725f 6964 202b 206f 6666  `(layer_id + off
-00023090: 7365 7429 202f 2f20 286c 6179 6572 7320  set) // (layers 
-000230a0: 2f20 7069 7065 6c69 6e65 5f73 7461 6765  / pipeline_stage
-000230b0: 2960 2e0a 0a20 2020 2020 2020 2041 7267  )`...        Arg
-000230c0: 733a 0a20 2020 2020 2020 2020 2020 206e  s:.            n
-000230d0: 6574 776f 726b 2843 656c 6c29 202d 2052  etwork(Cell) - R
-000230e0: 6570 7265 7365 6e74 7320 7468 6520 7472  epresents the tr
-000230f0: 616e 7366 6f72 6d65 7220 626c 6f63 6b0a  ansformer block.
-00023100: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
-00023110: 725f 6964 2869 6e74 2920 2d20 4d65 616e  r_id(int) - Mean
-00023120: 7320 7468 6520 6c61 7965 7220 696e 6465  s the layer inde
-00023130: 7820 666f 7220 7468 6520 6375 7272 656e  x for the curren
-00023140: 7420 6d6f 6475 6c65 2c20 636f 756e 7473  t module, counts
-00023150: 2066 726f 6d20 7a65 726f 2e0a 2020 2020   from zero..    
-00023160: 2020 2020 2020 2020 6f66 6673 6574 2869          offset(i
-00023170: 6e74 2920 2d20 4d65 616e 7320 7468 6520  nt) - Means the 
-00023180: 6c61 7965 725f 696e 6465 7820 6e65 6564  layer_index need
-00023190: 7320 616e 206f 6666 7365 742c 2069 6620  s an offset, if 
-000231a0: 7468 6572 6520 6172 6520 6f74 6865 7220  there are other 
-000231b0: 6d6f 6475 6c65 7320 696e 2074 6865 206e  modules in the n
-000231c0: 6574 2e0a 2020 2020 2020 2020 2020 2020  et..            
-000231d0: 6c61 7965 7273 2869 6e74 2920 2d20 5468  layers(int) - Th
-000231e0: 6520 746f 7461 6c20 6c61 7965 7273 2075  e total layers u
-000231f0: 7365 6420 666f 7220 7468 6520 6d6f 6465  sed for the mode
-00023200: 6c2e 0a20 2020 2020 2020 2022 2222 0a20  l..        """. 
-00023210: 2020 2020 2020 2023 206f 7665 7272 6964         # overrid
-00023220: 6520 7468 6520 6c61 7965 7273 0a20 2020  e the layers.   
-00023230: 2020 2020 2069 6620 746f 7461 6c5f 6c61       if total_la
-00023240: 7965 723a 0a20 2020 2020 2020 2020 2020  yer:.           
-00023250: 206c 6179 6572 7320 3d20 746f 7461 6c5f   layers = total_
-00023260: 6c61 7965 720a 2020 2020 2020 2020 2320  layer.        # 
-00023270: 5573 6564 2066 6f72 2074 6865 2070 6970  Used for the pip
-00023280: 656c 696e 6527 7320 7374 6167 6573 2073  eline's stages s
-00023290: 6574 7469 6e67 0a20 2020 2020 2020 2069  etting.        i
-000232a0: 6620 6c61 7965 7273 203c 2070 6172 616c  f layers < paral
-000232b0: 6c65 6c5f 636f 6e66 6967 2e70 6970 656c  lel_config.pipel
-000232c0: 696e 655f 7374 6167 653a 0a20 2020 2020  ine_stage:.     
-000232d0: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
-000232e0: 7565 4572 726f 7228 6622 6c61 7965 7273  ueError(f"layers
-000232f0: 207b 6c61 7965 7273 7d20 6d75 7374 2062   {layers} must b
-00023300: 6520 6c61 7267 6572 2074 6861 6e20 7069  e larger than pi
-00023310: 7065 6c69 6e65 2073 7461 6765 207b 7061  peline stage {pa
-00023320: 7261 6c6c 656c 5f63 6f6e 6669 672e 7069  rallel_config.pi
-00023330: 7065 6c69 6e65 5f73 7461 6765 7d22 290a  peline_stage}").
-00023340: 0a20 2020 2020 2020 2070 705f 6469 7320  .        pp_dis 
-00023350: 3d20 6d61 7828 6c61 7965 7273 202f 2f20  = max(layers // 
-00023360: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00023370: 7069 7065 6c69 6e65 5f73 7461 6765 2c20  pipeline_stage, 
-00023380: 3129 0a20 2020 2020 2020 2023 2074 6865  1).        # the
-00023390: 2070 6970 656c 696e 6520 7374 6167 6520   pipeline stage 
-000233a0: 6d75 7374 2062 6520 696e 205b 302c 2070  must be in [0, p
-000233b0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e70  arallel_config.p
-000233c0: 6970 656c 696e 655f 7374 6167 6520 2d20  ipeline_stage - 
-000233d0: 315d 0a20 2020 2020 2020 2070 705f 6964  1].        pp_id
-000233e0: 203d 206d 696e 2828 6c61 7965 725f 6964   = min((layer_id
-000233f0: 202b 206f 6666 7365 7429 202f 2f20 7070   + offset) // pp
-00023400: 5f64 6973 2c20 7061 7261 6c6c 656c 5f63  _dis, parallel_c
-00023410: 6f6e 6669 672e 7069 7065 6c69 6e65 5f73  onfig.pipeline_s
-00023420: 7461 6765 202d 2031 290a 2020 2020 2020  tage - 1).      
-00023430: 2020 6e65 7477 6f72 6b2e 7069 7065 6c69    network.pipeli
-00023440: 6e65 5f73 7461 6765 203d 2070 705f 6964  ne_stage = pp_id
-00023450: 0a0a 2020 2020 2020 2020 2320 5573 6564  ..        # Used
-00023460: 2066 6f72 206f 7074 696d 697a 6572 2773   for optimizer's
-00023470: 2066 7573 696f 6e20 7461 670a 2020 2020   fusion tag.    
-00023480: 2020 2020 6469 7320 3d20 6d61 7828 6c61      dis = max(la
-00023490: 7965 7273 202f 2f20 7061 7261 6c6c 656c  yers // parallel
-000234a0: 5f63 6f6e 6669 672e 6772 6164 6965 6e74  _config.gradient
-000234b0: 5f61 6767 7265 6761 7469 6f6e 5f67 726f  _aggregation_gro
-000234c0: 7570 2c20 3129 0a20 2020 2020 2020 206e  up, 1).        n
-000234d0: 6574 776f 726b 2e73 6574 5f63 6f6d 6d5f  etwork.set_comm_
-000234e0: 6675 7369 6f6e 2828 6c61 7965 725f 6964  fusion((layer_id
-000234f0: 202b 206f 6666 7365 7429 202f 2f20 6469   + offset) // di
-00023500: 7320 2b20 3129 0a20 2020 2020 2020 2023  s + 1).        #
-00023510: 2055 7365 6420 666f 7220 656e 6162 6c69   Used for enabli
-00023520: 6e67 2072 6563 6f6d 7075 7461 7469 6f6e  ng recomputation
-00023530: 206f 6620 7468 6520 626c 6f63 6b0a 2020   of the block.  
-00023540: 2020 2020 2020 6966 2069 7369 6e73 7461        if isinsta
-00023550: 6e63 6528 7061 7261 6c6c 656c 5f63 6f6e  nce(parallel_con
-00023560: 6669 672e 7265 636f 6d70 7574 652c 2062  fig.recompute, b
-00023570: 6f6f 6c29 3a0a 2020 2020 2020 2020 2020  ool):.          
-00023580: 2020 6966 2070 6172 616c 6c65 6c5f 636f    if parallel_co
-00023590: 6e66 6967 2e72 6563 6f6d 7075 7465 2061  nfig.recompute a
-000235a0: 6e64 206e 6f74 2070 6172 616c 6c65 6c5f  nd not parallel_
-000235b0: 636f 6e66 6967 2e73 656c 6563 745f 7265  config.select_re
-000235c0: 636f 6d70 7574 653a 0a20 2020 2020 2020  compute:.       
-000235d0: 2020 2020 2020 2020 206e 6574 776f 726b           network
-000235e0: 2e72 6563 6f6d 7075 7465 2829 0a20 2020  .recompute().   
-000235f0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00023600: 2020 2020 2020 2069 6620 7061 7261 6c6c         if parall
-00023610: 656c 5f63 6f6e 6669 672e 7265 636f 6d70  el_config.recomp
-00023620: 7574 652e 7265 636f 6d70 7574 6520 616e  ute.recompute an
-00023630: 6420 6e6f 7420 7061 7261 6c6c 656c 5f63  d not parallel_c
-00023640: 6f6e 6669 672e 7265 636f 6d70 7574 652e  onfig.recompute.
-00023650: 7365 6c65 6374 5f72 6563 6f6d 7075 7465  select_recompute
-00023660: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00023670: 2020 7061 7261 6c65 6c5f 6f70 5f63 6f6d    paralel_op_com
-00023680: 6d5f 636f 6d70 7574 6520 3d20 7061 7261  m_compute = para
-00023690: 6c6c 656c 5f63 6f6e 6669 672e 7265 636f  llel_config.reco
-000236a0: 6d70 7574 652e 7061 7261 6c6c 656c 5f6f  mpute.parallel_o
-000236b0: 7074 696d 697a 6572 5f63 6f6d 6d5f 7265  ptimizer_comm_re
-000236c0: 636f 6d70 7574 650a 2020 2020 2020 2020  compute.        
-000236d0: 2020 2020 2020 2020 6e65 7477 6f72 6b2e          network.
-000236e0: 7265 636f 6d70 7574 6528 7061 7261 6c6c  recompute(parall
-000236f0: 656c 5f6f 7074 696d 697a 6572 5f63 6f6d  el_optimizer_com
-00023700: 6d5f 7265 636f 6d70 7574 653d 7061 7261  m_recompute=para
-00023710: 6c65 6c5f 6f70 5f63 6f6d 6d5f 636f 6d70  lel_op_comm_comp
-00023720: 7574 652c 0a20 2020 2020 2020 2020 2020  ute,.           
-00023730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023740: 2020 2020 2020 206d 705f 636f 6d6d 5f72         mp_comm_r
-00023750: 6563 6f6d 7075 7465 3d70 6172 616c 6c65  ecompute=paralle
-00023760: 6c5f 636f 6e66 6967 2e72 6563 6f6d 7075  l_config.recompu
-00023770: 7465 2e6d 705f 636f 6d6d 5f72 6563 6f6d  te.mp_comm_recom
-00023780: 7075 7465 2c0a 2020 2020 2020 2020 2020  pute,.          
-00023790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000237a0: 2020 2020 2020 2020 7265 636f 6d70 7574          recomput
-000237b0: 655f 736c 6963 655f 6163 7469 7661 7469  e_slice_activati
-000237c0: 6f6e 3d70 6172 616c 6c65 6c5f 636f 6e66  on=parallel_conf
-000237d0: 6967 2e72 6563 6f6d 7075 7465 2e72 6563  ig.recompute.rec
-000237e0: 6f6d 7075 7465 5f73 6c69 6365 5f61 6374  ompute_slice_act
-000237f0: 6976 6174 696f 6e29 0a0a 2020 2020 7265  ivation)..    re
-00023800: 7475 726e 205f 7365 745f 7061 7261 6c6c  turn _set_parall
-00023810: 656c 5f63 6f6e 6669 6775 7265 5f66 6f72  el_configure_for
-00023820: 5f6c 6179 6572 0a0a 0a63 6c61 7373 2054  _layer...class T
-00023830: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00023840: 7228 4365 6c6c 293a 0a20 2020 2072 2222  r(Cell):.    r""
-00023850: 220a 2020 2020 2020 2020 5472 616e 7366  ".        Transf
-00023860: 6f72 6d65 7220 456e 636f 6465 7220 6d6f  ormer Encoder mo
-00023870: 6475 6c65 2077 6974 6820 6d75 6c74 692d  dule with multi-
-00023880: 6c61 7965 7220 7374 6163 6b65 6420 6f66  layer stacked of
-00023890: 2060 5472 616e 7366 6f72 6d65 7245 6e63   `TransformerEnc
-000238a0: 6f64 6572 4c61 7965 7260 2c20 696e 636c  oderLayer`, incl
-000238b0: 7564 696e 6720 6d75 6c74 6968 6561 6420  uding multihead 
-000238c0: 7365 6c66 0a20 2020 2020 2020 2061 7474  self.        att
-000238d0: 656e 7469 6f6e 2061 6e64 2066 6565 6466  ention and feedf
-000238e0: 6f72 7761 7264 206c 6179 6572 2e0a 0a20  orward layer... 
-000238f0: 2020 2020 2020 2041 7267 733a 0a20 2020         Args:.   
-00023900: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
-00023910: 697a 6528 696e 7429 3a20 5468 6520 6261  ize(int): The ba
-00023920: 7463 6820 7369 7a65 206f 6620 7468 6520  tch size of the 
-00023930: 696e 7075 7420 7465 6e73 6f72 2077 6865  input tensor whe
-00023940: 6e20 646f 2069 6e63 7265 6e6d 656e 7461  n do increnmenta
-00023950: 6c20 7072 6564 6963 7469 6f6e 2e20 5368  l prediction. Sh
-00023960: 6f75 6c64 2062 6520 6120 706f 7369 7469  ould be a positi
-00023970: 7665 0a20 2020 2020 2020 2020 2020 2020  ve.             
-00023980: 2020 2076 616c 7565 2e20 5768 656e 2064     value. When d
-00023990: 6f20 7472 6169 6e69 6e67 206f 7220 7072  o training or pr
-000239a0: 6564 6963 7469 6f6e 2c20 7468 6520 6172  ediction, the ar
-000239b0: 6775 6d65 6e74 2077 696c 6c20 6e6f 7420  gument will not 
-000239c0: 776f 726b 2061 6e64 2074 6865 2075 7365  work and the use
-000239d0: 7220 6361 6e20 6a75 7374 2070 6173 7320  r can just pass 
-000239e0: 4e6f 6e65 2074 6f0a 2020 2020 2020 2020  None to.        
-000239f0: 2020 2020 2020 2020 7468 6520 6172 6775          the argu
-00023a00: 6d65 6e74 2e0a 2020 2020 2020 2020 2020  ment..          
-00023a10: 2020 6e75 6d5f 6c61 7965 7273 2869 6e74    num_layers(int
-00023a20: 293a 2054 6865 206c 6179 6572 7320 6f66  ): The layers of
-00023a30: 2074 6865 2060 5472 616e 7366 6f72 6d65   the `Transforme
-00023a40: 7245 6e63 6f64 6572 4c61 7965 7260 0a20  rEncoderLayer`. 
-00023a50: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-00023a60: 6e5f 7369 7a65 2869 6e74 293a 2054 6865  n_size(int): The
-00023a70: 2068 6964 6465 6e20 7369 7a65 206f 6620   hidden size of 
-00023a80: 7468 6520 696e 7075 742e 0a20 2020 2020  the input..     
-00023a90: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00023aa0: 6e5f 7369 7a65 2869 6e74 293a 2054 6865  n_size(int): The
-00023ab0: 2068 6964 6465 6e20 7369 7a65 206f 6620   hidden size of 
-00023ac0: 626f 7474 6c65 6e65 636b 2069 6e20 7468  bottleneck in th
-00023ad0: 6520 6665 6564 666f 7277 6172 6420 6c61  e feedforward la
-00023ae0: 7965 722e 0a20 2020 2020 2020 2020 2020  yer..           
-00023af0: 2073 6571 5f6c 656e 6774 6828 696e 7429   seq_length(int)
-00023b00: 3a20 5468 6520 7365 715f 6c65 6e67 7468  : The seq_length
-00023b10: 206f 6620 7468 6520 696e 7075 7420 7465   of the input te
-00023b20: 6e73 6f72 2e0a 2020 2020 2020 2020 2020  nsor..          
-00023b30: 2020 6e75 6d5f 6865 6164 7328 696e 7429    num_heads(int)
-00023b40: 3a20 5468 6520 6e75 6d62 6572 206f 6620  : The number of 
-00023b50: 7468 6520 6865 6164 732e 0a20 2020 2020  the heads..     
-00023b60: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-00023b70: 5f64 726f 706f 7574 5f72 6174 6528 666c  _dropout_rate(fl
-00023b80: 6f61 7429 3a20 5468 6520 6472 6f70 6f75  oat): The dropou
-00023b90: 7420 7261 7465 206f 6620 7468 6520 6174  t rate of the at
-00023ba0: 7465 6e74 696f 6e20 7363 6f72 6573 2e20  tention scores. 
-00023bb0: 4465 6661 756c 743a 302e 312e 0a20 2020  Default:0.1..   
-00023bc0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00023bd0: 6472 6f70 6f75 745f 7261 7465 2866 6c6f  dropout_rate(flo
-00023be0: 6174 293a 2054 6865 2064 726f 706f 7574  at): The dropout
-00023bf0: 2072 6174 6520 6f66 2074 6865 2066 696e   rate of the fin
-00023c00: 616c 206f 7574 7075 7420 6f66 2074 6865  al output of the
-00023c10: 206c 6179 6572 2e20 4465 6661 756c 743a   layer. Default:
-00023c20: 2030 2e31 2e0a 2020 2020 2020 2020 2020   0.1..          
-00023c30: 2020 6869 6464 656e 5f61 6374 2028 7374    hidden_act (st
-00023c40: 722c 206e 6e2e 4365 6c6c 293a 2054 6865  r, nn.Cell): The
-00023c50: 2061 6374 6976 6174 696f 6e20 6f66 2074   activation of t
-00023c60: 6865 2069 6e74 6572 6e61 6c20 6665 6564  he internal feed
-00023c70: 666f 7277 6172 6420 6c61 7965 722e 2053  forward layer. S
-00023c80: 7570 706f 7274 7320 2772 656c 7527 2c0a  upports 'relu',.
-00023c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023ca0: 2772 656c 7536 272c 2027 7461 6e68 272c  'relu6', 'tanh',
-00023cb0: 2027 6765 6c75 272c 2027 6661 7374 5f67   'gelu', 'fast_g
-00023cc0: 656c 7527 2c20 2765 6c75 272c 2027 7369  elu', 'elu', 'si
-00023cd0: 676d 6f69 6427 2c20 2770 7265 6c75 272c  gmoid', 'prelu',
-00023ce0: 2027 6c65 616b 7972 656c 7527 2c20 2768   'leakyrelu', 'h
-00023cf0: 7377 6973 6827 2c0a 2020 2020 2020 2020  swish',.        
-00023d00: 2020 2020 2020 2020 2768 7369 676d 6f69          'hsigmoi
-00023d10: 6427 2c20 276c 6f67 7369 676d 6f69 6427  d', 'logsigmoid'
-00023d20: 2061 6e64 2073 6f20 6f6e 2e20 5573 6572   and so on. User
-00023d30: 2063 616e 2070 726f 7669 6465 2063 7573   can provide cus
-00023d40: 746f 6d20 6163 7469 7669 7469 6f6e 2074  tom activition t
-00023d50: 6f20 7468 6520 6172 6775 6d65 6e74 2e0a  o the argument..
-00023d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023d70: 4966 2075 7365 7220 7761 6e74 7320 746f  If user wants to
-00023d80: 2072 756e 2074 6865 206e 6574 2069 6e20   run the net in 
-00023d90: 7468 6520 7061 7261 6c6c 656c 206d 6f64  the parallel mod
-00023da0: 652c 2074 6865 2063 7573 746f 6d20 6163  e, the custom ac
-00023db0: 7469 7661 7469 6f6e 206d 7573 7420 616c  tivation must al
-00023dc0: 736f 2070 726f 7669 6465 0a20 2020 2020  so provide.     
-00023dd0: 2020 2020 2020 2020 2020 2074 6865 2060             the `
-00023de0: 6163 7469 7661 7469 6f6e 5f73 6861 7264  activation_shard
-00023df0: 6020 6675 6e63 7469 6f6e 2e20 506c 6561  ` function. Plea
-00023e00: 7365 2073 6565 2074 6865 2065 7861 6d70  se see the examp
-00023e10: 6c65 7320 6f66 2074 6865 0a20 2020 2020  les of the.     
-00023e20: 2020 2020 2020 2020 2020 2063 6c61 7373             class
-00023e30: 3a60 6d69 6e64 666f 726d 6572 732e 6d6f  :`mindformers.mo
-00023e40: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-00023e50: 722e 4665 6564 466f 7277 6172 6460 2e20  r.FeedForward`. 
-00023e60: 4465 6661 756c 743a 2067 656c 752e 0a20  Default: gelu.. 
-00023e70: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
-00023e80: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-00023e90: 616c 2862 6f6f 6c29 3a20 446f 2072 6573  al(bool): Do res
-00023ea0: 6964 7561 6c73 2061 6464 7320 6265 666f  iduals adds befo
-00023eb0: 7265 2074 6865 206c 6179 6572 6e6f 726d  re the layernorm
-00023ec0: 2e20 4465 6661 756c 7420 4661 6c73 652e  . Default False.
-00023ed0: 0a20 2020 2020 2020 2020 2020 206c 6179  .            lay
-00023ee0: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-00023ef0: 7970 6528 6474 7970 652e 4e75 6d62 6572  ype(dtype.Number
-00023f00: 293a 2054 6865 2063 6f6d 7075 7461 7469  ): The computati
-00023f10: 6f6e 2074 7970 6520 6f66 2074 6865 206c  on type of the l
-00023f20: 6179 6572 6e6f 726d 2e0a 2020 2020 2020  ayernorm..      
-00023f30: 2020 2020 2020 2020 2020 5368 6f75 6c64            Should
-00023f40: 2062 6520 6d73 7479 7065 2e66 6c6f 6174   be mstype.float
-00023f50: 3332 206f 7220 6d73 7479 7065 2e66 6c6f  32 or mstype.flo
-00023f60: 6174 3136 2e20 4465 6661 756c 7420 6d73  at16. Default ms
-00023f70: 7479 7065 2e66 6c6f 6174 3332 2e0a 2020  type.float32..  
-00023f80: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
-00023f90: 785f 636f 6d70 7574 655f 7479 7065 2864  x_compute_type(d
-00023fa0: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
-00023fb0: 6520 636f 6d70 7574 6174 696f 6e20 7479  e computation ty
-00023fc0: 7065 206f 6620 7468 6520 736f 6674 6d61  pe of the softma
-00023fd0: 7820 696e 2074 6865 2061 7474 656e 7469  x in the attenti
-00023fe0: 6f6e 2e0a 2020 2020 2020 2020 2020 2020  on..            
-00023ff0: 2020 2020 5368 6f75 6c64 2062 6520 6d73      Should be ms
-00024000: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
-00024010: 6d73 7479 7065 2e66 6c6f 6174 3136 2e20  mstype.float16. 
-00024020: 4465 6661 756c 743a 206d 7374 7970 652e  Default: mstype.
-00024030: 666c 6f61 7433 322e 0a20 2020 2020 2020  float32..       
-00024040: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
-00024050: 7479 7065 2864 7479 7065 2e4e 756d 6265  type(dtype.Numbe
-00024060: 7229 3a20 5468 6520 7061 7261 6d65 7465  r): The paramete
-00024070: 7220 696e 6974 6961 6c69 7a61 7469 6f6e  r initialization
-00024080: 2074 7970 6520 6f66 2074 6865 206d 6f64   type of the mod
-00024090: 756c 652e 0a20 2020 2020 2020 2020 2020  ule..           
-000240a0: 2020 2020 2053 686f 756c 6420 6265 206d       Should be m
-000240b0: 7374 7970 652e 666c 6f61 7433 3220 6f72  stype.float32 or
-000240c0: 206d 7374 7970 652e 666c 6f61 7431 362e   mstype.float16.
-000240d0: 2044 6566 6175 6c74 3a20 6d73 7479 7065   Default: mstype
-000240e0: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
-000240f0: 2020 2020 2020 6c61 6d62 6461 5f66 756e        lambda_fun
-00024100: 6328 6675 6e63 7469 6f6e 293a 2041 2066  c(function): A f
-00024110: 756e 6374 696f 6e20 6361 6e20 6465 7465  unction can dete
-00024120: 726d 696e 6520 7468 6520 6675 7369 6f6e  rmine the fusion
-00024130: 2069 6e64 6578 2c0a 2020 2020 2020 2020   index,.        
-00024140: 2020 2020 2020 2020 7069 7065 6c69 6e65          pipeline
-00024150: 2073 7461 6765 7320 616e 6420 7265 636f   stages and reco
-00024160: 6d70 7574 6520 6174 7472 6962 7574 652e  mpute attribute.
-00024170: 2049 6620 7468 650a 2020 2020 2020 2020   If the.        
-00024180: 2020 2020 2020 2020 7573 6572 2077 616e          user wan
-00024190: 7473 2074 6f20 6465 7465 726d 696e 6520  ts to determine 
-000241a0: 7468 6520 7069 7065 6c69 6e65 2073 7461  the pipeline sta
-000241b0: 6765 2061 6e64 2067 7261 6469 656e 7420  ge and gradient 
-000241c0: 6167 6772 6567 6174 696f 6e20 6675 7369  aggregation fusi
-000241d0: 6f6e 2c20 7468 6520 7573 6572 2063 616e  on, the user can
-000241e0: 2070 6173 7320 610a 2020 2020 2020 2020   pass a.        
-000241f0: 2020 2020 2020 2020 6675 6e63 7469 6f6e          function
-00024200: 2074 6861 7420 6163 6365 7074 7320 606e   that accepts `n
-00024210: 6574 776f 726b 602c 2060 6c61 7965 725f  etwork`, `layer_
-00024220: 6964 602c 2060 6f66 6673 6574 602c 2060  id`, `offset`, `
-00024230: 7061 7261 6c6c 656c 5f63 6f6e 6669 6760  parallel_config`
-00024240: 2c20 606c 6179 6572 7360 2e20 5468 6520  , `layers`. The 
-00024250: 606e 6574 776f 726b 2843 656c 6c29 600a  `network(Cell)`.
-00024260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024270: 7265 7072 6573 656e 7473 2074 6865 2074  represents the t
-00024280: 7261 6e73 666f 726d 6572 2062 6c6f 636b  ransformer block
-00024290: 2c20 606c 6179 6572 5f69 6428 696e 7429  , `layer_id(int)
-000242a0: 6020 6d65 616e 7320 7468 6520 6c61 7965  ` means the laye
-000242b0: 7220 696e 6465 7820 666f 7220 7468 6520  r index for the 
-000242c0: 6375 7272 656e 7420 6d6f 6475 6c65 2c20  current module, 
-000242d0: 636f 756e 7473 0a20 2020 2020 2020 2020  counts.         
-000242e0: 2020 2020 2020 2066 726f 6d20 7a65 726f         from zero
-000242f0: 2c20 606f 6666 7365 7428 696e 7429 6020  , `offset(int)` 
-00024300: 6d65 616e 7320 7468 6520 6c61 7965 725f  means the layer_
-00024310: 696e 6465 7820 6e65 6564 7320 616e 206f  index needs an o
-00024320: 6666 7365 742c 2069 6620 7468 6572 6520  ffset, if there 
-00024330: 6172 6520 6f74 6865 7220 6d6f 6475 6c65  are other module
-00024340: 7320 696e 2074 6865 206e 6574 2e0a 2020  s in the net..  
-00024350: 2020 2020 2020 2020 2020 2020 2020 5468                Th
-00024360: 6520 6465 6661 756c 7420 7365 7474 696e  e default settin
-00024370: 6720 666f 7220 7468 6520 7069 7065 6c69  g for the pipeli
-00024380: 6e65 2069 733a 2060 286c 6179 6572 5f69  ne is: `(layer_i
-00024390: 6420 2b20 6f66 6673 6574 2920 2f2f 2028  d + offset) // (
-000243a0: 6c61 7965 7273 202f 2070 6970 656c 696e  layers / pipelin
-000243b0: 655f 7374 6167 6529 602e 0a20 2020 2020  e_stage)`..     
-000243c0: 2020 2020 2020 2020 2020 2044 6566 6175             Defau
-000243d0: 6c74 3a20 4e6f 6e65 2e0a 2020 2020 2020  lt: None..      
-000243e0: 2020 2020 2020 6f66 6673 6574 2869 6e74        offset(int
-000243f0: 293a 2054 6865 2069 6e69 7469 616c 206c  ): The initial l
-00024400: 6179 6572 2069 6e64 6578 2066 6f72 2074  ayer index for t
-00024410: 6865 2060 656e 636f 6465 7260 2e20 5573  he `encoder`. Us
-00024420: 6564 2066 6f72 2073 6574 7469 6e67 2074  ed for setting t
-00024430: 6865 2066 7573 696f 6e20 6964 2061 6e64  he fusion id and
-00024440: 2073 7461 6765 2069 642c 2074 6f20 6e6f   stage id, to no
-00024450: 740a 2020 2020 2020 2020 2020 2020 2020  t.              
-00024460: 2020 6f76 6572 6c61 7020 7769 7468 2074    overlap with t
-00024470: 6865 2065 6e63 6f64 6572 206c 6179 6572  he encoder layer
-00024480: 2e20 4465 6661 756c 7420 302e 0a20 2020  . Default 0..   
-00024490: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-000244a0: 7428 626f 6f6c 293a 2055 7365 2074 6865  t(bool): Use the
-000244b0: 2070 6173 7420 7374 6174 6520 746f 2063   past state to c
-000244c0: 6f6d 7075 7465 2c20 7573 6564 2066 6f72  ompute, used for
-000244d0: 2069 6e63 7265 6d65 6e74 616c 2070 7265   incremental pre
-000244e0: 6469 6374 696f 6e2e 2046 6f72 2065 7861  diction. For exa
-000244f0: 6d70 6c65 2c20 6966 2077 6520 6861 7665  mple, if we have
-00024500: 2074 776f 0a20 2020 2020 2020 2020 2020   two.           
-00024510: 2020 2020 2077 6f72 6473 2061 6e64 2077       words and w
-00024520: 616e 7420 746f 2067 656e 6572 6174 6520  ant to generate 
-00024530: 7468 6520 7465 6e20 6d6f 7265 2077 6f72  the ten more wor
-00024540: 6473 2e20 5765 206a 7573 7420 6e65 6564  ds. We just need
-00024550: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
-00024560: 7477 6f20 776f 7264 7327 2073 7461 7465  two words' state
-00024570: 206f 6e6c 7920 6f6e 6365 2c0a 2020 2020   only once,.    
-00024580: 2020 2020 2020 2020 2020 2020 616e 6420              and 
-00024590: 6765 6e65 7261 7465 2074 6865 206e 6578  generate the nex
-000245a0: 7420 776f 7264 206f 6e65 2062 7920 6f6e  t word one by on
-000245b0: 652e 2057 6865 6e20 7573 655f 7061 7374  e. When use_past
-000245c0: 2069 7320 5472 7565 2c20 7468 6572 6520   is True, there 
-000245d0: 6172 6520 7477 6f20 7374 6570 7320 746f  are two steps to
-000245e0: 2072 756e 2074 6865 2070 7265 6469 6374   run the predict
-000245f0: 696f 6e2e 0a20 2020 2020 2020 2020 2020  ion..           
-00024600: 2020 2020 2049 6e20 7468 6520 6669 7273       In the firs
-00024610: 7420 7374 6570 2c20 7365 7420 7468 6520  t step, set the 
-00024620: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00024630: 6f6e 2074 6f20 6265 2054 7275 6520 6279  on to be True by
-00024640: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00024650: 2060 6d6f 6465 6c2e 6164 645f 666c 6167   `model.add_flag
-00024660: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
-00024670: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
-00024680: 7275 6529 602c 2061 6e64 2070 6173 7320  rue)`, and pass 
-00024690: 7468 6520 6675 6c6c 2069 6e70 7574 732e  the full inputs.
-000246a0: 2054 6865 6e2c 2073 6574 2074 6865 0a20   Then, set the. 
-000246b0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-000246c0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-000246d0: 6e20 746f 2062 6520 4661 6c73 6520 6279  n to be False by
-000246e0: 2060 6d6f 6465 6c2e 6164 645f 666c 6167   `model.add_flag
-000246f0: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
-00024700: 6972 7374 5f69 7465 7261 7469 6f6e 3d46  irst_iteration=F
-00024710: 616c 7365 2960 2e20 4174 2074 6869 7320  alse)`. At this 
-00024720: 6d6f 6d65 6e74 2c0a 2020 2020 2020 2020  moment,.        
-00024730: 2020 2020 2020 2020 7061 7373 2074 6865          pass the
-00024740: 2073 696e 676c 6520 7374 6570 2773 2069   single step's i
-00024750: 6e70 7574 2074 656e 736f 722c 2061 6e64  nput tensor, and
-00024760: 206c 6f6f 7020 6974 2e20 4465 6661 756c   loop it. Defaul
-00024770: 743a 2046 616c 7365 2e0a 2020 2020 2020  t: False..      
-00024780: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
-00024790: 284d 6f45 436f 6e66 6967 293a 2054 6865  (MoEConfig): The
-000247a0: 2063 6f6e 6669 6775 7261 7469 6f6e 206f   configuration o
-000247b0: 6620 4d6f 4520 284d 6978 7475 7265 206f  f MoE (Mixture o
-000247c0: 6620 4578 7065 7274 292e 2044 6566 6175  f Expert). Defau
-000247d0: 6c74 2069 7320 616e 2069 6e73 7461 6e63  lt is an instanc
-000247e0: 6520 6f66 204d 6f45 436f 6e66 6967 0a20  e of MoEConfig. 
-000247f0: 2020 2020 2020 2020 2020 2020 2020 2077                 w
-00024800: 6974 6820 6465 6661 756c 7420 7661 6c75  ith default valu
-00024810: 6573 2e20 506c 6561 7365 2073 6565 2060  es. Please see `
-00024820: 4d6f 4543 6f6e 6669 6760 2e0a 2020 2020  MoEConfig`..    
-00024830: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-00024840: 5f63 6f6e 6669 6728 5472 616e 7366 6f72  _config(Transfor
-00024850: 6d65 724f 7050 6172 616c 6c65 6c43 6f6e  merOpParallelCon
-00024860: 6669 6729 3a20 5468 6520 7061 7261 6c6c  fig): The parall
-00024870: 656c 2063 6f6e 6669 6775 7265 2e20 4465  el configure. De
-00024880: 6661 756c 7420 6064 6566 6175 6c74 5f74  fault `default_t
-00024890: 7261 6e73 666f 726d 6572 5f63 6f6e 6669  ransformer_confi
-000248a0: 6760 2c0a 2020 2020 2020 2020 2020 2020  g`,.            
-000248b0: 2020 2020 616e 2069 6e73 7461 6e63 6520      an instance 
-000248c0: 6f66 2060 5472 616e 7366 6f72 6d65 724f  of `TransformerO
-000248d0: 7050 6172 616c 6c65 6c43 6f6e 6669 6760  pParallelConfig`
-000248e0: 2077 6974 6820 6465 6661 756c 7420 6172   with default ar
-000248f0: 6773 2e0a 0a20 2020 2020 2020 2049 6e70  gs...        Inp
-00024900: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-00024910: 202d 202a 2a68 6964 6465 6e5f 7374 6174   - **hidden_stat
-00024920: 6573 2a2a 2028 5465 6e73 6f72 2920 2d20  es** (Tensor) - 
-00024930: 5465 6e73 6f72 2c20 7368 6170 6520 7368  Tensor, shape sh
-00024940: 6f75 6c64 2062 6520 5b62 6174 6368 5f73  ould be [batch_s
-00024950: 697a 652c 2073 6571 5f6c 656e 6774 682c  ize, seq_length,
-00024960: 2068 6964 6465 6e5f 7369 7a65 5d20 6f72   hidden_size] or
-00024970: 0a20 2020 2020 2020 2020 2020 2020 205b  .              [
-00024980: 6261 7463 685f 7369 7a65 202a 2073 6571  batch_size * seq
-00024990: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
-000249a0: 7369 7a65 5d2c 2069 6620 7468 6520 7573  size], if the us
-000249b0: 655f 7061 7374 2069 7320 4661 6c73 6520  e_past is False 
-000249c0: 6f72 2069 735f 6669 7273 745f 6974 6572  or is_first_iter
-000249d0: 6174 696f 6e3d 5472 7565 2e20 4f74 6865  ation=True. Othe
-000249e0: 7277 6973 652c 0a20 2020 2020 2020 2020  rwise,.         
-000249f0: 2020 2020 2073 686f 756c 6420 6265 205b       should be [
-00024a00: 6261 7463 685f 7369 7a65 2c20 312c 2068  batch_size, 1, h
-00024a10: 6964 6465 6e5f 7369 7a65 5d2e 0a20 2020  idden_size]..   
-00024a20: 2020 2020 2020 2020 202d 202a 2a61 7474           - **att
-00024a30: 656e 7469 6f6e 5f6d 6173 6b2a 2a20 2854  ention_mask** (T
-00024a40: 656e 736f 7229 202d 2046 6c6f 6174 2054  ensor) - Float T
-00024a50: 656e 736f 722c 2049 6620 7468 6520 7573  ensor, If the us
-00024a60: 655f 7061 7374 2069 7320 4661 6c73 6520  e_past is False 
-00024a70: 6f72 2069 735f 6669 7273 745f 6974 6572  or is_first_iter
-00024a80: 6174 696f 6e3d 5472 7565 2c0a 2020 2020  ation=True,.    
-00024a90: 2020 2020 2020 2020 2020 7468 6520 6174            the at
-00024aa0: 7465 6e74 696f 6e20 6d61 736b 206d 6174  tention mask mat
-00024ab0: 7269 7820 7368 6f75 6c64 2062 6120 5b62  rix should ba [b
-00024ac0: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
-00024ad0: 656e 6774 682c 2073 6571 5f6c 656e 6774  ength, seq_lengt
-00024ae0: 685d 2c20 6f72 204e 6f6e 652e 204e 6f6e  h], or None. Non
-00024af0: 6520 6d65 616e 7320 7468 6572 6520 7769  e means there wi
-00024b00: 6c6c 0a20 2020 2020 2020 2020 2020 2020  ll.             
-00024b10: 2062 6520 6e6f 206d 6173 6b20 696e 2073   be no mask in s
-00024b20: 6f66 746d 6178 2063 6f6d 7075 7461 7469  oftmax computati
-00024b30: 6f6e 2e20 4f74 6865 7277 6973 652c 2073  on. Otherwise, s
-00024b40: 686f 756c 6420 6265 205b 6261 7463 685f  hould be [batch_
-00024b50: 7369 7a65 2c20 312c 2068 6964 6465 6e5f  size, 1, hidden_
-00024b60: 7369 7a65 5d0a 2020 2020 2020 2020 2020  size].          
-00024b70: 2020 2d20 2a2a 696e 6974 5f72 6573 6574    - **init_reset
-00024b80: 2a2a 2028 5465 6e73 6f72 2920 2d20 4120  ** (Tensor) - A 
-00024b90: 626f 6f6c 2074 656e 736f 7220 7769 7468  bool tensor with
-00024ba0: 2073 6861 7065 205b 315d 2c20 7573 6564   shape [1], used
-00024bb0: 2074 6f20 636c 6561 7220 7468 6520 7061   to clear the pa
-00024bc0: 7374 206b 6579 2070 6172 616d 6574 6572  st key parameter
-00024bd0: 2061 6e64 0a20 2020 2020 2020 2020 2020   and.           
-00024be0: 2020 2070 6173 7420 7661 6c75 6520 7061     past value pa
-00024bf0: 7261 6d65 7465 7220 7573 6564 2069 6e20  rameter used in 
-00024c00: 7468 6520 696e 6372 656d 656e 7461 6c20  the incremental 
-00024c10: 7072 6564 6963 7469 6f6e 2e20 4f6e 6c79  prediction. Only
-00024c20: 2076 616c 6964 2077 6865 6e20 7573 655f   valid when use_
-00024c30: 7061 7374 2069 7320 5472 7565 2e20 4465  past is True. De
-00024c40: 6661 756c 7420 5472 7565 2e0a 2020 2020  fault True..    
-00024c50: 2020 2020 2020 2020 2d20 2a2a 6261 7463          - **batc
-00024c60: 685f 7661 6c69 645f 6c65 6e67 7468 2a2a  h_valid_length**
-00024c70: 2028 5465 6e73 6f72 2920 2d20 496e 7433   (Tensor) - Int3
-00024c80: 3220 7465 6e73 6f72 2077 6974 6820 7368  2 tensor with sh
-00024c90: 6170 6520 5b62 6174 6368 5f73 697a 655d  ape [batch_size]
-00024ca0: 2074 6865 2070 6173 7420 6361 6c63 756c   the past calcul
-00024cb0: 6174 6564 2074 6865 2069 6e64 6578 2e0a  ated the index..
-00024cc0: 2020 2020 2020 2020 2020 2020 2020 5573                Us
-00024cd0: 6564 2066 6f72 2069 6e63 7265 6d65 6e74  ed for increment
-00024ce0: 616c 2070 7265 6469 6374 696f 6e20 7768  al prediction wh
-00024cf0: 656e 2074 6865 2075 7365 5f70 6173 7420  en the use_past 
-00024d00: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
-00024d10: 204e 6f6e 652e 0a0a 2020 2020 2020 2020   None...        
-00024d20: 4f75 7470 7574 733a 0a20 2020 2020 2020  Outputs:.       
-00024d30: 2020 2020 2054 7570 6c65 2c20 6120 7475       Tuple, a tu
-00024d40: 706c 6520 636f 6e74 6169 6e73 2860 6f75  ple contains(`ou
-00024d50: 7470 7574 602c 2060 6c61 7965 725f 7072  tput`, `layer_pr
-00024d60: 6573 656e 7460 290a 0a20 2020 2020 2020  esent`)..       
-00024d70: 2020 2020 202d 202a 2a6f 7574 7075 742a       - **output*
-00024d80: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
-00024d90: 2066 6c6f 6174 2074 656e 736f 7220 6f66   float tensor of
-00024da0: 2074 6865 206f 7574 7075 7420 6f66 2074   the output of t
-00024db0: 6865 206c 6179 6572 2077 6974 680a 2020  he layer with.  
-00024dc0: 2020 2020 2020 2020 2020 2020 7368 6170              shap
-00024dd0: 6520 2862 6174 6368 5f73 697a 652c 2073  e (batch_size, s
-00024de0: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-00024df0: 6e5f 7369 7a65 2920 6f72 2028 6261 7463  n_size) or (batc
-00024e00: 685f 7369 7a65 202a 2073 6571 5f6c 656e  h_size * seq_len
-00024e10: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-00024e20: 292c 2069 6620 7468 6520 7573 655f 7061  ), if the use_pa
-00024e30: 7374 2069 730a 2020 2020 2020 2020 2020  st is.          
-00024e40: 2020 2020 4661 6c73 6520 6f72 2069 735f      False or is_
-00024e50: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-00024e60: 5472 7565 2e20 4f74 6865 7277 6973 652c  True. Otherwise,
-00024e70: 2069 7420 7769 6c6c 2062 6520 2862 6174   it will be (bat
-00024e80: 6368 5f73 697a 652c 2031 2c20 6869 6464  ch_size, 1, hidd
-00024e90: 656e 5f73 697a 6529 2e0a 2020 2020 2020  en_size)..      
-00024ea0: 2020 2020 2020 2d20 2a2a 6c61 7965 725f        - **layer_
-00024eb0: 7072 6573 656e 742a 2a20 2854 7570 6c65  present** (Tuple
-00024ec0: 2920 2d20 4120 7475 706c 6520 7769 7468  ) - A tuple with
-00024ed0: 2073 697a 6520 6f66 206e 756d 5f6c 6179   size of num_lay
-00024ee0: 6572 732c 2077 6865 7265 2065 6163 6820  ers, where each 
-00024ef0: 7475 706c 6520 636f 6e74 6169 6e73 2074  tuple contains t
-00024f00: 6865 2054 656e 736f 7220 7468 650a 2020  he Tensor the.  
-00024f10: 2020 2020 2020 2020 2020 2020 7072 6f6a              proj
-00024f20: 6563 7465 6420 6b65 7920 616e 6420 7661  ected key and va
-00024f30: 6c75 6520 7665 6374 6f72 2077 6974 6820  lue vector with 
-00024f40: 7368 6170 6520 2828 6261 7463 685f 7369  shape ((batch_si
-00024f50: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
-00024f60: 697a 655f 7065 725f 6865 6164 2c20 7365  ize_per_head, se
-00024f70: 715f 6c65 6e67 7468 292c 0a20 2020 2020  q_length),.     
-00024f80: 2020 2020 2020 2020 2061 6e64 2028 6261           and (ba
-00024f90: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
-00024fa0: 6164 732c 2073 6571 5f6c 656e 6774 682c  ads, seq_length,
-00024fb0: 2073 697a 655f 7065 725f 6865 6164 2929   size_per_head))
-00024fc0: 2e0a 0a20 2020 2020 2020 2053 7570 706f  ...        Suppo
-00024fd0: 7274 6564 2050 6c61 7466 6f72 6d73 3a0a  rted Platforms:.
-00024fe0: 2020 2020 2020 2020 2020 2020 6060 4173              ``As
-00024ff0: 6365 6e64 6060 2060 6047 5055 6060 0a0a  cend`` ``GPU``..
-00025000: 2020 2020 2020 2020 4578 616d 706c 6573          Examples
-00025010: 3a0a 2020 2020 2020 2020 2020 2020 3e3e  :.            >>
-00025020: 3e20 696d 706f 7274 206e 756d 7079 2061  > import numpy a
-00025030: 7320 6e70 0a20 2020 2020 2020 2020 2020  s np.           
-00025040: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
-00025050: 6f72 6520 696d 706f 7274 2064 7479 7065  ore import dtype
-00025060: 2061 7320 6d73 7479 7065 0a20 2020 2020   as mstype.     
-00025070: 2020 2020 2020 203e 3e3e 2066 726f 6d20         >>> from 
-00025080: 6d69 6e64 666f 726d 6572 732e 6d6f 6475  mindformers.modu
-00025090: 6c65 732e 7472 616e 7366 6f72 6d65 7220  les.transformer 
-000250a0: 696d 706f 7274 2054 7261 6e73 666f 726d  import Transform
-000250b0: 6572 456e 636f 6465 720a 2020 2020 2020  erEncoder.      
-000250c0: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
-000250d0: 696e 6473 706f 7265 2069 6d70 6f72 7420  indspore import 
-000250e0: 5465 6e73 6f72 0a20 2020 2020 2020 2020  Tensor.         
-000250f0: 2020 203e 3e3e 206d 6f64 656c 203d 2054     >>> model = T
-00025100: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00025110: 7228 6261 7463 685f 7369 7a65 3d32 2c20  r(batch_size=2, 
-00025120: 6e75 6d5f 6c61 7965 7273 3d32 2c20 6869  num_layers=2, hi
-00025130: 6464 656e 5f73 697a 653d 382c 2066 666e  dden_size=8, ffn
-00025140: 5f68 6964 6465 6e5f 7369 7a65 3d36 342c  _hidden_size=64,
-00025150: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
-00025160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025170: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
-00025180: 6c65 6e67 7468 3d31 362c 206e 756d 5f68  length=16, num_h
-00025190: 6561 6473 3d32 290a 2020 2020 2020 2020  eads=2).        
-000251a0: 2020 2020 3e3e 3e20 656e 636f 6465 725f      >>> encoder_
-000251b0: 696e 7075 745f 7661 6c75 6520 3d20 5465  input_value = Te
-000251c0: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-000251d0: 2031 362c 2038 2929 2c20 6d73 7479 7065   16, 8)), mstype
-000251e0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
-000251f0: 2020 2020 2020 3e3e 3e20 656e 636f 6465        >>> encode
-00025200: 725f 696e 7075 745f 6d61 736b 203d 2054  r_input_mask = T
-00025210: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
-00025220: 2c20 3136 2c20 3136 2929 2c20 6d73 7479  , 16, 16)), msty
-00025230: 7065 2e66 6c6f 6174 3136 290a 2020 2020  pe.float16).    
-00025240: 2020 2020 2020 2020 3e3e 3e20 6f75 7470          >>> outp
-00025250: 7574 2c20 7061 7374 203d 206d 6f64 656c  ut, past = model
-00025260: 2865 6e63 6f64 6572 5f69 6e70 7574 5f76  (encoder_input_v
-00025270: 616c 7565 2c20 656e 636f 6465 725f 696e  alue, encoder_in
-00025280: 7075 745f 6d61 736b 290a 2020 2020 2020  put_mask).      
-00025290: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
-000252a0: 6f75 7470 7574 2e73 6861 7065 290a 2020  output.shape).  
-000252b0: 2020 2020 2020 2020 2020 2832 2c20 3136            (2, 16
-000252c0: 2c20 3829 0a20 2020 2020 2020 2020 2020  , 8).           
-000252d0: 203e 3e3e 2070 7269 6e74 286c 656e 2870   >>> print(len(p
-000252e0: 6173 7429 290a 2020 2020 2020 2020 2020  ast)).          
-000252f0: 2020 320a 2020 2020 2020 2020 2020 2020    2.            
-00025300: 3e3e 3e20 7072 696e 7428 7061 7374 5b30  >>> print(past[0
-00025310: 5d5b 305d 2e73 6861 7065 290a 2020 2020  ][0].shape).    
-00025320: 2020 2020 2020 2020 2832 2c20 322c 2034          (2, 2, 4
-00025330: 2c20 3136 290a 2020 2020 2020 2020 2020  , 16).          
-00025340: 2020 3e3e 3e20 7072 696e 7428 7061 7374    >>> print(past
-00025350: 5b30 5d5b 315d 2e73 6861 7065 290a 2020  [0][1].shape).  
-00025360: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-00025370: 2031 362c 2034 290a 2020 2020 2020 2020   16, 4).        
-00025380: 2020 2020 3e3e 3e20 2320 5768 656e 2075      >>> # When u
-00025390: 7365 2075 7365 5f70 6173 743d 5472 7565  se use_past=True
-000253a0: 2c20 6974 2069 6e63 6c75 6465 7320 7477  , it includes tw
-000253b0: 6f20 7374 6570 7320 746f 2069 6d70 6c65  o steps to imple
-000253c0: 6d65 6e74 2074 6865 2069 6e63 7265 6d65  ment the increme
-000253d0: 6e74 616c 2070 7265 6469 6374 696f 6e2e  ntal prediction.
-000253e0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000253f0: 2023 2053 7465 7020 313a 2073 6574 2069   # Step 1: set i
-00025400: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00025410: 6e3d 5472 7565 2c20 616e 6420 696e 7075  n=True, and inpu
-00025420: 7420 7468 6520 6675 6c6c 2073 6571 7565  t the full seque
-00025430: 6e63 6520 6c65 6e67 7468 2773 2073 7461  nce length's sta
-00025440: 7465 2e0a 2020 2020 2020 2020 2020 2020  te..            
-00025450: 3e3e 3e20 6261 7463 685f 7661 6c69 645f  >>> batch_valid_
-00025460: 6c65 6e67 7468 203d 2054 656e 736f 7228  length = Tensor(
-00025470: 6e70 2e6f 6e65 7328 2832 2c29 292c 206d  np.ones((2,)), m
-00025480: 7374 7970 652e 696e 7433 3229 0a20 2020  stype.int32).   
-00025490: 2020 2020 2020 2020 203e 3e3e 2069 6e69           >>> ini
-000254a0: 745f 7265 7365 7420 3d20 5465 6e73 6f72  t_reset = Tensor
-000254b0: 285b 5472 7565 5d2c 206d 7374 7970 652e  ([True], mstype.
-000254c0: 626f 6f6c 5f29 0a20 2020 2020 2020 2020  bool_).         
-000254d0: 2020 203e 3e3e 2023 2053 6574 2069 735f     >>> # Set is_
-000254e0: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-000254f0: 5472 7565 2074 6f20 6765 6e65 7261 7465  True to generate
-00025500: 2074 6865 2066 756c 6c20 6d65 6d6f 7279   the full memory
-00025510: 2073 7461 7465 730a 2020 2020 2020 2020   states.        
-00025520: 2020 2020 3e3e 3e20 6d6f 6465 6c20 3d20      >>> model = 
-00025530: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
-00025540: 6572 2862 6174 6368 5f73 697a 653d 322c  er(batch_size=2,
-00025550: 2068 6964 6465 6e5f 7369 7a65 3d38 2c20   hidden_size=8, 
-00025560: 6666 6e5f 6869 6464 656e 5f73 697a 653d  ffn_hidden_size=
-00025570: 3634 2c20 7365 715f 6c65 6e67 7468 3d31  64, seq_length=1
-00025580: 362c 0a20 2020 2020 2020 2020 2020 202e  6,.            .
-00025590: 2e2e 2020 2020 2020 2020 2020 2020 2020  ..              
-000255a0: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
-000255b0: 6d5f 6865 6164 733d 322c 206e 756d 5f6c  m_heads=2, num_l
-000255c0: 6179 6572 733d 322c 2075 7365 5f70 6173  ayers=2, use_pas
-000255d0: 743d 5472 7565 290a 2020 2020 2020 2020  t=True).        
-000255e0: 2020 2020 3e3e 3e20 6d6f 6465 6c2e 6164      >>> model.ad
-000255f0: 645f 666c 6167 735f 7265 6375 7273 6976  d_flags_recursiv
-00025600: 6528 6973 5f66 6972 7374 5f69 7465 7261  e(is_first_itera
-00025610: 7469 6f6e 3d54 7275 6529 0a20 2020 2020  tion=True).     
-00025620: 2020 2020 2020 203e 3e3e 2068 6964 6465         >>> hidde
-00025630: 6e2c 2070 6173 7420 3d20 6d6f 6465 6c28  n, past = model(
-00025640: 656e 636f 6465 725f 696e 7075 745f 7661  encoder_input_va
-00025650: 6c75 652c 2065 6e63 6f64 6572 5f69 6e70  lue, encoder_inp
-00025660: 7574 5f6d 6173 6b2c 2069 6e69 745f 7265  ut_mask, init_re
-00025670: 7365 742c 2062 6174 6368 5f76 616c 6964  set, batch_valid
-00025680: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-00025690: 2020 2020 203e 3e3e 2070 7269 6e74 2868       >>> print(h
-000256a0: 6964 6465 6e2e 7368 6170 6529 0a20 2020  idden.shape).   
-000256b0: 2020 2020 2020 2020 2028 322c 2031 362c           (2, 16,
-000256c0: 2038 290a 2020 2020 2020 2020 2020 2020   8).            
-000256d0: 3e3e 3e20 7072 696e 7428 7061 7374 5b30  >>> print(past[0
-000256e0: 5d5b 305d 2e73 6861 7065 290a 2020 2020  ][0].shape).    
-000256f0: 2020 2020 2020 2020 2832 2c20 322c 2034          (2, 2, 4
-00025700: 2c20 3136 290a 2020 2020 2020 2020 2020  , 16).          
-00025710: 2020 3e3e 3e20 7072 696e 7428 7061 7374    >>> print(past
-00025720: 5b30 5d5b 315d 2e73 6861 7065 290a 2020  [0][1].shape).  
-00025730: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-00025740: 2031 362c 2034 290a 2020 2020 2020 2020   16, 4).        
-00025750: 2020 2020 3e3e 3e20 656e 636f 6465 725f      >>> encoder_
-00025760: 696e 7075 745f 7661 6c75 6520 3d20 5465  input_value = Te
-00025770: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-00025780: 2031 2c20 3829 292c 206d 7374 7970 652e   1, 8)), mstype.
-00025790: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-000257a0: 2020 2020 203e 3e3e 2065 6e63 6f64 6572       >>> encoder
-000257b0: 5f69 6e70 7574 5f6d 6173 6b20 3d20 5465  _input_mask = Te
-000257c0: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-000257d0: 2031 2c20 3136 2929 2c20 6d73 7479 7065   1, 16)), mstype
-000257e0: 2e66 6c6f 6174 3136 290a 2020 2020 2020  .float16).      
-000257f0: 2020 2020 2020 3e3e 3e20 696e 6974 5f72        >>> init_r
-00025800: 6573 6574 203d 2054 656e 736f 7228 5b46  eset = Tensor([F
-00025810: 616c 7365 5d2c 206d 7374 7970 652e 626f  alse], mstype.bo
-00025820: 6f6c 5f29 0a20 2020 2020 2020 2020 2020  ol_).           
-00025830: 203e 3e3e 2023 2053 7465 7020 323a 2073   >>> # Step 2: s
-00025840: 6574 2069 735f 6669 7273 745f 6974 6572  et is_first_iter
-00025850: 6174 696f 6e3d 4661 6c73 652c 2061 6e64  ation=False, and
-00025860: 2070 6173 7320 7468 6520 7369 6e67 6c65   pass the single
-00025870: 2077 6f72 6420 746f 2072 756e 2074 6865   word to run the
-00025880: 2070 7265 6469 6374 696f 6e20 7261 7468   prediction rath
-00025890: 6572 2074 6861 6e0a 2020 2020 2020 2020  er than.        
-000258a0: 2020 2020 3e3e 3e20 2320 7468 6520 6675      >>> # the fu
-000258b0: 6c6c 2073 6571 7565 6e63 652e 0a20 2020  ll sequence..   
-000258c0: 2020 2020 2020 2020 203e 3e3e 206d 6f64           >>> mod
-000258d0: 656c 2e61 6464 5f66 6c61 6773 5f72 6563  el.add_flags_rec
-000258e0: 7572 7369 7665 2869 735f 6669 7273 745f  ursive(is_first_
-000258f0: 6974 6572 6174 696f 6e3d 4661 6c73 6529  iteration=False)
-00025900: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00025910: 2068 6964 6465 6e2c 2070 6173 7420 3d20   hidden, past = 
-00025920: 6d6f 6465 6c28 656e 636f 6465 725f 696e  model(encoder_in
-00025930: 7075 745f 7661 6c75 652c 2065 6e63 6f64  put_value, encod
-00025940: 6572 5f69 6e70 7574 5f6d 6173 6b2c 2069  er_input_mask, i
-00025950: 6e69 745f 7265 7365 742c 2062 6174 6368  nit_reset, batch
-00025960: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
-00025970: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
-00025980: 7269 6e74 2868 6964 6465 6e2e 7368 6170  rint(hidden.shap
-00025990: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-000259a0: 322c 2031 2c20 3829 0a20 2020 2020 2020  2, 1, 8).       
-000259b0: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-000259c0: 6173 745b 305d 5b30 5d2e 7368 6170 6529  ast[0][0].shape)
-000259d0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-000259e0: 2032 2c20 342c 2031 3629 0a20 2020 2020   2, 4, 16).     
-000259f0: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
-00025a00: 2870 6173 745b 305d 5b31 5d2e 7368 6170  (past[0][1].shap
-00025a10: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-00025a20: 322c 2032 2c20 3136 2c20 3429 0a20 2020  2, 2, 16, 4).   
-00025a30: 2022 2222 0a0a 2020 2020 405f 4c6f 6741   """..    @_LogA
-00025a40: 6374 696f 6e4f 6e63 6528 6d5f 6c6f 6767  ctionOnce(m_logg
-00025a50: 6572 3d6c 6f67 6765 722c 206b 6579 3d27  er=logger, key='
-00025a60: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
-00025a70: 6572 272c 0a20 2020 2020 2020 2020 2020  er',.           
-00025a80: 2020 2020 2020 2020 206e 6f5f 7761 726e           no_warn
-00025a90: 696e 673d 5f67 6574 5f70 6172 616c 6c65  ing=_get_paralle
-00025aa0: 6c5f 6d6f 6465 2829 2069 6e20 2850 6172  l_mode() in (Par
-00025ab0: 616c 6c65 6c4d 6f64 652e 5354 414e 445f  allelMode.STAND_
-00025ac0: 414c 4f4e 452c 2929 0a20 2020 2040 5f61  ALONE,)).    @_a
-00025ad0: 7267 735f 7479 7065 5f76 616c 6964 6174  rgs_type_validat
-00025ae0: 6f72 5f63 6865 636b 2868 6964 6465 6e5f  or_check(hidden_
-00025af0: 7369 7a65 3d56 616c 6964 6174 6f72 2e63  size=Validator.c
-00025b00: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
-00025b10: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-00025b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025b30: 2020 206e 756d 5f68 6561 6473 3d56 616c     num_heads=Val
-00025b40: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-00025b50: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
-00025b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025b70: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-00025b80: 6964 6465 6e5f 7369 7a65 3d56 616c 6964  idden_size=Valid
-00025b90: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-00025ba0: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-00025bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025bc0: 2020 2020 2020 2020 2073 6571 5f6c 656e           seq_len
-00025bd0: 6774 683d 5661 6c69 6461 746f 722e 6368  gth=Validator.ch
-00025be0: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-00025bf0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00025c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025c10: 2020 6e75 6d5f 6c61 7965 7273 3d56 616c    num_layers=Val
-00025c20: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-00025c30: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
-00025c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025c50: 2020 2020 2020 2020 2020 206f 6666 7365             offse
-00025c60: 743d 5661 6c69 6461 746f 722e 6368 6563  t=Validator.chec
-00025c70: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f69  k_non_negative_i
-00025c80: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-00025c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025ca0: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-00025cb0: 6f70 6f75 745f 7261 7465 3d56 616c 6964  opout_rate=Valid
-00025cc0: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
-00025cd0: 6567 6174 6976 655f 666c 6f61 742c 0a20  egative_float,. 
-00025ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025cf0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-00025d00: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-00025d10: 7465 3d56 616c 6964 6174 6f72 2e63 6865  te=Validator.che
-00025d20: 636b 5f6e 6f6e 5f6e 6567 6174 6976 655f  ck_non_negative_
-00025d30: 666c 6f61 742c 0a20 2020 2020 2020 2020  float,.         
-00025d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025d50: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
-00025d60: 726e 6f72 6d5f 7265 7369 6475 616c 3d56  rnorm_residual=V
-00025d70: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
-00025d80: 6f6f 6c2c 0a20 2020 2020 2020 2020 2020  ool,.           
-00025d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025da0: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
-00025db0: 6f6d 7075 7465 5f74 7970 653d 5f76 616c  ompute_type=_val
-00025dc0: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
-00025dd0: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00025de0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00025df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021f00: 2020 7365 6c66 2e76 616c 7565 5f70 6173    self.value_pas
+00021f10: 742c 2062 6174 6368 5f76 616c 6964 5f6c  t, batch_valid_l
+00021f20: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
+00021f30: 2020 206c 6179 6572 5f70 7265 7365 6e74     layer_present
+00021f40: 202b 3d20 6372 6f73 735f 6c61 7965 725f   += cross_layer_
+00021f50: 7072 6573 656e 740a 2020 2020 2020 2020  present.        
+00021f60: 2020 2020 6966 2073 656c 662e 706f 7374      if self.post
+00021f70: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
+00021f80: 7561 6c3a 0a20 2020 2020 2020 2020 2020  ual:.           
+00021f90: 2020 2020 2078 203d 2073 656c 662e 6164       x = self.ad
+00021fa0: 6428 6d69 6464 6c65 5f6f 7574 7075 742c  d(middle_output,
+00021fb0: 2063 726f 7373 5f61 7474 6e5f 6f75 7470   cross_attn_outp
+00021fc0: 7574 290a 2020 2020 2020 2020 2020 2020  ut).            
+00021fd0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00021fe0: 2020 2020 2020 7820 3d20 7365 6c66 2e61        x = self.a
+00021ff0: 6464 2878 2c20 6372 6f73 735f 6174 746e  dd(x, cross_attn
+00022000: 5f6f 7574 7075 7429 0a0a 2020 2020 2020  _output)..      
+00022010: 2020 6f75 7470 7574 5f78 203d 2073 656c    output_x = sel
+00022020: 662e 6c61 7965 726e 6f72 6d32 2878 290a  f.layernorm2(x).
+00022030: 2020 2020 2020 2020 6f75 7470 7574 5f78          output_x
+00022040: 203d 2046 2e63 6173 7428 6f75 7470 7574   = F.cast(output
+00022050: 5f78 2c20 7365 6c66 2e64 7479 7065 290a  _x, self.dtype).
+00022060: 2020 2020 2020 2020 6175 785f 6c6f 7373          aux_loss
+00022070: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
+00022080: 6966 2073 656c 662e 7573 655f 6d6f 653a  if self.use_moe:
+00022090: 0a20 2020 2020 2020 2020 2020 206d 6c70  .            mlp
+000220a0: 5f6c 6f67 6974 2c20 6175 785f 6c6f 7373  _logit, aux_loss
+000220b0: 203d 2073 656c 662e 6f75 7470 7574 286f   = self.output(o
+000220c0: 7574 7075 745f 7829 0a20 2020 2020 2020  utput_x).       
+000220d0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+000220e0: 2020 206d 6c70 5f6c 6f67 6974 203d 2073     mlp_logit = s
+000220f0: 656c 662e 6f75 7470 7574 286f 7574 7075  elf.output(outpu
+00022100: 745f 7829 0a0a 2020 2020 2020 2020 7661  t_x)..        va
+00022110: 6c75 655f 7570 6461 7465 203d 204e 6f6e  lue_update = Non
+00022120: 650a 2020 2020 2020 2020 6b65 795f 7570  e.        key_up
+00022130: 6461 7465 203d 204e 6f6e 650a 2020 2020  date = None.    
+00022140: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+00022150: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+00022160: 2020 2320 6375 7272 656e 7420 6b65 7920    # current key 
+00022170: 616e 6420 7661 6c75 650a 2020 2020 2020  and value.      
+00022180: 2020 2020 2020 6b65 795f 7072 6573 656e        key_presen
+00022190: 742c 2076 616c 7565 5f70 7265 7365 6e74  t, value_present
+000221a0: 203d 206c 6179 6572 5f70 7265 7365 6e74   = layer_present
+000221b0: 0a20 2020 2020 2020 2020 2020 2023 2075  .            # u
+000221c0: 7064 6174 6520 6b65 7920 616e 6420 7661  pdate key and va
+000221d0: 6c75 6520 6361 6c63 756c 6174 6564 2074  lue calculated t
+000221e0: 6869 7320 7374 6570 0a20 2020 2020 2020  his step.       
+000221f0: 2020 2020 2073 656c 662e 6173 7369 676e       self.assign
+00022200: 2873 656c 662e 6b65 795f 7061 7374 2c20  (self.key_past, 
+00022210: 6b65 795f 7072 6573 656e 7429 0a20 2020  key_present).   
+00022220: 2020 2020 2020 2020 206b 6579 5f75 7064           key_upd
+00022230: 6174 6520 3d20 7365 6c66 2e6b 6579 5f70  ate = self.key_p
+00022240: 6173 740a 2020 2020 2020 2020 2020 2020  ast.            
+00022250: 7365 6c66 2e61 7373 6967 6e28 7365 6c66  self.assign(self
+00022260: 2e76 616c 7565 5f70 6173 742c 2076 616c  .value_past, val
+00022270: 7565 5f70 7265 7365 6e74 290a 2020 2020  ue_present).    
+00022280: 2020 2020 2020 2020 7661 6c75 655f 7570          value_up
+00022290: 6461 7465 203d 2073 656c 662e 7661 6c75  date = self.valu
+000222a0: 655f 7061 7374 0a20 2020 2020 2020 2020  e_past.         
+000222b0: 2020 2023 2061 6464 2064 6570 656e 6465     # add depende
+000222c0: 6e63 7920 666f 7220 6465 7369 7265 6420  ncy for desired 
+000222d0: 6578 6563 7574 696f 6e20 6f72 6465 720a  execution order.
+000222e0: 2020 2020 2020 2020 2020 2020 6b65 795f              key_
+000222f0: 7570 6461 7465 203d 2046 2e64 6570 656e  update = F.depen
+00022300: 6428 6b65 795f 7570 6461 7465 2c20 6b65  d(key_update, ke
+00022310: 795f 7265 7365 7429 0a20 2020 2020 2020  y_reset).       
+00022320: 2020 2020 2076 616c 7565 5f75 7064 6174       value_updat
+00022330: 6520 3d20 462e 6465 7065 6e64 2876 616c  e = F.depend(val
+00022340: 7565 5f75 7064 6174 652c 2076 616c 7565  ue_update, value
+00022350: 5f72 6573 6574 290a 0a20 2020 2020 2020  _reset)..       
+00022360: 2023 2061 6464 2064 6570 656e 6465 6e63   # add dependenc
+00022370: 7920 666f 7220 6465 7369 7265 6420 6578  y for desired ex
+00022380: 6563 7574 696f 6e20 6f72 6465 720a 2020  ecution order.  
+00022390: 2020 2020 2020 6d6c 705f 6c6f 6769 7420        mlp_logit 
+000223a0: 3d20 462e 6465 7065 6e64 286d 6c70 5f6c  = F.depend(mlp_l
+000223b0: 6f67 6974 2c20 7661 6c75 655f 7570 6461  ogit, value_upda
+000223c0: 7465 290a 2020 2020 2020 2020 6d6c 705f  te).        mlp_
+000223d0: 6c6f 6769 7420 3d20 462e 6465 7065 6e64  logit = F.depend
+000223e0: 286d 6c70 5f6c 6f67 6974 2c20 6b65 795f  (mlp_logit, key_
+000223f0: 7570 6461 7465 290a 0a20 2020 2020 2020  update)..       
+00022400: 2023 2069 6620 7368 6170 6520 6973 2033   # if shape is 3
+00022410: 642c 2077 6520 7265 7368 6170 6520 7468  d, we reshape th
+00022420: 6520 696e 7075 7473 206f 6620 7468 6520  e inputs of the 
+00022430: 6164 640a 2020 2020 2020 2020 6966 206c  add.        if l
+00022440: 656e 2868 6964 6465 6e5f 7368 6170 6529  en(hidden_shape)
+00022450: 203d 3d20 333a 0a20 2020 2020 2020 2020   == 3:.         
+00022460: 2020 206f 7574 7075 745f 7820 3d20 502e     output_x = P.
+00022470: 5265 7368 6170 6528 2928 6f75 7470 7574  Reshape()(output
+00022480: 5f78 2c20 6869 6464 656e 5f73 6861 7065  _x, hidden_shape
+00022490: 290a 2020 2020 2020 2020 2020 2020 6d6c  ).            ml
+000224a0: 705f 6c6f 6769 7420 3d20 502e 5265 7368  p_logit = P.Resh
+000224b0: 6170 6528 2928 6d6c 705f 6c6f 6769 742c  ape()(mlp_logit,
+000224c0: 2068 6964 6465 6e5f 7368 6170 6529 0a20   hidden_shape). 
+000224d0: 2020 2020 2020 2020 2020 2078 203d 2050             x = P
+000224e0: 2e52 6573 6861 7065 2829 2878 2c20 6869  .Reshape()(x, hi
+000224f0: 6464 656e 5f73 6861 7065 290a 0a20 2020  dden_shape)..   
+00022500: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+00022510: 2e70 6f73 745f 6c61 7965 726e 6f72 6d5f  .post_layernorm_
+00022520: 7265 7369 6475 616c 3a0a 2020 2020 2020  residual:.      
+00022530: 2020 2020 2020 2020 2020 6f75 7470 7574            output
+00022540: 203d 2073 656c 662e 6164 645f 3364 286f   = self.add_3d(o
+00022550: 7574 7075 745f 782c 206d 6c70 5f6c 6f67  utput_x, mlp_log
+00022560: 6974 290a 2020 2020 2020 2020 2020 2020  it).            
+00022570: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00022580: 2020 2020 2020 6f75 7470 7574 203d 2073        output = s
+00022590: 656c 662e 6164 645f 3364 2878 2c20 6d6c  elf.add_3d(x, ml
+000225a0: 705f 6c6f 6769 7429 0a20 2020 2020 2020  p_logit).       
+000225b0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+000225c0: 2020 2069 6620 7365 6c66 2e70 6f73 745f     if self.post_
+000225d0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+000225e0: 616c 3a0a 2020 2020 2020 2020 2020 2020  al:.            
+000225f0: 2020 2020 6f75 7470 7574 203d 2073 656c      output = sel
+00022600: 662e 6164 6428 6f75 7470 7574 5f78 2c20  f.add(output_x, 
+00022610: 6d6c 705f 6c6f 6769 7429 0a20 2020 2020  mlp_logit).     
+00022620: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00022630: 2020 2020 2020 2020 2020 2020 206f 7574               out
+00022640: 7075 7420 3d20 7365 6c66 2e61 6464 2878  put = self.add(x
+00022650: 2c20 6d6c 705f 6c6f 6769 7429 0a20 2020  , mlp_logit).   
+00022660: 2020 2020 2020 2020 206f 7574 7075 7420           output 
+00022670: 3d20 462e 7265 7368 6170 6528 6f75 7470  = F.reshape(outp
+00022680: 7574 2c20 6869 6464 656e 5f73 6861 7065  ut, hidden_shape
+00022690: 290a 0a20 2020 2020 2020 2069 6620 7365  )..        if se
+000226a0: 6c66 2e75 7365 5f6d 6f65 3a0a 2020 2020  lf.use_moe:.    
+000226b0: 2020 2020 2020 2020 7265 7475 726e 206f          return o
+000226c0: 7574 7075 742c 206c 6179 6572 5f70 7265  utput, layer_pre
+000226d0: 7365 6e74 2c20 6175 785f 6c6f 7373 0a20  sent, aux_loss. 
+000226e0: 2020 2020 2020 2072 6574 7572 6e20 6f75         return ou
+000226f0: 7470 7574 2c20 6c61 7965 725f 7072 6573  tput, layer_pres
+00022700: 656e 740a 0a20 2020 2064 6566 205f 6368  ent..    def _ch
+00022710: 6563 6b5f 696e 7075 7428 7365 6c66 2c20  eck_input(self, 
+00022720: 6869 6464 656e 5f73 7461 7465 732c 2061  hidden_states, a
+00022730: 7474 656e 7469 6f6e 5f6d 6173 6b2c 2065  ttention_mask, e
+00022740: 6e63 6f64 6572 5f6f 7574 7075 742c 206d  ncoder_output, m
+00022750: 656d 6f72 795f 6d61 736b 2c20 696e 6974  emory_mask, init
+00022760: 5f72 6573 6574 2c20 6261 7463 685f 7661  _reset, batch_va
+00022770: 6c69 645f 6c65 6e67 7468 293a 0a20 2020  lid_length):.   
+00022780: 2020 2020 2072 2222 2243 6865 636b 2069       r"""Check i
+00022790: 6e70 7574 7322 2222 0a20 2020 2020 2020  nputs""".       
+000227a0: 205f 6368 6563 6b5f 696e 7075 745f 6474   _check_input_dt
+000227b0: 7970 6528 462e 6474 7970 6528 6869 6464  ype(F.dtype(hidd
+000227c0: 656e 5f73 7461 7465 7329 2c20 2268 6964  en_states), "hid
+000227d0: 6465 6e5f 7374 6174 6573 222c 0a20 2020  den_states",.   
+000227e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000227f0: 2020 2020 2020 2020 5b6d 7374 7970 652e          [mstype.
+00022800: 666c 6f61 7433 322c 206d 7374 7970 652e  float32, mstype.
+00022810: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+00022820: 6266 6c6f 6174 3136 5d2c 2073 656c 662e  bfloat16], self.
+00022830: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+00022840: 2020 6966 2061 7474 656e 7469 6f6e 5f6d    if attention_m
+00022850: 6173 6b20 6973 206e 6f74 204e 6f6e 653a  ask is not None:
+00022860: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+00022870: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
+00022880: 462e 6474 7970 6528 6174 7465 6e74 696f  F.dtype(attentio
+00022890: 6e5f 6d61 736b 292c 2022 6174 7465 6e74  n_mask), "attent
+000228a0: 696f 6e5f 6d61 736b 222c 0a20 2020 2020  ion_mask",.     
+000228b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000228c0: 2020 2020 2020 2020 2020 5b6d 7374 7970            [mstyp
+000228d0: 652e 666c 6f61 7433 322c 206d 7374 7970  e.float32, mstyp
+000228e0: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
+000228f0: 652e 6266 6c6f 6174 3136 5d2c 0a20 2020  e.bfloat16],.   
+00022900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022910: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00022920: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
+00022930: 2020 2069 6620 656e 636f 6465 725f 6f75     if encoder_ou
+00022940: 7470 7574 2069 7320 6e6f 7420 4e6f 6e65  tput is not None
+00022950: 3a0a 2020 2020 2020 2020 2020 2020 5f63  :.            _c
+00022960: 6865 636b 5f69 6e70 7574 5f64 7479 7065  heck_input_dtype
+00022970: 2846 2e64 7479 7065 2865 6e63 6f64 6572  (F.dtype(encoder
+00022980: 5f6f 7574 7075 7429 2c20 2265 6e63 6f64  _output), "encod
+00022990: 6572 5f6f 7574 7075 7422 2c0a 2020 2020  er_output",.    
+000229a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000229b0: 2020 2020 2020 2020 2020 205b 6d73 7479             [msty
+000229c0: 7065 2e66 6c6f 6174 3332 2c20 6d73 7479  pe.float32, msty
+000229d0: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
+000229e0: 7065 2e62 666c 6f61 7431 365d 2c20 7365  pe.bfloat16], se
+000229f0: 6c66 2e63 6c73 5f6e 616d 6529 0a20 2020  lf.cls_name).   
+00022a00: 2020 2020 2069 6620 6d65 6d6f 7279 5f6d       if memory_m
+00022a10: 6173 6b20 6973 206e 6f74 204e 6f6e 653a  ask is not None:
+00022a20: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+00022a30: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
+00022a40: 462e 6474 7970 6528 6d65 6d6f 7279 5f6d  F.dtype(memory_m
+00022a50: 6173 6b29 2c20 226d 656d 6f72 795f 6d61  ask), "memory_ma
+00022a60: 736b 222c 0a20 2020 2020 2020 2020 2020  sk",.           
+00022a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022a80: 2020 2020 5b6d 7374 7970 652e 666c 6f61      [mstype.floa
+00022a90: 7433 322c 206d 7374 7970 652e 666c 6f61  t32, mstype.floa
+00022aa0: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
+00022ab0: 6174 3136 5d2c 2073 656c 662e 636c 735f  at16], self.cls_
+00022ac0: 6e61 6d65 290a 0a20 2020 2020 2020 2069  name)..        i
+00022ad0: 6e69 745f 7265 7365 745f 6973 5f74 656e  nit_reset_is_ten
+00022ae0: 736f 7220 3d20 6973 696e 7374 616e 6365  sor = isinstance
+00022af0: 2869 6e69 745f 7265 7365 742c 2054 656e  (init_reset, Ten
+00022b00: 736f 7229 0a20 2020 2020 2020 2069 6e69  sor).        ini
+00022b10: 745f 7265 7365 745f 6973 5f64 6566 6175  t_reset_is_defau
+00022b20: 6c74 203d 2069 6e69 745f 7265 7365 7420  lt = init_reset 
+00022b30: 6973 2054 7275 650a 2020 2020 2020 2020  is True.        
+00022b40: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+00022b50: 7468 5f69 735f 7465 6e73 6f72 203d 2069  th_is_tensor = i
+00022b60: 7369 6e73 7461 6e63 6528 6261 7463 685f  sinstance(batch_
+00022b70: 7661 6c69 645f 6c65 6e67 7468 2c20 5465  valid_length, Te
+00022b80: 6e73 6f72 290a 2020 2020 2020 2020 6261  nsor).        ba
+00022b90: 7463 685f 6973 5f64 6566 6175 6c74 203d  tch_is_default =
+00022ba0: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
+00022bb0: 6774 6820 6973 204e 6f6e 650a 2020 2020  gth is None.    
+00022bc0: 2020 2020 5f63 6865 636b 5f70 6173 745f      _check_past_
+00022bd0: 6e6f 6e65 5f69 6e70 7574 5f6e 6f6e 6528  none_input_none(
+00022be0: 7365 6c66 2e75 7365 5f70 6173 742c 2022  self.use_past, "
+00022bf0: 696e 6974 5f72 6573 6574 222c 2073 656c  init_reset", sel
+00022c00: 662e 636c 735f 6e61 6d65 2c20 5472 7565  f.cls_name, True
+00022c10: 2c20 696e 6974 5f72 6573 6574 5f69 735f  , init_reset_is_
+00022c20: 7465 6e73 6f72 2c0a 2020 2020 2020 2020  tensor,.        
+00022c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022c40: 2020 2020 2020 2020 2020 2020 696e 6974              init
+00022c50: 5f72 6573 6574 5f69 735f 6465 6661 756c  _reset_is_defaul
+00022c60: 7429 0a20 2020 2020 2020 205f 6368 6563  t).        _chec
+00022c70: 6b5f 7061 7374 5f6e 6f6e 655f 696e 7075  k_past_none_inpu
+00022c80: 745f 6e6f 6e65 2873 656c 662e 7573 655f  t_none(self.use_
+00022c90: 7061 7374 2c20 2262 6174 6368 5f76 616c  past, "batch_val
+00022ca0: 6964 5f6c 656e 6774 6822 2c20 7365 6c66  id_length", self
+00022cb0: 2e63 6c73 5f6e 616d 652c 204e 6f6e 652c  .cls_name, None,
+00022cc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00022cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022ce0: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
+00022cf0: 5f6c 656e 6774 685f 6973 5f74 656e 736f  _length_is_tenso
+00022d00: 722c 2062 6174 6368 5f69 735f 6465 6661  r, batch_is_defa
+00022d10: 756c 7429 0a0a 2020 2020 2020 2020 6966  ult)..        if
+00022d20: 2073 656c 662e 7573 655f 7061 7374 3a0a   self.use_past:.
+00022d30: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+00022d40: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+00022d50: 2e64 7479 7065 2869 6e69 745f 7265 7365  .dtype(init_rese
+00022d60: 7429 2c20 2269 6e69 745f 7265 7365 7422  t), "init_reset"
+00022d70: 2c20 5b6d 7374 7970 652e 626f 6f6c 5f5d  , [mstype.bool_]
+00022d80: 2c20 7365 6c66 2e63 6c73 5f6e 616d 6529  , self.cls_name)
+00022d90: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+00022da0: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
+00022db0: 462e 6474 7970 6528 6261 7463 685f 7661  F.dtype(batch_va
+00022dc0: 6c69 645f 6c65 6e67 7468 292c 2022 6261  lid_length), "ba
+00022dd0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+00022de0: 222c 205b 6d73 7479 7065 2e69 6e74 3332  ", [mstype.int32
+00022df0: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
+00022e00: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+00022e10: 2054 7275 650a 0a0a 6465 6620 5f67 6574   True...def _get
+00022e20: 5f6c 616d 6264 615f 6675 6e63 2874 6f74  _lambda_func(tot
+00022e30: 616c 5f6c 6179 6572 3d4e 6f6e 6529 3a0a  al_layer=None):.
+00022e40: 2020 2020 7222 2222 0a20 2020 2041 2077      r""".    A w
+00022e50: 7261 7070 6572 2066 756e 6374 696f 6e20  rapper function 
+00022e60: 6f66 2073 7065 6369 6679 696e 6720 7069  of specifying pi
+00022e70: 7065 6c69 6e65 2073 7461 6765 2061 6e64  peline stage and
+00022e80: 2067 7261 6469 656e 7420 6167 6772 6567   gradient aggreg
+00022e90: 6174 696f 6e20 6675 7369 6f6e 2e20 4966  ation fusion. If
+00022ea0: 2074 6865 2074 6f74 616c 206c 6179 6572   the total layer
+00022eb0: 0a20 2020 2069 7320 6e6f 7420 4e6f 6e65  .    is not None
+00022ec0: 2c20 666f 7220 6578 616d 706c 652c 2073  , for example, s
+00022ed0: 6574 2069 6e20 7468 6520 7472 616e 7366  et in the transf
+00022ee0: 6f72 6d65 7220 6d6f 6465 6c2c 2074 6865  ormer model, the
+00022ef0: 2070 6970 656c 696e 6520 7374 6167 6520   pipeline stage 
+00022f00: 7365 7474 696e 6720 6675 6e63 7469 6f6e  setting function
+00022f10: 2077 696c 6c20 6265 0a20 2020 2060 286c   will be.    `(l
+00022f20: 6179 6572 5f69 6420 2b20 3029 202f 2f20  ayer_id + 0) // 
+00022f30: 2874 6f74 616c 5f6c 6179 6572 7320 2f20  (total_layers / 
+00022f40: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00022f50: 7069 7065 6c69 6e65 5f73 7461 6765 2960  pipeline_stage)`
+00022f60: 2066 6f72 2074 6865 2065 6e63 6f64 6572   for the encoder
+00022f70: 2061 6e64 2c0a 2020 2020 6028 6c61 7965   and,.    `(laye
+00022f80: 725f 6964 202b 206f 6666 7365 7429 202f  r_id + offset) /
+00022f90: 2f0a 2020 2020 2874 6f74 616c 5f6c 6179  /.    (total_lay
+00022fa0: 6572 7320 2f20 7061 7261 6c6c 656c 5f63  ers / parallel_c
+00022fb0: 6f6e 6669 672e 7069 7065 6c69 6e65 5f73  onfig.pipeline_s
+00022fc0: 7461 6765 2960 2066 6f72 2074 6865 2064  tage)` for the d
+00022fd0: 6563 6f64 6572 2c20 7768 6572 6520 606f  ecoder, where `o
+00022fe0: 6666 7365 7460 2069 7320 7468 6520 6c61  ffset` is the la
+00022ff0: 7965 7273 2069 6e20 7468 6520 656e 636f  yers in the enco
+00023000: 6465 722e 0a20 2020 2022 2222 0a0a 2020  der..    """..  
+00023010: 2020 6465 6620 5f73 6574 5f70 6172 616c    def _set_paral
+00023020: 6c65 6c5f 636f 6e66 6967 7572 655f 666f  lel_configure_fo
+00023030: 725f 6c61 7965 7228 6e65 7477 6f72 6b2c  r_layer(network,
+00023040: 206c 6179 6572 5f69 642c 206f 6666 7365   layer_id, offse
+00023050: 742c 2070 6172 616c 6c65 6c5f 636f 6e66  t, parallel_conf
+00023060: 6967 2c20 6c61 7965 7273 293a 0a20 2020  ig, layers):.   
+00023070: 2020 2020 2072 2222 220a 2020 2020 2020       r""".      
+00023080: 2020 4465 6661 756c 7420 7365 7474 696e    Default settin
+00023090: 6720 666f 7220 7468 6520 7069 7065 6c69  g for the pipeli
+000230a0: 6e65 2069 733a 2060 286c 6179 6572 5f69  ne is: `(layer_i
+000230b0: 6420 2b20 6f66 6673 6574 2920 2f2f 2028  d + offset) // (
+000230c0: 6c61 7965 7273 202f 2070 6970 656c 696e  layers / pipelin
+000230d0: 655f 7374 6167 6529 602e 0a0a 2020 2020  e_stage)`...    
+000230e0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+000230f0: 2020 2020 2020 6e65 7477 6f72 6b28 4365        network(Ce
+00023100: 6c6c 2920 2d20 5265 7072 6573 656e 7473  ll) - Represents
+00023110: 2074 6865 2074 7261 6e73 666f 726d 6572   the transformer
+00023120: 2062 6c6f 636b 0a20 2020 2020 2020 2020   block.         
+00023130: 2020 206c 6179 6572 5f69 6428 696e 7429     layer_id(int)
+00023140: 202d 204d 6561 6e73 2074 6865 206c 6179   - Means the lay
+00023150: 6572 2069 6e64 6578 2066 6f72 2074 6865  er index for the
+00023160: 2063 7572 7265 6e74 206d 6f64 756c 652c   current module,
+00023170: 2063 6f75 6e74 7320 6672 6f6d 207a 6572   counts from zer
+00023180: 6f2e 0a20 2020 2020 2020 2020 2020 206f  o..            o
+00023190: 6666 7365 7428 696e 7429 202d 204d 6561  ffset(int) - Mea
+000231a0: 6e73 2074 6865 206c 6179 6572 5f69 6e64  ns the layer_ind
+000231b0: 6578 206e 6565 6473 2061 6e20 6f66 6673  ex needs an offs
+000231c0: 6574 2c20 6966 2074 6865 7265 2061 7265  et, if there are
+000231d0: 206f 7468 6572 206d 6f64 756c 6573 2069   other modules i
+000231e0: 6e20 7468 6520 6e65 742e 0a20 2020 2020  n the net..     
+000231f0: 2020 2020 2020 206c 6179 6572 7328 696e         layers(in
+00023200: 7429 202d 2054 6865 2074 6f74 616c 206c  t) - The total l
+00023210: 6179 6572 7320 7573 6564 2066 6f72 2074  ayers used for t
+00023220: 6865 206d 6f64 656c 2e0a 2020 2020 2020  he model..      
+00023230: 2020 2222 220a 2020 2020 2020 2020 2320    """.        # 
+00023240: 6f76 6572 7269 6465 2074 6865 206c 6179  override the lay
+00023250: 6572 730a 2020 2020 2020 2020 6966 2074  ers.        if t
+00023260: 6f74 616c 5f6c 6179 6572 3a0a 2020 2020  otal_layer:.    
+00023270: 2020 2020 2020 2020 6c61 7965 7273 203d          layers =
+00023280: 2074 6f74 616c 5f6c 6179 6572 0a20 2020   total_layer.   
+00023290: 2020 2020 2023 2055 7365 6420 666f 7220       # Used for 
+000232a0: 7468 6520 7069 7065 6c69 6e65 2773 2073  the pipeline's s
+000232b0: 7461 6765 7320 7365 7474 696e 670a 2020  tages setting.  
+000232c0: 2020 2020 2020 6966 206c 6179 6572 7320        if layers 
+000232d0: 3c20 7061 7261 6c6c 656c 5f63 6f6e 6669  < parallel_confi
+000232e0: 672e 7069 7065 6c69 6e65 5f73 7461 6765  g.pipeline_stage
+000232f0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
+00023300: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
+00023310: 226c 6179 6572 7320 7b6c 6179 6572 737d  "layers {layers}
+00023320: 206d 7573 7420 6265 206c 6172 6765 7220   must be larger 
+00023330: 7468 616e 2070 6970 656c 696e 6520 7374  than pipeline st
+00023340: 6167 6520 7b70 6172 616c 6c65 6c5f 636f  age {parallel_co
+00023350: 6e66 6967 2e70 6970 656c 696e 655f 7374  nfig.pipeline_st
+00023360: 6167 657d 2229 0a0a 2020 2020 2020 2020  age}")..        
+00023370: 7070 5f64 6973 203d 206d 6178 286c 6179  pp_dis = max(lay
+00023380: 6572 7320 2f2f 2070 6172 616c 6c65 6c5f  ers // parallel_
+00023390: 636f 6e66 6967 2e70 6970 656c 696e 655f  config.pipeline_
+000233a0: 7374 6167 652c 2031 290a 2020 2020 2020  stage, 1).      
+000233b0: 2020 2320 7468 6520 7069 7065 6c69 6e65    # the pipeline
+000233c0: 2073 7461 6765 206d 7573 7420 6265 2069   stage must be i
+000233d0: 6e20 5b30 2c20 7061 7261 6c6c 656c 5f63  n [0, parallel_c
+000233e0: 6f6e 6669 672e 7069 7065 6c69 6e65 5f73  onfig.pipeline_s
+000233f0: 7461 6765 202d 2031 5d0a 2020 2020 2020  tage - 1].      
+00023400: 2020 7070 5f69 6420 3d20 6d69 6e28 286c    pp_id = min((l
+00023410: 6179 6572 5f69 6420 2b20 6f66 6673 6574  ayer_id + offset
+00023420: 2920 2f2f 2070 705f 6469 732c 2070 6172  ) // pp_dis, par
+00023430: 616c 6c65 6c5f 636f 6e66 6967 2e70 6970  allel_config.pip
+00023440: 656c 696e 655f 7374 6167 6520 2d20 3129  eline_stage - 1)
+00023450: 0a20 2020 2020 2020 206e 6574 776f 726b  .        network
+00023460: 2e70 6970 656c 696e 655f 7374 6167 6520  .pipeline_stage 
+00023470: 3d20 7070 5f69 640a 0a20 2020 2020 2020  = pp_id..       
+00023480: 2023 2055 7365 6420 666f 7220 6f70 7469   # Used for opti
+00023490: 6d69 7a65 7227 7320 6675 7369 6f6e 2074  mizer's fusion t
+000234a0: 6167 0a20 2020 2020 2020 2064 6973 203d  ag.        dis =
+000234b0: 206d 6178 286c 6179 6572 7320 2f2f 2070   max(layers // p
+000234c0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e67  arallel_config.g
+000234d0: 7261 6469 656e 745f 6167 6772 6567 6174  radient_aggregat
+000234e0: 696f 6e5f 6772 6f75 702c 2031 290a 2020  ion_group, 1).  
+000234f0: 2020 2020 2020 6e65 7477 6f72 6b2e 7365        network.se
+00023500: 745f 636f 6d6d 5f66 7573 696f 6e28 286c  t_comm_fusion((l
+00023510: 6179 6572 5f69 6420 2b20 6f66 6673 6574  ayer_id + offset
+00023520: 2920 2f2f 2064 6973 202b 2031 290a 2020  ) // dis + 1).  
+00023530: 2020 2020 2020 2320 5573 6564 2066 6f72        # Used for
+00023540: 2065 6e61 626c 696e 6720 7265 636f 6d70   enabling recomp
+00023550: 7574 6174 696f 6e20 6f66 2074 6865 2062  utation of the b
+00023560: 6c6f 636b 0a20 2020 2020 2020 2069 6620  lock.        if 
+00023570: 6973 696e 7374 616e 6365 2870 6172 616c  isinstance(paral
+00023580: 6c65 6c5f 636f 6e66 6967 2e72 6563 6f6d  lel_config.recom
+00023590: 7075 7465 2c20 626f 6f6c 293a 0a20 2020  pute, bool):.   
+000235a0: 2020 2020 2020 2020 2069 6620 7061 7261           if para
+000235b0: 6c6c 656c 5f63 6f6e 6669 672e 7265 636f  llel_config.reco
+000235c0: 6d70 7574 6520 616e 6420 6e6f 7420 7061  mpute and not pa
+000235d0: 7261 6c6c 656c 5f63 6f6e 6669 672e 7365  rallel_config.se
+000235e0: 6c65 6374 5f72 6563 6f6d 7075 7465 3a0a  lect_recompute:.
+000235f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023600: 6e65 7477 6f72 6b2e 7265 636f 6d70 7574  network.recomput
+00023610: 6528 290a 2020 2020 2020 2020 656c 7365  e().        else
+00023620: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
+00023630: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00023640: 2e72 6563 6f6d 7075 7465 2e72 6563 6f6d  .recompute.recom
+00023650: 7075 7465 2061 6e64 206e 6f74 2070 6172  pute and not par
+00023660: 616c 6c65 6c5f 636f 6e66 6967 2e72 6563  allel_config.rec
+00023670: 6f6d 7075 7465 2e73 656c 6563 745f 7265  ompute.select_re
+00023680: 636f 6d70 7574 653a 0a20 2020 2020 2020  compute:.       
+00023690: 2020 2020 2020 2020 2070 6172 616c 656c           paralel
+000236a0: 5f6f 705f 636f 6d6d 5f63 6f6d 7075 7465  _op_comm_compute
+000236b0: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
+000236c0: 6967 2e72 6563 6f6d 7075 7465 2e70 6172  ig.recompute.par
+000236d0: 616c 6c65 6c5f 6f70 7469 6d69 7a65 725f  allel_optimizer_
+000236e0: 636f 6d6d 5f72 6563 6f6d 7075 7465 0a20  comm_recompute. 
+000236f0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+00023700: 6574 776f 726b 2e72 6563 6f6d 7075 7465  etwork.recompute
+00023710: 2870 6172 616c 6c65 6c5f 6f70 7469 6d69  (parallel_optimi
+00023720: 7a65 725f 636f 6d6d 5f72 6563 6f6d 7075  zer_comm_recompu
+00023730: 7465 3d70 6172 616c 656c 5f6f 705f 636f  te=paralel_op_co
+00023740: 6d6d 5f63 6f6d 7075 7465 2c0a 2020 2020  mm_compute,.    
+00023750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023760: 2020 2020 2020 2020 2020 2020 2020 6d70                mp
+00023770: 5f63 6f6d 6d5f 7265 636f 6d70 7574 653d  _comm_recompute=
+00023780: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023790: 7265 636f 6d70 7574 652e 6d70 5f63 6f6d  recompute.mp_com
+000237a0: 6d5f 7265 636f 6d70 7574 652c 0a20 2020  m_recompute,.   
+000237b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000237c0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+000237d0: 6563 6f6d 7075 7465 5f73 6c69 6365 5f61  ecompute_slice_a
+000237e0: 6374 6976 6174 696f 6e3d 7061 7261 6c6c  ctivation=parall
+000237f0: 656c 5f63 6f6e 6669 672e 7265 636f 6d70  el_config.recomp
+00023800: 7574 652e 7265 636f 6d70 7574 655f 736c  ute.recompute_sl
+00023810: 6963 655f 6163 7469 7661 7469 6f6e 290a  ice_activation).
+00023820: 0a20 2020 2072 6574 7572 6e20 5f73 6574  .    return _set
+00023830: 5f70 6172 616c 6c65 6c5f 636f 6e66 6967  _parallel_config
+00023840: 7572 655f 666f 725f 6c61 7965 720a 0a0a  ure_for_layer...
+00023850: 636c 6173 7320 5472 616e 7366 6f72 6d65  class Transforme
+00023860: 7245 6e63 6f64 6572 2843 656c 6c29 3a0a  rEncoder(Cell):.
+00023870: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
+00023880: 2054 7261 6e73 666f 726d 6572 2045 6e63   Transformer Enc
+00023890: 6f64 6572 206d 6f64 756c 6520 7769 7468  oder module with
+000238a0: 206d 756c 7469 2d6c 6179 6572 2073 7461   multi-layer sta
+000238b0: 636b 6564 206f 6620 6054 7261 6e73 666f  cked of `Transfo
+000238c0: 726d 6572 456e 636f 6465 724c 6179 6572  rmerEncoderLayer
+000238d0: 602c 2069 6e63 6c75 6469 6e67 206d 756c  `, including mul
+000238e0: 7469 6865 6164 2073 656c 660a 2020 2020  tihead self.    
+000238f0: 2020 2020 6174 7465 6e74 696f 6e20 616e      attention an
+00023900: 6420 6665 6564 666f 7277 6172 6420 6c61  d feedforward la
+00023910: 7965 722e 0a0a 2020 2020 2020 2020 4172  yer...        Ar
+00023920: 6773 3a0a 2020 2020 2020 2020 2020 2020  gs:.            
+00023930: 6261 7463 685f 7369 7a65 2869 6e74 293a  batch_size(int):
+00023940: 2054 6865 2062 6174 6368 2073 697a 6520   The batch size 
+00023950: 6f66 2074 6865 2069 6e70 7574 2074 656e  of the input ten
+00023960: 736f 7220 7768 656e 2064 6f20 696e 6372  sor when do incr
+00023970: 656e 6d65 6e74 616c 2070 7265 6469 6374  enmental predict
+00023980: 696f 6e2e 2053 686f 756c 6420 6265 2061  ion. Should be a
+00023990: 2070 6f73 6974 6976 650a 2020 2020 2020   positive.      
+000239a0: 2020 2020 2020 2020 2020 7661 6c75 652e            value.
+000239b0: 2057 6865 6e20 646f 2074 7261 696e 696e   When do trainin
+000239c0: 6720 6f72 2070 7265 6469 6374 696f 6e2c  g or prediction,
+000239d0: 2074 6865 2061 7267 756d 656e 7420 7769   the argument wi
+000239e0: 6c6c 206e 6f74 2077 6f72 6b20 616e 6420  ll not work and 
+000239f0: 7468 6520 7573 6572 2063 616e 206a 7573  the user can jus
+00023a00: 7420 7061 7373 204e 6f6e 6520 746f 0a20  t pass None to. 
+00023a10: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+00023a20: 6865 2061 7267 756d 656e 742e 0a20 2020  he argument..   
+00023a30: 2020 2020 2020 2020 206e 756d 5f6c 6179           num_lay
+00023a40: 6572 7328 696e 7429 3a20 5468 6520 6c61  ers(int): The la
+00023a50: 7965 7273 206f 6620 7468 6520 6054 7261  yers of the `Tra
+00023a60: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
+00023a70: 6179 6572 600a 2020 2020 2020 2020 2020  ayer`.          
+00023a80: 2020 6869 6464 656e 5f73 697a 6528 696e    hidden_size(in
+00023a90: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+00023aa0: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
+00023ab0: 2e0a 2020 2020 2020 2020 2020 2020 6666  ..            ff
+00023ac0: 6e5f 6869 6464 656e 5f73 697a 6528 696e  n_hidden_size(in
+00023ad0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+00023ae0: 697a 6520 6f66 2062 6f74 746c 656e 6563  ize of bottlenec
+00023af0: 6b20 696e 2074 6865 2066 6565 6466 6f72  k in the feedfor
+00023b00: 7761 7264 206c 6179 6572 2e0a 2020 2020  ward layer..    
+00023b10: 2020 2020 2020 2020 7365 715f 6c65 6e67          seq_leng
+00023b20: 7468 2869 6e74 293a 2054 6865 2073 6571  th(int): The seq
+00023b30: 5f6c 656e 6774 6820 6f66 2074 6865 2069  _length of the i
+00023b40: 6e70 7574 2074 656e 736f 722e 0a20 2020  nput tensor..   
+00023b50: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+00023b60: 6473 2869 6e74 293a 2054 6865 206e 756d  ds(int): The num
+00023b70: 6265 7220 6f66 2074 6865 2068 6561 6473  ber of the heads
+00023b80: 2e0a 2020 2020 2020 2020 2020 2020 6174  ..            at
+00023b90: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+00023ba0: 7261 7465 2866 6c6f 6174 293a 2054 6865  rate(float): The
+00023bb0: 2064 726f 706f 7574 2072 6174 6520 6f66   dropout rate of
+00023bc0: 2074 6865 2061 7474 656e 7469 6f6e 2073   the attention s
+00023bd0: 636f 7265 732e 2044 6566 6175 6c74 3a30  cores. Default:0
+00023be0: 2e31 2e0a 2020 2020 2020 2020 2020 2020  .1..            
+00023bf0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00023c00: 6174 6528 666c 6f61 7429 3a20 5468 6520  ate(float): The 
+00023c10: 6472 6f70 6f75 7420 7261 7465 206f 6620  dropout rate of 
+00023c20: 7468 6520 6669 6e61 6c20 6f75 7470 7574  the final output
+00023c30: 206f 6620 7468 6520 6c61 7965 722e 2044   of the layer. D
+00023c40: 6566 6175 6c74 3a20 302e 312e 0a20 2020  efault: 0.1..   
+00023c50: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00023c60: 6163 7420 2873 7472 2c20 6e6e 2e43 656c  act (str, nn.Cel
+00023c70: 6c29 3a20 5468 6520 6163 7469 7661 7469  l): The activati
+00023c80: 6f6e 206f 6620 7468 6520 696e 7465 726e  on of the intern
+00023c90: 616c 2066 6565 6466 6f72 7761 7264 206c  al feedforward l
+00023ca0: 6179 6572 2e20 5375 7070 6f72 7473 2027  ayer. Supports '
+00023cb0: 7265 6c75 272c 0a20 2020 2020 2020 2020  relu',.         
+00023cc0: 2020 2020 2020 2027 7265 6c75 3627 2c20         'relu6', 
+00023cd0: 2774 616e 6827 2c20 2767 656c 7527 2c20  'tanh', 'gelu', 
+00023ce0: 2766 6173 745f 6765 6c75 272c 2027 656c  'fast_gelu', 'el
+00023cf0: 7527 2c20 2773 6967 6d6f 6964 272c 2027  u', 'sigmoid', '
+00023d00: 7072 656c 7527 2c20 276c 6561 6b79 7265  prelu', 'leakyre
+00023d10: 6c75 272c 2027 6873 7769 7368 272c 0a20  lu', 'hswish',. 
+00023d20: 2020 2020 2020 2020 2020 2020 2020 2027                 '
+00023d30: 6873 6967 6d6f 6964 272c 2027 6c6f 6773  hsigmoid', 'logs
+00023d40: 6967 6d6f 6964 2720 616e 6420 736f 206f  igmoid' and so o
+00023d50: 6e2e 2055 7365 7220 6361 6e20 7072 6f76  n. User can prov
+00023d60: 6964 6520 6375 7374 6f6d 2061 6374 6976  ide custom activ
+00023d70: 6974 696f 6e20 746f 2074 6865 2061 7267  ition to the arg
+00023d80: 756d 656e 742e 0a20 2020 2020 2020 2020  ument..         
+00023d90: 2020 2020 2020 2049 6620 7573 6572 2077         If user w
+00023da0: 616e 7473 2074 6f20 7275 6e20 7468 6520  ants to run the 
+00023db0: 6e65 7420 696e 2074 6865 2070 6172 616c  net in the paral
+00023dc0: 6c65 6c20 6d6f 6465 2c20 7468 6520 6375  lel mode, the cu
+00023dd0: 7374 6f6d 2061 6374 6976 6174 696f 6e20  stom activation 
+00023de0: 6d75 7374 2061 6c73 6f20 7072 6f76 6964  must also provid
+00023df0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00023e00: 2020 7468 6520 6061 6374 6976 6174 696f    the `activatio
+00023e10: 6e5f 7368 6172 6460 2066 756e 6374 696f  n_shard` functio
+00023e20: 6e2e 2050 6c65 6173 6520 7365 6520 7468  n. Please see th
+00023e30: 6520 6578 616d 706c 6573 206f 6620 7468  e examples of th
+00023e40: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00023e50: 2020 636c 6173 733a 606d 696e 6466 6f72    class:`mindfor
+00023e60: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
+00023e70: 6e73 666f 726d 6572 2e46 6565 6446 6f72  nsformer.FeedFor
+00023e80: 7761 7264 602e 2044 6566 6175 6c74 3a20  ward`. Default: 
+00023e90: 6765 6c75 2e0a 2020 2020 2020 2020 2020  gelu..          
+00023ea0: 2020 706f 7374 5f6c 6179 6572 6e6f 726d    post_layernorm
+00023eb0: 5f72 6573 6964 7561 6c28 626f 6f6c 293a  _residual(bool):
+00023ec0: 2044 6f20 7265 7369 6475 616c 7320 6164   Do residuals ad
+00023ed0: 6473 2062 6566 6f72 6520 7468 6520 6c61  ds before the la
+00023ee0: 7965 726e 6f72 6d2e 2044 6566 6175 6c74  yernorm. Default
+00023ef0: 2046 616c 7365 2e0a 2020 2020 2020 2020   False..        
+00023f00: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
+00023f10: 6d70 7574 655f 7479 7065 2864 7479 7065  mpute_type(dtype
+00023f20: 2e4e 756d 6265 7229 3a20 5468 6520 636f  .Number): The co
+00023f30: 6d70 7574 6174 696f 6e20 7479 7065 206f  mputation type o
+00023f40: 6620 7468 6520 6c61 7965 726e 6f72 6d2e  f the layernorm.
+00023f50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00023f60: 2053 686f 756c 6420 6265 206d 7374 7970   Should be mstyp
+00023f70: 652e 666c 6f61 7433 3220 6f72 206d 7374  e.float32 or mst
+00023f80: 7970 652e 666c 6f61 7431 362e 2044 6566  ype.float16. Def
+00023f90: 6175 6c74 206d 7374 7970 652e 666c 6f61  ault mstype.floa
+00023fa0: 7433 322e 0a20 2020 2020 2020 2020 2020  t32..           
+00023fb0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+00023fc0: 5f74 7970 6528 6474 7970 652e 4e75 6d62  _type(dtype.Numb
+00023fd0: 6572 293a 2054 6865 2063 6f6d 7075 7461  er): The computa
+00023fe0: 7469 6f6e 2074 7970 6520 6f66 2074 6865  tion type of the
+00023ff0: 2073 6f66 746d 6178 2069 6e20 7468 6520   softmax in the 
+00024000: 6174 7465 6e74 696f 6e2e 0a20 2020 2020  attention..     
+00024010: 2020 2020 2020 2020 2020 2053 686f 756c             Shoul
+00024020: 6420 6265 206d 7374 7970 652e 666c 6f61  d be mstype.floa
+00024030: 7433 3220 6f72 206d 7374 7970 652e 666c  t32 or mstype.fl
+00024040: 6f61 7431 362e 2044 6566 6175 6c74 3a20  oat16. Default: 
+00024050: 6d73 7479 7065 2e66 6c6f 6174 3332 2e0a  mstype.float32..
+00024060: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00024070: 6d5f 696e 6974 5f74 7970 6528 6474 7970  m_init_type(dtyp
+00024080: 652e 4e75 6d62 6572 293a 2054 6865 2070  e.Number): The p
+00024090: 6172 616d 6574 6572 2069 6e69 7469 616c  arameter initial
+000240a0: 697a 6174 696f 6e20 7479 7065 206f 6620  ization type of 
+000240b0: 7468 6520 6d6f 6475 6c65 2e0a 2020 2020  the module..    
+000240c0: 2020 2020 2020 2020 2020 2020 5368 6f75              Shou
+000240d0: 6c64 2062 6520 6d73 7479 7065 2e66 6c6f  ld be mstype.flo
+000240e0: 6174 3332 206f 7220 6d73 7479 7065 2e66  at32 or mstype.f
+000240f0: 6c6f 6174 3136 2e20 4465 6661 756c 743a  loat16. Default:
+00024100: 206d 7374 7970 652e 666c 6f61 7433 322e   mstype.float32.
+00024110: 0a20 2020 2020 2020 2020 2020 206c 616d  .            lam
+00024120: 6264 615f 6675 6e63 2866 756e 6374 696f  bda_func(functio
+00024130: 6e29 3a20 4120 6675 6e63 7469 6f6e 2063  n): A function c
+00024140: 616e 2064 6574 6572 6d69 6e65 2074 6865  an determine the
+00024150: 2066 7573 696f 6e20 696e 6465 782c 0a20   fusion index,. 
+00024160: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00024170: 6970 656c 696e 6520 7374 6167 6573 2061  ipeline stages a
+00024180: 6e64 2072 6563 6f6d 7075 7465 2061 7474  nd recompute att
+00024190: 7269 6275 7465 2e20 4966 2074 6865 0a20  ribute. If the. 
+000241a0: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+000241b0: 7365 7220 7761 6e74 7320 746f 2064 6574  ser wants to det
+000241c0: 6572 6d69 6e65 2074 6865 2070 6970 656c  ermine the pipel
+000241d0: 696e 6520 7374 6167 6520 616e 6420 6772  ine stage and gr
+000241e0: 6164 6965 6e74 2061 6767 7265 6761 7469  adient aggregati
+000241f0: 6f6e 2066 7573 696f 6e2c 2074 6865 2075  on fusion, the u
+00024200: 7365 7220 6361 6e20 7061 7373 2061 0a20  ser can pass a. 
+00024210: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+00024220: 756e 6374 696f 6e20 7468 6174 2061 6363  unction that acc
+00024230: 6570 7473 2060 6e65 7477 6f72 6b60 2c20  epts `network`, 
+00024240: 606c 6179 6572 5f69 6460 2c20 606f 6666  `layer_id`, `off
+00024250: 7365 7460 2c20 6070 6172 616c 6c65 6c5f  set`, `parallel_
+00024260: 636f 6e66 6967 602c 2060 6c61 7965 7273  config`, `layers
+00024270: 602e 2054 6865 2060 6e65 7477 6f72 6b28  `. The `network(
+00024280: 4365 6c6c 2960 0a20 2020 2020 2020 2020  Cell)`.         
+00024290: 2020 2020 2020 2072 6570 7265 7365 6e74         represent
+000242a0: 7320 7468 6520 7472 616e 7366 6f72 6d65  s the transforme
+000242b0: 7220 626c 6f63 6b2c 2060 6c61 7965 725f  r block, `layer_
+000242c0: 6964 2869 6e74 2960 206d 6561 6e73 2074  id(int)` means t
+000242d0: 6865 206c 6179 6572 2069 6e64 6578 2066  he layer index f
+000242e0: 6f72 2074 6865 2063 7572 7265 6e74 206d  or the current m
+000242f0: 6f64 756c 652c 2063 6f75 6e74 730a 2020  odule, counts.  
+00024300: 2020 2020 2020 2020 2020 2020 2020 6672                fr
+00024310: 6f6d 207a 6572 6f2c 2060 6f66 6673 6574  om zero, `offset
+00024320: 2869 6e74 2960 206d 6561 6e73 2074 6865  (int)` means the
+00024330: 206c 6179 6572 5f69 6e64 6578 206e 6565   layer_index nee
+00024340: 6473 2061 6e20 6f66 6673 6574 2c20 6966  ds an offset, if
+00024350: 2074 6865 7265 2061 7265 206f 7468 6572   there are other
+00024360: 206d 6f64 756c 6573 2069 6e20 7468 6520   modules in the 
+00024370: 6e65 742e 0a20 2020 2020 2020 2020 2020  net..           
+00024380: 2020 2020 2054 6865 2064 6566 6175 6c74       The default
+00024390: 2073 6574 7469 6e67 2066 6f72 2074 6865   setting for the
+000243a0: 2070 6970 656c 696e 6520 6973 3a20 6028   pipeline is: `(
+000243b0: 6c61 7965 725f 6964 202b 206f 6666 7365  layer_id + offse
+000243c0: 7429 202f 2f20 286c 6179 6572 7320 2f20  t) // (layers / 
+000243d0: 7069 7065 6c69 6e65 5f73 7461 6765 2960  pipeline_stage)`
+000243e0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+000243f0: 2020 4465 6661 756c 743a 204e 6f6e 652e    Default: None.
+00024400: 0a20 2020 2020 2020 2020 2020 206f 6666  .            off
+00024410: 7365 7428 696e 7429 3a20 5468 6520 696e  set(int): The in
+00024420: 6974 6961 6c20 6c61 7965 7220 696e 6465  itial layer inde
+00024430: 7820 666f 7220 7468 6520 6065 6e63 6f64  x for the `encod
+00024440: 6572 602e 2055 7365 6420 666f 7220 7365  er`. Used for se
+00024450: 7474 696e 6720 7468 6520 6675 7369 6f6e  tting the fusion
+00024460: 2069 6420 616e 6420 7374 6167 6520 6964   id and stage id
+00024470: 2c20 746f 206e 6f74 0a20 2020 2020 2020  , to not.       
+00024480: 2020 2020 2020 2020 206f 7665 726c 6170           overlap
+00024490: 2077 6974 6820 7468 6520 656e 636f 6465   with the encode
+000244a0: 7220 6c61 7965 722e 2044 6566 6175 6c74  r layer. Default
+000244b0: 2030 2e0a 2020 2020 2020 2020 2020 2020   0..            
+000244c0: 7573 655f 7061 7374 2862 6f6f 6c29 3a20  use_past(bool): 
+000244d0: 5573 6520 7468 6520 7061 7374 2073 7461  Use the past sta
+000244e0: 7465 2074 6f20 636f 6d70 7574 652c 2075  te to compute, u
+000244f0: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
+00024500: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
+00024510: 466f 7220 6578 616d 706c 652c 2069 6620  For example, if 
+00024520: 7765 2068 6176 6520 7477 6f0a 2020 2020  we have two.    
+00024530: 2020 2020 2020 2020 2020 2020 776f 7264              word
+00024540: 7320 616e 6420 7761 6e74 2074 6f20 6765  s and want to ge
+00024550: 6e65 7261 7465 2074 6865 2074 656e 206d  nerate the ten m
+00024560: 6f72 6520 776f 7264 732e 2057 6520 6a75  ore words. We ju
+00024570: 7374 206e 6565 6420 746f 2063 6f6d 7075  st need to compu
+00024580: 7465 2074 6865 2074 776f 2077 6f72 6473  te the two words
+00024590: 2720 7374 6174 6520 6f6e 6c79 206f 6e63  ' state only onc
+000245a0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000245b0: 2020 2061 6e64 2067 656e 6572 6174 6520     and generate 
+000245c0: 7468 6520 6e65 7874 2077 6f72 6420 6f6e  the next word on
+000245d0: 6520 6279 206f 6e65 2e20 5768 656e 2075  e by one. When u
+000245e0: 7365 5f70 6173 7420 6973 2054 7275 652c  se_past is True,
+000245f0: 2074 6865 7265 2061 7265 2074 776f 2073   there are two s
+00024600: 7465 7073 2074 6f20 7275 6e20 7468 6520  teps to run the 
+00024610: 7072 6564 6963 7469 6f6e 2e0a 2020 2020  prediction..    
+00024620: 2020 2020 2020 2020 2020 2020 496e 2074              In t
+00024630: 6865 2066 6972 7374 2073 7465 702c 2073  he first step, s
+00024640: 6574 2074 6865 2069 735f 6669 7273 745f  et the is_first_
+00024650: 6974 6572 6174 696f 6e20 746f 2062 6520  iteration to be 
+00024660: 5472 7565 2062 790a 2020 2020 2020 2020  True by.        
+00024670: 2020 2020 2020 2020 606d 6f64 656c 2e61          `model.a
+00024680: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
+00024690: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
+000246a0: 6174 696f 6e3d 5472 7565 2960 2c20 616e  ation=True)`, an
+000246b0: 6420 7061 7373 2074 6865 2066 756c 6c20  d pass the full 
+000246c0: 696e 7075 7473 2e20 5468 656e 2c20 7365  inputs. Then, se
+000246d0: 7420 7468 650a 2020 2020 2020 2020 2020  t the.          
+000246e0: 2020 2020 2020 6973 5f66 6972 7374 5f69        is_first_i
+000246f0: 7465 7261 7469 6f6e 2074 6f20 6265 2046  teration to be F
+00024700: 616c 7365 2062 7920 606d 6f64 656c 2e61  alse by `model.a
+00024710: 6464 5f66 6c61 6773 5f72 6563 7572 7369  dd_flags_recursi
+00024720: 7665 2869 735f 6669 7273 745f 6974 6572  ve(is_first_iter
+00024730: 6174 696f 6e3d 4661 6c73 6529 602e 2041  ation=False)`. A
+00024740: 7420 7468 6973 206d 6f6d 656e 742c 0a20  t this moment,. 
+00024750: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00024760: 6173 7320 7468 6520 7369 6e67 6c65 2073  ass the single s
+00024770: 7465 7027 7320 696e 7075 7420 7465 6e73  tep's input tens
+00024780: 6f72 2c20 616e 6420 6c6f 6f70 2069 742e  or, and loop it.
+00024790: 2044 6566 6175 6c74 3a20 4661 6c73 652e   Default: False.
+000247a0: 0a20 2020 2020 2020 2020 2020 206d 6f65  .            moe
+000247b0: 5f63 6f6e 6669 6728 4d6f 4543 6f6e 6669  _config(MoEConfi
+000247c0: 6729 3a20 5468 6520 636f 6e66 6967 7572  g): The configur
+000247d0: 6174 696f 6e20 6f66 204d 6f45 2028 4d69  ation of MoE (Mi
+000247e0: 7874 7572 6520 6f66 2045 7870 6572 7429  xture of Expert)
+000247f0: 2e20 4465 6661 756c 7420 6973 2061 6e20  . Default is an 
+00024800: 696e 7374 616e 6365 206f 6620 4d6f 4543  instance of MoEC
+00024810: 6f6e 6669 670a 2020 2020 2020 2020 2020  onfig.          
+00024820: 2020 2020 2020 7769 7468 2064 6566 6175        with defau
+00024830: 6c74 2076 616c 7565 732e 2050 6c65 6173  lt values. Pleas
+00024840: 6520 7365 6520 604d 6f45 436f 6e66 6967  e see `MoEConfig
+00024850: 602e 0a20 2020 2020 2020 2020 2020 2070  `..            p
+00024860: 6172 616c 6c65 6c5f 636f 6e66 6967 2854  arallel_config(T
+00024870: 7261 6e73 666f 726d 6572 4f70 5061 7261  ransformerOpPara
+00024880: 6c6c 656c 436f 6e66 6967 293a 2054 6865  llelConfig): The
+00024890: 2070 6172 616c 6c65 6c20 636f 6e66 6967   parallel config
+000248a0: 7572 652e 2044 6566 6175 6c74 2060 6465  ure. Default `de
+000248b0: 6661 756c 745f 7472 616e 7366 6f72 6d65  fault_transforme
+000248c0: 725f 636f 6e66 6967 602c 0a20 2020 2020  r_config`,.     
+000248d0: 2020 2020 2020 2020 2020 2061 6e20 696e             an in
+000248e0: 7374 616e 6365 206f 6620 6054 7261 6e73  stance of `Trans
+000248f0: 666f 726d 6572 4f70 5061 7261 6c6c 656c  formerOpParallel
+00024900: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
+00024910: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
+00024920: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
+00024930: 2020 2020 2020 2020 2d20 2a2a 6869 6464          - **hidd
+00024940: 656e 5f73 7461 7465 732a 2a20 2854 656e  en_states** (Ten
+00024950: 736f 7229 202d 2054 656e 736f 722c 2073  sor) - Tensor, s
+00024960: 6861 7065 2073 686f 756c 6420 6265 205b  hape should be [
+00024970: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
+00024980: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
+00024990: 697a 655d 206f 720a 2020 2020 2020 2020  ize] or.        
+000249a0: 2020 2020 2020 5b62 6174 6368 5f73 697a        [batch_siz
+000249b0: 6520 2a20 7365 715f 6c65 6e67 7468 2c20  e * seq_length, 
+000249c0: 6869 6464 656e 5f73 697a 655d 2c20 6966  hidden_size], if
+000249d0: 2074 6865 2075 7365 5f70 6173 7420 6973   the use_past is
+000249e0: 2046 616c 7365 206f 7220 6973 5f66 6972   False or is_fir
+000249f0: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
+00024a00: 652e 204f 7468 6572 7769 7365 2c0a 2020  e. Otherwise,.  
+00024a10: 2020 2020 2020 2020 2020 2020 7368 6f75              shou
+00024a20: 6c64 2062 6520 5b62 6174 6368 5f73 697a  ld be [batch_siz
+00024a30: 652c 2031 2c20 6869 6464 656e 5f73 697a  e, 1, hidden_siz
+00024a40: 655d 2e0a 2020 2020 2020 2020 2020 2020  e]..            
+00024a50: 2d20 2a2a 6174 7465 6e74 696f 6e5f 6d61  - **attention_ma
+00024a60: 736b 2a2a 2028 5465 6e73 6f72 2920 2d20  sk** (Tensor) - 
+00024a70: 466c 6f61 7420 5465 6e73 6f72 2c20 4966  Float Tensor, If
+00024a80: 2074 6865 2075 7365 5f70 6173 7420 6973   the use_past is
+00024a90: 2046 616c 7365 206f 7220 6973 5f66 6972   False or is_fir
+00024aa0: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
+00024ab0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00024ac0: 2074 6865 2061 7474 656e 7469 6f6e 206d   the attention m
+00024ad0: 6173 6b20 6d61 7472 6978 2073 686f 756c  ask matrix shoul
+00024ae0: 6420 6261 205b 6261 7463 685f 7369 7a65  d ba [batch_size
+00024af0: 2c20 7365 715f 6c65 6e67 7468 2c20 7365  , seq_length, se
+00024b00: 715f 6c65 6e67 7468 5d2c 206f 7220 4e6f  q_length], or No
+00024b10: 6e65 2e20 4e6f 6e65 206d 6561 6e73 2074  ne. None means t
+00024b20: 6865 7265 2077 696c 6c0a 2020 2020 2020  here will.      
+00024b30: 2020 2020 2020 2020 6265 206e 6f20 6d61          be no ma
+00024b40: 736b 2069 6e20 736f 6674 6d61 7820 636f  sk in softmax co
+00024b50: 6d70 7574 6174 696f 6e2e 204f 7468 6572  mputation. Other
+00024b60: 7769 7365 2c20 7368 6f75 6c64 2062 6520  wise, should be 
+00024b70: 5b62 6174 6368 5f73 697a 652c 2031 2c20  [batch_size, 1, 
+00024b80: 6869 6464 656e 5f73 697a 655d 0a20 2020  hidden_size].   
+00024b90: 2020 2020 2020 2020 202d 202a 2a69 6e69           - **ini
+00024ba0: 745f 7265 7365 742a 2a20 2854 656e 736f  t_reset** (Tenso
+00024bb0: 7229 202d 2041 2062 6f6f 6c20 7465 6e73  r) - A bool tens
+00024bc0: 6f72 2077 6974 6820 7368 6170 6520 5b31  or with shape [1
+00024bd0: 5d2c 2075 7365 6420 746f 2063 6c65 6172  ], used to clear
+00024be0: 2074 6865 2070 6173 7420 6b65 7920 7061   the past key pa
+00024bf0: 7261 6d65 7465 7220 616e 640a 2020 2020  rameter and.    
+00024c00: 2020 2020 2020 2020 2020 7061 7374 2076            past v
+00024c10: 616c 7565 2070 6172 616d 6574 6572 2075  alue parameter u
+00024c20: 7365 6420 696e 2074 6865 2069 6e63 7265  sed in the incre
+00024c30: 6d65 6e74 616c 2070 7265 6469 6374 696f  mental predictio
+00024c40: 6e2e 204f 6e6c 7920 7661 6c69 6420 7768  n. Only valid wh
+00024c50: 656e 2075 7365 5f70 6173 7420 6973 2054  en use_past is T
+00024c60: 7275 652e 2044 6566 6175 6c74 2054 7275  rue. Default Tru
+00024c70: 652e 0a20 2020 2020 2020 2020 2020 202d  e..            -
+00024c80: 202a 2a62 6174 6368 5f76 616c 6964 5f6c   **batch_valid_l
+00024c90: 656e 6774 682a 2a20 2854 656e 736f 7229  ength** (Tensor)
+00024ca0: 202d 2049 6e74 3332 2074 656e 736f 7220   - Int32 tensor 
+00024cb0: 7769 7468 2073 6861 7065 205b 6261 7463  with shape [batc
+00024cc0: 685f 7369 7a65 5d20 7468 6520 7061 7374  h_size] the past
+00024cd0: 2063 616c 6375 6c61 7465 6420 7468 6520   calculated the 
+00024ce0: 696e 6465 782e 0a20 2020 2020 2020 2020  index..         
+00024cf0: 2020 2020 2055 7365 6420 666f 7220 696e       Used for in
+00024d00: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+00024d10: 7469 6f6e 2077 6865 6e20 7468 6520 7573  tion when the us
+00024d20: 655f 7061 7374 2069 7320 5472 7565 2e20  e_past is True. 
+00024d30: 4465 6661 756c 7420 4e6f 6e65 2e0a 0a20  Default None... 
+00024d40: 2020 2020 2020 204f 7574 7075 7473 3a0a         Outputs:.
+00024d50: 2020 2020 2020 2020 2020 2020 5475 706c              Tupl
+00024d60: 652c 2061 2074 7570 6c65 2063 6f6e 7461  e, a tuple conta
+00024d70: 696e 7328 606f 7574 7075 7460 2c20 606c  ins(`output`, `l
+00024d80: 6179 6572 5f70 7265 7365 6e74 6029 0a0a  ayer_present`)..
+00024d90: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+00024da0: 6f75 7470 7574 2a2a 2028 5465 6e73 6f72  output** (Tensor
+00024db0: 2920 2d20 5468 6520 666c 6f61 7420 7465  ) - The float te
+00024dc0: 6e73 6f72 206f 6620 7468 6520 6f75 7470  nsor of the outp
+00024dd0: 7574 206f 6620 7468 6520 6c61 7965 7220  ut of the layer 
+00024de0: 7769 7468 0a20 2020 2020 2020 2020 2020  with.           
+00024df0: 2020 2073 6861 7065 2028 6261 7463 685f     shape (batch_
+00024e00: 7369 7a65 2c20 7365 715f 6c65 6e67 7468  size, seq_length
+00024e10: 2c20 6869 6464 656e 5f73 697a 6529 206f  , hidden_size) o
+00024e20: 7220 2862 6174 6368 5f73 697a 6520 2a20  r (batch_size * 
+00024e30: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+00024e40: 656e 5f73 697a 6529 2c20 6966 2074 6865  en_size), if the
+00024e50: 2075 7365 5f70 6173 7420 6973 0a20 2020   use_past is.   
+00024e60: 2020 2020 2020 2020 2020 2046 616c 7365             False
+00024e70: 206f 7220 6973 5f66 6972 7374 5f69 7465   or is_first_ite
+00024e80: 7261 7469 6f6e 3d54 7275 652e 204f 7468  ration=True. Oth
+00024e90: 6572 7769 7365 2c20 6974 2077 696c 6c20  erwise, it will 
+00024ea0: 6265 2028 6261 7463 685f 7369 7a65 2c20  be (batch_size, 
+00024eb0: 312c 2068 6964 6465 6e5f 7369 7a65 292e  1, hidden_size).
+00024ec0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+00024ed0: 2a6c 6179 6572 5f70 7265 7365 6e74 2a2a  *layer_present**
+00024ee0: 2028 5475 706c 6529 202d 2041 2074 7570   (Tuple) - A tup
+00024ef0: 6c65 2077 6974 6820 7369 7a65 206f 6620  le with size of 
+00024f00: 6e75 6d5f 6c61 7965 7273 2c20 7768 6572  num_layers, wher
+00024f10: 6520 6561 6368 2074 7570 6c65 2063 6f6e  e each tuple con
+00024f20: 7461 696e 7320 7468 6520 5465 6e73 6f72  tains the Tensor
+00024f30: 2074 6865 0a20 2020 2020 2020 2020 2020   the.           
+00024f40: 2020 2070 726f 6a65 6374 6564 206b 6579     projected key
+00024f50: 2061 6e64 2076 616c 7565 2076 6563 746f   and value vecto
+00024f60: 7220 7769 7468 2073 6861 7065 2028 2862  r with shape ((b
+00024f70: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+00024f80: 6561 6473 2c20 7369 7a65 5f70 6572 5f68  eads, size_per_h
+00024f90: 6561 642c 2073 6571 5f6c 656e 6774 6829  ead, seq_length)
+00024fa0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00024fb0: 616e 6420 2862 6174 6368 5f73 697a 652c  and (batch_size,
+00024fc0: 206e 756d 5f68 6561 6473 2c20 7365 715f   num_heads, seq_
+00024fd0: 6c65 6e67 7468 2c20 7369 7a65 5f70 6572  length, size_per
+00024fe0: 5f68 6561 6429 292e 0a0a 2020 2020 2020  _head))...      
+00024ff0: 2020 5375 7070 6f72 7465 6420 506c 6174    Supported Plat
+00025000: 666f 726d 733a 0a20 2020 2020 2020 2020  forms:.         
+00025010: 2020 2060 6041 7363 656e 6460 6020 6060     ``Ascend`` ``
+00025020: 4750 5560 600a 0a20 2020 2020 2020 2045  GPU``..        E
+00025030: 7861 6d70 6c65 733a 0a20 2020 2020 2020  xamples:.       
+00025040: 2020 2020 203e 3e3e 2069 6d70 6f72 7420       >>> import 
+00025050: 6e75 6d70 7920 6173 206e 700a 2020 2020  numpy as np.    
+00025060: 2020 2020 2020 2020 3e3e 3e20 6672 6f6d          >>> from
+00025070: 206d 696e 6473 706f 7265 2069 6d70 6f72   mindspore impor
+00025080: 7420 6474 7970 6520 6173 206d 7374 7970  t dtype as mstyp
+00025090: 650a 2020 2020 2020 2020 2020 2020 3e3e  e.            >>
+000250a0: 3e20 6672 6f6d 206d 696e 6466 6f72 6d65  > from mindforme
+000250b0: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+000250c0: 666f 726d 6572 2069 6d70 6f72 7420 5472  former import Tr
+000250d0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
+000250e0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+000250f0: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
+00025100: 696d 706f 7274 2054 656e 736f 720a 2020  import Tensor.  
+00025110: 2020 2020 2020 2020 2020 3e3e 3e20 6d6f            >>> mo
+00025120: 6465 6c20 3d20 5472 616e 7366 6f72 6d65  del = Transforme
+00025130: 7245 6e63 6f64 6572 2862 6174 6368 5f73  rEncoder(batch_s
+00025140: 697a 653d 322c 206e 756d 5f6c 6179 6572  ize=2, num_layer
+00025150: 733d 322c 2068 6964 6465 6e5f 7369 7a65  s=2, hidden_size
+00025160: 3d38 2c20 6666 6e5f 6869 6464 656e 5f73  =8, ffn_hidden_s
+00025170: 697a 653d 3634 2c0a 2020 2020 2020 2020  ize=64,.        
+00025180: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
+00025190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000251a0: 2020 2073 6571 5f6c 656e 6774 683d 3136     seq_length=16
+000251b0: 2c20 6e75 6d5f 6865 6164 733d 3229 0a20  , num_heads=2). 
+000251c0: 2020 2020 2020 2020 2020 203e 3e3e 2065             >>> e
+000251d0: 6e63 6f64 6572 5f69 6e70 7574 5f76 616c  ncoder_input_val
+000251e0: 7565 203d 2054 656e 736f 7228 6e70 2e6f  ue = Tensor(np.o
+000251f0: 6e65 7328 2832 2c20 3136 2c20 3829 292c  nes((2, 16, 8)),
+00025200: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
+00025210: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00025220: 2065 6e63 6f64 6572 5f69 6e70 7574 5f6d   encoder_input_m
+00025230: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
+00025240: 6f6e 6573 2828 322c 2031 362c 2031 3629  ones((2, 16, 16)
+00025250: 292c 206d 7374 7970 652e 666c 6f61 7431  ), mstype.float1
+00025260: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
+00025270: 3e3e 206f 7574 7075 742c 2070 6173 7420  >> output, past 
+00025280: 3d20 6d6f 6465 6c28 656e 636f 6465 725f  = model(encoder_
+00025290: 696e 7075 745f 7661 6c75 652c 2065 6e63  input_value, enc
+000252a0: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b29  oder_input_mask)
+000252b0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+000252c0: 2070 7269 6e74 286f 7574 7075 742e 7368   print(output.sh
+000252d0: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+000252e0: 2028 322c 2031 362c 2038 290a 2020 2020   (2, 16, 8).    
+000252f0: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+00025300: 7428 6c65 6e28 7061 7374 2929 0a20 2020  t(len(past)).   
+00025310: 2020 2020 2020 2020 2032 0a20 2020 2020           2.     
+00025320: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00025330: 2870 6173 745b 305d 5b30 5d2e 7368 6170  (past[0][0].shap
+00025340: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+00025350: 322c 2032 2c20 342c 2031 3629 0a20 2020  2, 2, 4, 16).   
+00025360: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+00025370: 6e74 2870 6173 745b 305d 5b31 5d2e 7368  nt(past[0][1].sh
+00025380: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00025390: 2028 322c 2032 2c20 3136 2c20 3429 0a20   (2, 2, 16, 4). 
+000253a0: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
+000253b0: 2057 6865 6e20 7573 6520 7573 655f 7061   When use use_pa
+000253c0: 7374 3d54 7275 652c 2069 7420 696e 636c  st=True, it incl
+000253d0: 7564 6573 2074 776f 2073 7465 7073 2074  udes two steps t
+000253e0: 6f20 696d 706c 656d 656e 7420 7468 6520  o implement the 
+000253f0: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
+00025400: 6963 7469 6f6e 2e0a 2020 2020 2020 2020  iction..        
+00025410: 2020 2020 3e3e 3e20 2320 5374 6570 2031      >>> # Step 1
+00025420: 3a20 7365 7420 6973 5f66 6972 7374 5f69  : set is_first_i
+00025430: 7465 7261 7469 6f6e 3d54 7275 652c 2061  teration=True, a
+00025440: 6e64 2069 6e70 7574 2074 6865 2066 756c  nd input the ful
+00025450: 6c20 7365 7175 656e 6365 206c 656e 6774  l sequence lengt
+00025460: 6827 7320 7374 6174 652e 0a20 2020 2020  h's state..     
+00025470: 2020 2020 2020 203e 3e3e 2062 6174 6368         >>> batch
+00025480: 5f76 616c 6964 5f6c 656e 6774 6820 3d20  _valid_length = 
+00025490: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
+000254a0: 322c 2929 2c20 6d73 7479 7065 2e69 6e74  2,)), mstype.int
+000254b0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
+000254c0: 3e3e 3e20 696e 6974 5f72 6573 6574 203d  >>> init_reset =
+000254d0: 2054 656e 736f 7228 5b54 7275 655d 2c20   Tensor([True], 
+000254e0: 6d73 7479 7065 2e62 6f6f 6c5f 290a 2020  mstype.bool_).  
+000254f0: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
+00025500: 5365 7420 6973 5f66 6972 7374 5f69 7465  Set is_first_ite
+00025510: 7261 7469 6f6e 3d54 7275 6520 746f 2067  ration=True to g
+00025520: 656e 6572 6174 6520 7468 6520 6675 6c6c  enerate the full
+00025530: 206d 656d 6f72 7920 7374 6174 6573 0a20   memory states. 
+00025540: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
+00025550: 6f64 656c 203d 2054 7261 6e73 666f 726d  odel = Transform
+00025560: 6572 456e 636f 6465 7228 6261 7463 685f  erEncoder(batch_
+00025570: 7369 7a65 3d32 2c20 6869 6464 656e 5f73  size=2, hidden_s
+00025580: 697a 653d 382c 2066 666e 5f68 6964 6465  ize=8, ffn_hidde
+00025590: 6e5f 7369 7a65 3d36 342c 2073 6571 5f6c  n_size=64, seq_l
+000255a0: 656e 6774 683d 3136 2c0a 2020 2020 2020  ength=16,.      
+000255b0: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
+000255c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000255d0: 2020 2020 206e 756d 5f68 6561 6473 3d32       num_heads=2
+000255e0: 2c20 6e75 6d5f 6c61 7965 7273 3d32 2c20  , num_layers=2, 
+000255f0: 7573 655f 7061 7374 3d54 7275 6529 0a20  use_past=True). 
+00025600: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
+00025610: 6f64 656c 2e61 6464 5f66 6c61 6773 5f72  odel.add_flags_r
+00025620: 6563 7572 7369 7665 2869 735f 6669 7273  ecursive(is_firs
+00025630: 745f 6974 6572 6174 696f 6e3d 5472 7565  t_iteration=True
+00025640: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00025650: 3e20 6869 6464 656e 2c20 7061 7374 203d  > hidden, past =
+00025660: 206d 6f64 656c 2865 6e63 6f64 6572 5f69   model(encoder_i
+00025670: 6e70 7574 5f76 616c 7565 2c20 656e 636f  nput_value, enco
+00025680: 6465 725f 696e 7075 745f 6d61 736b 2c20  der_input_mask, 
+00025690: 696e 6974 5f72 6573 6574 2c20 6261 7463  init_reset, batc
+000256a0: 685f 7661 6c69 645f 6c65 6e67 7468 290a  h_valid_length).
+000256b0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000256c0: 7072 696e 7428 6869 6464 656e 2e73 6861  print(hidden.sha
+000256d0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+000256e0: 2832 2c20 3136 2c20 3829 0a20 2020 2020  (2, 16, 8).     
+000256f0: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00025700: 2870 6173 745b 305d 5b30 5d2e 7368 6170  (past[0][0].shap
+00025710: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+00025720: 322c 2032 2c20 342c 2031 3629 0a20 2020  2, 2, 4, 16).   
+00025730: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+00025740: 6e74 2870 6173 745b 305d 5b31 5d2e 7368  nt(past[0][1].sh
+00025750: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00025760: 2028 322c 2032 2c20 3136 2c20 3429 0a20   (2, 2, 16, 4). 
+00025770: 2020 2020 2020 2020 2020 203e 3e3e 2065             >>> e
+00025780: 6e63 6f64 6572 5f69 6e70 7574 5f76 616c  ncoder_input_val
+00025790: 7565 203d 2054 656e 736f 7228 6e70 2e6f  ue = Tensor(np.o
+000257a0: 6e65 7328 2832 2c20 312c 2038 2929 2c20  nes((2, 1, 8)), 
+000257b0: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+000257c0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000257d0: 656e 636f 6465 725f 696e 7075 745f 6d61  encoder_input_ma
+000257e0: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
+000257f0: 6e65 7328 2832 2c20 312c 2031 3629 292c  nes((2, 1, 16)),
+00025800: 206d 7374 7970 652e 666c 6f61 7431 3629   mstype.float16)
+00025810: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00025820: 2069 6e69 745f 7265 7365 7420 3d20 5465   init_reset = Te
+00025830: 6e73 6f72 285b 4661 6c73 655d 2c20 6d73  nsor([False], ms
+00025840: 7479 7065 2e62 6f6f 6c5f 290a 2020 2020  type.bool_).    
+00025850: 2020 2020 2020 2020 3e3e 3e20 2320 5374          >>> # St
+00025860: 6570 2032 3a20 7365 7420 6973 5f66 6972  ep 2: set is_fir
+00025870: 7374 5f69 7465 7261 7469 6f6e 3d46 616c  st_iteration=Fal
+00025880: 7365 2c20 616e 6420 7061 7373 2074 6865  se, and pass the
+00025890: 2073 696e 676c 6520 776f 7264 2074 6f20   single word to 
+000258a0: 7275 6e20 7468 6520 7072 6564 6963 7469  run the predicti
+000258b0: 6f6e 2072 6174 6865 7220 7468 616e 0a20  on rather than. 
+000258c0: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
+000258d0: 2074 6865 2066 756c 6c20 7365 7175 656e   the full sequen
+000258e0: 6365 2e0a 2020 2020 2020 2020 2020 2020  ce..            
+000258f0: 3e3e 3e20 6d6f 6465 6c2e 6164 645f 666c  >>> model.add_fl
+00025900: 6167 735f 7265 6375 7273 6976 6528 6973  ags_recursive(is
+00025910: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
+00025920: 3d46 616c 7365 290a 2020 2020 2020 2020  =False).        
+00025930: 2020 2020 3e3e 3e20 6869 6464 656e 2c20      >>> hidden, 
+00025940: 7061 7374 203d 206d 6f64 656c 2865 6e63  past = model(enc
+00025950: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+00025960: 2c20 656e 636f 6465 725f 696e 7075 745f  , encoder_input_
+00025970: 6d61 736b 2c20 696e 6974 5f72 6573 6574  mask, init_reset
+00025980: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+00025990: 6e67 7468 290a 2020 2020 2020 2020 2020  ngth).          
+000259a0: 2020 3e3e 3e20 7072 696e 7428 6869 6464    >>> print(hidd
+000259b0: 656e 2e73 6861 7065 290a 2020 2020 2020  en.shape).      
+000259c0: 2020 2020 2020 2832 2c20 312c 2038 290a        (2, 1, 8).
+000259d0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000259e0: 7072 696e 7428 7061 7374 5b30 5d5b 305d  print(past[0][0]
+000259f0: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+00025a00: 2020 2020 2832 2c20 322c 2034 2c20 3136      (2, 2, 4, 16
+00025a10: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00025a20: 3e20 7072 696e 7428 7061 7374 5b30 5d5b  > print(past[0][
+00025a30: 315d 2e73 6861 7065 290a 2020 2020 2020  1].shape).      
+00025a40: 2020 2020 2020 2832 2c20 322c 2031 362c        (2, 2, 16,
+00025a50: 2034 290a 2020 2020 2222 220a 0a20 2020   4).    """..   
+00025a60: 2040 5f4c 6f67 4163 7469 6f6e 4f6e 6365   @_LogActionOnce
+00025a70: 286d 5f6c 6f67 6765 723d 6c6f 6767 6572  (m_logger=logger
+00025a80: 2c20 6b65 793d 2754 7261 6e73 666f 726d  , key='Transform
+00025a90: 6572 456e 636f 6465 7227 2c0a 2020 2020  erEncoder',.    
+00025aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ab0: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
+00025ac0: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
+00025ad0: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
+00025ae0: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
+00025af0: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
+00025b00: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
+00025b10: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
+00025b20: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
+00025b30: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
+00025b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025b50: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
+00025b60: 6164 733d 5661 6c69 6461 746f 722e 6368  ads=Validator.ch
+00025b70: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+00025b80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00025b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ba0: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+00025bb0: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
+00025bc0: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+00025bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025bf0: 7365 715f 6c65 6e67 7468 3d56 616c 6964  seq_length=Valid
+00025c00: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
+00025c10: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
+00025c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025c30: 2020 2020 2020 2020 206e 756d 5f6c 6179           num_lay
+00025c40: 6572 733d 5661 6c69 6461 746f 722e 6368  ers=Validator.ch
+00025c50: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+00025c60: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00025c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025c80: 2020 6f66 6673 6574 3d56 616c 6964 6174    offset=Validat
+00025c90: 6f72 2e63 6865 636b 5f6e 6f6e 5f6e 6567  or.check_non_neg
+00025ca0: 6174 6976 655f 696e 742c 0a20 2020 2020  ative_int,.     
+00025cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025cc0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00025cd0: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+00025ce0: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
+00025cf0: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f66  k_non_negative_f
+00025d00: 6c6f 6174 2c0a 2020 2020 2020 2020 2020  loat,.          
+00025d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025d20: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+00025d30: 706f 7574 5f72 6174 653d 5661 6c69 6461  pout_rate=Valida
+00025d40: 746f 722e 6368 6563 6b5f 6e6f 6e5f 6e65  tor.check_non_ne
+00025d50: 6761 7469 7665 5f66 6c6f 6174 2c0a 2020  gative_float,.  
+00025d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025d70: 2020 2020 2020 2020 2020 2020 2020 706f                po
+00025d80: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00025d90: 6964 7561 6c3d 5661 6c69 6461 746f 722e  idual=Validator.
+00025da0: 6368 6563 6b5f 626f 6f6c 2c0a 2020 2020  check_bool,.    
+00025db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025dc0: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
+00025dd0: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
+00025de0: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
+00025df0: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
+00025e00: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
 00025e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025e20: 2020 2020 2020 2020 2020 2020 206d 7374               mst
-00025e30: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
-00025e40: 7970 652e 6266 6c6f 6174 3136 5d2c 0a20  ype.bfloat16],. 
-00025e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025e50: 2020 2020 6d73 7479 7065 2e66 6c6f 6174      mstype.float
+00025e60: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
+00025e70: 7431 365d 2c0a 2020 2020 2020 2020 2020  t16],.          
 00025e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025e90: 2020 2020 2020 2020 2020 2254 7261 6e73            "Trans
-00025ea0: 666f 726d 6572 456e 636f 6465 7222 292c  formerEncoder"),
-00025eb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00025ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025ed0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00025ee0: 5f74 7970 653d 5f76 616c 6964 5f76 616c  _type=_valid_val
-00025ef0: 7565 5f63 6865 636b 7328 5b6d 7374 7970  ue_checks([mstyp
-00025f00: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
-00025f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ec0: 2022 5472 616e 7366 6f72 6d65 7245 6e63   "TransformerEnc
+00025ed0: 6f64 6572 2229 2c0a 2020 2020 2020 2020  oder"),.        
+00025ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ef0: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+00025f00: 636f 6d70 7574 655f 7479 7065 3d5f 7661  compute_type=_va
+00025f10: 6c69 645f 7661 6c75 655f 6368 6563 6b73  lid_value_checks
+00025f20: 285b 6d73 7479 7065 2e66 6c6f 6174 3332  ([mstype.float32
+00025f30: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00025f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025f50: 2020 2020 206d 7374 7970 652e 666c 6f61       mstype.floa
-00025f60: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
-00025f70: 6174 3136 5d2c 0a20 2020 2020 2020 2020  at16],.         
-00025f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025f70: 2020 2020 2020 2020 2020 2020 6d73 7479              msty
+00025f80: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
+00025f90: 7065 2e62 666c 6f61 7431 365d 2c0a 2020  pe.bfloat16],.  
 00025fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00025fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025fc0: 2254 7261 6e73 666f 726d 6572 456e 636f  "TransformerEnco
-00025fd0: 6465 7222 292c 0a20 2020 2020 2020 2020  der"),.         
-00025fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025ff0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
-00026000: 745f 7479 7065 3d5f 7661 6c69 645f 7661  t_type=_valid_va
-00026010: 6c75 655f 6368 6563 6b73 285b 6d73 7479  lue_checks([msty
-00026020: 7065 2e66 6c6f 6174 3332 2c20 6d73 7479  pe.float32, msty
-00026030: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
-00026040: 7065 2e62 666c 6f61 7431 365d 2c0a 2020  pe.bfloat16],.  
-00026050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025fe0: 2020 2020 2020 2022 5472 616e 7366 6f72         "Transfor
+00025ff0: 6d65 7245 6e63 6f64 6572 2229 2c0a 2020  merEncoder"),.  
+00026000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026010: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00026020: 7261 6d5f 696e 6974 5f74 7970 653d 5f76  ram_init_type=_v
+00026030: 616c 6964 5f76 616c 7565 5f63 6865 636b  alid_value_check
+00026040: 7328 5b6d 7374 7970 652e 666c 6f61 7433  s([mstype.float3
+00026050: 322c 206d 7374 7970 652e 666c 6f61 7431  2, mstype.float1
+00026060: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
+00026070: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
 00026080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026090: 2020 2254 7261 6e73 666f 726d 6572 456e    "TransformerEn
-000260a0: 636f 6465 7222 292c 0a20 2020 2020 2020  coder"),.       
-000260b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000260c0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-000260d0: 6c5f 636f 6e66 6967 3d5f 7661 6c69 645f  l_config=_valid_
-000260e0: 7479 7065 5f63 6865 636b 7328 5b54 7261  type_checks([Tra
-000260f0: 6e73 666f 726d 6572 4f70 5061 7261 6c6c  nsformerOpParall
-00026100: 656c 436f 6e66 6967 5d2c 0a20 2020 2020  elConfig],.     
-00026110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026140: 2020 2020 2020 2020 2020 2020 2020 2254                "T
-00026150: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00026160: 7222 292c 0a20 2020 2020 2020 2020 2020  r"),.           
-00026170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026180: 2020 2020 2075 7365 5f70 6173 743d 5661       use_past=Va
-00026190: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-000261a0: 6f6c 290a 2020 2020 6465 6620 5f5f 696e  ol).    def __in
-000261b0: 6974 5f5f 2873 656c 662c 0a20 2020 2020  it__(self,.     
-000261c0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-000261d0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
-000261e0: 2020 2020 2020 2020 206e 756d 5f6c 6179           num_lay
-000261f0: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
-00026200: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-00026210: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00026220: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
-00026230: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00026240: 2020 2020 2020 7365 715f 6c65 6e67 7468        seq_length
-00026250: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00026260: 2020 206e 756d 5f68 6561 6473 2c0a 2020     num_heads,.  
-00026270: 2020 2020 2020 2020 2020 2020 2020 2061                 a
-00026280: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
-00026290: 5f72 6174 653d 302e 312c 0a20 2020 2020  _rate=0.1,.     
-000262a0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-000262b0: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
-000262c0: 302e 312c 0a20 2020 2020 2020 2020 2020  0.1,.           
-000262d0: 2020 2020 2020 6869 6464 656e 5f61 6374        hidden_act
-000262e0: 3d27 6765 6c75 272c 0a20 2020 2020 2020  ='gelu',.       
-000262f0: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
-00026300: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-00026310: 6c3d 4661 6c73 652c 0a20 2020 2020 2020  l=False,.       
-00026320: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
-00026330: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-00026340: 3d6d 7374 7970 652e 666c 6f61 7433 322c  =mstype.float32,
-00026350: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00026360: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
-00026370: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
-00026380: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-00026390: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
-000263a0: 6974 5f74 7970 653d 6d73 7479 7065 2e66  it_type=mstype.f
-000263b0: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
-000263c0: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
-000263d0: 6675 6e63 3d4e 6f6e 652c 0a20 2020 2020  func=None,.     
-000263e0: 2020 2020 2020 2020 2020 2020 6f66 6673              offs
-000263f0: 6574 3d30 2c0a 2020 2020 2020 2020 2020  et=0,.          
-00026400: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
-00026410: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-00026420: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
-00026430: 6967 3d64 6566 6175 6c74 5f6d 6f65 5f63  ig=default_moe_c
-00026440: 6f6e 6669 672c 0a20 2020 2020 2020 2020  onfig,.         
-00026450: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-00026460: 5f63 6f6e 6669 673d 6465 6661 756c 745f  _config=default_
-00026470: 7472 616e 7366 6f72 6d65 725f 636f 6e66  transformer_conf
-00026480: 6967 293a 0a20 2020 2020 2020 2073 7570  ig):.        sup
-00026490: 6572 2854 7261 6e73 666f 726d 6572 456e  er(TransformerEn
-000264a0: 636f 6465 722c 2073 656c 6629 2e5f 5f69  coder, self).__i
-000264b0: 6e69 745f 5f28 290a 2020 2020 2020 2020  nit__().        
-000264c0: 5f63 6865 636b 5f63 6f6e 6669 6728 7061  _check_config(pa
-000264d0: 7261 6c6c 656c 5f63 6f6e 6669 6729 0a20  rallel_config). 
-000264e0: 2020 2020 2020 205f 6368 6563 6b5f 6d6f         _check_mo
-000264f0: 655f 636f 6e66 6967 286d 6f65 5f63 6f6e  e_config(moe_con
-00026500: 6669 672c 2070 6172 616c 6c65 6c5f 636f  fig, parallel_co
-00026510: 6e66 6967 290a 2020 2020 2020 2020 7365  nfig).        se
-00026520: 6c66 2e75 7365 5f6d 6f65 203d 2028 6d6f  lf.use_moe = (mo
-00026530: 655f 636f 6e66 6967 2e65 7870 6572 745f  e_config.expert_
-00026540: 6e75 6d20 3e20 3129 0a20 2020 2020 2020  num > 1).       
-00026550: 2069 6620 6261 7463 685f 7369 7a65 206f   if batch_size o
-00026560: 7220 7573 655f 7061 7374 3a0a 2020 2020  r use_past:.    
-00026570: 2020 2020 2020 2020 5661 6c69 6461 746f          Validato
-00026580: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
-00026590: 5f69 6e74 2862 6174 6368 5f73 697a 6529  _int(batch_size)
-000265a0: 0a20 2020 2020 2020 2063 6f6e 6669 675f  .        config_
-000265b0: 746f 5f6c 6179 6572 203d 2070 6172 616c  to_layer = paral
-000265c0: 6c65 6c5f 636f 6e66 6967 2e6d 6f65 5f70  lel_config.moe_p
-000265d0: 6172 616c 6c65 6c5f 636f 6e66 6967 2069  arallel_config i
-000265e0: 6620 7365 6c66 2e75 7365 5f6d 6f65 2065  f self.use_moe e
-000265f0: 6c73 6520 7061 7261 6c6c 656c 5f63 6f6e  lse parallel_con
-00026600: 6669 672e 6470 5f6d 705f 636f 6e66 6967  fig.dp_mp_config
-00026610: 0a20 2020 2020 2020 2069 6620 5f67 6574  .        if _get
-00026620: 5f70 6172 616c 6c65 6c5f 6d6f 6465 2829  _parallel_mode()
-00026630: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
-00026640: 652e 4155 544f 5f50 4152 414c 4c45 4c2c  e.AUTO_PARALLEL,
-00026650: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s
-00026660: 656c 662e 6164 6420 3d20 502e 4164 6428  elf.add = P.Add(
-00026670: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00026680: 6c66 2e61 7578 5f6c 6f73 7320 3d20 5465  lf.aux_loss = Te
-00026690: 6e73 6f72 2830 2e30 2c20 6d73 7479 7065  nsor(0.0, mstype
-000266a0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
-000266b0: 2020 2020 2020 7365 6c66 2e6e 756d 5f6c        self.num_l
-000266c0: 6179 6572 7320 3d20 6e75 6d5f 6c61 7965  ayers = num_laye
-000266d0: 7273 0a20 2020 2020 2020 2020 2020 2073  rs.            s
-000266e0: 656c 662e 626c 6f63 6b73 203d 206e 6e2e  elf.blocks = nn.
-000266f0: 4365 6c6c 4c69 7374 2829 0a20 2020 2020  CellList().     
-00026700: 2020 2020 2020 2066 6f72 2069 2069 6e20         for i in 
-00026710: 7261 6e67 6528 6e75 6d5f 6c61 7965 7273  range(num_layers
-00026720: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-00026730: 2020 2062 6c6f 636b 203d 2054 7261 6e73     block = Trans
-00026740: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
-00026750: 6572 2868 6964 6465 6e5f 7369 7a65 3d68  er(hidden_size=h
-00026760: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00026770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026790: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-000267a0: 685f 7369 7a65 3d62 6174 6368 5f73 697a  h_size=batch_siz
-000267b0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000267c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000267d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000267e0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-000267f0: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
-00026800: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00026810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026830: 2020 2020 7365 715f 6c65 6e67 7468 3d73      seq_length=s
-00026840: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-00026850: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026870: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-00026880: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00026890: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
-000268a0: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-000268b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000268c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000268d0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-000268e0: 5f64 726f 706f 7574 5f72 6174 653d 6869  _dropout_rate=hi
-000268f0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-00026900: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00026910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026930: 2020 206c 6179 6572 6e6f 726d 5f63 6f6d     layernorm_com
-00026940: 7075 7465 5f74 7970 653d 6c61 7965 726e  pute_type=layern
-00026950: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-00026960: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00026970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026990: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
-000269a0: 655f 7479 7065 3d73 6f66 746d 6178 5f63  e_type=softmax_c
-000269b0: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
-000269c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000269d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000269e0: 2020 2020 2020 2020 2020 2020 206e 756d               num
-000269f0: 5f68 6561 6473 3d6e 756d 5f68 6561 6473  _heads=num_heads
-00026a00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00026a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026a30: 2020 6869 6464 656e 5f61 6374 3d68 6964    hidden_act=hid
-00026a40: 6465 6e5f 6163 742c 0a20 2020 2020 2020  den_act,.       
-00026a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026a70: 2020 2020 2020 2020 2070 6f73 745f 6c61           post_la
-00026a80: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-00026a90: 3d70 6f73 745f 6c61 7965 726e 6f72 6d5f  =post_layernorm_
-00026aa0: 7265 7369 6475 616c 2c0a 2020 2020 2020  residual,.      
-00026ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026ad0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-00026ae0: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-00026af0: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-00026b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026b20: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-00026b30: 6173 743d 7573 655f 7061 7374 2c0a 2020  ast=use_past,.  
+00026090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000260a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000260b0: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
+000260c0: 6f72 6d65 7245 6e63 6f64 6572 2229 2c0a  ormerEncoder"),.
+000260d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000260e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000260f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+00026100: 5f76 616c 6964 5f74 7970 655f 6368 6563  _valid_type_chec
+00026110: 6b73 285b 5472 616e 7366 6f72 6d65 724f  ks([TransformerO
+00026120: 7050 6172 616c 6c65 6c43 6f6e 6669 675d  pParallelConfig]
+00026130: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00026140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026170: 2020 2020 2022 5472 616e 7366 6f72 6d65       "Transforme
+00026180: 7245 6e63 6f64 6572 2229 2c0a 2020 2020  rEncoder"),.    
+00026190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000261a0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
+000261b0: 7061 7374 3d56 616c 6964 6174 6f72 2e63  past=Validator.c
+000261c0: 6865 636b 5f62 6f6f 6c29 0a20 2020 2064  heck_bool).    d
+000261d0: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
+000261e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000261f0: 2020 2062 6174 6368 5f73 697a 652c 0a20     batch_size,. 
+00026200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026210: 6e75 6d5f 6c61 7965 7273 2c0a 2020 2020  num_layers,.    
+00026220: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00026230: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+00026240: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+00026250: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+00026260: 2020 2020 2020 2020 2020 2020 2073 6571               seq
+00026270: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
+00026280: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
+00026290: 6164 732c 0a20 2020 2020 2020 2020 2020  ads,.           
+000262a0: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
+000262b0: 6472 6f70 6f75 745f 7261 7465 3d30 2e31  dropout_rate=0.1
+000262c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000262d0: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
+000262e0: 745f 7261 7465 3d30 2e31 2c0a 2020 2020  t_rate=0.1,.    
+000262f0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00026300: 6465 6e5f 6163 743d 2767 656c 7527 2c0a  den_act='gelu',.
+00026310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026320: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
+00026330: 7265 7369 6475 616c 3d46 616c 7365 2c0a  residual=False,.
+00026340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026350: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
+00026360: 7465 5f74 7970 653d 6d73 7479 7065 2e66  te_type=mstype.f
+00026370: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
+00026380: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+00026390: 5f63 6f6d 7075 7465 5f74 7970 653d 6d73  _compute_type=ms
+000263a0: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+000263b0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+000263c0: 6172 616d 5f69 6e69 745f 7479 7065 3d6d  aram_init_type=m
+000263d0: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
+000263e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000263f0: 6c61 6d62 6461 5f66 756e 633d 4e6f 6e65  lambda_func=None
+00026400: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00026410: 2020 206f 6666 7365 743d 302c 0a20 2020     offset=0,.   
+00026420: 2020 2020 2020 2020 2020 2020 2020 7573                us
+00026430: 655f 7061 7374 3d46 616c 7365 2c0a 2020  e_past=False,.  
+00026440: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+00026450: 6f65 5f63 6f6e 6669 673d 6465 6661 756c  oe_config=defaul
+00026460: 745f 6d6f 655f 636f 6e66 6967 2c0a 2020  t_moe_config,.  
+00026470: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00026480: 6172 616c 6c65 6c5f 636f 6e66 6967 3d64  arallel_config=d
+00026490: 6566 6175 6c74 5f74 7261 6e73 666f 726d  efault_transform
+000264a0: 6572 5f63 6f6e 6669 6729 3a0a 2020 2020  er_config):.    
+000264b0: 2020 2020 7375 7065 7228 5472 616e 7366      super(Transf
+000264c0: 6f72 6d65 7245 6e63 6f64 6572 2c20 7365  ormerEncoder, se
+000264d0: 6c66 292e 5f5f 696e 6974 5f5f 2829 0a20  lf).__init__(). 
+000264e0: 2020 2020 2020 205f 6368 6563 6b5f 636f         _check_co
+000264f0: 6e66 6967 2870 6172 616c 6c65 6c5f 636f  nfig(parallel_co
+00026500: 6e66 6967 290a 2020 2020 2020 2020 5f63  nfig).        _c
+00026510: 6865 636b 5f6d 6f65 5f63 6f6e 6669 6728  heck_moe_config(
+00026520: 6d6f 655f 636f 6e66 6967 2c20 7061 7261  moe_config, para
+00026530: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+00026540: 2020 2020 2073 656c 662e 7573 655f 6d6f       self.use_mo
+00026550: 6520 3d20 286d 6f65 5f63 6f6e 6669 672e  e = (moe_config.
+00026560: 6578 7065 7274 5f6e 756d 203e 2031 290a  expert_num > 1).
+00026570: 2020 2020 2020 2020 6966 2062 6174 6368          if batch
+00026580: 5f73 697a 6520 6f72 2075 7365 5f70 6173  _size or use_pas
+00026590: 743a 0a20 2020 2020 2020 2020 2020 2056  t:.            V
+000265a0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+000265b0: 6f73 6974 6976 655f 696e 7428 6261 7463  ositive_int(batc
+000265c0: 685f 7369 7a65 290a 2020 2020 2020 2020  h_size).        
+000265d0: 636f 6e66 6967 5f74 6f5f 6c61 7965 7220  config_to_layer 
+000265e0: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
+000265f0: 672e 6d6f 655f 7061 7261 6c6c 656c 5f63  g.moe_parallel_c
+00026600: 6f6e 6669 6720 6966 2073 656c 662e 7573  onfig if self.us
+00026610: 655f 6d6f 6520 656c 7365 2070 6172 616c  e_moe else paral
+00026620: 6c65 6c5f 636f 6e66 6967 2e64 705f 6d70  lel_config.dp_mp
+00026630: 5f63 6f6e 6669 670a 2020 2020 2020 2020  _config.        
+00026640: 6966 205f 6765 745f 7061 7261 6c6c 656c  if _get_parallel
+00026650: 5f6d 6f64 6528 2920 696e 2028 5061 7261  _mode() in (Para
+00026660: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
+00026670: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
+00026680: 2020 2020 2020 7365 6c66 2e61 6464 203d        self.add =
+00026690: 2050 2e41 6464 2829 0a20 2020 2020 2020   P.Add().       
+000266a0: 2020 2020 2073 656c 662e 6175 785f 6c6f       self.aux_lo
+000266b0: 7373 203d 2054 656e 736f 7228 302e 302c  ss = Tensor(0.0,
+000266c0: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
+000266d0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+000266e0: 662e 6e75 6d5f 6c61 7965 7273 203d 206e  f.num_layers = n
+000266f0: 756d 5f6c 6179 6572 730a 2020 2020 2020  um_layers.      
+00026700: 2020 2020 2020 7365 6c66 2e62 6c6f 636b        self.block
+00026710: 7320 3d20 6e6e 2e43 656c 6c4c 6973 7428  s = nn.CellList(
+00026720: 290a 2020 2020 2020 2020 2020 2020 666f  ).            fo
+00026730: 7220 6920 696e 2072 616e 6765 286e 756d  r i in range(num
+00026740: 5f6c 6179 6572 7329 3a0a 2020 2020 2020  _layers):.      
+00026750: 2020 2020 2020 2020 2020 626c 6f63 6b20            block 
+00026760: 3d20 5472 616e 7366 6f72 6d65 7245 6e63  = TransformerEnc
+00026770: 6f64 6572 4c61 7965 7228 6869 6464 656e  oderLayer(hidden
+00026780: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+00026790: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000267a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000267b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000267c0: 2020 2062 6174 6368 5f73 697a 653d 6261     batch_size=ba
+000267d0: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
+000267e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000267f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026800: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+00026810: 6464 656e 5f73 697a 653d 6666 6e5f 6869  dden_size=ffn_hi
+00026820: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+00026830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026850: 2020 2020 2020 2020 2020 2073 6571 5f6c             seq_l
+00026860: 656e 6774 683d 7365 715f 6c65 6e67 7468  ength=seq_length
+00026870: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00026880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000268a0: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+000268b0: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
+000268c0: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
+000268d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000268e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000268f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026900: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+00026910: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
+00026920: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+00026930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026950: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
+00026960: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
+00026970: 3d6c 6179 6572 6e6f 726d 5f63 6f6d 7075  =layernorm_compu
+00026980: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+00026990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000269a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000269b0: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+000269c0: 5f63 6f6d 7075 7465 5f74 7970 653d 736f  _compute_type=so
+000269d0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+000269e0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+000269f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a10: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
+00026a20: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
+00026a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a50: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00026a60: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+00026a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026aa0: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+00026ab0: 6573 6964 7561 6c3d 706f 7374 5f6c 6179  esidual=post_lay
+00026ac0: 6572 6e6f 726d 5f72 6573 6964 7561 6c2c  ernorm_residual,
+00026ad0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00026ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026b00: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+00026b10: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+00026b20: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00026b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00026b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026b60: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-00026b70: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
-00026b80: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
-00026b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026bb0: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00026bc0: 6e66 6967 3d63 6f6e 6669 675f 746f 5f6c  nfig=config_to_l
-00026bd0: 6179 6572 290a 2020 2020 2020 2020 2020  ayer).          
-00026be0: 2020 2020 2020 2320 4966 2074 6865 2075        # If the u
-00026bf0: 7365 7220 646f 6573 6e27 7420 7061 7373  ser doesn't pass
-00026c00: 2074 6865 2066 7573 696f 6e20 6675 6e63   the fusion func
-00026c10: 7469 6f6e 2c20 7573 6520 7468 6520 6465  tion, use the de
-00026c20: 6661 756c 7420 6f6e 650a 2020 2020 2020  fault one.      
-00026c30: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-00026c40: 206c 616d 6264 615f 6675 6e63 3a0a 2020   lambda_func:.  
-00026c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026c60: 2020 6c61 6d62 6461 5f66 756e 6320 3d20    lambda_func = 
-00026c70: 5f67 6574 5f6c 616d 6264 615f 6675 6e63  _get_lambda_func
-00026c80: 2829 0a0a 2020 2020 2020 2020 2020 2020  ()..            
-00026c90: 2020 2020 6c61 6d62 6461 5f66 756e 6328      lambda_func(
-00026ca0: 626c 6f63 6b2c 206c 6179 6572 5f69 643d  block, layer_id=
-00026cb0: 692c 206c 6179 6572 733d 6e75 6d5f 6c61  i, layers=num_la
-00026cc0: 7965 7273 2c0a 2020 2020 2020 2020 2020  yers,.          
-00026cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026ce0: 2020 6f66 6673 6574 3d6f 6666 7365 742c    offset=offset,
-00026cf0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-00026d00: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
-00026d10: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00026d20: 2020 7365 6c66 2e62 6c6f 636b 732e 6170    self.blocks.ap
-00026d30: 7065 6e64 2862 6c6f 636b 290a 2020 2020  pend(block).    
-00026d40: 2020 2020 656c 6966 205f 6765 745f 7061      elif _get_pa
-00026d50: 7261 6c6c 656c 5f6d 6f64 6528 2920 6e6f  rallel_mode() no
-00026d60: 7420 696e 2028 5061 7261 6c6c 656c 4d6f  t in (ParallelMo
-00026d70: 6465 2e41 5554 4f5f 5041 5241 4c4c 454c  de.AUTO_PARALLEL
-00026d80: 2c29 3a0a 2020 2020 2020 2020 2020 2020  ,):.            
-00026d90: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
-00026da0: 2829 2e73 6861 7264 2828 2829 2c20 2829  ().shard(((), ()
-00026db0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-00026dc0: 656c 662e 6175 785f 6c6f 7373 203d 2054  elf.aux_loss = T
-00026dd0: 656e 736f 7228 302e 302c 206d 7374 7970  ensor(0.0, mstyp
-00026de0: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
-00026df0: 2020 2020 2020 206c 6f67 6765 722e 7761         logger.wa
-00026e00: 726e 696e 6728 2246 6f72 2070 6172 616c  rning("For paral
-00026e10: 6c65 6c20 6d6f 6465 2c20 7368 6172 6469  lel mode, shardi
-00026e20: 6e67 2070 726f 7061 6761 7469 6f6e 2069  ng propagation i
-00026e30: 7320 7265 636f 6d6d 656e 6465 642c 2079  s recommended, y
-00026e40: 6f75 2063 616e 2075 7365 2069 7420 6279  ou can use it by
-00026e50: 2073 6574 7469 6e67 2022 0a20 2020 2020   setting ".     
-00026e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026e70: 2020 2020 2020 2227 7365 745f 6175 746f        "'set_auto
-00026e80: 5f70 6172 616c 6c65 6c5f 636f 6e74 6578  _parallel_contex
-00026e90: 7428 7061 7261 6c6c 656c 5f6d 6f64 653d  t(parallel_mode=
-00026ea0: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
-00026eb0: 4f5f 5041 5241 4c4c 454c 2c20 220a 2020  O_PARALLEL, ".  
-00026ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026ed0: 2020 2020 2020 2020 2022 7365 6172 6368           "search
-00026ee0: 5f6d 6f64 653d 5c22 7368 6172 6469 6e67  _mode=\"sharding
-00026ef0: 5f70 726f 7061 6761 7469 6f6e 5c22 2927  _propagation\")'
-00026f00: 2061 6e64 2022 0a20 2020 2020 2020 2020   and ".         
-00026f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026f20: 2020 2227 7365 745f 616c 676f 5f70 6172    "'set_algo_par
-00026f30: 616d 6574 6572 7328 656c 656d 656e 7477  ameters(elementw
-00026f40: 6973 655f 6f70 5f73 7472 6174 6567 795f  ise_op_strategy_
-00026f50: 666f 6c6c 6f77 3d46 616c 7365 2c20 6675  follow=False, fu
-00026f60: 6c6c 795f 7573 655f 6465 7669 6365 733d  lly_use_devices=
-00026f70: 4661 6c73 6529 2722 290a 2020 2020 2020  False)'").      
-00026f80: 2020 2020 2020 7365 6c66 2e6e 756d 5f6c        self.num_l
-00026f90: 6179 6572 7320 3d20 6e75 6d5f 6c61 7965  ayers = num_laye
-00026fa0: 7273 0a20 2020 2020 2020 2020 2020 2073  rs.            s
-00026fb0: 656c 662e 626c 6f63 6b73 203d 206e 6e2e  elf.blocks = nn.
-00026fc0: 4365 6c6c 4c69 7374 2829 0a20 2020 2020  CellList().     
-00026fd0: 2020 2020 2020 2066 6f72 2069 2069 6e20         for i in 
-00026fe0: 7261 6e67 6528 6e75 6d5f 6c61 7965 7273  range(num_layers
-00026ff0: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-00027000: 2020 2062 6c6f 636b 203d 2054 7261 6e73     block = Trans
-00027010: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
-00027020: 6572 2868 6964 6465 6e5f 7369 7a65 3d68  er(hidden_size=h
-00027030: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00027040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027060: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-00027070: 685f 7369 7a65 3d62 6174 6368 5f73 697a  h_size=batch_siz
-00027080: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00027090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000270a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000270b0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-000270c0: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
-000270d0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-000270e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000270f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027100: 2020 2020 7365 715f 6c65 6e67 7468 3d73      seq_length=s
-00027110: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-00027120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027140: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-00027150: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00027160: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
-00027170: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-00027180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000271a0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-000271b0: 5f64 726f 706f 7574 5f72 6174 653d 6869  _dropout_rate=hi
-000271c0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-000271d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000271e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000271f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027200: 2020 206c 6179 6572 6e6f 726d 5f63 6f6d     layernorm_com
-00027210: 7075 7465 5f74 7970 653d 6c61 7965 726e  pute_type=layern
-00027220: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-00027230: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00027240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027260: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
-00027270: 655f 7479 7065 3d73 6f66 746d 6178 5f63  e_type=softmax_c
-00027280: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
-00027290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000272a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000272b0: 2020 2020 2020 2020 2020 2020 206e 756d               num
-000272c0: 5f68 6561 6473 3d6e 756d 5f68 6561 6473  _heads=num_heads
-000272d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000272e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000272f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027300: 2020 6869 6464 656e 5f61 6374 3d68 6964    hidden_act=hid
-00027310: 6465 6e5f 6163 742c 0a20 2020 2020 2020  den_act,.       
-00027320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027340: 2020 2020 2020 2020 2070 6f73 745f 6c61           post_la
-00027350: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-00027360: 3d70 6f73 745f 6c61 7965 726e 6f72 6d5f  =post_layernorm_
-00027370: 7265 7369 6475 616c 2c0a 2020 2020 2020  residual,.      
-00027380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000273a0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-000273b0: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-000273c0: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-000273d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000273e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000273f0: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-00027400: 6173 743d 7573 655f 7061 7374 2c0a 2020  ast=use_past,.  
+00026b50: 2020 7573 655f 7061 7374 3d75 7365 5f70    use_past=use_p
+00026b60: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+00026b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026b90: 2020 2020 206d 6f65 5f63 6f6e 6669 673d       moe_config=
+00026ba0: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
+00026bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026bd0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00026be0: 6c6c 656c 5f63 6f6e 6669 673d 636f 6e66  llel_config=conf
+00026bf0: 6967 5f74 6f5f 6c61 7965 7229 0a20 2020  ig_to_layer).   
+00026c00: 2020 2020 2020 2020 2020 2020 2023 2049               # I
+00026c10: 6620 7468 6520 7573 6572 2064 6f65 736e  f the user doesn
+00026c20: 2774 2070 6173 7320 7468 6520 6675 7369  't pass the fusi
+00026c30: 6f6e 2066 756e 6374 696f 6e2c 2075 7365  on function, use
+00026c40: 2074 6865 2064 6566 6175 6c74 206f 6e65   the default one
+00026c50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00026c60: 2069 6620 6e6f 7420 6c61 6d62 6461 5f66   if not lambda_f
+00026c70: 756e 633a 0a20 2020 2020 2020 2020 2020  unc:.           
+00026c80: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
+00026c90: 6675 6e63 203d 205f 6765 745f 6c61 6d62  func = _get_lamb
+00026ca0: 6461 5f66 756e 6328 290a 0a20 2020 2020  da_func()..     
+00026cb0: 2020 2020 2020 2020 2020 206c 616d 6264             lambd
+00026cc0: 615f 6675 6e63 2862 6c6f 636b 2c20 6c61  a_func(block, la
+00026cd0: 7965 725f 6964 3d69 2c20 6c61 7965 7273  yer_id=i, layers
+00026ce0: 3d6e 756d 5f6c 6179 6572 732c 0a20 2020  =num_layers,.   
+00026cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026d00: 2020 2020 2020 2020 206f 6666 7365 743d           offset=
+00026d10: 6f66 6673 6574 2c20 7061 7261 6c6c 656c  offset, parallel
+00026d20: 5f63 6f6e 6669 673d 7061 7261 6c6c 656c  _config=parallel
+00026d30: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
+00026d40: 2020 2020 2020 2020 2073 656c 662e 626c           self.bl
+00026d50: 6f63 6b73 2e61 7070 656e 6428 626c 6f63  ocks.append(bloc
+00026d60: 6b29 0a20 2020 2020 2020 2065 6c69 6620  k).        elif 
+00026d70: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
+00026d80: 6465 2829 206e 6f74 2069 6e20 2850 6172  de() not in (Par
+00026d90: 616c 6c65 6c4d 6f64 652e 4155 544f 5f50  allelMode.AUTO_P
+00026da0: 4152 414c 4c45 4c2c 293a 0a20 2020 2020  ARALLEL,):.     
+00026db0: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+00026dc0: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+00026dd0: 2828 292c 2028 2929 290a 2020 2020 2020  ((), ())).      
+00026de0: 2020 2020 2020 7365 6c66 2e61 7578 5f6c        self.aux_l
+00026df0: 6f73 7320 3d20 5465 6e73 6f72 2830 2e30  oss = Tensor(0.0
+00026e00: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+00026e10: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
+00026e20: 6767 6572 2e77 6172 6e69 6e67 2822 466f  gger.warning("Fo
+00026e30: 7220 7061 7261 6c6c 656c 206d 6f64 652c  r parallel mode,
+00026e40: 2073 6861 7264 696e 6720 7072 6f70 6167   sharding propag
+00026e50: 6174 696f 6e20 6973 2072 6563 6f6d 6d65  ation is recomme
+00026e60: 6e64 6564 2c20 796f 7520 6361 6e20 7573  nded, you can us
+00026e70: 6520 6974 2062 7920 7365 7474 696e 6720  e it by setting 
+00026e80: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00026e90: 2020 2020 2020 2020 2020 2020 2022 2773               "'s
+00026ea0: 6574 5f61 7574 6f5f 7061 7261 6c6c 656c  et_auto_parallel
+00026eb0: 5f63 6f6e 7465 7874 2870 6172 616c 6c65  _context(paralle
+00026ec0: 6c5f 6d6f 6465 3d50 6172 616c 6c65 6c4d  l_mode=ParallelM
+00026ed0: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
+00026ee0: 4c2c 2022 0a20 2020 2020 2020 2020 2020  L, ".           
+00026ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026f00: 2273 6561 7263 685f 6d6f 6465 3d5c 2273  "search_mode=\"s
+00026f10: 6861 7264 696e 675f 7072 6f70 6167 6174  harding_propagat
+00026f20: 696f 6e5c 2229 2720 616e 6420 220a 2020  ion\")' and ".  
+00026f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026f40: 2020 2020 2020 2020 2022 2773 6574 5f61           "'set_a
+00026f50: 6c67 6f5f 7061 7261 6d65 7465 7273 2865  lgo_parameters(e
+00026f60: 6c65 6d65 6e74 7769 7365 5f6f 705f 7374  lementwise_op_st
+00026f70: 7261 7465 6779 5f66 6f6c 6c6f 773d 4661  rategy_follow=Fa
+00026f80: 6c73 652c 2066 756c 6c79 5f75 7365 5f64  lse, fully_use_d
+00026f90: 6576 6963 6573 3d46 616c 7365 2927 2229  evices=False)'")
+00026fa0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00026fb0: 662e 6e75 6d5f 6c61 7965 7273 203d 206e  f.num_layers = n
+00026fc0: 756d 5f6c 6179 6572 730a 2020 2020 2020  um_layers.      
+00026fd0: 2020 2020 2020 7365 6c66 2e62 6c6f 636b        self.block
+00026fe0: 7320 3d20 6e6e 2e43 656c 6c4c 6973 7428  s = nn.CellList(
+00026ff0: 290a 2020 2020 2020 2020 2020 2020 666f  ).            fo
+00027000: 7220 6920 696e 2072 616e 6765 286e 756d  r i in range(num
+00027010: 5f6c 6179 6572 7329 3a0a 2020 2020 2020  _layers):.      
+00027020: 2020 2020 2020 2020 2020 626c 6f63 6b20            block 
+00027030: 3d20 5472 616e 7366 6f72 6d65 7245 6e63  = TransformerEnc
+00027040: 6f64 6572 4c61 7965 7228 6869 6464 656e  oderLayer(hidden
+00027050: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+00027060: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00027070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027090: 2020 2062 6174 6368 5f73 697a 653d 6261     batch_size=ba
+000270a0: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
+000270b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000270c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000270d0: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+000270e0: 6464 656e 5f73 697a 653d 6666 6e5f 6869  dden_size=ffn_hi
+000270f0: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+00027100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027120: 2020 2020 2020 2020 2020 2073 6571 5f6c             seq_l
+00027130: 656e 6774 683d 7365 715f 6c65 6e67 7468  ength=seq_length
+00027140: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00027150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027170: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+00027180: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
+00027190: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
+000271a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000271b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000271c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000271d0: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+000271e0: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
+000271f0: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+00027200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027220: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
+00027230: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
+00027240: 3d6c 6179 6572 6e6f 726d 5f63 6f6d 7075  =layernorm_compu
+00027250: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+00027260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027280: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+00027290: 5f63 6f6d 7075 7465 5f74 7970 653d 736f  _compute_type=so
+000272a0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+000272b0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+000272c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000272d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000272e0: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
+000272f0: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
+00027300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027320: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00027330: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+00027340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027370: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+00027380: 6573 6964 7561 6c3d 706f 7374 5f6c 6179  esidual=post_lay
+00027390: 6572 6e6f 726d 5f72 6573 6964 7561 6c2c  ernorm_residual,
+000273a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000273b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000273c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000273d0: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+000273e0: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+000273f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00027400: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00027410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027430: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-00027440: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
-00027450: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
-00027460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027480: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00027490: 6e66 6967 3d63 6f6e 6669 675f 746f 5f6c  nfig=config_to_l
-000274a0: 6179 6572 290a 2020 2020 2020 2020 2020  ayer).          
-000274b0: 2020 2020 2020 2320 4966 2074 6865 2075        # If the u
-000274c0: 7365 7220 646f 6573 6e27 7420 7061 7373  ser doesn't pass
-000274d0: 2074 6865 2066 7573 696f 6e20 6675 6e63   the fusion func
-000274e0: 7469 6f6e 2c20 7573 6520 7468 6520 6465  tion, use the de
-000274f0: 6661 756c 7420 6f6e 650a 2020 2020 2020  fault one.      
-00027500: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-00027510: 206c 616d 6264 615f 6675 6e63 3a0a 2020   lambda_func:.  
-00027520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027530: 2020 6c61 6d62 6461 5f66 756e 6320 3d20    lambda_func = 
-00027540: 5f67 6574 5f6c 616d 6264 615f 6675 6e63  _get_lambda_func
-00027550: 2829 0a0a 2020 2020 2020 2020 2020 2020  ()..            
-00027560: 2020 2020 6c61 6d62 6461 5f66 756e 6328      lambda_func(
-00027570: 626c 6f63 6b2c 206c 6179 6572 5f69 643d  block, layer_id=
-00027580: 692c 206c 6179 6572 733d 6e75 6d5f 6c61  i, layers=num_la
-00027590: 7965 7273 2c0a 2020 2020 2020 2020 2020  yers,.          
-000275a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000275b0: 2020 6f66 6673 6574 3d6f 6666 7365 742c    offset=offset,
-000275c0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-000275d0: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
-000275e0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-000275f0: 2020 7365 6c66 2e62 6c6f 636b 732e 6170    self.blocks.ap
-00027600: 7065 6e64 2862 6c6f 636b 290a 2020 2020  pend(block).    
-00027610: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-00027620: 2020 2020 2020 7261 6973 6520 5275 6e74        raise Runt
-00027630: 696d 6545 7272 6f72 2866 2254 6865 207b  imeError(f"The {
-00027640: 7365 6c66 2e63 6c73 5f6e 616d 657d 206f  self.cls_name} o
-00027650: 6e6c 7920 7375 7070 6f72 7420 7368 6172  nly support shar
-00027660: 6469 6e67 2070 726f 7061 6761 7469 6f6e  ding propagation
-00027670: 206f 7220 220a 2020 2020 2020 2020 2020   or ".          
-00027680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027690: 2020 2020 2066 2273 656d 692d 6175 746f       f"semi-auto
-000276a0: 2070 6172 616c 6c65 6c20 6d6f 6465 206e   parallel mode n
-000276b0: 6f77 2e22 290a 0a20 2020 2064 6566 2063  ow.")..    def c
-000276c0: 6f6e 7374 7275 6374 2873 656c 662c 2068  onstruct(self, h
-000276d0: 6964 6465 6e5f 7374 6174 6573 2c20 6174  idden_states, at
-000276e0: 7465 6e74 696f 6e5f 6d61 736b 2c20 696e  tention_mask, in
-000276f0: 6974 5f72 6573 6574 3d54 7275 652c 2062  it_reset=True, b
-00027700: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00027710: 683d 4e6f 6e65 293a 0a20 2020 2020 2020  h=None):.       
-00027720: 2022 2222 666f 7277 6172 6420 7072 6f63   """forward proc
-00027730: 6573 7322 2222 0a20 2020 2020 2020 2070  ess""".        p
-00027740: 7265 7365 6e74 5f6c 6179 6572 203d 2028  resent_layer = (
-00027750: 290a 2020 2020 2020 2020 6966 2073 656c  ).        if sel
-00027760: 662e 7573 655f 6d6f 653a 0a20 2020 2020  f.use_moe:.     
-00027770: 2020 2020 2020 2061 6363 756d 5f6c 6f73         accum_los
-00027780: 7320 3d20 7365 6c66 2e61 7578 5f6c 6f73  s = self.aux_los
-00027790: 730a 2020 2020 2020 2020 2020 2020 666f  s.            fo
-000277a0: 7220 6920 696e 2072 616e 6765 2873 656c  r i in range(sel
-000277b0: 662e 6e75 6d5f 6c61 7965 7273 293a 0a20  f.num_layers):. 
-000277c0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-000277d0: 6964 6465 6e5f 7374 6174 6573 2c20 7072  idden_states, pr
-000277e0: 6573 656e 742c 2061 7578 5f6c 6f73 7320  esent, aux_loss 
-000277f0: 3d20 7365 6c66 2e62 6c6f 636b 735b 695d  = self.blocks[i]
-00027800: 2868 6964 6465 6e5f 7374 6174 6573 2c0a  (hidden_states,.
-00027810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027420: 2020 7573 655f 7061 7374 3d75 7365 5f70    use_past=use_p
+00027430: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+00027440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027460: 2020 2020 206d 6f65 5f63 6f6e 6669 673d       moe_config=
+00027470: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
+00027480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000274a0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+000274b0: 6c6c 656c 5f63 6f6e 6669 673d 636f 6e66  llel_config=conf
+000274c0: 6967 5f74 6f5f 6c61 7965 7229 0a20 2020  ig_to_layer).   
+000274d0: 2020 2020 2020 2020 2020 2020 2023 2049               # I
+000274e0: 6620 7468 6520 7573 6572 2064 6f65 736e  f the user doesn
+000274f0: 2774 2070 6173 7320 7468 6520 6675 7369  't pass the fusi
+00027500: 6f6e 2066 756e 6374 696f 6e2c 2075 7365  on function, use
+00027510: 2074 6865 2064 6566 6175 6c74 206f 6e65   the default one
+00027520: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00027530: 2069 6620 6e6f 7420 6c61 6d62 6461 5f66   if not lambda_f
+00027540: 756e 633a 0a20 2020 2020 2020 2020 2020  unc:.           
+00027550: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
+00027560: 6675 6e63 203d 205f 6765 745f 6c61 6d62  func = _get_lamb
+00027570: 6461 5f66 756e 6328 290a 0a20 2020 2020  da_func()..     
+00027580: 2020 2020 2020 2020 2020 206c 616d 6264             lambd
+00027590: 615f 6675 6e63 2862 6c6f 636b 2c20 6c61  a_func(block, la
+000275a0: 7965 725f 6964 3d69 2c20 6c61 7965 7273  yer_id=i, layers
+000275b0: 3d6e 756d 5f6c 6179 6572 732c 0a20 2020  =num_layers,.   
+000275c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000275d0: 2020 2020 2020 2020 206f 6666 7365 743d           offset=
+000275e0: 6f66 6673 6574 2c20 7061 7261 6c6c 656c  offset, parallel
+000275f0: 5f63 6f6e 6669 673d 7061 7261 6c6c 656c  _config=parallel
+00027600: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
+00027610: 2020 2020 2020 2020 2073 656c 662e 626c           self.bl
+00027620: 6f63 6b73 2e61 7070 656e 6428 626c 6f63  ocks.append(bloc
+00027630: 6b29 0a20 2020 2020 2020 2065 6c73 653a  k).        else:
+00027640: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
+00027650: 7365 2052 756e 7469 6d65 4572 726f 7228  se RuntimeError(
+00027660: 6622 5468 6520 7b73 656c 662e 636c 735f  f"The {self.cls_
+00027670: 6e61 6d65 7d20 6f6e 6c79 2073 7570 706f  name} only suppo
+00027680: 7274 2073 6861 7264 696e 6720 7072 6f70  rt sharding prop
+00027690: 6167 6174 696f 6e20 6f72 2022 0a20 2020  agation or ".   
+000276a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000276b0: 2020 2020 2020 2020 2020 2020 6622 7365              f"se
+000276c0: 6d69 2d61 7574 6f20 7061 7261 6c6c 656c  mi-auto parallel
+000276d0: 206d 6f64 6520 6e6f 772e 2229 0a0a 2020   mode now.")..  
+000276e0: 2020 6465 6620 636f 6e73 7472 7563 7428    def construct(
+000276f0: 7365 6c66 2c20 6869 6464 656e 5f73 7461  self, hidden_sta
+00027700: 7465 732c 2061 7474 656e 7469 6f6e 5f6d  tes, attention_m
+00027710: 6173 6b2c 2069 6e69 745f 7265 7365 743d  ask, init_reset=
+00027720: 5472 7565 2c20 6261 7463 685f 7661 6c69  True, batch_vali
+00027730: 645f 6c65 6e67 7468 3d4e 6f6e 6529 3a0a  d_length=None):.
+00027740: 2020 2020 2020 2020 2222 2266 6f72 7761          """forwa
+00027750: 7264 2070 726f 6365 7373 2222 220a 2020  rd process""".  
+00027760: 2020 2020 2020 7072 6573 656e 745f 6c61        present_la
+00027770: 7965 7220 3d20 2829 0a20 2020 2020 2020  yer = ().       
+00027780: 2069 6620 7365 6c66 2e75 7365 5f6d 6f65   if self.use_moe
+00027790: 3a0a 2020 2020 2020 2020 2020 2020 6163  :.            ac
+000277a0: 6375 6d5f 6c6f 7373 203d 2073 656c 662e  cum_loss = self.
+000277b0: 6175 785f 6c6f 7373 0a20 2020 2020 2020  aux_loss.       
+000277c0: 2020 2020 2066 6f72 2069 2069 6e20 7261       for i in ra
+000277d0: 6e67 6528 7365 6c66 2e6e 756d 5f6c 6179  nge(self.num_lay
+000277e0: 6572 7329 3a0a 2020 2020 2020 2020 2020  ers):.          
+000277f0: 2020 2020 2020 6869 6464 656e 5f73 7461        hidden_sta
+00027800: 7465 732c 2070 7265 7365 6e74 2c20 6175  tes, present, au
+00027810: 785f 6c6f 7373 203d 2073 656c 662e 626c  x_loss = self.bl
+00027820: 6f63 6b73 5b69 5d28 6869 6464 656e 5f73  ocks[i](hidden_s
+00027830: 7461 7465 732c 0a20 2020 2020 2020 2020  tates,.         
 00027840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027850: 2020 6174 7465 6e74 696f 6e5f 6d61 736b    attention_mask
-00027860: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00027870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027870: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
+00027880: 6f6e 5f6d 6173 6b2c 0a20 2020 2020 2020  on_mask,.       
 00027890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000278a0: 2020 2020 696e 6974 5f72 6573 6574 2c0a      init_reset,.
+000278a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000278b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000278c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000278d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000278c0: 2020 2020 2020 2020 2020 2069 6e69 745f             init_
+000278d0: 7265 7365 742c 0a20 2020 2020 2020 2020  reset,.         
 000278e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000278f0: 2020 6261 7463 685f 7661 6c69 645f 6c65    batch_valid_le
-00027900: 6e67 7468 290a 2020 2020 2020 2020 2020  ngth).          
-00027910: 2020 2020 2020 7072 6573 656e 745f 6c61        present_la
-00027920: 7965 7220 3d20 7072 6573 656e 745f 6c61  yer = present_la
-00027930: 7965 7220 2b20 2870 7265 7365 6e74 2c29  yer + (present,)
-00027940: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00027950: 2061 6363 756d 5f6c 6f73 7320 3d20 7365   accum_loss = se
-00027960: 6c66 2e61 6464 2861 6363 756d 5f6c 6f73  lf.add(accum_los
-00027970: 732c 2061 7578 5f6c 6f73 7329 0a20 2020  s, aux_loss).   
-00027980: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-00027990: 6869 6464 656e 5f73 7461 7465 732c 2070  hidden_states, p
-000279a0: 7265 7365 6e74 5f6c 6179 6572 2c20 6163  resent_layer, ac
-000279b0: 6375 6d5f 6c6f 7373 0a0a 2020 2020 2020  cum_loss..      
-000279c0: 2020 666f 7220 6920 696e 2072 616e 6765    for i in range
-000279d0: 2873 656c 662e 6e75 6d5f 6c61 7965 7273  (self.num_layers
-000279e0: 293a 0a20 2020 2020 2020 2020 2020 2068  ):.            h
-000279f0: 6964 6465 6e5f 7374 6174 6573 2c20 7072  idden_states, pr
-00027a00: 6573 656e 7420 3d20 7365 6c66 2e62 6c6f  esent = self.blo
-00027a10: 636b 735b 695d 2868 6964 6465 6e5f 7374  cks[i](hidden_st
-00027a20: 6174 6573 2c0a 2020 2020 2020 2020 2020  ates,.          
-00027a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027a50: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-00027a60: 696f 6e5f 6d61 736b 2c0a 2020 2020 2020  ion_mask,.      
+000278f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027910: 2020 2020 2020 2020 2062 6174 6368 5f76           batch_v
+00027920: 616c 6964 5f6c 656e 6774 6829 0a20 2020  alid_length).   
+00027930: 2020 2020 2020 2020 2020 2020 2070 7265               pre
+00027940: 7365 6e74 5f6c 6179 6572 203d 2070 7265  sent_layer = pre
+00027950: 7365 6e74 5f6c 6179 6572 202b 2028 7072  sent_layer + (pr
+00027960: 6573 656e 742c 290a 2020 2020 2020 2020  esent,).        
+00027970: 2020 2020 2020 2020 6163 6375 6d5f 6c6f          accum_lo
+00027980: 7373 203d 2073 656c 662e 6164 6428 6163  ss = self.add(ac
+00027990: 6375 6d5f 6c6f 7373 2c20 6175 785f 6c6f  cum_loss, aux_lo
+000279a0: 7373 290a 2020 2020 2020 2020 2020 2020  ss).            
+000279b0: 7265 7475 726e 2068 6964 6465 6e5f 7374  return hidden_st
+000279c0: 6174 6573 2c20 7072 6573 656e 745f 6c61  ates, present_la
+000279d0: 7965 722c 2061 6363 756d 5f6c 6f73 730a  yer, accum_loss.
+000279e0: 0a20 2020 2020 2020 2066 6f72 2069 2069  .        for i i
+000279f0: 6e20 7261 6e67 6528 7365 6c66 2e6e 756d  n range(self.num
+00027a00: 5f6c 6179 6572 7329 3a0a 2020 2020 2020  _layers):.      
+00027a10: 2020 2020 2020 6869 6464 656e 5f73 7461        hidden_sta
+00027a20: 7465 732c 2070 7265 7365 6e74 203d 2073  tes, present = s
+00027a30: 656c 662e 626c 6f63 6b73 5b69 5d28 6869  elf.blocks[i](hi
+00027a40: 6464 656e 5f73 7461 7465 732c 0a20 2020  dden_states,.   
+00027a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00027a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027a90: 2020 2020 2020 2020 2020 2020 2020 696e                in
-00027aa0: 6974 5f72 6573 6574 2c0a 2020 2020 2020  it_reset,.      
+00027a80: 2061 7474 656e 7469 6f6e 5f6d 6173 6b2c   attention_mask,
+00027a90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00027aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00027ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027ad0: 2020 2020 2020 2020 2020 2020 2020 6261                ba
-00027ae0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-00027af0: 290a 2020 2020 2020 2020 2020 2020 7072  ).            pr
-00027b00: 6573 656e 745f 6c61 7965 7220 3d20 7072  esent_layer = pr
-00027b10: 6573 656e 745f 6c61 7965 7220 2b20 2870  esent_layer + (p
-00027b20: 7265 7365 6e74 2c29 0a0a 2020 2020 2020  resent,)..      
-00027b30: 2020 7265 7475 726e 2068 6964 6465 6e5f    return hidden_
-00027b40: 7374 6174 6573 2c20 7072 6573 656e 745f  states, present_
-00027b50: 6c61 7965 720a 0a0a 636c 6173 7320 5472  layer...class Tr
-00027b60: 616e 7366 6f72 6d65 7244 6563 6f64 6572  ansformerDecoder
-00027b70: 2843 656c 6c29 3a0a 2020 2020 7222 2222  (Cell):.    r"""
-00027b80: 0a20 2020 2020 2020 2054 7261 6e73 666f  .        Transfo
-00027b90: 726d 6572 2044 6563 6f64 6572 206d 6f64  rmer Decoder mod
-00027ba0: 756c 6520 7769 7468 206d 756c 7469 2d6c  ule with multi-l
-00027bb0: 6179 6572 2073 7461 636b 6564 206f 6620  ayer stacked of 
-00027bc0: 6054 7261 6e73 666f 726d 6572 4465 636f  `TransformerDeco
-00027bd0: 6465 724c 6179 6572 602c 2069 6e63 6c75  derLayer`, inclu
-00027be0: 6469 6e67 206d 756c 7469 6865 6164 2073  ding multihead s
-00027bf0: 656c 660a 2020 2020 2020 2020 6174 7465  elf.        atte
-00027c00: 6e74 696f 6e2c 2063 726f 7373 2061 7474  ntion, cross att
-00027c10: 656e 7469 6f6e 2061 6e64 2066 6565 6466  ention and feedf
-00027c20: 6f72 7761 7264 206c 6179 6572 2e0a 0a20  orward layer... 
-00027c30: 2020 2020 2020 2041 7267 733a 0a20 2020         Args:.   
-00027c40: 2020 2020 2020 2020 206e 756d 5f6c 6179           num_lay
-00027c50: 6572 7328 696e 7429 3a20 5468 6520 6c61  ers(int): The la
-00027c60: 7965 7273 206f 6620 7468 6520 6054 7261  yers of the `Tra
-00027c70: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
-00027c80: 6179 6572 602e 0a20 2020 2020 2020 2020  ayer`..         
-00027c90: 2020 2062 6174 6368 5f73 697a 6528 696e     batch_size(in
-00027ca0: 7429 3a20 5468 6520 6261 7463 6820 7369  t): The batch si
-00027cb0: 7a65 206f 6620 7468 6520 696e 7075 7420  ze of the input 
-00027cc0: 7465 6e73 6f72 2077 6865 6e20 646f 2069  tensor when do i
-00027cd0: 6e63 7265 6e6d 656e 7461 6c20 7072 6564  ncrenmental pred
-00027ce0: 6963 7469 6f6e 2e20 5368 6f75 6c64 2062  iction. Should b
-00027cf0: 6520 6120 706f 7369 7469 7665 0a20 2020  e a positive.   
-00027d00: 2020 2020 2020 2020 2020 2020 2076 616c               val
-00027d10: 7565 2e20 5768 656e 2064 6f20 7472 6169  ue. When do trai
-00027d20: 6e69 6e67 206f 7220 7072 6564 6963 7469  ning or predicti
-00027d30: 6f6e 2c20 7468 6520 6172 6775 6d65 6e74  on, the argument
-00027d40: 2077 696c 6c20 6e6f 7420 776f 726b 2061   will not work a
-00027d50: 6e64 2074 6865 2075 7365 7220 6361 6e20  nd the user can 
-00027d60: 6a75 7374 2070 6173 7320 4e6f 6e65 2074  just pass None t
-00027d70: 6f0a 2020 2020 2020 2020 2020 2020 2020  o.              
-00027d80: 2020 7468 6520 6172 6775 6d65 6e74 2e0a    the argument..
-00027d90: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-00027da0: 656e 5f73 697a 6528 696e 7429 3a20 5468  en_size(int): Th
-00027db0: 6520 6869 6464 656e 2073 697a 6520 6f66  e hidden size of
-00027dc0: 2074 6865 2069 6e70 7574 2e0a 2020 2020   the input..    
-00027dd0: 2020 2020 2020 2020 6666 6e5f 6869 6464          ffn_hidd
-00027de0: 656e 5f73 697a 6528 696e 7429 3a20 5468  en_size(int): Th
-00027df0: 6520 6869 6464 656e 2073 697a 6520 6f66  e hidden size of
-00027e00: 2062 6f74 746c 656e 6563 6b20 696e 2074   bottleneck in t
-00027e10: 6865 2066 6565 6466 6f72 7761 7264 206c  he feedforward l
-00027e20: 6179 6572 2e0a 2020 2020 2020 2020 2020  ayer..          
-00027e30: 2020 7372 635f 7365 715f 6c65 6e67 7468    src_seq_length
-00027e40: 2869 6e74 293a 2054 6865 2069 6e70 7574  (int): The input
-00027e50: 2073 6f75 7263 6520 7365 7175 656e 6365   source sequence
-00027e60: 206c 656e 6774 682e 0a20 2020 2020 2020   length..       
-00027e70: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
-00027e80: 6774 6828 696e 7429 3a20 5468 6520 696e  gth(int): The in
-00027e90: 7075 7420 7461 7267 6574 2073 6571 7565  put target seque
-00027ea0: 6e63 6520 6c65 6e67 7468 2e0a 2020 2020  nce length..    
-00027eb0: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
-00027ec0: 7328 696e 7429 3a20 5468 6520 6e75 6d62  s(int): The numb
-00027ed0: 6572 206f 6620 7468 6520 6865 6164 732e  er of the heads.
-00027ee0: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
-00027ef0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
-00027f00: 6174 6528 666c 6f61 7429 3a20 5468 6520  ate(float): The 
-00027f10: 6472 6f70 6f75 7420 7261 7465 206f 6620  dropout rate of 
-00027f20: 7468 6520 6174 7465 6e74 696f 6e20 7363  the attention sc
-00027f30: 6f72 6573 2e20 4465 6661 756c 743a 302e  ores. Default:0.
-00027f40: 312e 0a20 2020 2020 2020 2020 2020 2068  1..            h
-00027f50: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-00027f60: 7465 2866 6c6f 6174 293a 2054 6865 2064  te(float): The d
-00027f70: 726f 706f 7574 2072 6174 6520 6f66 2074  ropout rate of t
-00027f80: 6865 2066 696e 616c 206f 7574 7075 7420  he final output 
-00027f90: 6f66 2074 6865 206c 6179 6572 2e20 4465  of the layer. De
-00027fa0: 6661 756c 743a 302e 312e 0a20 2020 2020  fault:0.1..     
-00027fb0: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
-00027fc0: 726e 6f72 6d5f 7265 7369 6475 616c 2862  rnorm_residual(b
-00027fd0: 6f6f 6c29 3a20 446f 2072 6573 6964 7561  ool): Do residua
-00027fe0: 6c73 2061 6464 7320 6265 666f 7265 2074  ls adds before t
-00027ff0: 6865 206c 6179 6572 6e6f 726d 2e20 4465  he layernorm. De
-00028000: 6661 756c 7420 4661 6c73 652e 0a20 2020  fault False..   
-00028010: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-00028020: 726d 5f63 6f6d 7075 7465 5f74 7970 6528  rm_compute_type(
-00028030: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-00028040: 6865 2063 6f6d 7075 7461 7469 6f6e 2074  he computation t
-00028050: 7970 6520 6f66 2074 6865 206c 6179 6572  ype of the layer
-00028060: 6e6f 726d 2e0a 2020 2020 2020 2020 2020  norm..          
-00028070: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
-00028080: 6d73 7479 7065 2e66 6c6f 6174 3332 206f  mstype.float32 o
-00028090: 7220 6d73 7479 7065 2e66 6c6f 6174 3136  r mstype.float16
-000280a0: 2e20 4465 6661 756c 7420 6d73 7479 7065  . Default mstype
-000280b0: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
-000280c0: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
-000280d0: 6d70 7574 655f 7479 7065 2864 7479 7065  mpute_type(dtype
-000280e0: 2e4e 756d 6265 7229 3a20 5468 6520 636f  .Number): The co
-000280f0: 6d70 7574 6174 696f 6e20 7479 7065 206f  mputation type o
-00028100: 6620 7468 6520 736f 6674 6d61 7820 696e  f the softmax in
-00028110: 2074 6865 2061 7474 656e 7469 6f6e 2e0a   the attention..
-00028120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028130: 5368 6f75 6c64 2062 6520 6d73 7479 7065  Should be mstype
-00028140: 2e66 6c6f 6174 3332 206f 7220 6d73 7479  .float32 or msty
-00028150: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-00028160: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
-00028170: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-00028180: 7061 7261 6d5f 696e 6974 5f74 7970 6528  param_init_type(
-00028190: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-000281a0: 6865 2070 6172 616d 6574 6572 2069 6e69  he parameter ini
-000281b0: 7469 616c 697a 6174 696f 6e20 7479 7065  tialization type
-000281c0: 206f 6620 7468 6520 6d6f 6475 6c65 2e0a   of the module..
-000281d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000281e0: 5368 6f75 6c64 2062 6520 6d73 7479 7065  Should be mstype
-000281f0: 2e66 6c6f 6174 3332 206f 7220 6d73 7479  .float32 or msty
-00028200: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-00028210: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
-00028220: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-00028230: 6869 6464 656e 5f61 6374 2028 7374 722c  hidden_act (str,
-00028240: 206e 6e2e 4365 6c6c 293a 2054 6865 2061   nn.Cell): The a
-00028250: 6374 6976 6174 696f 6e20 6f66 2074 6865  ctivation of the
-00028260: 2069 6e74 6572 6e61 6c20 6665 6564 666f   internal feedfo
-00028270: 7277 6172 6420 6c61 7965 722e 2053 7570  rward layer. Sup
-00028280: 706f 7274 7320 2772 656c 7527 2c0a 2020  ports 'relu',.  
-00028290: 2020 2020 2020 2020 2020 2020 2020 2772                'r
-000282a0: 656c 7536 272c 2027 7461 6e68 272c 2027  elu6', 'tanh', '
-000282b0: 6765 6c75 272c 2027 6661 7374 5f67 656c  gelu', 'fast_gel
-000282c0: 7527 2c20 2765 6c75 272c 2027 7369 676d  u', 'elu', 'sigm
-000282d0: 6f69 6427 2c20 2770 7265 6c75 272c 2027  oid', 'prelu', '
-000282e0: 6c65 616b 7972 656c 7527 2c20 2768 7377  leakyrelu', 'hsw
-000282f0: 6973 6827 2c0a 2020 2020 2020 2020 2020  ish',.          
-00028300: 2020 2020 2020 2768 7369 676d 6f69 6427        'hsigmoid'
-00028310: 2c20 276c 6f67 7369 676d 6f69 6427 2061  , 'logsigmoid' a
-00028320: 6e64 2073 6f20 6f6e 2e20 5573 6572 2063  nd so on. User c
-00028330: 616e 2070 726f 7669 6465 2063 7573 746f  an provide custo
-00028340: 6d20 6163 7469 7669 7469 6f6e 2074 6f20  m activition to 
-00028350: 7468 6520 6172 6775 6d65 6e74 2e0a 2020  the argument..  
-00028360: 2020 2020 2020 2020 2020 2020 2020 4966                If
-00028370: 2075 7365 7220 7761 6e74 7320 746f 2072   user wants to r
-00028380: 756e 2074 6865 206e 6574 2069 6e20 7468  un the net in th
-00028390: 6520 7061 7261 6c6c 656c 206d 6f64 652c  e parallel mode,
-000283a0: 2074 6865 2063 7573 746f 6d20 6163 7469   the custom acti
-000283b0: 7661 7469 6f6e 206d 7573 7420 616c 736f  vation must also
-000283c0: 2070 726f 7669 6465 0a20 2020 2020 2020   provide.       
-000283d0: 2020 2020 2020 2020 2074 6865 2060 6163           the `ac
-000283e0: 7469 7661 7469 6f6e 5f73 6861 7264 6020  tivation_shard` 
-000283f0: 6675 6e63 7469 6f6e 2e20 506c 6561 7365  function. Please
-00028400: 2073 6565 2074 6865 2065 7861 6d70 6c65   see the example
-00028410: 7320 6f66 2074 6865 0a20 2020 2020 2020  s of the.       
-00028420: 2020 2020 2020 2020 2063 6c61 7373 3a60           class:`
-00028430: 6d69 6e64 666f 726d 6572 732e 6d6f 6475  mindformers.modu
-00028440: 6c65 732e 7472 616e 7366 6f72 6d65 722e  les.transformer.
-00028450: 4665 6564 466f 7277 6172 6460 2e20 4465  FeedForward`. De
-00028460: 6661 756c 743a 2067 656c 752e 0a20 2020  fault: gelu..   
-00028470: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
-00028480: 6675 6e63 2866 756e 6374 696f 6e29 3a20  func(function): 
-00028490: 4120 6675 6e63 7469 6f6e 2063 616e 2064  A function can d
-000284a0: 6574 6572 6d69 6e65 2074 6865 2066 7573  etermine the fus
-000284b0: 696f 6e20 696e 6465 782c 0a20 2020 2020  ion index,.     
-000284c0: 2020 2020 2020 2020 2020 2070 6970 656c             pipel
-000284d0: 696e 6520 7374 6167 6573 2061 6e64 2072  ine stages and r
-000284e0: 6563 6f6d 7075 7465 2061 7474 7269 6275  ecompute attribu
-000284f0: 7465 2e20 4966 2074 6865 0a20 2020 2020  te. If the.     
-00028500: 2020 2020 2020 2020 2020 2075 7365 7220             user 
-00028510: 7761 6e74 7320 746f 2064 6574 6572 6d69  wants to determi
-00028520: 6e65 2074 6865 2070 6970 656c 696e 6520  ne the pipeline 
-00028530: 7374 6167 6520 616e 6420 6772 6164 6965  stage and gradie
-00028540: 6e74 2061 6767 7265 6761 7469 6f6e 2066  nt aggregation f
-00028550: 7573 696f 6e2c 2074 6865 2075 7365 7220  usion, the user 
-00028560: 6361 6e20 7061 7373 2061 0a20 2020 2020  can pass a.     
-00028570: 2020 2020 2020 2020 2020 2066 756e 6374             funct
-00028580: 696f 6e20 7468 6174 2061 6363 6570 7473  ion that accepts
-00028590: 2060 6e65 7477 6f72 6b60 2c20 606c 6179   `network`, `lay
-000285a0: 6572 5f69 6460 2c20 606f 6666 7365 7460  er_id`, `offset`
-000285b0: 2c20 6070 6172 616c 6c65 6c5f 636f 6e66  , `parallel_conf
-000285c0: 6967 602c 2060 6c61 7965 7273 602e 2054  ig`, `layers`. T
-000285d0: 6865 2060 6e65 7477 6f72 6b28 4365 6c6c  he `network(Cell
-000285e0: 2960 0a20 2020 2020 2020 2020 2020 2020  )`.             
-000285f0: 2020 2072 6570 7265 7365 6e74 7320 7468     represents th
-00028600: 6520 7472 616e 7366 6f72 6d65 7220 626c  e transformer bl
-00028610: 6f63 6b2c 2060 6c61 7965 725f 6964 2869  ock, `layer_id(i
-00028620: 6e74 2960 206d 6561 6e73 2074 6865 206c  nt)` means the l
-00028630: 6179 6572 2069 6e64 6578 2066 6f72 2074  ayer index for t
-00028640: 6865 2063 7572 7265 6e74 206d 6f64 756c  he current modul
-00028650: 652c 2063 6f75 6e74 730a 2020 2020 2020  e, counts.      
-00028660: 2020 2020 2020 2020 2020 6672 6f6d 207a            from z
-00028670: 6572 6f2c 2060 6f66 6673 6574 2869 6e74  ero, `offset(int
-00028680: 2960 206d 6561 6e73 2074 6865 206c 6179  )` means the lay
-00028690: 6572 5f69 6e64 6578 206e 6565 6473 2061  er_index needs a
-000286a0: 6e20 6f66 6673 6574 2c20 6966 2074 6865  n offset, if the
-000286b0: 7265 2061 7265 206f 7468 6572 206d 6f64  re are other mod
-000286c0: 756c 6573 2069 6e20 7468 6520 6e65 742e  ules in the net.
-000286d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000286e0: 2054 6865 2064 6566 6175 6c74 2073 6574   The default set
-000286f0: 7469 6e67 2066 6f72 2074 6865 2070 6970  ting for the pip
-00028700: 656c 696e 6520 6973 3a20 6028 6c61 7965  eline is: `(laye
-00028710: 725f 6964 202b 206f 6666 7365 7429 202f  r_id + offset) /
-00028720: 2f20 286c 6179 6572 7320 2f20 7069 7065  / (layers / pipe
-00028730: 6c69 6e65 5f73 7461 6765 2960 2e0a 2020  line_stage)`..  
-00028740: 2020 2020 2020 2020 2020 2020 2020 4465                De
-00028750: 6661 756c 743a 204e 6f6e 652e 0a20 2020  fault: None..   
-00028760: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-00028770: 7428 626f 6f6c 293a 2055 7365 2074 6865  t(bool): Use the
-00028780: 2070 6173 7420 7374 6174 6520 746f 2063   past state to c
-00028790: 6f6d 7075 7465 2c20 7573 6564 2066 6f72  ompute, used for
-000287a0: 2069 6e63 7265 6d65 6e74 616c 2070 7265   incremental pre
-000287b0: 6469 6374 696f 6e2e 2044 6566 6175 6c74  diction. Default
-000287c0: 2046 616c 7365 2e0a 2020 2020 2020 2020   False..        
-000287d0: 2020 2020 6f66 6673 6574 2869 6e74 293a      offset(int):
-000287e0: 2054 6865 2069 6e69 7469 616c 206c 6179   The initial lay
-000287f0: 6572 2069 6e64 6578 2066 6f72 2074 6865  er index for the
-00028800: 2060 6465 636f 6465 7260 2e20 5573 6564   `decoder`. Used
-00028810: 2066 6f72 2073 6574 7469 6e67 2074 6865   for setting the
-00028820: 2066 7573 696f 6e20 6964 2061 6e64 2073   fusion id and s
-00028830: 7461 6765 2069 642c 2074 6f20 6e6f 740a  tage id, to not.
-00028840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028850: 6f76 6572 6c61 7020 7769 7468 2074 6865  overlap with the
-00028860: 2065 6e63 6f64 6572 206c 6179 6572 2e20   encoder layer. 
-00028870: 4465 6661 756c 7420 302e 0a20 2020 2020  Default 0..     
-00028880: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
-00028890: 6728 4d6f 4543 6f6e 6669 6729 3a20 5468  g(MoEConfig): Th
-000288a0: 6520 636f 6e66 6967 7572 6174 696f 6e20  e configuration 
-000288b0: 6f66 204d 6f45 2028 4d69 7874 7572 6520  of MoE (Mixture 
-000288c0: 6f66 2045 7870 6572 7429 2e20 4465 6661  of Expert). Defa
-000288d0: 756c 7420 6973 2061 6e20 696e 7374 616e  ult is an instan
-000288e0: 6365 206f 6620 4d6f 4543 6f6e 6669 670a  ce of MoEConfig.
-000288f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028900: 7769 7468 2064 6566 6175 6c74 2076 616c  with default val
-00028910: 7565 732e 2050 6c65 6173 6520 7365 6520  ues. Please see 
-00028920: 604d 6f45 436f 6e66 6967 602e 0a20 2020  `MoEConfig`..   
-00028930: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-00028940: 6c5f 636f 6e66 6967 2854 7261 6e73 666f  l_config(Transfo
-00028950: 726d 6572 4f70 5061 7261 6c6c 656c 436f  rmerOpParallelCo
-00028960: 6e66 6967 293a 2054 6865 2070 6172 616c  nfig): The paral
-00028970: 6c65 6c20 636f 6e66 6967 7572 652e 2044  lel configure. D
-00028980: 6566 6175 6c74 2060 6465 6661 756c 745f  efault `default_
-00028990: 7472 616e 7366 6f72 6d65 725f 636f 6e66  transformer_conf
-000289a0: 6967 602c 0a20 2020 2020 2020 2020 2020  ig`,.           
-000289b0: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
-000289c0: 206f 6620 6054 7261 6e73 666f 726d 6572   of `Transformer
-000289d0: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
-000289e0: 6020 7769 7468 2064 6566 6175 6c74 2061  ` with default a
-000289f0: 7267 732e 0a0a 2020 2020 2020 2020 496e  rgs...        In
-00028a00: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
-00028a10: 2020 2d20 2a2a 6869 6464 656e 5f73 7461    - **hidden_sta
-00028a20: 7473 2a2a 2028 5465 6e73 6f72 2920 2d20  ts** (Tensor) - 
-00028a30: 5468 6520 696e 7075 7420 7465 6e73 6f72  The input tensor
-00028a40: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
-00028a50: 6368 5f73 697a 652c 2073 6571 5f6c 656e  ch_size, seq_len
-00028a60: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-00028a70: 5d20 6f72 0a20 2020 2020 2020 2020 2020  ] or.           
-00028a80: 2020 205b 6261 7463 685f 7369 7a65 202a     [batch_size *
-00028a90: 2073 6571 5f6c 656e 6774 682c 2068 6964   seq_length, hid
-00028aa0: 6465 6e5f 7369 7a65 5d0a 2020 2020 2020  den_size].      
-00028ab0: 2020 2020 2020 2d20 2a2a 6174 7465 6e74        - **attent
-00028ac0: 696f 6e5f 6d61 736b 2a2a 2028 5465 6e73  ion_mask** (Tens
-00028ad0: 6f72 2920 2d20 5468 6520 6174 7465 6e74  or) - The attent
-00028ae0: 696f 6e20 6d61 736b 2066 6f72 2064 6563  ion mask for dec
-00028af0: 6f64 6572 2077 6974 6820 7368 6170 650a  oder with shape.
-00028b00: 2020 2020 2020 2020 2020 2020 2020 5b62                [b
-00028b10: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
-00028b20: 656e 6774 682c 2073 6571 5f6c 656e 6774  ength, seq_lengt
-00028b30: 685d 206f 7220 4e6f 6e65 2e20 4e6f 6e65  h] or None. None
-00028b40: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
-00028b50: 6c20 6265 206e 6f20 6d61 736b 2069 6e20  l be no mask in 
-00028b60: 736f 6674 6d61 780a 2020 2020 2020 2020  softmax.        
-00028b70: 2020 2020 2020 636f 6d70 7574 6174 696f        computatio
-00028b80: 6e20 696e 2073 656c 6620 6174 7465 6e74  n in self attent
-00028b90: 696f 6e2e 0a20 2020 2020 2020 2020 2020  ion..           
-00028ba0: 202d 202a 2a65 6e63 6f64 6572 5f6f 7574   - **encoder_out
-00028bb0: 7075 742a 2a20 2854 656e 736f 7229 202d  put** (Tensor) -
-00028bc0: 2054 6865 206f 7574 7075 7420 6f66 2074   The output of t
-00028bd0: 6865 2065 6e63 6f64 6572 2077 6974 6820  he encoder with 
-00028be0: 7368 6170 6520 5b62 6174 6368 5f73 697a  shape [batch_siz
-00028bf0: 652c 2073 6571 5f6c 656e 6774 682c 2068  e, seq_length, h
-00028c00: 6964 6465 6e5f 7369 7a65 5d0a 2020 2020  idden_size].    
-00028c10: 2020 2020 2020 2020 2020 6f72 205b 6261            or [ba
-00028c20: 7463 685f 7369 7a65 202a 2073 6571 5f6c  tch_size * seq_l
-00028c30: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-00028c40: 7a65 5d2e 204e 6f74 6520 7468 6973 2061  ze]. Note this a
-00028c50: 7267 7320 6361 6e20 6e6f 7420 6265 2070  rgs can not be p
-00028c60: 6173 7365 6420 6279 204e 6f6e 6520 7768  assed by None wh
-00028c70: 656e 2074 6865 206e 6574 2069 7320 696e  en the net is in
-00028c80: 0a20 2020 2020 2020 2020 2020 2020 206f  .              o
-00028c90: 7574 6572 6d6f 7374 206c 6179 6572 2e20  utermost layer. 
-00028ca0: 4465 6661 756c 7420 4e6f 6e65 2e0a 2020  Default None..  
-00028cb0: 2020 2020 2020 2020 2020 2d20 2a2a 6d65            - **me
-00028cc0: 6d6f 7279 5f6d 6173 6b2a 2a20 2854 656e  mory_mask** (Ten
-00028cd0: 736f 7229 202d 2054 6865 206d 656d 6f72  sor) - The memor
-00028ce0: 7920 6d61 736b 206f 6620 7468 6520 6372  y mask of the cr
-00028cf0: 6f73 7320 6174 7465 6e74 696f 6e20 7769  oss attention wi
-00028d00: 7468 2073 6861 7065 205b 6261 7463 682c  th shape [batch,
-00028d10: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
-00028d20: 0a20 2020 2020 2020 2020 2020 2020 2073  .              s
-00028d30: 7263 5f73 6571 5f6c 656e 6774 685d 2077  rc_seq_length] w
-00028d40: 6865 7265 2074 6774 5f73 6571 5f6c 656e  here tgt_seq_len
-00028d50: 6774 6820 6973 2074 6865 206c 656e 6774  gth is the lengt
-00028d60: 6820 6f66 2074 6865 2064 6563 6f64 6572  h of the decoder
-00028d70: 2e20 5468 6520 7573 6572 2063 616e 2061  . The user can a
-00028d80: 6c73 6f20 7061 7373 204e 6f6e 652e 204e  lso pass None. N
-00028d90: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
-00028da0: 2020 6d65 616e 7320 7468 6572 6520 7769    means there wi
-00028db0: 6c6c 2062 6520 6e6f 206d 6173 6b20 696e  ll be no mask in
-00028dc0: 2073 6f66 746d 6178 2063 6f6d 7075 7461   softmax computa
-00028dd0: 7469 6f6e 2069 6e20 6372 6f73 7320 6174  tion in cross at
-00028de0: 7465 6e74 696f 6e2e 2044 6566 6175 6c74  tention. Default
-00028df0: 204e 6f6e 652e 0a20 2020 2020 2020 2020   None..         
-00028e00: 2020 202d 202a 2a69 6e69 745f 7265 7365     - **init_rese
-00028e10: 742a 2a20 2854 656e 736f 7229 202d 2041  t** (Tensor) - A
-00028e20: 2062 6f6f 6c20 7465 6e73 6f72 2077 6974   bool tensor wit
-00028e30: 6820 7368 6170 6520 5b31 5d2c 2075 7365  h shape [1], use
-00028e40: 6420 746f 2063 6c65 6172 2074 6865 2070  d to clear the p
-00028e50: 6173 7420 6b65 7920 7061 7261 6d65 7465  ast key paramete
-00028e60: 7220 616e 640a 2020 2020 2020 2020 2020  r and.          
-00028e70: 2020 2020 7061 7374 2076 616c 7565 2070      past value p
-00028e80: 6172 616d 6574 6572 2075 7365 6420 696e  arameter used in
-00028e90: 2074 6865 2069 6e63 7265 6d65 6e74 616c   the incremental
-00028ea0: 2070 7265 6469 6374 696f 6e2e 204f 6e6c   prediction. Onl
-00028eb0: 7920 7661 6c69 6420 7768 656e 2075 7365  y valid when use
-00028ec0: 5f70 6173 7420 6973 2054 7275 652e 2044  _past is True. D
-00028ed0: 6566 6175 6c74 2054 7275 652e 0a20 2020  efault True..   
-00028ee0: 2020 2020 2020 2020 202d 202a 2a62 6174           - **bat
-00028ef0: 6368 5f76 616c 6964 5f6c 656e 6774 682a  ch_valid_length*
-00028f00: 2a20 2854 656e 736f 7229 202d 2049 6e74  * (Tensor) - Int
-00028f10: 3332 2074 656e 736f 7220 7769 7468 2073  32 tensor with s
-00028f20: 6861 7065 205b 6261 7463 685f 7369 7a65  hape [batch_size
-00028f30: 5d20 7468 6520 7061 7374 2063 616c 6375  ] the past calcu
-00028f40: 6c61 7465 6420 7468 6520 696e 6465 782e  lated the index.
-00028f50: 0a20 2020 2020 2020 2020 2020 2020 2055  .              U
-00028f60: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
-00028f70: 7461 6c20 7072 6564 6963 7469 6f6e 2077  tal prediction w
-00028f80: 6865 6e20 7468 6520 7573 655f 7061 7374  hen the use_past
-00028f90: 2069 7320 5472 7565 2e20 4465 6661 756c   is True. Defaul
-00028fa0: 7420 4e6f 6e65 2e0a 0a20 2020 2020 2020  t None...       
-00028fb0: 204f 7574 7075 7473 3a0a 2020 2020 2020   Outputs:.      
-00028fc0: 2020 2020 2020 5475 706c 652c 2061 2074        Tuple, a t
-00028fd0: 7570 6c65 2063 6f6e 7461 696e 7328 606f  uple contains(`o
-00028fe0: 7574 7075 7460 2c20 606c 6179 6572 5f70  utput`, `layer_p
-00028ff0: 7265 7365 6e74 6029 0a0a 2020 2020 2020  resent`)..      
-00029000: 2020 2020 2020 2d20 2a2a 6f75 7470 7574        - **output
-00029010: 2a2a 2028 5465 6e73 6f72 2920 2d20 5468  ** (Tensor) - Th
-00029020: 6520 6f75 7470 7574 206c 6f67 6974 206f  e output logit o
-00029030: 6620 7468 6973 206c 6179 6572 2e20 5468  f this layer. Th
-00029040: 6520 7368 6170 6520 6973 205b 6261 7463  e shape is [batc
-00029050: 682c 2074 6774 5f73 6571 5f6c 656e 6774  h, tgt_seq_lengt
-00029060: 682c 2068 6964 6465 6e5f 7369 7a65 5d20  h, hidden_size] 
-00029070: 6f72 0a20 2020 2020 2020 2020 2020 2020  or.             
-00029080: 205b 6261 7463 6820 2a20 7467 745f 7365   [batch * tgt_se
-00029090: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-000290a0: 5f73 697a 655d 0a20 2020 2020 2020 2020  _size].         
-000290b0: 2020 202d 202a 2a6c 6179 6572 5f70 7265     - **layer_pre
-000290c0: 7365 6e74 2a2a 2028 5475 706c 6529 202d  sent** (Tuple) -
-000290d0: 2041 2074 7570 6c65 2077 6974 6820 7369   A tuple with si
-000290e0: 7a65 206f 6620 6e75 6d5f 6c61 7965 7273  ze of num_layers
-000290f0: 2c20 7768 6572 6520 6561 6368 2074 7570  , where each tup
-00029100: 6c65 2069 7320 7468 6520 7465 6e73 6f72  le is the tensor
-00029110: 206f 6620 7468 650a 2020 2020 2020 2020   of the.        
-00029120: 2020 2020 2020 7072 6f6a 6563 7465 6420        projected 
-00029130: 6b65 7920 616e 6420 7661 6c75 6520 7665  key and value ve
-00029140: 6374 6f72 2069 6e20 7365 6c66 2061 7474  ctor in self att
-00029150: 656e 7469 6f6e 2077 6974 6820 7368 6170  ention with shap
-00029160: 6520 2828 6261 7463 685f 7369 7a65 2c20  e ((batch_size, 
-00029170: 6e75 6d5f 6865 6164 732c 2073 697a 655f  num_heads, size_
-00029180: 7065 725f 6865 6164 2c0a 2020 2020 2020  per_head,.      
-00029190: 2020 2020 2020 2020 7467 745f 7365 715f          tgt_seq_
-000291a0: 6c65 6e67 7468 292c 2028 6261 7463 685f  length), (batch_
-000291b0: 7369 7a65 2c20 6e75 6d5f 6865 6164 732c  size, num_heads,
-000291c0: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
-000291d0: 2073 697a 655f 7065 725f 6865 6164 292c   size_per_head),
-000291e0: 2061 6e64 206f 6620 7468 6520 7072 6f6a   and of the proj
-000291f0: 6563 7465 6420 6b65 790a 2020 2020 2020  ected key.      
-00029200: 2020 2020 2020 2020 616e 6420 7661 6c75          and valu
-00029210: 6520 7665 6374 6f72 2069 6e20 6372 6f73  e vector in cros
-00029220: 7320 6174 7465 6e74 696f 6e20 7769 7468  s attention with
-00029230: 2073 6861 7065 2020 2862 6174 6368 5f73   shape  (batch_s
-00029240: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
-00029250: 7369 7a65 5f70 6572 5f68 6561 642c 2073  size_per_head, s
-00029260: 7263 5f73 6571 5f6c 656e 6774 6829 2c0a  rc_seq_length),.
-00029270: 2020 2020 2020 2020 2020 2020 2020 2862                (b
-00029280: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
-00029290: 6561 6473 2c20 7372 635f 7365 715f 6c65  eads, src_seq_le
-000292a0: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
-000292b0: 6561 6429 292e 0a0a 2020 2020 2020 2020  ead))...        
-000292c0: 5375 7070 6f72 7465 6420 506c 6174 666f  Supported Platfo
-000292d0: 726d 733a 0a20 2020 2020 2020 2020 2020  rms:.           
-000292e0: 2060 6041 7363 656e 6460 6020 6060 4750   ``Ascend`` ``GP
-000292f0: 5560 600a 0a20 2020 2020 2020 2045 7861  U``..        Exa
-00029300: 6d70 6c65 733a 0a20 2020 2020 2020 2020  mples:.         
-00029310: 2020 203e 3e3e 2069 6d70 6f72 7420 6e75     >>> import nu
-00029320: 6d70 7920 6173 206e 700a 2020 2020 2020  mpy as np.      
-00029330: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
-00029340: 696e 6473 706f 7265 2069 6d70 6f72 7420  indspore import 
-00029350: 6474 7970 6520 6173 206d 7374 7970 650a  dtype as mstype.
-00029360: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00029370: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
-00029380: 2e6d 6f64 756c 6573 2e74 7261 6e73 666f  .modules.transfo
-00029390: 726d 6572 2069 6d70 6f72 7420 5472 616e  rmer import Tran
-000293a0: 7366 6f72 6d65 7244 6563 6f64 6572 0a20  sformerDecoder. 
-000293b0: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
-000293c0: 726f 6d20 6d69 6e64 7370 6f72 6520 696d  rom mindspore im
-000293d0: 706f 7274 2054 656e 736f 720a 2020 2020  port Tensor.    
-000293e0: 2020 2020 2020 2020 3e3e 3e20 6d6f 6465          >>> mode
-000293f0: 6c20 3d20 5472 616e 7366 6f72 6d65 7244  l = TransformerD
-00029400: 6563 6f64 6572 2862 6174 6368 5f73 697a  ecoder(batch_siz
-00029410: 653d 322c 206e 756d 5f6c 6179 6572 733d  e=2, num_layers=
-00029420: 312c 2068 6964 6465 6e5f 7369 7a65 3d36  1, hidden_size=6
-00029430: 342c 2066 666e 5f68 6964 6465 6e5f 7369  4, ffn_hidden_si
-00029440: 7a65 3d36 342c 0a20 2020 2020 2020 2020  ze=64,.         
-00029450: 2020 202e 2e2e 2020 2020 2020 2020 2020     ...          
-00029460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029470: 2020 6e75 6d5f 6865 6164 733d 322c 2073    num_heads=2, s
-00029480: 7263 5f73 6571 5f6c 656e 6774 683d 3230  rc_seq_length=20
-00029490: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
-000294a0: 3d31 3029 0a20 2020 2020 2020 2020 2020  =10).           
-000294b0: 203e 3e3e 2065 6e63 6f64 6572 5f69 6e70   >>> encoder_inp
-000294c0: 7574 5f76 616c 7565 203d 2054 656e 736f  ut_value = Tenso
-000294d0: 7228 6e70 2e6f 6e65 7328 2832 2c20 3230  r(np.ones((2, 20
-000294e0: 2c20 3634 2929 2c20 6d73 7479 7065 2e66  , 64)), mstype.f
-000294f0: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
-00029500: 2020 2020 3e3e 3e20 6465 636f 6465 725f      >>> decoder_
-00029510: 696e 7075 745f 7661 6c75 6520 3d20 5465  input_value = Te
-00029520: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-00029530: 2031 302c 2036 3429 292c 206d 7374 7970   10, 64)), mstyp
-00029540: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
-00029550: 2020 2020 2020 203e 3e3e 2064 6563 6f64         >>> decod
-00029560: 6572 5f69 6e70 7574 5f6d 6173 6b20 3d20  er_input_mask = 
-00029570: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
-00029580: 322c 2031 302c 2031 3029 292c 206d 7374  2, 10, 10)), mst
-00029590: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
-000295a0: 2020 2020 2020 2020 203e 3e3e 206d 656d           >>> mem
-000295b0: 6f72 795f 6d61 736b 203d 2054 656e 736f  ory_mask = Tenso
-000295c0: 7228 6e70 2e6f 6e65 7328 2832 2c20 3130  r(np.ones((2, 10
-000295d0: 2c20 3230 2929 2c20 6d73 7479 7065 2e66  , 20)), mstype.f
-000295e0: 6c6f 6174 3136 290a 2020 2020 2020 2020  loat16).        
-000295f0: 2020 2020 3e3e 3e20 6f75 7470 7574 2c20      >>> output, 
-00029600: 7061 7374 203d 206d 6f64 656c 2864 6563  past = model(dec
-00029610: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
-00029620: 2c20 6465 636f 6465 725f 696e 7075 745f  , decoder_input_
-00029630: 6d61 736b 2c20 656e 636f 6465 725f 696e  mask, encoder_in
-00029640: 7075 745f 7661 6c75 652c 206d 656d 6f72  put_value, memor
-00029650: 795f 6d61 736b 290a 2020 2020 2020 2020  y_mask).        
-00029660: 2020 2020 3e3e 3e20 7072 696e 7428 6f75      >>> print(ou
-00029670: 7470 7574 2e73 6861 7065 290a 2020 2020  tput.shape).    
-00029680: 2020 2020 2020 2020 2832 2c20 3130 2c20          (2, 10, 
-00029690: 3634 290a 2020 2020 2020 2020 2020 2020  64).            
-000296a0: 3e3e 3e20 7072 696e 7428 6c65 6e28 7061  >>> print(len(pa
-000296b0: 7374 2929 0a20 2020 2020 2020 2020 2020  st)).           
-000296c0: 2031 0a20 2020 2020 2020 2020 2020 203e   1.            >
-000296d0: 3e3e 2070 7269 6e74 2870 6173 745b 305d  >> print(past[0]
-000296e0: 5b30 5d2e 7368 6170 6529 0a20 2020 2020  [0].shape).     
-000296f0: 2020 2020 2020 2028 322c 2032 2c20 3332         (2, 2, 32
-00029700: 2c20 3130 290a 2020 2020 2020 2020 2020  , 10).          
-00029710: 2020 3e3e 3e20 7072 696e 7428 7061 7374    >>> print(past
-00029720: 5b30 5d5b 315d 2e73 6861 7065 290a 2020  [0][1].shape).  
-00029730: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-00029740: 2031 302c 2033 3229 0a20 2020 2020 2020   10, 32).       
-00029750: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
-00029760: 6173 745b 305d 5b32 5d2e 7368 6170 6529  ast[0][2].shape)
-00029770: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-00029780: 2032 2c20 3332 2c20 3230 290a 2020 2020   2, 32, 20).    
-00029790: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-000297a0: 7428 7061 7374 5b30 5d5b 335d 2e73 6861  t(past[0][3].sha
-000297b0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-000297c0: 2832 2c20 322c 2032 302c 2033 3229 0a20  (2, 2, 20, 32). 
-000297d0: 2020 2022 2222 0a0a 2020 2020 405f 4c6f     """..    @_Lo
-000297e0: 6741 6374 696f 6e4f 6e63 6528 6d5f 6c6f  gActionOnce(m_lo
-000297f0: 6767 6572 3d6c 6f67 6765 722c 206b 6579  gger=logger, key
-00029800: 3d27 5472 616e 7366 6f72 6d65 7244 6563  ='TransformerDec
-00029810: 6f64 6572 272c 0a20 2020 2020 2020 2020  oder',.         
-00029820: 2020 2020 2020 2020 2020 206e 6f5f 7761             no_wa
-00029830: 726e 696e 673d 5f67 6574 5f70 6172 616c  rning=_get_paral
-00029840: 6c65 6c5f 6d6f 6465 2829 2069 6e20 2850  lel_mode() in (P
-00029850: 6172 616c 6c65 6c4d 6f64 652e 5354 414e  arallelMode.STAN
-00029860: 445f 414c 4f4e 452c 2929 0a20 2020 2040  D_ALONE,)).    @
-00029870: 5f61 7267 735f 7479 7065 5f76 616c 6964  _args_type_valid
-00029880: 6174 6f72 5f63 6865 636b 2868 6964 6465  ator_check(hidde
-00029890: 6e5f 7369 7a65 3d56 616c 6964 6174 6f72  n_size=Validator
-000298a0: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-000298b0: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
-000298c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000298d0: 2020 2020 206e 756d 5f68 6561 6473 3d56       num_heads=V
-000298e0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-000298f0: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
-00029900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029910: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-00029920: 5f68 6964 6465 6e5f 7369 7a65 3d56 616c  _hidden_size=Val
-00029930: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-00029940: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
-00029950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029960: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
-00029970: 6571 5f6c 656e 6774 683d 5661 6c69 6461  eq_length=Valida
-00029980: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-00029990: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
-000299a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000299b0: 2020 2020 2020 2020 6e75 6d5f 6c61 7965          num_laye
-000299c0: 7273 3d56 616c 6964 6174 6f72 2e63 6865  rs=Validator.che
-000299d0: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
-000299e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000299f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a00: 2074 6774 5f73 6571 5f6c 656e 6774 683d   tgt_seq_length=
-00029a10: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00029a20: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
-00029a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a40: 2020 2020 2020 2020 2020 2020 2020 6f66                of
-00029a50: 6673 6574 3d56 616c 6964 6174 6f72 2e63  fset=Validator.c
-00029a60: 6865 636b 5f6e 6f6e 5f6e 6567 6174 6976  heck_non_negativ
-00029a70: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
-00029a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a90: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-00029aa0: 5f64 726f 706f 7574 5f72 6174 653d 5661  _dropout_rate=Va
-00029ab0: 6c69 6461 746f 722e 6368 6563 6b5f 6e6f  lidator.check_no
-00029ac0: 6e5f 6e65 6761 7469 7665 5f66 6c6f 6174  n_negative_float
-00029ad0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00029ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029af0: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
-00029b00: 5f72 6174 653d 5661 6c69 6461 746f 722e  _rate=Validator.
-00029b10: 6368 6563 6b5f 6e6f 6e5f 6e65 6761 7469  check_non_negati
-00029b20: 7665 5f66 6c6f 6174 2c0a 2020 2020 2020  ve_float,.      
-00029b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b40: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
-00029b50: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-00029b60: 6c3d 5661 6c69 6461 746f 722e 6368 6563  l=Validator.chec
-00029b70: 6b5f 626f 6f6c 2c0a 2020 2020 2020 2020  k_bool,.        
-00029b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b90: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
-00029ba0: 6d5f 636f 6d70 7574 655f 7479 7065 3d5f  m_compute_type=_
-00029bb0: 7661 6c69 645f 7661 6c75 655f 6368 6563  valid_value_chec
-00029bc0: 6b73 285b 6d73 7479 7065 2e66 6c6f 6174  ks([mstype.float
-00029bd0: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
-00029be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027ac0: 2020 2020 2069 6e69 745f 7265 7365 742c       init_reset,
+00027ad0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00027ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027b00: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
+00027b10: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
+00027b20: 2020 2020 2070 7265 7365 6e74 5f6c 6179       present_lay
+00027b30: 6572 203d 2070 7265 7365 6e74 5f6c 6179  er = present_lay
+00027b40: 6572 202b 2028 7072 6573 656e 742c 290a  er + (present,).
+00027b50: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00027b60: 6869 6464 656e 5f73 7461 7465 732c 2070  hidden_states, p
+00027b70: 7265 7365 6e74 5f6c 6179 6572 0a0a 0a63  resent_layer...c
+00027b80: 6c61 7373 2054 7261 6e73 666f 726d 6572  lass Transformer
+00027b90: 4465 636f 6465 7228 4365 6c6c 293a 0a20  Decoder(Cell):. 
+00027ba0: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
+00027bb0: 5472 616e 7366 6f72 6d65 7220 4465 636f  Transformer Deco
+00027bc0: 6465 7220 6d6f 6475 6c65 2077 6974 6820  der module with 
+00027bd0: 6d75 6c74 692d 6c61 7965 7220 7374 6163  multi-layer stac
+00027be0: 6b65 6420 6f66 2060 5472 616e 7366 6f72  ked of `Transfor
+00027bf0: 6d65 7244 6563 6f64 6572 4c61 7965 7260  merDecoderLayer`
+00027c00: 2c20 696e 636c 7564 696e 6720 6d75 6c74  , including mult
+00027c10: 6968 6561 6420 7365 6c66 0a20 2020 2020  ihead self.     
+00027c20: 2020 2061 7474 656e 7469 6f6e 2c20 6372     attention, cr
+00027c30: 6f73 7320 6174 7465 6e74 696f 6e20 616e  oss attention an
+00027c40: 6420 6665 6564 666f 7277 6172 6420 6c61  d feedforward la
+00027c50: 7965 722e 0a0a 2020 2020 2020 2020 4172  yer...        Ar
+00027c60: 6773 3a0a 2020 2020 2020 2020 2020 2020  gs:.            
+00027c70: 6e75 6d5f 6c61 7965 7273 2869 6e74 293a  num_layers(int):
+00027c80: 2054 6865 206c 6179 6572 7320 6f66 2074   The layers of t
+00027c90: 6865 2060 5472 616e 7366 6f72 6d65 7244  he `TransformerD
+00027ca0: 6563 6f64 6572 4c61 7965 7260 2e0a 2020  ecoderLayer`..  
+00027cb0: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+00027cc0: 7369 7a65 2869 6e74 293a 2054 6865 2062  size(int): The b
+00027cd0: 6174 6368 2073 697a 6520 6f66 2074 6865  atch size of the
+00027ce0: 2069 6e70 7574 2074 656e 736f 7220 7768   input tensor wh
+00027cf0: 656e 2064 6f20 696e 6372 656e 6d65 6e74  en do increnment
+00027d00: 616c 2070 7265 6469 6374 696f 6e2e 2053  al prediction. S
+00027d10: 686f 756c 6420 6265 2061 2070 6f73 6974  hould be a posit
+00027d20: 6976 650a 2020 2020 2020 2020 2020 2020  ive.            
+00027d30: 2020 2020 7661 6c75 652e 2057 6865 6e20      value. When 
+00027d40: 646f 2074 7261 696e 696e 6720 6f72 2070  do training or p
+00027d50: 7265 6469 6374 696f 6e2c 2074 6865 2061  rediction, the a
+00027d60: 7267 756d 656e 7420 7769 6c6c 206e 6f74  rgument will not
+00027d70: 2077 6f72 6b20 616e 6420 7468 6520 7573   work and the us
+00027d80: 6572 2063 616e 206a 7573 7420 7061 7373  er can just pass
+00027d90: 204e 6f6e 6520 746f 0a20 2020 2020 2020   None to.       
+00027da0: 2020 2020 2020 2020 2074 6865 2061 7267           the arg
+00027db0: 756d 656e 742e 0a20 2020 2020 2020 2020  ument..         
+00027dc0: 2020 2068 6964 6465 6e5f 7369 7a65 2869     hidden_size(i
+00027dd0: 6e74 293a 2054 6865 2068 6964 6465 6e20  nt): The hidden 
+00027de0: 7369 7a65 206f 6620 7468 6520 696e 7075  size of the inpu
+00027df0: 742e 0a20 2020 2020 2020 2020 2020 2066  t..            f
+00027e00: 666e 5f68 6964 6465 6e5f 7369 7a65 2869  fn_hidden_size(i
+00027e10: 6e74 293a 2054 6865 2068 6964 6465 6e20  nt): The hidden 
+00027e20: 7369 7a65 206f 6620 626f 7474 6c65 6e65  size of bottlene
+00027e30: 636b 2069 6e20 7468 6520 6665 6564 666f  ck in the feedfo
+00027e40: 7277 6172 6420 6c61 7965 722e 0a20 2020  rward layer..   
+00027e50: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
+00027e60: 5f6c 656e 6774 6828 696e 7429 3a20 5468  _length(int): Th
+00027e70: 6520 696e 7075 7420 736f 7572 6365 2073  e input source s
+00027e80: 6571 7565 6e63 6520 6c65 6e67 7468 2e0a  equence length..
+00027e90: 2020 2020 2020 2020 2020 2020 7467 745f              tgt_
+00027ea0: 7365 715f 6c65 6e67 7468 2869 6e74 293a  seq_length(int):
+00027eb0: 2054 6865 2069 6e70 7574 2074 6172 6765   The input targe
+00027ec0: 7420 7365 7175 656e 6365 206c 656e 6774  t sequence lengt
+00027ed0: 682e 0a20 2020 2020 2020 2020 2020 206e  h..            n
+00027ee0: 756d 5f68 6561 6473 2869 6e74 293a 2054  um_heads(int): T
+00027ef0: 6865 206e 756d 6265 7220 6f66 2074 6865  he number of the
+00027f00: 2068 6561 6473 2e0a 2020 2020 2020 2020   heads..        
+00027f10: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
+00027f20: 6f70 6f75 745f 7261 7465 2866 6c6f 6174  opout_rate(float
+00027f30: 293a 2054 6865 2064 726f 706f 7574 2072  ): The dropout r
+00027f40: 6174 6520 6f66 2074 6865 2061 7474 656e  ate of the atten
+00027f50: 7469 6f6e 2073 636f 7265 732e 2044 6566  tion scores. Def
+00027f60: 6175 6c74 3a30 2e31 2e0a 2020 2020 2020  ault:0.1..      
+00027f70: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+00027f80: 706f 7574 5f72 6174 6528 666c 6f61 7429  pout_rate(float)
+00027f90: 3a20 5468 6520 6472 6f70 6f75 7420 7261  : The dropout ra
+00027fa0: 7465 206f 6620 7468 6520 6669 6e61 6c20  te of the final 
+00027fb0: 6f75 7470 7574 206f 6620 7468 6520 6c61  output of the la
+00027fc0: 7965 722e 2044 6566 6175 6c74 3a30 2e31  yer. Default:0.1
+00027fd0: 2e0a 2020 2020 2020 2020 2020 2020 706f  ..            po
+00027fe0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00027ff0: 6964 7561 6c28 626f 6f6c 293a 2044 6f20  idual(bool): Do 
+00028000: 7265 7369 6475 616c 7320 6164 6473 2062  residuals adds b
+00028010: 6566 6f72 6520 7468 6520 6c61 7965 726e  efore the layern
+00028020: 6f72 6d2e 2044 6566 6175 6c74 2046 616c  orm. Default Fal
+00028030: 7365 2e0a 2020 2020 2020 2020 2020 2020  se..            
+00028040: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+00028050: 655f 7479 7065 2864 7479 7065 2e4e 756d  e_type(dtype.Num
+00028060: 6265 7229 3a20 5468 6520 636f 6d70 7574  ber): The comput
+00028070: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
+00028080: 6520 6c61 7965 726e 6f72 6d2e 0a20 2020  e layernorm..   
+00028090: 2020 2020 2020 2020 2020 2020 2053 686f               Sho
+000280a0: 756c 6420 6265 206d 7374 7970 652e 666c  uld be mstype.fl
+000280b0: 6f61 7433 3220 6f72 206d 7374 7970 652e  oat32 or mstype.
+000280c0: 666c 6f61 7431 362e 2044 6566 6175 6c74  float16. Default
+000280d0: 206d 7374 7970 652e 666c 6f61 7433 322e   mstype.float32.
+000280e0: 0a20 2020 2020 2020 2020 2020 2073 6f66  .            sof
+000280f0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+00028100: 6528 6474 7970 652e 4e75 6d62 6572 293a  e(dtype.Number):
+00028110: 2054 6865 2063 6f6d 7075 7461 7469 6f6e   The computation
+00028120: 2074 7970 6520 6f66 2074 6865 2073 6f66   type of the sof
+00028130: 746d 6178 2069 6e20 7468 6520 6174 7465  tmax in the atte
+00028140: 6e74 696f 6e2e 0a20 2020 2020 2020 2020  ntion..         
+00028150: 2020 2020 2020 2053 686f 756c 6420 6265         Should be
+00028160: 206d 7374 7970 652e 666c 6f61 7433 3220   mstype.float32 
+00028170: 6f72 206d 7374 7970 652e 666c 6f61 7431  or mstype.float1
+00028180: 362e 2044 6566 6175 6c74 206d 7374 7970  6. Default mstyp
+00028190: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+000281a0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+000281b0: 745f 7479 7065 2864 7479 7065 2e4e 756d  t_type(dtype.Num
+000281c0: 6265 7229 3a20 5468 6520 7061 7261 6d65  ber): The parame
+000281d0: 7465 7220 696e 6974 6961 6c69 7a61 7469  ter initializati
+000281e0: 6f6e 2074 7970 6520 6f66 2074 6865 206d  on type of the m
+000281f0: 6f64 756c 652e 0a20 2020 2020 2020 2020  odule..         
+00028200: 2020 2020 2020 2053 686f 756c 6420 6265         Should be
+00028210: 206d 7374 7970 652e 666c 6f61 7433 3220   mstype.float32 
+00028220: 6f72 206d 7374 7970 652e 666c 6f61 7431  or mstype.float1
+00028230: 362e 2044 6566 6175 6c74 206d 7374 7970  6. Default mstyp
+00028240: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+00028250: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
+00028260: 7420 2873 7472 2c20 6e6e 2e43 656c 6c29  t (str, nn.Cell)
+00028270: 3a20 5468 6520 6163 7469 7661 7469 6f6e  : The activation
+00028280: 206f 6620 7468 6520 696e 7465 726e 616c   of the internal
+00028290: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
+000282a0: 6572 2e20 5375 7070 6f72 7473 2027 7265  er. Supports 're
+000282b0: 6c75 272c 0a20 2020 2020 2020 2020 2020  lu',.           
+000282c0: 2020 2020 2027 7265 6c75 3627 2c20 2774       'relu6', 't
+000282d0: 616e 6827 2c20 2767 656c 7527 2c20 2766  anh', 'gelu', 'f
+000282e0: 6173 745f 6765 6c75 272c 2027 656c 7527  ast_gelu', 'elu'
+000282f0: 2c20 2773 6967 6d6f 6964 272c 2027 7072  , 'sigmoid', 'pr
+00028300: 656c 7527 2c20 276c 6561 6b79 7265 6c75  elu', 'leakyrelu
+00028310: 272c 2027 6873 7769 7368 272c 0a20 2020  ', 'hswish',.   
+00028320: 2020 2020 2020 2020 2020 2020 2027 6873               'hs
+00028330: 6967 6d6f 6964 272c 2027 6c6f 6773 6967  igmoid', 'logsig
+00028340: 6d6f 6964 2720 616e 6420 736f 206f 6e2e  moid' and so on.
+00028350: 2055 7365 7220 6361 6e20 7072 6f76 6964   User can provid
+00028360: 6520 6375 7374 6f6d 2061 6374 6976 6974  e custom activit
+00028370: 696f 6e20 746f 2074 6865 2061 7267 756d  ion to the argum
+00028380: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
+00028390: 2020 2020 2049 6620 7573 6572 2077 616e       If user wan
+000283a0: 7473 2074 6f20 7275 6e20 7468 6520 6e65  ts to run the ne
+000283b0: 7420 696e 2074 6865 2070 6172 616c 6c65  t in the paralle
+000283c0: 6c20 6d6f 6465 2c20 7468 6520 6375 7374  l mode, the cust
+000283d0: 6f6d 2061 6374 6976 6174 696f 6e20 6d75  om activation mu
+000283e0: 7374 2061 6c73 6f20 7072 6f76 6964 650a  st also provide.
+000283f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028400: 7468 6520 6061 6374 6976 6174 696f 6e5f  the `activation_
+00028410: 7368 6172 6460 2066 756e 6374 696f 6e2e  shard` function.
+00028420: 2050 6c65 6173 6520 7365 6520 7468 6520   Please see the 
+00028430: 6578 616d 706c 6573 206f 6620 7468 650a  examples of the.
+00028440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028450: 636c 6173 733a 606d 696e 6466 6f72 6d65  class:`mindforme
+00028460: 7273 2e6d 6f64 756c 6573 2e74 7261 6e73  rs.modules.trans
+00028470: 666f 726d 6572 2e46 6565 6446 6f72 7761  former.FeedForwa
+00028480: 7264 602e 2044 6566 6175 6c74 3a20 6765  rd`. Default: ge
+00028490: 6c75 2e0a 2020 2020 2020 2020 2020 2020  lu..            
+000284a0: 6c61 6d62 6461 5f66 756e 6328 6675 6e63  lambda_func(func
+000284b0: 7469 6f6e 293a 2041 2066 756e 6374 696f  tion): A functio
+000284c0: 6e20 6361 6e20 6465 7465 726d 696e 6520  n can determine 
+000284d0: 7468 6520 6675 7369 6f6e 2069 6e64 6578  the fusion index
+000284e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000284f0: 2020 7069 7065 6c69 6e65 2073 7461 6765    pipeline stage
+00028500: 7320 616e 6420 7265 636f 6d70 7574 6520  s and recompute 
+00028510: 6174 7472 6962 7574 652e 2049 6620 7468  attribute. If th
+00028520: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00028530: 2020 7573 6572 2077 616e 7473 2074 6f20    user wants to 
+00028540: 6465 7465 726d 696e 6520 7468 6520 7069  determine the pi
+00028550: 7065 6c69 6e65 2073 7461 6765 2061 6e64  peline stage and
+00028560: 2067 7261 6469 656e 7420 6167 6772 6567   gradient aggreg
+00028570: 6174 696f 6e20 6675 7369 6f6e 2c20 7468  ation fusion, th
+00028580: 6520 7573 6572 2063 616e 2070 6173 7320  e user can pass 
+00028590: 610a 2020 2020 2020 2020 2020 2020 2020  a.              
+000285a0: 2020 6675 6e63 7469 6f6e 2074 6861 7420    function that 
+000285b0: 6163 6365 7074 7320 606e 6574 776f 726b  accepts `network
+000285c0: 602c 2060 6c61 7965 725f 6964 602c 2060  `, `layer_id`, `
+000285d0: 6f66 6673 6574 602c 2060 7061 7261 6c6c  offset`, `parall
+000285e0: 656c 5f63 6f6e 6669 6760 2c20 606c 6179  el_config`, `lay
+000285f0: 6572 7360 2e20 5468 6520 606e 6574 776f  ers`. The `netwo
+00028600: 726b 2843 656c 6c29 600a 2020 2020 2020  rk(Cell)`.      
+00028610: 2020 2020 2020 2020 2020 7265 7072 6573            repres
+00028620: 656e 7473 2074 6865 2074 7261 6e73 666f  ents the transfo
+00028630: 726d 6572 2062 6c6f 636b 2c20 606c 6179  rmer block, `lay
+00028640: 6572 5f69 6428 696e 7429 6020 6d65 616e  er_id(int)` mean
+00028650: 7320 7468 6520 6c61 7965 7220 696e 6465  s the layer inde
+00028660: 7820 666f 7220 7468 6520 6375 7272 656e  x for the curren
+00028670: 7420 6d6f 6475 6c65 2c20 636f 756e 7473  t module, counts
+00028680: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028690: 2066 726f 6d20 7a65 726f 2c20 606f 6666   from zero, `off
+000286a0: 7365 7428 696e 7429 6020 6d65 616e 7320  set(int)` means 
+000286b0: 7468 6520 6c61 7965 725f 696e 6465 7820  the layer_index 
+000286c0: 6e65 6564 7320 616e 206f 6666 7365 742c  needs an offset,
+000286d0: 2069 6620 7468 6572 6520 6172 6520 6f74   if there are ot
+000286e0: 6865 7220 6d6f 6475 6c65 7320 696e 2074  her modules in t
+000286f0: 6865 206e 6574 2e0a 2020 2020 2020 2020  he net..        
+00028700: 2020 2020 2020 2020 5468 6520 6465 6661          The defa
+00028710: 756c 7420 7365 7474 696e 6720 666f 7220  ult setting for 
+00028720: 7468 6520 7069 7065 6c69 6e65 2069 733a  the pipeline is:
+00028730: 2060 286c 6179 6572 5f69 6420 2b20 6f66   `(layer_id + of
+00028740: 6673 6574 2920 2f2f 2028 6c61 7965 7273  fset) // (layers
+00028750: 202f 2070 6970 656c 696e 655f 7374 6167   / pipeline_stag
+00028760: 6529 602e 0a20 2020 2020 2020 2020 2020  e)`..           
+00028770: 2020 2020 2044 6566 6175 6c74 3a20 4e6f       Default: No
+00028780: 6e65 2e0a 2020 2020 2020 2020 2020 2020  ne..            
+00028790: 7573 655f 7061 7374 2862 6f6f 6c29 3a20  use_past(bool): 
+000287a0: 5573 6520 7468 6520 7061 7374 2073 7461  Use the past sta
+000287b0: 7465 2074 6f20 636f 6d70 7574 652c 2075  te to compute, u
+000287c0: 7365 6420 666f 7220 696e 6372 656d 656e  sed for incremen
+000287d0: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
+000287e0: 4465 6661 756c 7420 4661 6c73 652e 0a20  Default False.. 
+000287f0: 2020 2020 2020 2020 2020 206f 6666 7365             offse
+00028800: 7428 696e 7429 3a20 5468 6520 696e 6974  t(int): The init
+00028810: 6961 6c20 6c61 7965 7220 696e 6465 7820  ial layer index 
+00028820: 666f 7220 7468 6520 6064 6563 6f64 6572  for the `decoder
+00028830: 602e 2055 7365 6420 666f 7220 7365 7474  `. Used for sett
+00028840: 696e 6720 7468 6520 6675 7369 6f6e 2069  ing the fusion i
+00028850: 6420 616e 6420 7374 6167 6520 6964 2c20  d and stage id, 
+00028860: 746f 206e 6f74 0a20 2020 2020 2020 2020  to not.         
+00028870: 2020 2020 2020 206f 7665 726c 6170 2077         overlap w
+00028880: 6974 6820 7468 6520 656e 636f 6465 7220  ith the encoder 
+00028890: 6c61 7965 722e 2044 6566 6175 6c74 2030  layer. Default 0
+000288a0: 2e0a 2020 2020 2020 2020 2020 2020 6d6f  ..            mo
+000288b0: 655f 636f 6e66 6967 284d 6f45 436f 6e66  e_config(MoEConf
+000288c0: 6967 293a 2054 6865 2063 6f6e 6669 6775  ig): The configu
+000288d0: 7261 7469 6f6e 206f 6620 4d6f 4520 284d  ration of MoE (M
+000288e0: 6978 7475 7265 206f 6620 4578 7065 7274  ixture of Expert
+000288f0: 292e 2044 6566 6175 6c74 2069 7320 616e  ). Default is an
+00028900: 2069 6e73 7461 6e63 6520 6f66 204d 6f45   instance of MoE
+00028910: 436f 6e66 6967 0a20 2020 2020 2020 2020  Config.         
+00028920: 2020 2020 2020 2077 6974 6820 6465 6661         with defa
+00028930: 756c 7420 7661 6c75 6573 2e20 506c 6561  ult values. Plea
+00028940: 7365 2073 6565 2060 4d6f 4543 6f6e 6669  se see `MoEConfi
+00028950: 6760 2e0a 2020 2020 2020 2020 2020 2020  g`..            
+00028960: 7061 7261 6c6c 656c 5f63 6f6e 6669 6728  parallel_config(
+00028970: 5472 616e 7366 6f72 6d65 724f 7050 6172  TransformerOpPar
+00028980: 616c 6c65 6c43 6f6e 6669 6729 3a20 5468  allelConfig): Th
+00028990: 6520 7061 7261 6c6c 656c 2063 6f6e 6669  e parallel confi
+000289a0: 6775 7265 2e20 4465 6661 756c 7420 6064  gure. Default `d
+000289b0: 6566 6175 6c74 5f74 7261 6e73 666f 726d  efault_transform
+000289c0: 6572 5f63 6f6e 6669 6760 2c0a 2020 2020  er_config`,.    
+000289d0: 2020 2020 2020 2020 2020 2020 616e 2069              an i
+000289e0: 6e73 7461 6e63 6520 6f66 2060 5472 616e  nstance of `Tran
+000289f0: 7366 6f72 6d65 724f 7050 6172 616c 6c65  sformerOpParalle
+00028a00: 6c43 6f6e 6669 6760 2077 6974 6820 6465  lConfig` with de
+00028a10: 6661 756c 7420 6172 6773 2e0a 0a20 2020  fault args...   
+00028a20: 2020 2020 2049 6e70 7574 733a 0a20 2020       Inputs:.   
+00028a30: 2020 2020 2020 2020 202d 202a 2a68 6964           - **hid
+00028a40: 6465 6e5f 7374 6174 732a 2a20 2854 656e  den_stats** (Ten
+00028a50: 736f 7229 202d 2054 6865 2069 6e70 7574  sor) - The input
+00028a60: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
+00028a70: 7065 205b 6261 7463 685f 7369 7a65 2c20  pe [batch_size, 
+00028a80: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+00028a90: 656e 5f73 697a 655d 206f 720a 2020 2020  en_size] or.    
+00028aa0: 2020 2020 2020 2020 2020 5b62 6174 6368            [batch
+00028ab0: 5f73 697a 6520 2a20 7365 715f 6c65 6e67  _size * seq_leng
+00028ac0: 7468 2c20 6869 6464 656e 5f73 697a 655d  th, hidden_size]
+00028ad0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+00028ae0: 2a61 7474 656e 7469 6f6e 5f6d 6173 6b2a  *attention_mask*
+00028af0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+00028b00: 2061 7474 656e 7469 6f6e 206d 6173 6b20   attention mask 
+00028b10: 666f 7220 6465 636f 6465 7220 7769 7468  for decoder with
+00028b20: 2073 6861 7065 0a20 2020 2020 2020 2020   shape.         
+00028b30: 2020 2020 205b 6261 7463 685f 7369 7a65       [batch_size
+00028b40: 2c20 7365 715f 6c65 6e67 7468 2c20 7365  , seq_length, se
+00028b50: 715f 6c65 6e67 7468 5d20 6f72 204e 6f6e  q_length] or Non
+00028b60: 652e 204e 6f6e 6520 6d65 616e 7320 7468  e. None means th
+00028b70: 6572 6520 7769 6c6c 2062 6520 6e6f 206d  ere will be no m
+00028b80: 6173 6b20 696e 2073 6f66 746d 6178 0a20  ask in softmax. 
+00028b90: 2020 2020 2020 2020 2020 2020 2063 6f6d               com
+00028ba0: 7075 7461 7469 6f6e 2069 6e20 7365 6c66  putation in self
+00028bb0: 2061 7474 656e 7469 6f6e 2e0a 2020 2020   attention..    
+00028bc0: 2020 2020 2020 2020 2d20 2a2a 656e 636f          - **enco
+00028bd0: 6465 725f 6f75 7470 7574 2a2a 2028 5465  der_output** (Te
+00028be0: 6e73 6f72 2920 2d20 5468 6520 6f75 7470  nsor) - The outp
+00028bf0: 7574 206f 6620 7468 6520 656e 636f 6465  ut of the encode
+00028c00: 7220 7769 7468 2073 6861 7065 205b 6261  r with shape [ba
+00028c10: 7463 685f 7369 7a65 2c20 7365 715f 6c65  tch_size, seq_le
+00028c20: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+00028c30: 655d 0a20 2020 2020 2020 2020 2020 2020  e].             
+00028c40: 206f 7220 5b62 6174 6368 5f73 697a 6520   or [batch_size 
+00028c50: 2a20 7365 715f 6c65 6e67 7468 2c20 6869  * seq_length, hi
+00028c60: 6464 656e 5f73 697a 655d 2e20 4e6f 7465  dden_size]. Note
+00028c70: 2074 6869 7320 6172 6773 2063 616e 206e   this args can n
+00028c80: 6f74 2062 6520 7061 7373 6564 2062 7920  ot be passed by 
+00028c90: 4e6f 6e65 2077 6865 6e20 7468 6520 6e65  None when the ne
+00028ca0: 7420 6973 2069 6e0a 2020 2020 2020 2020  t is in.        
+00028cb0: 2020 2020 2020 6f75 7465 726d 6f73 7420        outermost 
+00028cc0: 6c61 7965 722e 2044 6566 6175 6c74 204e  layer. Default N
+00028cd0: 6f6e 652e 0a20 2020 2020 2020 2020 2020  one..           
+00028ce0: 202d 202a 2a6d 656d 6f72 795f 6d61 736b   - **memory_mask
+00028cf0: 2a2a 2028 5465 6e73 6f72 2920 2d20 5468  ** (Tensor) - Th
+00028d00: 6520 6d65 6d6f 7279 206d 6173 6b20 6f66  e memory mask of
+00028d10: 2074 6865 2063 726f 7373 2061 7474 656e   the cross atten
+00028d20: 7469 6f6e 2077 6974 6820 7368 6170 6520  tion with shape 
+00028d30: 5b62 6174 6368 2c20 7467 745f 7365 715f  [batch, tgt_seq_
+00028d40: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+00028d50: 2020 2020 2020 7372 635f 7365 715f 6c65        src_seq_le
+00028d60: 6e67 7468 5d20 7768 6572 6520 7467 745f  ngth] where tgt_
+00028d70: 7365 715f 6c65 6e67 7468 2069 7320 7468  seq_length is th
+00028d80: 6520 6c65 6e67 7468 206f 6620 7468 6520  e length of the 
+00028d90: 6465 636f 6465 722e 2054 6865 2075 7365  decoder. The use
+00028da0: 7220 6361 6e20 616c 736f 2070 6173 7320  r can also pass 
+00028db0: 4e6f 6e65 2e20 4e6f 6e65 0a20 2020 2020  None. None.     
+00028dc0: 2020 2020 2020 2020 206d 6561 6e73 2074           means t
+00028dd0: 6865 7265 2077 696c 6c20 6265 206e 6f20  here will be no 
+00028de0: 6d61 736b 2069 6e20 736f 6674 6d61 7820  mask in softmax 
+00028df0: 636f 6d70 7574 6174 696f 6e20 696e 2063  computation in c
+00028e00: 726f 7373 2061 7474 656e 7469 6f6e 2e20  ross attention. 
+00028e10: 4465 6661 756c 7420 4e6f 6e65 2e0a 2020  Default None..  
+00028e20: 2020 2020 2020 2020 2020 2d20 2a2a 696e            - **in
+00028e30: 6974 5f72 6573 6574 2a2a 2028 5465 6e73  it_reset** (Tens
+00028e40: 6f72 2920 2d20 4120 626f 6f6c 2074 656e  or) - A bool ten
+00028e50: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
+00028e60: 315d 2c20 7573 6564 2074 6f20 636c 6561  1], used to clea
+00028e70: 7220 7468 6520 7061 7374 206b 6579 2070  r the past key p
+00028e80: 6172 616d 6574 6572 2061 6e64 0a20 2020  arameter and.   
+00028e90: 2020 2020 2020 2020 2020 2070 6173 7420             past 
+00028ea0: 7661 6c75 6520 7061 7261 6d65 7465 7220  value parameter 
+00028eb0: 7573 6564 2069 6e20 7468 6520 696e 6372  used in the incr
+00028ec0: 656d 656e 7461 6c20 7072 6564 6963 7469  emental predicti
+00028ed0: 6f6e 2e20 4f6e 6c79 2076 616c 6964 2077  on. Only valid w
+00028ee0: 6865 6e20 7573 655f 7061 7374 2069 7320  hen use_past is 
+00028ef0: 5472 7565 2e20 4465 6661 756c 7420 5472  True. Default Tr
+00028f00: 7565 2e0a 2020 2020 2020 2020 2020 2020  ue..            
+00028f10: 2d20 2a2a 6261 7463 685f 7661 6c69 645f  - **batch_valid_
+00028f20: 6c65 6e67 7468 2a2a 2028 5465 6e73 6f72  length** (Tensor
+00028f30: 2920 2d20 496e 7433 3220 7465 6e73 6f72  ) - Int32 tensor
+00028f40: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
+00028f50: 6368 5f73 697a 655d 2074 6865 2070 6173  ch_size] the pas
+00028f60: 7420 6361 6c63 756c 6174 6564 2074 6865  t calculated the
+00028f70: 2069 6e64 6578 2e0a 2020 2020 2020 2020   index..        
+00028f80: 2020 2020 2020 5573 6564 2066 6f72 2069        Used for i
+00028f90: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
+00028fa0: 6374 696f 6e20 7768 656e 2074 6865 2075  ction when the u
+00028fb0: 7365 5f70 6173 7420 6973 2054 7275 652e  se_past is True.
+00028fc0: 2044 6566 6175 6c74 204e 6f6e 652e 0a0a   Default None...
+00028fd0: 2020 2020 2020 2020 4f75 7470 7574 733a          Outputs:
+00028fe0: 0a20 2020 2020 2020 2020 2020 2054 7570  .            Tup
+00028ff0: 6c65 2c20 6120 7475 706c 6520 636f 6e74  le, a tuple cont
+00029000: 6169 6e73 2860 6f75 7470 7574 602c 2060  ains(`output`, `
+00029010: 6c61 7965 725f 7072 6573 656e 7460 290a  layer_present`).
+00029020: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+00029030: 2a6f 7574 7075 742a 2a20 2854 656e 736f  *output** (Tenso
+00029040: 7229 202d 2054 6865 206f 7574 7075 7420  r) - The output 
+00029050: 6c6f 6769 7420 6f66 2074 6869 7320 6c61  logit of this la
+00029060: 7965 722e 2054 6865 2073 6861 7065 2069  yer. The shape i
+00029070: 7320 5b62 6174 6368 2c20 7467 745f 7365  s [batch, tgt_se
+00029080: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
+00029090: 5f73 697a 655d 206f 720a 2020 2020 2020  _size] or.      
+000290a0: 2020 2020 2020 2020 5b62 6174 6368 202a          [batch *
+000290b0: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
+000290c0: 2068 6964 6465 6e5f 7369 7a65 5d0a 2020   hidden_size].  
+000290d0: 2020 2020 2020 2020 2020 2d20 2a2a 6c61            - **la
+000290e0: 7965 725f 7072 6573 656e 742a 2a20 2854  yer_present** (T
+000290f0: 7570 6c65 2920 2d20 4120 7475 706c 6520  uple) - A tuple 
+00029100: 7769 7468 2073 697a 6520 6f66 206e 756d  with size of num
+00029110: 5f6c 6179 6572 732c 2077 6865 7265 2065  _layers, where e
+00029120: 6163 6820 7475 706c 6520 6973 2074 6865  ach tuple is the
+00029130: 2074 656e 736f 7220 6f66 2074 6865 0a20   tensor of the. 
+00029140: 2020 2020 2020 2020 2020 2020 2070 726f               pro
+00029150: 6a65 6374 6564 206b 6579 2061 6e64 2076  jected key and v
+00029160: 616c 7565 2076 6563 746f 7220 696e 2073  alue vector in s
+00029170: 656c 6620 6174 7465 6e74 696f 6e20 7769  elf attention wi
+00029180: 7468 2073 6861 7065 2028 2862 6174 6368  th shape ((batch
+00029190: 5f73 697a 652c 206e 756d 5f68 6561 6473  _size, num_heads
+000291a0: 2c20 7369 7a65 5f70 6572 5f68 6561 642c  , size_per_head,
+000291b0: 0a20 2020 2020 2020 2020 2020 2020 2074  .              t
+000291c0: 6774 5f73 6571 5f6c 656e 6774 6829 2c20  gt_seq_length), 
+000291d0: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
+000291e0: 5f68 6561 6473 2c20 7467 745f 7365 715f  _heads, tgt_seq_
+000291f0: 6c65 6e67 7468 2c20 7369 7a65 5f70 6572  length, size_per
+00029200: 5f68 6561 6429 2c20 616e 6420 6f66 2074  _head), and of t
+00029210: 6865 2070 726f 6a65 6374 6564 206b 6579  he projected key
+00029220: 0a20 2020 2020 2020 2020 2020 2020 2061  .              a
+00029230: 6e64 2076 616c 7565 2076 6563 746f 7220  nd value vector 
+00029240: 696e 2063 726f 7373 2061 7474 656e 7469  in cross attenti
+00029250: 6f6e 2077 6974 6820 7368 6170 6520 2028  on with shape  (
+00029260: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
+00029270: 6865 6164 732c 2073 697a 655f 7065 725f  heads, size_per_
+00029280: 6865 6164 2c20 7372 635f 7365 715f 6c65  head, src_seq_le
+00029290: 6e67 7468 292c 0a20 2020 2020 2020 2020  ngth),.         
+000292a0: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
+000292b0: 2c20 6e75 6d5f 6865 6164 732c 2073 7263  , num_heads, src
+000292c0: 5f73 6571 5f6c 656e 6774 682c 2073 697a  _seq_length, siz
+000292d0: 655f 7065 725f 6865 6164 2929 2e0a 0a20  e_per_head))... 
+000292e0: 2020 2020 2020 2053 7570 706f 7274 6564         Supported
+000292f0: 2050 6c61 7466 6f72 6d73 3a0a 2020 2020   Platforms:.    
+00029300: 2020 2020 2020 2020 6060 4173 6365 6e64          ``Ascend
+00029310: 6060 2060 6047 5055 6060 0a0a 2020 2020  `` ``GPU``..    
+00029320: 2020 2020 4578 616d 706c 6573 3a0a 2020      Examples:.  
+00029330: 2020 2020 2020 2020 2020 3e3e 3e20 696d            >>> im
+00029340: 706f 7274 206e 756d 7079 2061 7320 6e70  port numpy as np
+00029350: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00029360: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
+00029370: 696d 706f 7274 2064 7479 7065 2061 7320  import dtype as 
+00029380: 6d73 7479 7065 0a20 2020 2020 2020 2020  mstype.         
+00029390: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
+000293a0: 666f 726d 6572 732e 6d6f 6475 6c65 732e  formers.modules.
+000293b0: 7472 616e 7366 6f72 6d65 7220 696d 706f  transformer impo
+000293c0: 7274 2054 7261 6e73 666f 726d 6572 4465  rt TransformerDe
+000293d0: 636f 6465 720a 2020 2020 2020 2020 2020  coder.          
+000293e0: 2020 3e3e 3e20 6672 6f6d 206d 696e 6473    >>> from minds
+000293f0: 706f 7265 2069 6d70 6f72 7420 5465 6e73  pore import Tens
+00029400: 6f72 0a20 2020 2020 2020 2020 2020 203e  or.            >
+00029410: 3e3e 206d 6f64 656c 203d 2054 7261 6e73  >> model = Trans
+00029420: 666f 726d 6572 4465 636f 6465 7228 6261  formerDecoder(ba
+00029430: 7463 685f 7369 7a65 3d32 2c20 6e75 6d5f  tch_size=2, num_
+00029440: 6c61 7965 7273 3d31 2c20 6869 6464 656e  layers=1, hidden
+00029450: 5f73 697a 653d 3634 2c20 6666 6e5f 6869  _size=64, ffn_hi
+00029460: 6464 656e 5f73 697a 653d 3634 2c0a 2020  dden_size=64,.  
+00029470: 2020 2020 2020 2020 2020 2e2e 2e20 2020            ...   
+00029480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029490: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+000294a0: 6473 3d32 2c20 7372 635f 7365 715f 6c65  ds=2, src_seq_le
+000294b0: 6e67 7468 3d32 302c 2074 6774 5f73 6571  ngth=20, tgt_seq
+000294c0: 5f6c 656e 6774 683d 3130 290a 2020 2020  _length=10).    
+000294d0: 2020 2020 2020 2020 3e3e 3e20 656e 636f          >>> enco
+000294e0: 6465 725f 696e 7075 745f 7661 6c75 6520  der_input_value 
+000294f0: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
+00029500: 2828 322c 2032 302c 2036 3429 292c 206d  ((2, 20, 64)), m
+00029510: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
+00029520: 2020 2020 2020 2020 2020 203e 3e3e 2064             >>> d
+00029530: 6563 6f64 6572 5f69 6e70 7574 5f76 616c  ecoder_input_val
+00029540: 7565 203d 2054 656e 736f 7228 6e70 2e6f  ue = Tensor(np.o
+00029550: 6e65 7328 2832 2c20 3130 2c20 3634 2929  nes((2, 10, 64))
+00029560: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+00029570: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00029580: 3e20 6465 636f 6465 725f 696e 7075 745f  > decoder_input_
+00029590: 6d61 736b 203d 2054 656e 736f 7228 6e70  mask = Tensor(np
+000295a0: 2e6f 6e65 7328 2832 2c20 3130 2c20 3130  .ones((2, 10, 10
+000295b0: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+000295c0: 3136 290a 2020 2020 2020 2020 2020 2020  16).            
+000295d0: 3e3e 3e20 6d65 6d6f 7279 5f6d 6173 6b20  >>> memory_mask 
+000295e0: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
+000295f0: 2828 322c 2031 302c 2032 3029 292c 206d  ((2, 10, 20)), m
+00029600: 7374 7970 652e 666c 6f61 7431 3629 0a20  stype.float16). 
+00029610: 2020 2020 2020 2020 2020 203e 3e3e 206f             >>> o
+00029620: 7574 7075 742c 2070 6173 7420 3d20 6d6f  utput, past = mo
+00029630: 6465 6c28 6465 636f 6465 725f 696e 7075  del(decoder_inpu
+00029640: 745f 7661 6c75 652c 2064 6563 6f64 6572  t_value, decoder
+00029650: 5f69 6e70 7574 5f6d 6173 6b2c 2065 6e63  _input_mask, enc
+00029660: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+00029670: 2c20 6d65 6d6f 7279 5f6d 6173 6b29 0a20  , memory_mask). 
+00029680: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+00029690: 7269 6e74 286f 7574 7075 742e 7368 6170  rint(output.shap
+000296a0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+000296b0: 322c 2031 302c 2036 3429 0a20 2020 2020  2, 10, 64).     
+000296c0: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+000296d0: 286c 656e 2870 6173 7429 290a 2020 2020  (len(past)).    
+000296e0: 2020 2020 2020 2020 310a 2020 2020 2020          1.      
+000296f0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00029700: 7061 7374 5b30 5d5b 305d 2e73 6861 7065  past[0][0].shape
+00029710: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+00029720: 2c20 322c 2033 322c 2031 3029 0a20 2020  , 2, 32, 10).   
+00029730: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+00029740: 6e74 2870 6173 745b 305d 5b31 5d2e 7368  nt(past[0][1].sh
+00029750: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00029760: 2028 322c 2032 2c20 3130 2c20 3332 290a   (2, 2, 10, 32).
+00029770: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00029780: 7072 696e 7428 7061 7374 5b30 5d5b 325d  print(past[0][2]
+00029790: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+000297a0: 2020 2020 2832 2c20 322c 2033 322c 2032      (2, 2, 32, 2
+000297b0: 3029 0a20 2020 2020 2020 2020 2020 203e  0).            >
+000297c0: 3e3e 2070 7269 6e74 2870 6173 745b 305d  >> print(past[0]
+000297d0: 5b33 5d2e 7368 6170 6529 0a20 2020 2020  [3].shape).     
+000297e0: 2020 2020 2020 2028 322c 2032 2c20 3230         (2, 2, 20
+000297f0: 2c20 3332 290a 2020 2020 2222 220a 0a20  , 32).    """.. 
+00029800: 2020 2040 5f4c 6f67 4163 7469 6f6e 4f6e     @_LogActionOn
+00029810: 6365 286d 5f6c 6f67 6765 723d 6c6f 6767  ce(m_logger=logg
+00029820: 6572 2c20 6b65 793d 2754 7261 6e73 666f  er, key='Transfo
+00029830: 726d 6572 4465 636f 6465 7227 2c0a 2020  rmerDecoder',.  
+00029840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029850: 2020 6e6f 5f77 6172 6e69 6e67 3d5f 6765    no_warning=_ge
+00029860: 745f 7061 7261 6c6c 656c 5f6d 6f64 6528  t_parallel_mode(
+00029870: 2920 696e 2028 5061 7261 6c6c 656c 4d6f  ) in (ParallelMo
+00029880: 6465 2e53 5441 4e44 5f41 4c4f 4e45 2c29  de.STAND_ALONE,)
+00029890: 290a 2020 2020 405f 6172 6773 5f74 7970  ).    @_args_typ
+000298a0: 655f 7661 6c69 6461 746f 725f 6368 6563  e_validator_chec
+000298b0: 6b28 6869 6464 656e 5f73 697a 653d 5661  k(hidden_size=Va
+000298c0: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
+000298d0: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
+000298e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000298f0: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+00029900: 6865 6164 733d 5661 6c69 6461 746f 722e  heads=Validator.
+00029910: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+00029920: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
+00029930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029940: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+00029950: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
+00029960: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+00029970: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00029980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029990: 2020 7372 635f 7365 715f 6c65 6e67 7468    src_seq_length
+000299a0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+000299b0: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
+000299c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000299d0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+000299e0: 756d 5f6c 6179 6572 733d 5661 6c69 6461  um_layers=Valida
+000299f0: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
+00029a00: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
+00029a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029a20: 2020 2020 2020 2020 7467 745f 7365 715f          tgt_seq_
+00029a30: 6c65 6e67 7468 3d56 616c 6964 6174 6f72  length=Validator
+00029a40: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
+00029a50: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
+00029a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029a70: 2020 2020 206f 6666 7365 743d 5661 6c69       offset=Vali
+00029a80: 6461 746f 722e 6368 6563 6b5f 6e6f 6e5f  dator.check_non_
+00029a90: 6e65 6761 7469 7665 5f69 6e74 2c0a 2020  negative_int,.  
+00029aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ab0: 2020 2020 2020 2020 2020 2020 2020 6174                at
+00029ac0: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+00029ad0: 7261 7465 3d56 616c 6964 6174 6f72 2e63  rate=Validator.c
+00029ae0: 6865 636b 5f6e 6f6e 5f6e 6567 6174 6976  heck_non_negativ
+00029af0: 655f 666c 6f61 742c 0a20 2020 2020 2020  e_float,.       
+00029b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029b10: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00029b20: 6472 6f70 6f75 745f 7261 7465 3d56 616c  dropout_rate=Val
+00029b30: 6964 6174 6f72 2e63 6865 636b 5f6e 6f6e  idator.check_non
+00029b40: 5f6e 6567 6174 6976 655f 666c 6f61 742c  _negative_float,
+00029b50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029b70: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
+00029b80: 7265 7369 6475 616c 3d56 616c 6964 6174  residual=Validat
+00029b90: 6f72 2e63 6865 636b 5f62 6f6f 6c2c 0a20  or.check_bool,. 
+00029ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029bb0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+00029bc0: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
+00029bd0: 5f74 7970 653d 5f76 616c 6964 5f76 616c  _type=_valid_val
+00029be0: 7565 5f63 6865 636b 7328 5b6d 7374 7970  ue_checks([mstyp
+00029bf0: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
 00029c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00029c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c20: 6d73 7479 7065 2e66 6c6f 6174 3136 2c20  mstype.float16, 
-00029c30: 6d73 7479 7065 2e62 666c 6f61 7431 365d  mstype.bfloat16]
-00029c40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00029c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029c40: 2020 2020 2020 206d 7374 7970 652e 666c         mstype.fl
+00029c50: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
+00029c60: 6c6f 6174 3136 5d2c 0a20 2020 2020 2020  loat16],.       
 00029c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c80: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-00029c90: 616e 7366 6f72 6d65 7244 6563 6f64 6572  ansformerDecoder
-00029ca0: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
-00029cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029cc0: 2020 2020 736f 6674 6d61 785f 636f 6d70      softmax_comp
-00029cd0: 7574 655f 7479 7065 3d5f 7661 6c69 645f  ute_type=_valid_
-00029ce0: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
-00029cf0: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
-00029d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029cb0: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
+00029cc0: 4465 636f 6465 7222 292c 0a20 2020 2020  Decoder"),.     
+00029cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ce0: 2020 2020 2020 2020 2020 2073 6f66 746d             softm
+00029cf0: 6178 5f63 6f6d 7075 7465 5f74 7970 653d  ax_compute_type=
+00029d00: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
+00029d10: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
+00029d20: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
 00029d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d40: 2020 2020 2020 2020 6d73 7479 7065 2e66          mstype.f
-00029d50: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
-00029d60: 666c 6f61 7431 365d 2c0a 2020 2020 2020  float16],.      
-00029d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029d60: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+00029d70: 7374 7970 652e 666c 6f61 7431 362c 206d  stype.float16, m
+00029d80: 7374 7970 652e 6266 6c6f 6174 3136 5d2c  stype.bfloat16],
+00029d90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
 00029da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029db0: 2020 2022 5472 616e 7366 6f72 6d65 7244     "TransformerD
-00029dc0: 6563 6f64 6572 2229 2c0a 2020 2020 2020  ecoder"),.      
-00029dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029de0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-00029df0: 696e 6974 5f74 7970 653d 5f76 616c 6964  init_type=_valid
-00029e00: 5f76 616c 7565 5f63 6865 636b 7328 5b6d  _value_checks([m
-00029e10: 7374 7970 652e 666c 6f61 7433 322c 206d  stype.float32, m
-00029e20: 7374 7970 652e 666c 6f61 7431 362c 206d  stype.float16, m
-00029e30: 7374 7970 652e 6266 6c6f 6174 3136 5d2c  stype.bfloat16],
-00029e40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00029e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029dd0: 2020 2020 2020 2020 2020 2254 7261 6e73            "Trans
+00029de0: 666f 726d 6572 4465 636f 6465 7222 292c  formerDecoder"),
+00029df0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029e10: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+00029e20: 3d5f 7661 6c69 645f 7661 6c75 655f 6368  =_valid_value_ch
+00029e30: 6563 6b73 285b 6d73 7479 7065 2e66 6c6f  ecks([mstype.flo
+00029e40: 6174 3332 2c20 6d73 7479 7065 2e66 6c6f  at32, mstype.flo
+00029e50: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
+00029e60: 6f61 7431 365d 2c0a 2020 2020 2020 2020  oat16],.        
 00029e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029e80: 2020 2020 2022 5472 616e 7366 6f72 6d65       "Transforme
-00029e90: 7244 6563 6f64 6572 2229 2c0a 2020 2020  rDecoder"),.    
-00029ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029eb0: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00029ec0: 6c6c 656c 5f63 6f6e 6669 673d 5f76 616c  llel_config=_val
-00029ed0: 6964 5f74 7970 655f 6368 6563 6b73 285b  id_type_checks([
-00029ee0: 5472 616e 7366 6f72 6d65 724f 7050 6172  TransformerOpPar
-00029ef0: 616c 6c65 6c43 6f6e 6669 675d 2c0a 2020  allelConfig],.  
-00029f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ea0: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
+00029eb0: 6e73 666f 726d 6572 4465 636f 6465 7222  nsformerDecoder"
+00029ec0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00029ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ee0: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+00029ef0: 6967 3d5f 7661 6c69 645f 7479 7065 5f63  ig=_valid_type_c
+00029f00: 6865 636b 7328 5b54 7261 6e73 666f 726d  hecks([Transform
+00029f10: 6572 4f70 5061 7261 6c6c 656c 436f 6e66  erOpParallelConf
+00029f20: 6967 5d2c 0a20 2020 2020 2020 2020 2020  ig],.           
 00029f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029f40: 2022 5472 616e 7366 6f72 6d65 7244 6563   "TransformerDec
-00029f50: 6f64 6572 2229 2c0a 2020 2020 2020 2020  oder"),.        
-00029f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029f70: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-00029f80: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-00029f90: 5f62 6f6f 6c29 0a20 2020 2064 6566 205f  _bool).    def _
-00029fa0: 5f69 6e69 745f 5f28 7365 6c66 2c0a 2020  _init__(self,.  
-00029fb0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-00029fc0: 756d 5f6c 6179 6572 732c 0a20 2020 2020  um_layers,.     
-00029fd0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-00029fe0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
-00029ff0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-0002a000: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-0002a010: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-0002a020: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-0002a030: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-0002a040: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-0002a050: 2020 2020 2020 2020 2020 7467 745f 7365            tgt_se
-0002a060: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-0002a070: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
-0002a080: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
-0002a090: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-0002a0a0: 5f64 726f 706f 7574 5f72 6174 653d 302e  _dropout_rate=0.
-0002a0b0: 312c 0a20 2020 2020 2020 2020 2020 2020  1,.             
-0002a0c0: 2020 2020 6869 6464 656e 5f64 726f 706f      hidden_dropo
-0002a0d0: 7574 5f72 6174 653d 302e 312c 0a20 2020  ut_rate=0.1,.   
-0002a0e0: 2020 2020 2020 2020 2020 2020 2020 706f                po
-0002a0f0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
-0002a100: 6964 7561 6c3d 4661 6c73 652c 0a20 2020  idual=False,.   
-0002a110: 2020 2020 2020 2020 2020 2020 2020 6c61                la
-0002a120: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-0002a130: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
-0002a140: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
-0002a150: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
-0002a160: 6d70 7574 655f 7479 7065 3d6d 7374 7970  mpute_type=mstyp
-0002a170: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
-0002a180: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0002a190: 6d5f 696e 6974 5f74 7970 653d 6d73 7479  m_init_type=msty
-0002a1a0: 7065 2e66 6c6f 6174 3332 2c0a 2020 2020  pe.float32,.    
-0002a1b0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
-0002a1c0: 6465 6e5f 6163 743d 2767 656c 7527 2c0a  den_act='gelu',.
-0002a1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a1e0: 206c 616d 6264 615f 6675 6e63 3d4e 6f6e   lambda_func=Non
-0002a1f0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002a200: 2020 2020 7573 655f 7061 7374 3d46 616c      use_past=Fal
-0002a210: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
-0002a220: 2020 2020 206f 6666 7365 743d 302c 0a20       offset=0,. 
-0002a230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a240: 6d6f 655f 636f 6e66 6967 3d64 6566 6175  moe_config=defau
-0002a250: 6c74 5f6d 6f65 5f63 6f6e 6669 672c 0a20  lt_moe_config,. 
-0002a260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a270: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-0002a280: 6465 6661 756c 745f 7472 616e 7366 6f72  default_transfor
-0002a290: 6d65 725f 636f 6e66 6967 293a 0a20 2020  mer_config):.   
-0002a2a0: 2020 2020 2073 7570 6572 2854 7261 6e73       super(Trans
-0002a2b0: 666f 726d 6572 4465 636f 6465 722c 2073  formerDecoder, s
-0002a2c0: 656c 6629 2e5f 5f69 6e69 745f 5f28 290a  elf).__init__().
-0002a2d0: 2020 2020 2020 2020 5f63 6865 636b 5f6d          _check_m
-0002a2e0: 6f65 5f63 6f6e 6669 6728 6d6f 655f 636f  oe_config(moe_co
-0002a2f0: 6e66 6967 2c20 7061 7261 6c6c 656c 5f63  nfig, parallel_c
-0002a300: 6f6e 6669 6729 0a20 2020 2020 2020 205f  onfig).        _
-0002a310: 6368 6563 6b5f 636f 6e66 6967 2870 6172  check_config(par
+00029f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f60: 2020 2020 2020 2020 2254 7261 6e73 666f          "Transfo
+00029f70: 726d 6572 4465 636f 6465 7222 292c 0a20  rmerDecoder"),. 
+00029f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f90: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+00029fa0: 7365 5f70 6173 743d 5661 6c69 6461 746f  se_past=Validato
+00029fb0: 722e 6368 6563 6b5f 626f 6f6c 290a 2020  r.check_bool).  
+00029fc0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+00029fd0: 656c 662c 0a20 2020 2020 2020 2020 2020  elf,.           
+00029fe0: 2020 2020 2020 6e75 6d5f 6c61 7965 7273        num_layers
+00029ff0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002a000: 2020 2062 6174 6368 5f73 697a 652c 0a20     batch_size,. 
+0002a010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a020: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
+0002a030: 2020 2020 2020 2020 2020 2020 2020 6666                ff
+0002a040: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
+0002a050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a060: 7372 635f 7365 715f 6c65 6e67 7468 2c0a  src_seq_length,.
+0002a070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a080: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
+0002a090: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002a0a0: 2020 6e75 6d5f 6865 6164 732c 0a20 2020    num_heads,.   
+0002a0b0: 2020 2020 2020 2020 2020 2020 2020 6174                at
+0002a0c0: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+0002a0d0: 7261 7465 3d30 2e31 2c0a 2020 2020 2020  rate=0.1,.      
+0002a0e0: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+0002a0f0: 6e5f 6472 6f70 6f75 745f 7261 7465 3d30  n_dropout_rate=0
+0002a100: 2e31 2c0a 2020 2020 2020 2020 2020 2020  .1,.            
+0002a110: 2020 2020 2070 6f73 745f 6c61 7965 726e       post_layern
+0002a120: 6f72 6d5f 7265 7369 6475 616c 3d46 616c  orm_residual=Fal
+0002a130: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
+0002a140: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
+0002a150: 6f6d 7075 7465 5f74 7970 653d 6d73 7479  ompute_type=msty
+0002a160: 7065 2e66 6c6f 6174 3332 2c0a 2020 2020  pe.float32,.    
+0002a170: 2020 2020 2020 2020 2020 2020 2073 6f66               sof
+0002a180: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0002a190: 653d 6d73 7479 7065 2e66 6c6f 6174 3332  e=mstype.float32
+0002a1a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002a1b0: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+0002a1c0: 7065 3d6d 7374 7970 652e 666c 6f61 7433  pe=mstype.float3
+0002a1d0: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
+0002a1e0: 2020 2020 6869 6464 656e 5f61 6374 3d27      hidden_act='
+0002a1f0: 6765 6c75 272c 0a20 2020 2020 2020 2020  gelu',.         
+0002a200: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+0002a210: 756e 633d 4e6f 6e65 2c0a 2020 2020 2020  unc=None,.      
+0002a220: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
+0002a230: 6173 743d 4661 6c73 652c 0a20 2020 2020  ast=False,.     
+0002a240: 2020 2020 2020 2020 2020 2020 6f66 6673              offs
+0002a250: 6574 3d30 2c0a 2020 2020 2020 2020 2020  et=0,.          
+0002a260: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
+0002a270: 673d 6465 6661 756c 745f 6d6f 655f 636f  g=default_moe_co
+0002a280: 6e66 6967 2c0a 2020 2020 2020 2020 2020  nfig,.          
+0002a290: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+0002a2a0: 636f 6e66 6967 3d64 6566 6175 6c74 5f74  config=default_t
+0002a2b0: 7261 6e73 666f 726d 6572 5f63 6f6e 6669  ransformer_confi
+0002a2c0: 6729 3a0a 2020 2020 2020 2020 7375 7065  g):.        supe
+0002a2d0: 7228 5472 616e 7366 6f72 6d65 7244 6563  r(TransformerDec
+0002a2e0: 6f64 6572 2c20 7365 6c66 292e 5f5f 696e  oder, self).__in
+0002a2f0: 6974 5f5f 2829 0a20 2020 2020 2020 205f  it__().        _
+0002a300: 6368 6563 6b5f 6d6f 655f 636f 6e66 6967  check_moe_config
+0002a310: 286d 6f65 5f63 6f6e 6669 672c 2070 6172  (moe_config, par
 0002a320: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-0002a330: 2020 2020 2020 7365 6c66 2e75 7365 5f6d        self.use_m
-0002a340: 6f65 203d 2028 6d6f 655f 636f 6e66 6967  oe = (moe_config
-0002a350: 2e65 7870 6572 745f 6e75 6d20 3e20 3129  .expert_num > 1)
-0002a360: 0a20 2020 2020 2020 2069 6620 6261 7463  .        if batc
-0002a370: 685f 7369 7a65 206f 7220 7573 655f 7061  h_size or use_pa
-0002a380: 7374 3a0a 2020 2020 2020 2020 2020 2020  st:.            
-0002a390: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-0002a3a0: 706f 7369 7469 7665 5f69 6e74 2862 6174  positive_int(bat
-0002a3b0: 6368 5f73 697a 6529 0a20 2020 2020 2020  ch_size).       
-0002a3c0: 2063 6f6e 6669 675f 746f 5f6c 6179 6572   config_to_layer
-0002a3d0: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
-0002a3e0: 6967 2e6d 6f65 5f70 6172 616c 6c65 6c5f  ig.moe_parallel_
-0002a3f0: 636f 6e66 6967 2069 6620 7365 6c66 2e75  config if self.u
-0002a400: 7365 5f6d 6f65 2065 6c73 6520 7061 7261  se_moe else para
-0002a410: 6c6c 656c 5f63 6f6e 6669 672e 6470 5f6d  llel_config.dp_m
-0002a420: 705f 636f 6e66 6967 0a20 2020 2020 2020  p_config.       
-0002a430: 2069 6620 5f67 6574 5f70 6172 616c 6c65   if _get_paralle
-0002a440: 6c5f 6d6f 6465 2829 2069 6e20 2850 6172  l_mode() in (Par
-0002a450: 616c 6c65 6c4d 6f64 652e 4155 544f 5f50  allelMode.AUTO_P
-0002a460: 4152 414c 4c45 4c2c 293a 0a20 2020 2020  ARALLEL,):.     
-0002a470: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
-0002a480: 3d20 502e 4164 6428 290a 2020 2020 2020  = P.Add().      
-0002a490: 2020 2020 2020 7365 6c66 2e61 7578 5f6c        self.aux_l
-0002a4a0: 6f73 7320 3d20 5465 6e73 6f72 2830 2e30  oss = Tensor(0.0
-0002a4b0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
-0002a4c0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0002a4d0: 6c66 2e6e 756d 5f6c 6179 6572 7320 3d20  lf.num_layers = 
-0002a4e0: 6e75 6d5f 6c61 7965 7273 0a20 2020 2020  num_layers.     
-0002a4f0: 2020 2020 2020 2073 656c 662e 626c 6f63         self.bloc
-0002a500: 6b73 203d 206e 6e2e 4365 6c6c 4c69 7374  ks = nn.CellList
-0002a510: 2829 0a0a 2020 2020 2020 2020 2020 2020  ()..            
-0002a520: 666f 7220 6920 696e 2072 616e 6765 286e  for i in range(n
-0002a530: 756d 5f6c 6179 6572 7329 3a0a 2020 2020  um_layers):.    
-0002a540: 2020 2020 2020 2020 2020 2020 626c 6f63              bloc
-0002a550: 6b20 3d20 5472 616e 7366 6f72 6d65 7244  k = TransformerD
-0002a560: 6563 6f64 6572 4c61 7965 7228 6869 6464  ecoderLayer(hidd
-0002a570: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
-0002a580: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0002a590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a5b0: 2020 2020 2062 6174 6368 5f73 697a 653d       batch_size=
-0002a5c0: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
-0002a5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a5f0: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
-0002a600: 6869 6464 656e 5f73 697a 653d 6666 6e5f  hidden_size=ffn_
-0002a610: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-0002a620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a640: 2020 2020 2020 2020 2020 2020 2073 7263               src
-0002a650: 5f73 6571 5f6c 656e 6774 683d 7372 635f  _seq_length=src_
-0002a660: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
-0002a670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a690: 2020 2020 2020 2020 2020 2020 7467 745f              tgt_
-0002a6a0: 7365 715f 6c65 6e67 7468 3d74 6774 5f73  seq_length=tgt_s
-0002a6b0: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-0002a6c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a6e0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-0002a6f0: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-0002a700: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
-0002a710: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-0002a720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a740: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-0002a750: 5f64 726f 706f 7574 5f72 6174 653d 6869  _dropout_rate=hi
-0002a760: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-0002a770: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002a780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a7a0: 2020 206e 756d 5f68 6561 6473 3d6e 756d     num_heads=num
-0002a7b0: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
-0002a7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a7e0: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
-0002a7f0: 6d5f 636f 6d70 7574 655f 7479 7065 3d6c  m_compute_type=l
-0002a800: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-0002a810: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
-0002a820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a840: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-0002a850: 6f6d 7075 7465 5f74 7970 653d 736f 6674  ompute_type=soft
-0002a860: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-0002a870: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002a880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a8a0: 2020 6869 6464 656e 5f61 6374 3d68 6964    hidden_act=hid
-0002a8b0: 6465 6e5f 6163 742c 0a20 2020 2020 2020  den_act,.       
-0002a8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a8e0: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-0002a8f0: 743d 7573 655f 7061 7374 2c0a 2020 2020  t=use_past,.    
+0002a330: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
+0002a340: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
+0002a350: 6669 6729 0a20 2020 2020 2020 2073 656c  fig).        sel
+0002a360: 662e 7573 655f 6d6f 6520 3d20 286d 6f65  f.use_moe = (moe
+0002a370: 5f63 6f6e 6669 672e 6578 7065 7274 5f6e  _config.expert_n
+0002a380: 756d 203e 2031 290a 2020 2020 2020 2020  um > 1).        
+0002a390: 6966 2062 6174 6368 5f73 697a 6520 6f72  if batch_size or
+0002a3a0: 2075 7365 5f70 6173 743a 0a20 2020 2020   use_past:.     
+0002a3b0: 2020 2020 2020 2056 616c 6964 6174 6f72         Validator
+0002a3c0: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
+0002a3d0: 696e 7428 6261 7463 685f 7369 7a65 290a  int(batch_size).
+0002a3e0: 2020 2020 2020 2020 636f 6e66 6967 5f74          config_t
+0002a3f0: 6f5f 6c61 7965 7220 3d20 7061 7261 6c6c  o_layer = parall
+0002a400: 656c 5f63 6f6e 6669 672e 6d6f 655f 7061  el_config.moe_pa
+0002a410: 7261 6c6c 656c 5f63 6f6e 6669 6720 6966  rallel_config if
+0002a420: 2073 656c 662e 7573 655f 6d6f 6520 656c   self.use_moe el
+0002a430: 7365 2070 6172 616c 6c65 6c5f 636f 6e66  se parallel_conf
+0002a440: 6967 2e64 705f 6d70 5f63 6f6e 6669 670a  ig.dp_mp_config.
+0002a450: 2020 2020 2020 2020 6966 205f 6765 745f          if _get_
+0002a460: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
+0002a470: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
+0002a480: 2e41 5554 4f5f 5041 5241 4c4c 454c 2c29  .AUTO_PARALLEL,)
+0002a490: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+0002a4a0: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
+0002a4b0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0002a4c0: 662e 6175 785f 6c6f 7373 203d 2054 656e  f.aux_loss = Ten
+0002a4d0: 736f 7228 302e 302c 206d 7374 7970 652e  sor(0.0, mstype.
+0002a4e0: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
+0002a4f0: 2020 2020 2073 656c 662e 6e75 6d5f 6c61       self.num_la
+0002a500: 7965 7273 203d 206e 756d 5f6c 6179 6572  yers = num_layer
+0002a510: 730a 2020 2020 2020 2020 2020 2020 7365  s.            se
+0002a520: 6c66 2e62 6c6f 636b 7320 3d20 6e6e 2e43  lf.blocks = nn.C
+0002a530: 656c 6c4c 6973 7428 290a 0a20 2020 2020  ellList()..     
+0002a540: 2020 2020 2020 2066 6f72 2069 2069 6e20         for i in 
+0002a550: 7261 6e67 6528 6e75 6d5f 6c61 7965 7273  range(num_layers
+0002a560: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
+0002a570: 2020 2062 6c6f 636b 203d 2054 7261 6e73     block = Trans
+0002a580: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
+0002a590: 6572 2868 6964 6465 6e5f 7369 7a65 3d68  er(hidden_size=h
+0002a5a0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0002a5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a5d0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
+0002a5e0: 685f 7369 7a65 3d62 6174 6368 5f73 697a  h_size=batch_siz
+0002a5f0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002a600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a620: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0002a630: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
+0002a640: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+0002a650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a670: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
+0002a680: 7468 3d73 7263 5f73 6571 5f6c 656e 6774  th=src_seq_lengt
+0002a690: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
+0002a6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6c0: 2020 2074 6774 5f73 6571 5f6c 656e 6774     tgt_seq_lengt
+0002a6d0: 683d 7467 745f 7365 715f 6c65 6e67 7468  h=tgt_seq_length
+0002a6e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002a6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a710: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+0002a720: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
+0002a730: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
+0002a740: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002a750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a770: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+0002a780: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
+0002a790: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+0002a7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a7c0: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
+0002a7d0: 6164 733d 6e75 6d5f 6865 6164 732c 0a20  ads=num_heads,. 
+0002a7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a800: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+0002a810: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
+0002a820: 5f74 7970 653d 6c61 7965 726e 6f72 6d5f  _type=layernorm_
+0002a830: 636f 6d70 7574 655f 7479 7065 2c0a 2020  compute_type,.  
+0002a840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a860: 2020 2020 2020 2020 2020 2020 2020 736f                so
+0002a870: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+0002a880: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
+0002a890: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+0002a8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8c0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002a8d0: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+0002a8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002a900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a920: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0002a930: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-0002a940: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
-0002a950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a970: 2020 2020 2020 2020 2020 2020 2070 6f73               pos
-0002a980: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
-0002a990: 6475 616c 3d70 6f73 745f 6c61 7965 726e  dual=post_layern
-0002a9a0: 6f72 6d5f 7265 7369 6475 616c 2c0a 2020  orm_residual,.  
-0002a9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a9d0: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-0002a9e0: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
-0002a9f0: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
-0002aa00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002aa10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002aa20: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-0002aa30: 6e66 6967 3d63 6f6e 6669 675f 746f 5f6c  nfig=config_to_l
-0002aa40: 6179 6572 290a 2020 2020 2020 2020 2020  ayer).          
-0002aa50: 2020 2020 2020 2320 4966 2074 6865 2075        # If the u
-0002aa60: 7365 7220 646f 6573 6e27 7420 7061 7373  ser doesn't pass
-0002aa70: 2074 6865 2066 7573 696f 6e20 6675 6e63   the fusion func
-0002aa80: 7469 6f6e 2c20 7573 6520 7468 6520 6465  tion, use the de
-0002aa90: 6661 756c 7420 6f6e 650a 2020 2020 2020  fault one.      
-0002aaa0: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-0002aab0: 206c 616d 6264 615f 6675 6e63 3a0a 2020   lambda_func:.  
-0002aac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002aad0: 2020 6c61 6d62 6461 5f66 756e 6320 3d20    lambda_func = 
-0002aae0: 5f67 6574 5f6c 616d 6264 615f 6675 6e63  _get_lambda_func
-0002aaf0: 2829 0a0a 2020 2020 2020 2020 2020 2020  ()..            
-0002ab00: 2020 2020 6c61 6d62 6461 5f66 756e 6328      lambda_func(
-0002ab10: 626c 6f63 6b2c 206c 6179 6572 5f69 643d  block, layer_id=
-0002ab20: 692c 206c 6179 6572 733d 6e75 6d5f 6c61  i, layers=num_la
-0002ab30: 7965 7273 2c0a 2020 2020 2020 2020 2020  yers,.          
-0002ab40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ab50: 2020 6f66 6673 6574 3d6f 6666 7365 742c    offset=offset,
-0002ab60: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0002ab70: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
-0002ab80: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
-0002ab90: 2020 2073 656c 662e 626c 6f63 6b73 2e61     self.blocks.a
-0002aba0: 7070 656e 6428 626c 6f63 6b29 0a20 2020  ppend(block).   
-0002abb0: 2020 2020 2065 6c69 6620 5f67 6574 5f70       elif _get_p
-0002abc0: 6172 616c 6c65 6c5f 6d6f 6465 2829 206e  arallel_mode() n
-0002abd0: 6f74 2069 6e20 2850 6172 616c 6c65 6c4d  ot in (ParallelM
-0002abe0: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
-0002abf0: 4c2c 293a 0a20 2020 2020 2020 2020 2020  L,):.           
-0002ac00: 2073 656c 662e 6164 6420 3d20 502e 4164   self.add = P.Ad
-0002ac10: 6428 292e 7368 6172 6428 2828 292c 2028  d().shard(((), (
-0002ac20: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-0002ac30: 7365 6c66 2e61 7578 5f6c 6f73 7320 3d20  self.aux_loss = 
-0002ac40: 5465 6e73 6f72 2830 2e30 2c20 6d73 7479  Tensor(0.0, msty
-0002ac50: 7065 2e66 6c6f 6174 3332 290a 2020 2020  pe.float32).    
-0002ac60: 2020 2020 2020 2020 6c6f 6767 6572 2e77          logger.w
-0002ac70: 6172 6e69 6e67 2822 466f 7220 7061 7261  arning("For para
-0002ac80: 6c6c 656c 206d 6f64 652c 2073 6861 7264  llel mode, shard
-0002ac90: 696e 6720 7072 6f70 6167 6174 696f 6e20  ing propagation 
-0002aca0: 6973 2072 6563 6f6d 6d65 6e64 6564 2c20  is recommended, 
-0002acb0: 796f 7520 6361 6e20 7573 6520 6974 2062  you can use it b
-0002acc0: 7920 7365 7474 696e 6720 220a 2020 2020  y setting ".    
-0002acd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ace0: 2020 2020 2020 2022 2773 6574 5f61 7574         "'set_aut
-0002acf0: 6f5f 7061 7261 6c6c 656c 5f63 6f6e 7465  o_parallel_conte
-0002ad00: 7874 2870 6172 616c 6c65 6c5f 6d6f 6465  xt(parallel_mode
-0002ad10: 3d50 6172 616c 6c65 6c4d 6f64 652e 4155  =ParallelMode.AU
-0002ad20: 544f 5f50 4152 414c 4c45 4c2c 2022 0a20  TO_PARALLEL, ". 
-0002ad30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ad40: 2020 2020 2020 2020 2020 2273 6561 7263            "searc
-0002ad50: 685f 6d6f 6465 3d5c 2273 6861 7264 696e  h_mode=\"shardin
-0002ad60: 675f 7072 6f70 6167 6174 696f 6e5c 2229  g_propagation\")
-0002ad70: 2720 616e 6420 220a 2020 2020 2020 2020  ' and ".        
-0002ad80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ad90: 2020 2022 2773 6574 5f61 6c67 6f5f 7061     "'set_algo_pa
-0002ada0: 7261 6d65 7465 7273 2865 6c65 6d65 6e74  rameters(element
-0002adb0: 7769 7365 5f6f 705f 7374 7261 7465 6779  wise_op_strategy
-0002adc0: 5f66 6f6c 6c6f 773d 4661 6c73 652c 2066  _follow=False, f
-0002add0: 756c 6c79 5f75 7365 5f64 6576 6963 6573  ully_use_devices
-0002ade0: 3d46 616c 7365 2927 2229 0a20 2020 2020  =False)'").     
-0002adf0: 2020 2020 2020 2073 656c 662e 6e75 6d5f         self.num_
-0002ae00: 6c61 7965 7273 203d 206e 756d 5f6c 6179  layers = num_lay
-0002ae10: 6572 730a 2020 2020 2020 2020 2020 2020  ers.            
-0002ae20: 7365 6c66 2e62 6c6f 636b 7320 3d20 6e6e  self.blocks = nn
-0002ae30: 2e43 656c 6c4c 6973 7428 290a 2020 2020  .CellList().    
-0002ae40: 2020 2020 2020 2020 666f 7220 6920 696e          for i in
-0002ae50: 2072 616e 6765 286e 756d 5f6c 6179 6572   range(num_layer
-0002ae60: 7329 3a0a 2020 2020 2020 2020 2020 2020  s):.            
-0002ae70: 2020 2020 626c 6f63 6b20 3d20 5472 616e      block = Tran
-0002ae80: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
-0002ae90: 7965 7228 6869 6464 656e 5f73 697a 653d  yer(hidden_size=
-0002aea0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-0002aeb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002aec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002aed0: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-0002aee0: 6368 5f73 697a 653d 6261 7463 685f 7369  ch_size=batch_si
-0002aef0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-0002af00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002af10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002af20: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
-0002af30: 697a 653d 6666 6e5f 6869 6464 656e 5f73  ize=ffn_hidden_s
-0002af40: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0002af50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002af60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002af70: 2020 2020 2073 7263 5f73 6571 5f6c 656e       src_seq_len
-0002af80: 6774 683d 7372 635f 7365 715f 6c65 6e67  gth=src_seq_leng
-0002af90: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
-0002afa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002afb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002afc0: 2020 2020 7467 745f 7365 715f 6c65 6e67      tgt_seq_leng
-0002afd0: 7468 3d74 6774 5f73 6571 5f6c 656e 6774  th=tgt_seq_lengt
-0002afe0: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
-0002aff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b010: 2020 2061 7474 656e 7469 6f6e 5f64 726f     attention_dro
-0002b020: 706f 7574 5f72 6174 653d 6174 7465 6e74  pout_rate=attent
-0002b030: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-0002b040: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002b050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b070: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
-0002b080: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
-0002b090: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
-0002b0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b0c0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
-0002b0d0: 6561 6473 3d6e 756d 5f68 6561 6473 2c0a  eads=num_heads,.
+0002a910: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
+0002a920: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+0002a930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a950: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+0002a960: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
+0002a970: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+0002a980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a9a0: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
+0002a9b0: 726d 5f72 6573 6964 7561 6c3d 706f 7374  rm_residual=post
+0002a9c0: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
+0002a9d0: 7561 6c2c 0a20 2020 2020 2020 2020 2020  ual,.           
+0002a9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002aa00: 2020 2020 206d 6f65 5f63 6f6e 6669 673d       moe_config=
+0002aa10: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
+0002aa20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002aa30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002aa40: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0002aa50: 6c6c 656c 5f63 6f6e 6669 673d 636f 6e66  llel_config=conf
+0002aa60: 6967 5f74 6f5f 6c61 7965 7229 0a20 2020  ig_to_layer).   
+0002aa70: 2020 2020 2020 2020 2020 2020 2023 2049               # I
+0002aa80: 6620 7468 6520 7573 6572 2064 6f65 736e  f the user doesn
+0002aa90: 2774 2070 6173 7320 7468 6520 6675 7369  't pass the fusi
+0002aaa0: 6f6e 2066 756e 6374 696f 6e2c 2075 7365  on function, use
+0002aab0: 2074 6865 2064 6566 6175 6c74 206f 6e65   the default one
+0002aac0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002aad0: 2069 6620 6e6f 7420 6c61 6d62 6461 5f66   if not lambda_f
+0002aae0: 756e 633a 0a20 2020 2020 2020 2020 2020  unc:.           
+0002aaf0: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
+0002ab00: 6675 6e63 203d 205f 6765 745f 6c61 6d62  func = _get_lamb
+0002ab10: 6461 5f66 756e 6328 290a 0a20 2020 2020  da_func()..     
+0002ab20: 2020 2020 2020 2020 2020 206c 616d 6264             lambd
+0002ab30: 615f 6675 6e63 2862 6c6f 636b 2c20 6c61  a_func(block, la
+0002ab40: 7965 725f 6964 3d69 2c20 6c61 7965 7273  yer_id=i, layers
+0002ab50: 3d6e 756d 5f6c 6179 6572 732c 0a20 2020  =num_layers,.   
+0002ab60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ab70: 2020 2020 2020 2020 206f 6666 7365 743d           offset=
+0002ab80: 6f66 6673 6574 2c20 7061 7261 6c6c 656c  offset, parallel
+0002ab90: 5f63 6f6e 6669 673d 7061 7261 6c6c 656c  _config=parallel
+0002aba0: 5f63 6f6e 6669 6729 0a0a 2020 2020 2020  _config)..      
+0002abb0: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
+0002abc0: 6c6f 636b 732e 6170 7065 6e64 2862 6c6f  locks.append(blo
+0002abd0: 636b 290a 2020 2020 2020 2020 656c 6966  ck).        elif
+0002abe0: 205f 6765 745f 7061 7261 6c6c 656c 5f6d   _get_parallel_m
+0002abf0: 6f64 6528 2920 6e6f 7420 696e 2028 5061  ode() not in (Pa
+0002ac00: 7261 6c6c 656c 4d6f 6465 2e41 5554 4f5f  rallelMode.AUTO_
+0002ac10: 5041 5241 4c4c 454c 2c29 3a0a 2020 2020  PARALLEL,):.    
+0002ac20: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+0002ac30: 203d 2050 2e41 6464 2829 2e73 6861 7264   = P.Add().shard
+0002ac40: 2828 2829 2c20 2829 2929 0a20 2020 2020  (((), ())).     
+0002ac50: 2020 2020 2020 2073 656c 662e 6175 785f         self.aux_
+0002ac60: 6c6f 7373 203d 2054 656e 736f 7228 302e  loss = Tensor(0.
+0002ac70: 302c 206d 7374 7970 652e 666c 6f61 7433  0, mstype.float3
+0002ac80: 3229 0a20 2020 2020 2020 2020 2020 206c  2).            l
+0002ac90: 6f67 6765 722e 7761 726e 696e 6728 2246  ogger.warning("F
+0002aca0: 6f72 2070 6172 616c 6c65 6c20 6d6f 6465  or parallel mode
+0002acb0: 2c20 7368 6172 6469 6e67 2070 726f 7061  , sharding propa
+0002acc0: 6761 7469 6f6e 2069 7320 7265 636f 6d6d  gation is recomm
+0002acd0: 656e 6465 642c 2079 6f75 2063 616e 2075  ended, you can u
+0002ace0: 7365 2069 7420 6279 2073 6574 7469 6e67  se it by setting
+0002acf0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
+0002ad00: 2020 2020 2020 2020 2020 2020 2020 2227                "'
+0002ad10: 7365 745f 6175 746f 5f70 6172 616c 6c65  set_auto_paralle
+0002ad20: 6c5f 636f 6e74 6578 7428 7061 7261 6c6c  l_context(parall
+0002ad30: 656c 5f6d 6f64 653d 5061 7261 6c6c 656c  el_mode=Parallel
+0002ad40: 4d6f 6465 2e41 5554 4f5f 5041 5241 4c4c  Mode.AUTO_PARALL
+0002ad50: 454c 2c20 220a 2020 2020 2020 2020 2020  EL, ".          
+0002ad60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ad70: 2022 7365 6172 6368 5f6d 6f64 653d 5c22   "search_mode=\"
+0002ad80: 7368 6172 6469 6e67 5f70 726f 7061 6761  sharding_propaga
+0002ad90: 7469 6f6e 5c22 2927 2061 6e64 2022 0a20  tion\")' and ". 
+0002ada0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002adb0: 2020 2020 2020 2020 2020 2227 7365 745f            "'set_
+0002adc0: 616c 676f 5f70 6172 616d 6574 6572 7328  algo_parameters(
+0002add0: 656c 656d 656e 7477 6973 655f 6f70 5f73  elementwise_op_s
+0002ade0: 7472 6174 6567 795f 666f 6c6c 6f77 3d46  trategy_follow=F
+0002adf0: 616c 7365 2c20 6675 6c6c 795f 7573 655f  alse, fully_use_
+0002ae00: 6465 7669 6365 733d 4661 6c73 6529 2722  devices=False)'"
+0002ae10: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0002ae20: 6c66 2e6e 756d 5f6c 6179 6572 7320 3d20  lf.num_layers = 
+0002ae30: 6e75 6d5f 6c61 7965 7273 0a20 2020 2020  num_layers.     
+0002ae40: 2020 2020 2020 2073 656c 662e 626c 6f63         self.bloc
+0002ae50: 6b73 203d 206e 6e2e 4365 6c6c 4c69 7374  ks = nn.CellList
+0002ae60: 2829 0a20 2020 2020 2020 2020 2020 2066  ().            f
+0002ae70: 6f72 2069 2069 6e20 7261 6e67 6528 6e75  or i in range(nu
+0002ae80: 6d5f 6c61 7965 7273 293a 0a20 2020 2020  m_layers):.     
+0002ae90: 2020 2020 2020 2020 2020 2062 6c6f 636b             block
+0002aea0: 203d 2054 7261 6e73 666f 726d 6572 4465   = TransformerDe
+0002aeb0: 636f 6465 724c 6179 6572 2868 6964 6465  coderLayer(hidde
+0002aec0: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+0002aed0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+0002aee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002aef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002af00: 2020 2020 6261 7463 685f 7369 7a65 3d62      batch_size=b
+0002af10: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
+0002af20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002af30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002af40: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+0002af50: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
+0002af60: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0002af70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002af80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002af90: 2020 2020 2020 2020 2020 2020 7372 635f              src_
+0002afa0: 7365 715f 6c65 6e67 7468 3d73 7263 5f73  seq_length=src_s
+0002afb0: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
+0002afc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002afd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002afe0: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
+0002aff0: 6571 5f6c 656e 6774 683d 7467 745f 7365  eq_length=tgt_se
+0002b000: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
+0002b010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b030: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+0002b040: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0002b050: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
+0002b060: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+0002b070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b090: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002b0a0: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
+0002b0b0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+0002b0c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002b0d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002b0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b0f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b110: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-0002b120: 655f 7479 7065 3d6c 6179 6572 6e6f 726d  e_type=layernorm
-0002b130: 5f63 6f6d 7075 7465 5f74 7970 652c 0a20  _compute_type,. 
-0002b140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b160: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0002b170: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-0002b180: 7970 653d 736f 6674 6d61 785f 636f 6d70  ype=softmax_comp
-0002b190: 7574 655f 7479 7065 2c0a 2020 2020 2020  ute_type,.      
-0002b1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b1c0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-0002b1d0: 5f61 6374 3d68 6964 6465 6e5f 6163 742c  _act=hidden_act,
-0002b1e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002b1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b210: 2075 7365 5f70 6173 743d 7573 655f 7061   use_past=use_pa
-0002b220: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
-0002b230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b250: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-0002b260: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-0002b270: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-0002b280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b2a0: 2020 2020 2070 6f73 745f 6c61 7965 726e       post_layern
-0002b2b0: 6f72 6d5f 7265 7369 6475 616c 3d70 6f73  orm_residual=pos
-0002b2c0: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
-0002b2d0: 6475 616c 2c0a 2020 2020 2020 2020 2020  dual,.          
-0002b2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b2f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b300: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
-0002b310: 3d6d 6f65 5f63 6f6e 6669 672c 0a20 2020  =moe_config,.   
-0002b320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b340: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0002b350: 616c 6c65 6c5f 636f 6e66 6967 3d63 6f6e  allel_config=con
-0002b360: 6669 675f 746f 5f6c 6179 6572 290a 2020  fig_to_layer).  
-0002b370: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-0002b380: 4966 2074 6865 2075 7365 7220 646f 6573  If the user does
-0002b390: 6e27 7420 7061 7373 2074 6865 2066 7573  n't pass the fus
-0002b3a0: 696f 6e20 6675 6e63 7469 6f6e 2c20 7573  ion function, us
-0002b3b0: 6520 7468 6520 6465 6661 756c 7420 6f6e  e the default on
-0002b3c0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-0002b3d0: 2020 6966 206e 6f74 206c 616d 6264 615f    if not lambda_
-0002b3e0: 6675 6e63 3a0a 2020 2020 2020 2020 2020  func:.          
-0002b3f0: 2020 2020 2020 2020 2020 6c61 6d62 6461            lambda
-0002b400: 5f66 756e 6320 3d20 5f67 6574 5f6c 616d  _func = _get_lam
-0002b410: 6264 615f 6675 6e63 2829 0a0a 2020 2020  bda_func()..    
-0002b420: 2020 2020 2020 2020 2020 2020 6c61 6d62              lamb
-0002b430: 6461 5f66 756e 6328 626c 6f63 6b2c 206c  da_func(block, l
-0002b440: 6179 6572 5f69 643d 692c 206c 6179 6572  ayer_id=i, layer
-0002b450: 733d 6e75 6d5f 6c61 7965 7273 2c0a 2020  s=num_layers,.  
-0002b460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b470: 2020 2020 2020 2020 2020 6f66 6673 6574            offset
-0002b480: 3d6f 6666 7365 742c 2070 6172 616c 6c65  =offset, paralle
-0002b490: 6c5f 636f 6e66 6967 3d70 6172 616c 6c65  l_config=paralle
-0002b4a0: 6c5f 636f 6e66 6967 290a 0a20 2020 2020  l_config)..     
-0002b4b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0002b4c0: 626c 6f63 6b73 2e61 7070 656e 6428 626c  blocks.append(bl
-0002b4d0: 6f63 6b29 0a20 2020 2020 2020 2065 6c73  ock).        els
-0002b4e0: 653a 0a20 2020 2020 2020 2020 2020 2072  e:.            r
-0002b4f0: 6169 7365 2052 756e 7469 6d65 4572 726f  aise RuntimeErro
-0002b500: 7228 6622 5468 6520 7b73 656c 662e 636c  r(f"The {self.cl
-0002b510: 735f 6e61 6d65 7d20 6f6e 6c79 2073 7570  s_name} only sup
-0002b520: 706f 7274 2073 6861 7264 696e 6720 7072  port sharding pr
-0002b530: 6f70 6167 6174 696f 6e20 6f72 2022 0a20  opagation or ". 
-0002b540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b550: 2020 2020 2020 2020 2020 2020 2020 6622                f"
-0002b560: 7365 6d69 2d61 7574 6f20 7061 7261 6c6c  semi-auto parall
-0002b570: 656c 206d 6f64 6520 6e6f 772e 2229 0a0a  el mode now.")..
-0002b580: 2020 2020 6465 6620 636f 6e73 7472 7563      def construc
-0002b590: 7428 7365 6c66 2c20 6869 6464 656e 5f73  t(self, hidden_s
-0002b5a0: 7461 7465 732c 2061 7474 656e 7469 6f6e  tates, attention
-0002b5b0: 5f6d 6173 6b2c 2065 6e63 6f64 6572 5f6f  _mask, encoder_o
-0002b5c0: 7574 7075 743d 4e6f 6e65 2c20 6d65 6d6f  utput=None, memo
-0002b5d0: 7279 5f6d 6173 6b3d 4e6f 6e65 2c0a 2020  ry_mask=None,.  
-0002b5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b5f0: 696e 6974 5f72 6573 6574 3d54 7275 652c  init_reset=True,
-0002b600: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-0002b610: 6774 683d 4e6f 6e65 293a 0a20 2020 2020  gth=None):.     
-0002b620: 2020 2022 2222 666f 7277 6172 6420 7072     """forward pr
-0002b630: 6f63 6573 7322 2222 0a20 2020 2020 2020  ocess""".       
-0002b640: 2070 7265 7365 6e74 5f6c 6179 6572 203d   present_layer =
-0002b650: 2028 290a 2020 2020 2020 2020 6966 2073   ().        if s
-0002b660: 656c 662e 7573 655f 6d6f 653a 0a20 2020  elf.use_moe:.   
-0002b670: 2020 2020 2020 2020 2061 6363 756d 5f6c           accum_l
-0002b680: 6f73 7320 3d20 7365 6c66 2e61 7578 5f6c  oss = self.aux_l
-0002b690: 6f73 730a 2020 2020 2020 2020 2020 2020  oss.            
-0002b6a0: 666f 7220 6920 696e 2072 616e 6765 2873  for i in range(s
-0002b6b0: 656c 662e 6e75 6d5f 6c61 7965 7273 293a  elf.num_layers):
-0002b6c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002b6d0: 2068 6964 6465 6e5f 7374 6174 6573 2c20   hidden_states, 
-0002b6e0: 7072 6573 656e 742c 2061 7578 5f6c 6f73  present, aux_los
-0002b6f0: 7320 3d20 7365 6c66 2e62 6c6f 636b 735b  s = self.blocks[
-0002b700: 695d 2868 6964 6465 6e5f 7374 6174 6573  i](hidden_states
-0002b710: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002b720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b0f0: 2020 6e75 6d5f 6865 6164 733d 6e75 6d5f    num_heads=num_
+0002b100: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
+0002b110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b120: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b130: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
+0002b140: 5f63 6f6d 7075 7465 5f74 7970 653d 6c61  _compute_type=la
+0002b150: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
+0002b160: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+0002b170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b190: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
+0002b1a0: 6d70 7574 655f 7479 7065 3d73 6f66 746d  mpute_type=softm
+0002b1b0: 6178 5f63 6f6d 7075 7465 5f74 7970 652c  ax_compute_type,
+0002b1c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002b1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b1f0: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
+0002b200: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
+0002b210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b230: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
+0002b240: 3d75 7365 5f70 6173 742c 0a20 2020 2020  =use_past,.     
+0002b250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b270: 2020 2020 2020 2020 2020 2070 6172 616d             param
+0002b280: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
+0002b290: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
+0002b2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b2b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b2c0: 2020 2020 2020 2020 2020 2020 706f 7374              post
+0002b2d0: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
+0002b2e0: 7561 6c3d 706f 7374 5f6c 6179 6572 6e6f  ual=post_layerno
+0002b2f0: 726d 5f72 6573 6964 7561 6c2c 0a20 2020  rm_residual,.   
+0002b300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b320: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
+0002b330: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
+0002b340: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
+0002b350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b370: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
+0002b380: 6669 673d 636f 6e66 6967 5f74 6f5f 6c61  fig=config_to_la
+0002b390: 7965 7229 0a20 2020 2020 2020 2020 2020  yer).           
+0002b3a0: 2020 2020 2023 2049 6620 7468 6520 7573       # If the us
+0002b3b0: 6572 2064 6f65 736e 2774 2070 6173 7320  er doesn't pass 
+0002b3c0: 7468 6520 6675 7369 6f6e 2066 756e 6374  the fusion funct
+0002b3d0: 696f 6e2c 2075 7365 2074 6865 2064 6566  ion, use the def
+0002b3e0: 6175 6c74 206f 6e65 0a20 2020 2020 2020  ault one.       
+0002b3f0: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
+0002b400: 6c61 6d62 6461 5f66 756e 633a 0a20 2020  lambda_func:.   
+0002b410: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b420: 206c 616d 6264 615f 6675 6e63 203d 205f   lambda_func = _
+0002b430: 6765 745f 6c61 6d62 6461 5f66 756e 6328  get_lambda_func(
+0002b440: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
+0002b450: 2020 206c 616d 6264 615f 6675 6e63 2862     lambda_func(b
+0002b460: 6c6f 636b 2c20 6c61 7965 725f 6964 3d69  lock, layer_id=i
+0002b470: 2c20 6c61 7965 7273 3d6e 756d 5f6c 6179  , layers=num_lay
+0002b480: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
+0002b490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b4a0: 206f 6666 7365 743d 6f66 6673 6574 2c20   offset=offset, 
+0002b4b0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+0002b4c0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+0002b4d0: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0002b4e0: 2020 7365 6c66 2e62 6c6f 636b 732e 6170    self.blocks.ap
+0002b4f0: 7065 6e64 2862 6c6f 636b 290a 2020 2020  pend(block).    
+0002b500: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0002b510: 2020 2020 2020 7261 6973 6520 5275 6e74        raise Runt
+0002b520: 696d 6545 7272 6f72 2866 2254 6865 207b  imeError(f"The {
+0002b530: 7365 6c66 2e63 6c73 5f6e 616d 657d 206f  self.cls_name} o
+0002b540: 6e6c 7920 7375 7070 6f72 7420 7368 6172  nly support shar
+0002b550: 6469 6e67 2070 726f 7061 6761 7469 6f6e  ding propagation
+0002b560: 206f 7220 220a 2020 2020 2020 2020 2020   or ".          
+0002b570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b580: 2020 2020 2066 2273 656d 692d 6175 746f       f"semi-auto
+0002b590: 2070 6172 616c 6c65 6c20 6d6f 6465 206e   parallel mode n
+0002b5a0: 6f77 2e22 290a 0a20 2020 2064 6566 2063  ow.")..    def c
+0002b5b0: 6f6e 7374 7275 6374 2873 656c 662c 2068  onstruct(self, h
+0002b5c0: 6964 6465 6e5f 7374 6174 6573 2c20 6174  idden_states, at
+0002b5d0: 7465 6e74 696f 6e5f 6d61 736b 2c20 656e  tention_mask, en
+0002b5e0: 636f 6465 725f 6f75 7470 7574 3d4e 6f6e  coder_output=Non
+0002b5f0: 652c 206d 656d 6f72 795f 6d61 736b 3d4e  e, memory_mask=N
+0002b600: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
+0002b610: 2020 2020 2020 2069 6e69 745f 7265 7365         init_rese
+0002b620: 743d 5472 7565 2c20 6261 7463 685f 7661  t=True, batch_va
+0002b630: 6c69 645f 6c65 6e67 7468 3d4e 6f6e 6529  lid_length=None)
+0002b640: 3a0a 2020 2020 2020 2020 2222 2266 6f72  :.        """for
+0002b650: 7761 7264 2070 726f 6365 7373 2222 220a  ward process""".
+0002b660: 2020 2020 2020 2020 7072 6573 656e 745f          present_
+0002b670: 6c61 7965 7220 3d20 2829 0a20 2020 2020  layer = ().     
+0002b680: 2020 2069 6620 7365 6c66 2e75 7365 5f6d     if self.use_m
+0002b690: 6f65 3a0a 2020 2020 2020 2020 2020 2020  oe:.            
+0002b6a0: 6163 6375 6d5f 6c6f 7373 203d 2073 656c  accum_loss = sel
+0002b6b0: 662e 6175 785f 6c6f 7373 0a20 2020 2020  f.aux_loss.     
+0002b6c0: 2020 2020 2020 2066 6f72 2069 2069 6e20         for i in 
+0002b6d0: 7261 6e67 6528 7365 6c66 2e6e 756d 5f6c  range(self.num_l
+0002b6e0: 6179 6572 7329 3a0a 2020 2020 2020 2020  ayers):.        
+0002b6f0: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
+0002b700: 7461 7465 732c 2070 7265 7365 6e74 2c20  tates, present, 
+0002b710: 6175 785f 6c6f 7373 203d 2073 656c 662e  aux_loss = self.
+0002b720: 626c 6f63 6b73 5b69 5d28 6869 6464 656e  blocks[i](hidden
+0002b730: 5f73 7461 7465 732c 0a20 2020 2020 2020  _states,.       
 0002b740: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b750: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
-0002b760: 736b 2c0a 2020 2020 2020 2020 2020 2020  sk,.            
-0002b770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b770: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+0002b780: 7469 6f6e 5f6d 6173 6b2c 0a20 2020 2020  tion_mask,.     
 0002b790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b7a0: 2020 2020 2020 656e 636f 6465 725f 6f75        encoder_ou
-0002b7b0: 7470 7574 2c0a 2020 2020 2020 2020 2020  tput,.          
-0002b7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b7c0: 2020 2020 2020 2020 2020 2020 2065 6e63               enc
+0002b7d0: 6f64 6572 5f6f 7574 7075 742c 0a20 2020  oder_output,.   
 0002b7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b7f0: 2020 2020 2020 2020 6d65 6d6f 7279 5f6d          memory_m
-0002b800: 6173 6b2c 0a20 2020 2020 2020 2020 2020  ask,.           
-0002b810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b810: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+0002b820: 656d 6f72 795f 6d61 736b 2c0a 2020 2020  emory_mask,.    
 0002b830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b840: 2020 2020 2020 2069 6e69 745f 7265 7365         init_rese
-0002b850: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0002b860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b860: 2020 2020 2020 2020 2020 2020 2020 696e                in
+0002b870: 6974 5f72 6573 6574 2c0a 2020 2020 2020  it_reset,.      
 0002b880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b890: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
-0002b8a0: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-0002b8b0: 2020 2020 2020 2020 2070 7265 7365 6e74           present
-0002b8c0: 5f6c 6179 6572 203d 2070 7265 7365 6e74  _layer = present
-0002b8d0: 5f6c 6179 6572 202b 2028 7072 6573 656e  _layer + (presen
-0002b8e0: 742c 290a 2020 2020 2020 2020 2020 2020  t,).            
-0002b8f0: 2020 2020 6163 6375 6d5f 6c6f 7373 203d      accum_loss =
-0002b900: 2073 656c 662e 6164 6428 6163 6375 6d5f   self.add(accum_
-0002b910: 6c6f 7373 2c20 6175 785f 6c6f 7373 290a  loss, aux_loss).
-0002b920: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-0002b930: 726e 2068 6964 6465 6e5f 7374 6174 6573  rn hidden_states
-0002b940: 2c20 7072 6573 656e 745f 6c61 7965 722c  , present_layer,
-0002b950: 2061 6363 756d 5f6c 6f73 730a 0a20 2020   accum_loss..   
-0002b960: 2020 2020 2023 204c 6f6f 7020 7468 726f       # Loop thro
-0002b970: 7567 6820 6561 6368 2073 656c 662d 6174  ugh each self-at
-0002b980: 7465 6e74 696f 6e20 6c61 7965 720a 2020  tention layer.  
-0002b990: 2020 2020 2020 666f 7220 6920 696e 2072        for i in r
-0002b9a0: 616e 6765 2873 656c 662e 6e75 6d5f 6c61  ange(self.num_la
-0002b9b0: 7965 7273 293a 0a20 2020 2020 2020 2020  yers):.         
-0002b9c0: 2020 2068 6964 6465 6e5f 7374 6174 6573     hidden_states
-0002b9d0: 2c20 7072 6573 656e 7420 3d20 7365 6c66  , present = self
-0002b9e0: 2e62 6c6f 636b 735b 695d 2868 6964 6465  .blocks[i](hidde
-0002b9f0: 6e5f 7374 6174 6573 2c0a 2020 2020 2020  n_states,.      
-0002ba00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ba10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ba20: 2020 2020 2020 2020 2020 2020 2020 6174                at
-0002ba30: 7465 6e74 696f 6e5f 6d61 736b 2c0a 2020  tention_mask,.  
+0002b890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b8b0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
+0002b8c0: 685f 7661 6c69 645f 6c65 6e67 7468 290a  h_valid_length).
+0002b8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b8e0: 7072 6573 656e 745f 6c61 7965 7220 3d20  present_layer = 
+0002b8f0: 7072 6573 656e 745f 6c61 7965 7220 2b20  present_layer + 
+0002b900: 2870 7265 7365 6e74 2c29 0a20 2020 2020  (present,).     
+0002b910: 2020 2020 2020 2020 2020 2061 6363 756d             accum
+0002b920: 5f6c 6f73 7320 3d20 7365 6c66 2e61 6464  _loss = self.add
+0002b930: 2861 6363 756d 5f6c 6f73 732c 2061 7578  (accum_loss, aux
+0002b940: 5f6c 6f73 7329 0a20 2020 2020 2020 2020  _loss).         
+0002b950: 2020 2072 6574 7572 6e20 6869 6464 656e     return hidden
+0002b960: 5f73 7461 7465 732c 2070 7265 7365 6e74  _states, present
+0002b970: 5f6c 6179 6572 2c20 6163 6375 6d5f 6c6f  _layer, accum_lo
+0002b980: 7373 0a0a 2020 2020 2020 2020 2320 4c6f  ss..        # Lo
+0002b990: 6f70 2074 6872 6f75 6768 2065 6163 6820  op through each 
+0002b9a0: 7365 6c66 2d61 7474 656e 7469 6f6e 206c  self-attention l
+0002b9b0: 6179 6572 0a20 2020 2020 2020 2066 6f72  ayer.        for
+0002b9c0: 2069 2069 6e20 7261 6e67 6528 7365 6c66   i in range(self
+0002b9d0: 2e6e 756d 5f6c 6179 6572 7329 3a0a 2020  .num_layers):.  
+0002b9e0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0002b9f0: 5f73 7461 7465 732c 2070 7265 7365 6e74  _states, present
+0002ba00: 203d 2073 656c 662e 626c 6f63 6b73 5b69   = self.blocks[i
+0002ba10: 5d28 6869 6464 656e 5f73 7461 7465 732c  ](hidden_states,
+0002ba20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002ba30: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002ba40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ba50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ba60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ba70: 2020 656e 636f 6465 725f 6f75 7470 7574    encoder_output
-0002ba80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002ba90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002baa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002bab0: 2020 2020 2020 6d65 6d6f 7279 5f6d 6173        memory_mas
-0002bac0: 6b2c 0a20 2020 2020 2020 2020 2020 2020  k,.             
-0002bad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002bae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002baf0: 2020 2020 2020 2069 6e69 745f 7265 7365         init_rese
-0002bb00: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0002bb10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002bb20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002bb30: 2020 2020 2020 2062 6174 6368 5f76 616c         batch_val
-0002bb40: 6964 5f6c 656e 6774 6829 0a20 2020 2020  id_length).     
-0002bb50: 2020 2020 2020 2070 7265 7365 6e74 5f6c         present_l
-0002bb60: 6179 6572 203d 2070 7265 7365 6e74 5f6c  ayer = present_l
-0002bb70: 6179 6572 202b 2028 7072 6573 656e 742c  ayer + (present,
-0002bb80: 290a 0a20 2020 2020 2020 2072 6574 7572  )..        retur
-0002bb90: 6e20 6869 6464 656e 5f73 7461 7465 732c  n hidden_states,
-0002bba0: 2070 7265 7365 6e74 5f6c 6179 6572 0a0a   present_layer..
-0002bbb0: 0a63 6c61 7373 2054 7261 6e73 666f 726d  .class Transform
-0002bbc0: 6572 2843 656c 6c29 3a0a 2020 2020 7222  er(Cell):.    r"
-0002bbd0: 2222 0a20 2020 2020 2020 2054 7261 6e73  "".        Trans
-0002bbe0: 666f 726d 6572 206d 6f64 756c 6520 696e  former module in
-0002bbf0: 636c 7564 696e 6720 656e 636f 6465 7220  cluding encoder 
-0002bc00: 616e 6420 6465 636f 6465 722e 2054 6865  and decoder. The
-0002bc10: 2064 6966 6665 7265 6e63 6520 7769 7468   difference with
-0002bc20: 2074 6865 206f 7269 6769 6e61 6c20 696d   the original im
-0002bc30: 706c 656d 656e 7473 2069 7320 7468 6520  plements is the 
-0002bc40: 6d6f 6475 6c65 2075 7365 0a20 2020 2020  module use.     
-0002bc50: 2020 2074 6865 2072 6573 6964 7561 6c20     the residual 
-0002bc60: 6164 6469 7469 6f6e 2062 6566 6f72 6520  addition before 
-0002bc70: 7468 6520 6c61 7965 7220 6e6f 726d 616c  the layer normal
-0002bc80: 697a 6174 696f 6e2e 2041 6e64 2074 6865  ization. And the
-0002bc90: 2064 6566 6175 6c74 2068 6964 6465 6e20   default hidden 
-0002bca0: 6163 7420 6973 2060 6765 6c75 602e 0a20  act is `gelu`.. 
-0002bcb0: 2020 2020 2020 2054 6865 2064 6574 6169         The detai
-0002bcc0: 6c73 2063 616e 2062 6520 666f 756e 6420  ls can be found 
-0002bcd0: 696e 2060 4174 7465 6e74 696f 6e20 6973  in `Attention is
-0002bce0: 2061 6c6c 2079 6f75 206e 6565 6420 3c68   all you need <h
-0002bcf0: 7474 7073 3a2f 2f61 7278 6976 2e6f 7267  ttps://arxiv.org
-0002bd00: 2f70 6466 2f31 3730 362e 3033 3736 3276  /pdf/1706.03762v
-0002bd10: 352e 7064 663e 605f 2e0a 0a20 2020 2020  5.pdf>`_...     
-0002bd20: 2020 204e 6f74 653a 0a20 2020 2020 2020     Note:.       
-0002bd30: 2020 2020 2054 6869 7320 6973 2061 6e20       This is an 
-0002bd40: 6578 7065 7269 6d65 6e74 616c 2069 6e74  experimental int
-0002bd50: 6572 6661 6365 2074 6861 7420 6973 2073  erface that is s
-0002bd60: 7562 6a65 6374 2074 6f20 6368 616e 6765  ubject to change
-0002bd70: 206f 7220 6465 6c65 7469 6f6e 2e0a 0a20   or deletion... 
-0002bd80: 2020 2020 2020 2041 7267 733a 0a20 2020         Args:.   
-0002bd90: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-0002bda0: 7369 7a65 2869 6e74 293a 2054 6865 2068  size(int): The h
-0002bdb0: 6964 6465 6e20 7369 7a65 206f 6620 7468  idden size of th
-0002bdc0: 6520 696e 7075 742e 0a20 2020 2020 2020  e input..       
-0002bdd0: 2020 2020 2062 6174 6368 5f73 697a 6528       batch_size(
-0002bde0: 696e 7429 3a20 5468 6520 6261 7463 6820  int): The batch 
-0002bdf0: 7369 7a65 206f 6620 7468 6520 696e 7075  size of the inpu
-0002be00: 7420 7465 6e73 6f72 2077 6865 6e20 646f  t tensor when do
-0002be10: 2069 6e63 7265 6e6d 656e 7461 6c20 7072   increnmental pr
-0002be20: 6564 6963 7469 6f6e 2e20 5368 6f75 6c64  ediction. Should
-0002be30: 2062 6520 6120 706f 7369 7469 7665 0a20   be a positive. 
-0002be40: 2020 2020 2020 2020 2020 2020 2020 2076                 v
-0002be50: 616c 7565 2e20 5768 656e 2064 6f20 7472  alue. When do tr
-0002be60: 6169 6e69 6e67 206f 7220 7072 6564 6963  aining or predic
-0002be70: 7469 6f6e 2c20 7468 6520 6172 6775 6d65  tion, the argume
-0002be80: 6e74 2077 696c 6c20 6e6f 7420 776f 726b  nt will not work
-0002be90: 2061 6e64 2074 6865 2075 7365 7220 6361   and the user ca
-0002bea0: 6e20 6a75 7374 2070 6173 7320 4e6f 6e65  n just pass None
-0002beb0: 2074 6f0a 2020 2020 2020 2020 2020 2020   to.            
-0002bec0: 2020 2020 7468 6520 6172 6775 6d65 6e74      the argument
-0002bed0: 2e0a 2020 2020 2020 2020 2020 2020 6666  ..            ff
-0002bee0: 6e5f 6869 6464 656e 5f73 697a 6528 696e  n_hidden_size(in
-0002bef0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
-0002bf00: 697a 6520 6f66 2062 6f74 746c 656e 6563  ize of bottlenec
-0002bf10: 6b20 696e 2074 6865 2066 6565 6466 6f72  k in the feedfor
-0002bf20: 7761 7264 206c 6179 6572 2e0a 2020 2020  ward layer..    
-0002bf30: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
-0002bf40: 6c65 6e67 7468 2869 6e74 293a 2054 6865  length(int): The
-0002bf50: 2073 6571 5f6c 656e 6774 6820 6f66 2074   seq_length of t
-0002bf60: 6865 2065 6e63 6f64 6572 2773 2069 6e70  he encoder's inp
-0002bf70: 7574 2074 656e 736f 722e 0a20 2020 2020  ut tensor..     
-0002bf80: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
-0002bf90: 656e 6774 6828 696e 7429 3a20 5468 6520  ength(int): The 
-0002bfa0: 7365 715f 6c65 6e67 7468 206f 6620 7468  seq_length of th
-0002bfb0: 6520 6465 636f 6465 7227 7320 696e 7075  e decoder's inpu
-0002bfc0: 7420 7465 6e73 6f72 2e0a 2020 2020 2020  t tensor..      
-0002bfd0: 2020 2020 2020 656e 636f 6465 725f 6c61        encoder_la
-0002bfe0: 7965 7273 2869 6e74 293a 2054 6865 206c  yers(int): The l
-0002bff0: 6179 6572 7320 6f66 2074 6865 2060 5472  ayers of the `Tr
-0002c000: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-0002c010: 4c61 7965 7260 2e20 4465 6661 756c 7420  Layer`. Default 
-0002c020: 332e 0a20 2020 2020 2020 2020 2020 2064  3..            d
-0002c030: 6563 6f64 6572 5f6c 6179 6572 7328 696e  ecoder_layers(in
-0002c040: 7429 3a20 5468 6520 6c61 7965 7273 206f  t): The layers o
-0002c050: 6620 7468 6520 6054 7261 6e73 666f 726d  f the `Transform
-0002c060: 6572 4465 636f 6465 724c 6179 6572 602e  erDecoderLayer`.
-0002c070: 2044 6566 6175 6c74 2033 2e0a 2020 2020   Default 3..    
-0002c080: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
-0002c090: 7328 696e 7429 3a20 5468 6520 6e75 6d62  s(int): The numb
-0002c0a0: 6572 206f 6620 7468 6520 6865 6164 732e  er of the heads.
-0002c0b0: 2044 6566 6175 6c74 3a20 322e 0a20 2020   Default: 2..   
-0002c0c0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-0002c0d0: 6f6e 5f64 726f 706f 7574 5f72 6174 6528  on_dropout_rate(
-0002c0e0: 666c 6f61 7429 3a20 5468 6520 6472 6f70  float): The drop
-0002c0f0: 6f75 7420 7261 7465 206f 6620 7468 6520  out rate of the 
-0002c100: 6174 7465 6e74 696f 6e20 7363 6f72 6573  attention scores
-0002c110: 2e20 4465 6661 756c 743a 302e 312e 0a20  . Default:0.1.. 
-0002c120: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0002c130: 6e5f 6472 6f70 6f75 745f 7261 7465 2866  n_dropout_rate(f
-0002c140: 6c6f 6174 293a 2054 6865 2064 726f 706f  loat): The dropo
-0002c150: 7574 2072 6174 6520 6f66 2074 6865 2066  ut rate of the f
-0002c160: 696e 616c 206f 7574 7075 7420 6f66 2074  inal output of t
-0002c170: 6865 206c 6179 6572 2e20 4465 6661 756c  he layer. Defaul
-0002c180: 743a 302e 312e 0a20 2020 2020 2020 2020  t:0.1..         
-0002c190: 2020 2068 6964 6465 6e5f 6163 7420 2873     hidden_act (s
-0002c1a0: 7472 2c20 6e6e 2e43 656c 6c29 3a20 5468  tr, nn.Cell): Th
-0002c1b0: 6520 6163 7469 7661 7469 6f6e 206f 6620  e activation of 
-0002c1c0: 7468 6520 696e 7465 726e 616c 2066 6565  the internal fee
-0002c1d0: 6466 6f72 7761 7264 206c 6179 6572 2e20  dforward layer. 
-0002c1e0: 5375 7070 6f72 7473 2027 7265 6c75 272c  Supports 'relu',
-0002c1f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002c200: 2027 7265 6c75 3627 2c20 2774 616e 6827   'relu6', 'tanh'
-0002c210: 2c20 2767 656c 7527 2c20 2766 6173 745f  , 'gelu', 'fast_
-0002c220: 6765 6c75 272c 2027 656c 7527 2c20 2773  gelu', 'elu', 's
-0002c230: 6967 6d6f 6964 272c 2027 7072 656c 7527  igmoid', 'prelu'
-0002c240: 2c20 276c 6561 6b79 7265 6c75 272c 2027  , 'leakyrelu', '
-0002c250: 6873 7769 7368 272c 0a20 2020 2020 2020  hswish',.       
-0002c260: 2020 2020 2020 2020 2027 6873 6967 6d6f           'hsigmo
-0002c270: 6964 272c 2027 6c6f 6773 6967 6d6f 6964  id', 'logsigmoid
-0002c280: 2720 616e 6420 736f 206f 6e2e 2055 7365  ' and so on. Use
-0002c290: 7220 6361 6e20 7072 6f76 6964 6520 6375  r can provide cu
-0002c2a0: 7374 6f6d 2061 6374 6976 6974 696f 6e20  stom activition 
-0002c2b0: 746f 2074 6865 2061 7267 756d 656e 742e  to the argument.
-0002c2c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002c2d0: 2049 6620 7573 6572 2077 616e 7473 2074   If user wants t
-0002c2e0: 6f20 7275 6e20 7468 6520 6e65 7420 696e  o run the net in
-0002c2f0: 2074 6865 2070 6172 616c 6c65 6c20 6d6f   the parallel mo
-0002c300: 6465 2c20 7468 6520 6375 7374 6f6d 2061  de, the custom a
-0002c310: 6374 6976 6174 696f 6e20 6d75 7374 2061  ctivation must a
-0002c320: 6c73 6f20 7072 6f76 6964 650a 2020 2020  lso provide.    
-0002c330: 2020 2020 2020 2020 2020 2020 7468 6520              the 
-0002c340: 6061 6374 6976 6174 696f 6e5f 7368 6172  `activation_shar
-0002c350: 6460 2066 756e 6374 696f 6e2e 2050 6c65  d` function. Ple
-0002c360: 6173 6520 7365 6520 7468 6520 6578 616d  ase see the exam
-0002c370: 706c 6573 206f 6620 7468 650a 2020 2020  ples of the.    
-0002c380: 2020 2020 2020 2020 2020 2020 636c 6173              clas
-0002c390: 733a 606d 696e 6466 6f72 6d65 7273 2e6d  s:`mindformers.m
-0002c3a0: 6f64 756c 6573 2e74 7261 6e73 666f 726d  odules.transform
-0002c3b0: 6572 2e46 6565 6446 6f72 7761 7264 602e  er.FeedForward`.
-0002c3c0: 2044 6566 6175 6c74 3a20 6765 6c75 2e0a   Default: gelu..
-0002c3d0: 2020 2020 2020 2020 2020 2020 706f 7374              post
-0002c3e0: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0002c3f0: 7561 6c28 626f 6f6c 293a 2044 6f20 7265  ual(bool): Do re
-0002c400: 7369 6475 616c 7320 6164 6473 2062 6566  siduals adds bef
-0002c410: 6f72 6520 7468 6520 6c61 7965 726e 6f72  ore the layernor
-0002c420: 6d2e 2044 6566 6175 6c74 2046 616c 7365  m. Default False
-0002c430: 2e0a 2020 2020 2020 2020 2020 2020 6c61  ..            la
-0002c440: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-0002c450: 7479 7065 2864 7479 7065 2e4e 756d 6265  type(dtype.Numbe
-0002c460: 7229 3a20 5468 6520 636f 6d70 7574 6174  r): The computat
-0002c470: 696f 6e20 7479 7065 206f 6620 7468 6520  ion type of the 
-0002c480: 6c61 7965 726e 6f72 6d2e 0a20 2020 2020  layernorm..     
-0002c490: 2020 2020 2020 2020 2020 2053 686f 756c             Shoul
-0002c4a0: 6420 6265 2064 7479 7065 2e66 6c6f 6174  d be dtype.float
-0002c4b0: 3332 206f 7220 6474 7970 652e 666c 6f61  32 or dtype.floa
-0002c4c0: 7431 362e 2044 6566 6175 6c74 2064 7479  t16. Default dty
-0002c4d0: 7065 2e66 6c6f 6174 3332 2e0a 2020 2020  pe.float32..    
-0002c4e0: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
-0002c4f0: 636f 6d70 7574 655f 7479 7065 2864 7479  compute_type(dty
-0002c500: 7065 2e4e 756d 6265 7229 3a20 5468 6520  pe.Number): The 
-0002c510: 636f 6d70 7574 6174 696f 6e20 7479 7065  computation type
-0002c520: 206f 6620 7468 6520 736f 6674 6d61 7820   of the softmax 
-0002c530: 696e 2074 6865 2061 7474 656e 7469 6f6e  in the attention
-0002c540: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
-0002c550: 2020 5368 6f75 6c64 2062 6520 6474 7970    Should be dtyp
-0002c560: 652e 666c 6f61 7433 3220 6f72 2064 7479  e.float32 or dty
-0002c570: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-0002c580: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
-0002c590: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
-0002c5a0: 7061 7261 6d5f 696e 6974 5f74 7970 6528  param_init_type(
-0002c5b0: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-0002c5c0: 6865 2070 6172 616d 6574 6572 2069 6e69  he parameter ini
-0002c5d0: 7469 616c 697a 6174 696f 6e20 7479 7065  tialization type
-0002c5e0: 206f 6620 7468 6520 6d6f 6475 6c65 2e0a   of the module..
-0002c5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c600: 5368 6f75 6c64 2062 6520 6474 7970 652e  Should be dtype.
-0002c610: 666c 6f61 7433 3220 6f72 2064 7479 7065  float32 or dtype
-0002c620: 2e66 6c6f 6174 3136 2e20 4465 6661 756c  .float16. Defaul
-0002c630: 7420 6474 7970 652e 666c 6f61 7433 322e  t dtype.float32.
-0002c640: 0a20 2020 2020 2020 2020 2020 206c 616d  .            lam
-0002c650: 6264 615f 6675 6e63 3a20 4120 6675 6e63  bda_func: A func
-0002c660: 7469 6f6e 2063 616e 2064 6574 6572 6d69  tion can determi
-0002c670: 6e65 2074 6865 2066 7573 696f 6e20 696e  ne the fusion in
-0002c680: 6465 782c 2070 6970 656c 696e 6520 7374  dex, pipeline st
-0002c690: 6167 6573 2061 6e64 2072 6563 6f6d 7075  ages and recompu
-0002c6a0: 7465 2061 7474 7269 6275 7465 2e20 4966  te attribute. If
-0002c6b0: 2074 6865 2075 7365 720a 2020 2020 2020   the user.      
-0002c6c0: 2020 2020 2020 2020 2020 7761 6e74 7320            wants 
-0002c6d0: 746f 2064 6574 6572 6d69 6e65 2074 6865  to determine the
-0002c6e0: 2070 6970 656c 696e 6520 7374 6167 6520   pipeline stage 
-0002c6f0: 616e 6420 6772 6164 6965 6e74 2061 6767  and gradient agg
-0002c700: 7265 6761 7469 6f6e 2066 7573 696f 6e2c  regation fusion,
-0002c710: 2074 6865 2075 7365 7220 6361 6e20 7061   the user can pa
-0002c720: 7373 2061 2066 756e 6374 696f 6e0a 2020  ss a function.  
-0002c730: 2020 2020 2020 2020 2020 2020 2020 7468                th
-0002c740: 6174 2061 6363 6570 7473 2060 6e65 7477  at accepts `netw
-0002c750: 6f72 6b60 2c20 606c 6179 6572 5f69 6460  ork`, `layer_id`
-0002c760: 2c20 606f 6666 7365 7460 2c20 6070 6172  , `offset`, `par
-0002c770: 616c 6c65 6c5f 636f 6e66 6967 602c 2060  allel_config`, `
-0002c780: 6c61 7965 7273 602e 2054 6865 2060 6e65  layers`. The `ne
-0002c790: 7477 6f72 6b28 4365 6c6c 2960 0a20 2020  twork(Cell)`.   
-0002c7a0: 2020 2020 2020 2020 2020 2020 2072 6570               rep
-0002c7b0: 7265 7365 6e74 7320 7468 6520 7472 616e  resents the tran
-0002c7c0: 7366 6f72 6d65 7220 626c 6f63 6b2c 2060  sformer block, `
-0002c7d0: 6c61 7965 725f 6964 2869 6e74 2960 206d  layer_id(int)` m
-0002c7e0: 6561 6e73 2074 6865 206c 6179 6572 2069  eans the layer i
-0002c7f0: 6e64 6578 2066 6f72 2074 6865 2063 7572  ndex for the cur
-0002c800: 7265 6e74 206d 6f64 756c 652c 2063 6f75  rent module, cou
-0002c810: 6e74 730a 2020 2020 2020 2020 2020 2020  nts.            
-0002c820: 2020 2020 6672 6f6d 207a 6572 6f2c 2060      from zero, `
-0002c830: 6f66 6673 6574 2869 6e74 2960 206d 6561  offset(int)` mea
-0002c840: 6e73 2074 6865 206c 6179 6572 5f69 6e64  ns the layer_ind
-0002c850: 6578 206e 6565 6473 2061 6e20 6f66 6673  ex needs an offs
-0002c860: 6574 2c20 6966 2074 6865 7265 2061 7265  et, if there are
-0002c870: 206f 7468 6572 206d 6f64 756c 6573 2069   other modules i
-0002c880: 6e20 7468 6520 6e65 742e 0a20 2020 2020  n the net..     
-0002c890: 2020 2020 2020 2020 2020 2054 6865 2064             The d
-0002c8a0: 6566 6175 6c74 2073 6574 7469 6e67 2066  efault setting f
-0002c8b0: 6f72 2074 6865 2070 6970 656c 696e 6520  or the pipeline 
-0002c8c0: 6973 3a20 6028 6c61 7965 725f 6964 202b  is: `(layer_id +
-0002c8d0: 206f 6666 7365 7429 202f 2f20 2828 656e   offset) // ((en
-0002c8e0: 636f 6465 725f 6c61 7965 7273 202b 2064  coder_layers + d
-0002c8f0: 6563 6f64 6572 5f6c 6179 6572 7329 0a20  ecoder_layers). 
-0002c900: 2020 2020 2020 2020 2020 2020 2020 202f                 /
-0002c910: 2070 6970 656c 696e 655f 7374 6167 6529   pipeline_stage)
-0002c920: 602e 2044 6566 6175 6c74 204e 6f6e 652e  `. Default None.
-0002c930: 0a20 2020 2020 2020 2020 2020 2075 7365  .            use
-0002c940: 5f70 6173 7428 626f 6f6c 293a 2055 7365  _past(bool): Use
-0002c950: 2074 6865 2070 6173 7420 7374 6174 6520   the past state 
-0002c960: 746f 2063 6f6d 7075 7465 2c20 7573 6564  to compute, used
-0002c970: 2066 6f72 2069 6e63 7265 6d65 6e74 616c   for incremental
-0002c980: 2070 7265 6469 6374 696f 6e2e 2044 6566   prediction. Def
-0002c990: 6175 6c74 2046 616c 7365 2e0a 2020 2020  ault False..    
-0002c9a0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
-0002c9b0: 6967 284d 6f45 436f 6e66 6967 293a 2054  ig(MoEConfig): T
-0002c9c0: 6865 2063 6f6e 6669 6775 7261 7469 6f6e  he configuration
-0002c9d0: 206f 6620 4d6f 4520 284d 6978 7475 7265   of MoE (Mixture
-0002c9e0: 206f 6620 4578 7065 7274 292e 2044 6566   of Expert). Def
-0002c9f0: 6175 6c74 2069 7320 616e 2069 6e73 7461  ault is an insta
-0002ca00: 6e63 6520 6f66 204d 6f45 436f 6e66 6967  nce of MoEConfig
-0002ca10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002ca20: 2077 6974 6820 6465 6661 756c 7420 7661   with default va
-0002ca30: 6c75 6573 2e20 506c 6561 7365 2073 6565  lues. Please see
-0002ca40: 2060 4d6f 4543 6f6e 6669 6760 2e0a 2020   `MoEConfig`..  
-0002ca50: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-0002ca60: 656c 5f63 6f6e 6669 6728 5472 616e 7366  el_config(Transf
-0002ca70: 6f72 6d65 724f 7050 6172 616c 6c65 6c43  ormerOpParallelC
-0002ca80: 6f6e 6669 6729 3a20 5468 6520 7061 7261  onfig): The para
-0002ca90: 6c6c 656c 2063 6f6e 6669 6775 7265 2e20  llel configure. 
-0002caa0: 4465 6661 756c 7420 6064 6566 6175 6c74  Default `default
-0002cab0: 5f74 7261 6e73 666f 726d 6572 5f63 6f6e  _transformer_con
-0002cac0: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
-0002cad0: 2020 2020 2020 616e 2069 6e73 7461 6e63        an instanc
-0002cae0: 6520 6f66 2060 5472 616e 7366 6f72 6d65  e of `Transforme
-0002caf0: 724f 7050 6172 616c 6c65 6c43 6f6e 6669  rOpParallelConfi
-0002cb00: 6760 2077 6974 6820 6465 6661 756c 7420  g` with default 
-0002cb10: 6172 6773 2e0a 0a20 2020 2020 2020 2049  args...        I
-0002cb20: 6e70 7574 733a 0a20 2020 2020 2020 2020  nputs:.         
-0002cb30: 2020 202d 202a 2a65 6e63 6f64 6572 5f69     - **encoder_i
-0002cb40: 6e70 7574 732a 2a20 2854 656e 736f 7229  nputs** (Tensor)
-0002cb50: 202d 2054 6865 2069 6e70 7574 2074 656e   - The input ten
-0002cb60: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
-0002cb70: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
-0002cb80: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
-0002cb90: 697a 655d 206f 720a 2020 2020 2020 2020  ize] or.        
-0002cba0: 2020 2020 2020 5b62 6174 6368 5f73 697a        [batch_siz
-0002cbb0: 6520 2a20 7365 715f 6c65 6e67 7468 2c20  e * seq_length, 
-0002cbc0: 6869 6464 656e 5f73 697a 655d 2e0a 2020  hidden_size]..  
-0002cbd0: 2020 2020 2020 2020 2020 2d20 2a2a 656e            - **en
-0002cbe0: 636f 6465 725f 6d61 736b 732a 2a20 2854  coder_masks** (T
-0002cbf0: 656e 736f 7229 202d 2054 6865 2061 7474  ensor) - The att
-0002cc00: 656e 7469 6f6e 206d 6173 6b20 666f 7220  ention mask for 
-0002cc10: 6465 636f 6465 7220 7769 7468 2073 6861  decoder with sha
-0002cc20: 7065 0a20 2020 2020 2020 2020 2020 2020  pe.             
-0002cc30: 205b 6261 7463 685f 7369 7a65 2c20 7365   [batch_size, se
-0002cc40: 715f 6c65 6e67 7468 2c20 7365 715f 6c65  q_length, seq_le
-0002cc50: 6e67 7468 5d20 6f72 204e 6f6e 652e 204e  ngth] or None. N
-0002cc60: 6f6e 6520 6d65 616e 7320 7468 6572 6520  one means there 
-0002cc70: 7769 6c6c 2062 6520 6e6f 206d 6173 6b20  will be no mask 
-0002cc80: 696e 2073 6f66 746d 6178 2063 6f6d 7075  in softmax compu
-0002cc90: 7461 7469 6f6e 0a20 2020 2020 2020 2020  tation.         
-0002cca0: 2020 2020 2069 6e20 7365 6c66 2061 7474       in self att
-0002ccb0: 656e 7469 6f6e 206f 6620 7468 6520 656e  ention of the en
-0002ccc0: 636f 6465 7220 6d6f 6475 6c65 2e0a 2020  coder module..  
-0002ccd0: 2020 2020 2020 2020 2020 2d20 2a2a 6465            - **de
-0002cce0: 636f 6465 725f 696e 7075 7473 2a2a 2028  coder_inputs** (
-0002ccf0: 5465 6e73 6f72 2920 2d20 5468 6520 6f75  Tensor) - The ou
-0002cd00: 7470 7574 206f 6620 7468 6520 656e 636f  tput of the enco
-0002cd10: 6465 7220 7769 7468 2073 6861 7065 205b  der with shape [
-0002cd20: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
-0002cd30: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
-0002cd40: 697a 655d 0a20 2020 2020 2020 2020 2020  ize].           
-0002cd50: 2020 206f 7220 5b62 6174 6368 5f73 697a     or [batch_siz
-0002cd60: 6520 2a20 7365 715f 6c65 6e67 7468 2c20  e * seq_length, 
-0002cd70: 6869 6464 656e 5f73 697a 655d 2c20 7468  hidden_size], th
-0002cd80: 6973 2073 686f 756c 6420 6265 206e 6f6e  is should be non
-0002cd90: 6520 6966 2074 6865 2064 6563 6f64 6572  e if the decoder
-0002cda0: 206c 6179 6572 2069 7320 302e 0a20 2020   layer is 0..   
-0002cdb0: 2020 2020 2020 2020 202d 202a 2a64 6563           - **dec
-0002cdc0: 6f64 6572 5f6d 6173 6b73 2a2a 2028 5465  oder_masks** (Te
-0002cdd0: 6e73 6f72 2920 2d20 5468 6520 6174 7465  nsor) - The atte
-0002cde0: 6e74 696f 6e20 6d61 736b 2066 6f72 2064  ntion mask for d
-0002cdf0: 6563 6f64 6572 2077 6974 6820 7368 6170  ecoder with shap
-0002ce00: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-0002ce10: 5b62 6174 6368 5f73 697a 652c 2073 6571  [batch_size, seq
-0002ce20: 5f6c 656e 6774 682c 2073 6571 5f6c 656e  _length, seq_len
-0002ce30: 6774 685d 206f 7220 4e6f 6e65 2e20 4e6f  gth] or None. No
-0002ce40: 6e65 206d 6561 6e73 2074 6865 7265 2077  ne means there w
-0002ce50: 696c 6c20 6265 206e 6f20 6d61 736b 2069  ill be no mask i
-0002ce60: 6e20 736f 6674 6d61 7820 636f 6d70 7574  n softmax comput
-0002ce70: 6174 696f 6e0a 2020 2020 2020 2020 2020  ation.          
-0002ce80: 2020 2020 696e 2073 656c 6620 6174 7465      in self atte
-0002ce90: 6e74 696f 6e20 6f66 2074 6865 2064 6563  ntion of the dec
-0002cea0: 6f64 6572 206d 6f64 756c 652e 0a20 2020  oder module..   
-0002ceb0: 2020 2020 2020 2020 202d 202a 2a6d 656d           - **mem
-0002cec0: 6f72 795f 6d61 736b 2a2a 2028 5465 6e73  ory_mask** (Tens
-0002ced0: 6f72 2920 2d20 5468 6520 6d65 6d6f 7279  or) - The memory
-0002cee0: 206d 6173 6b20 6f66 2074 6865 2063 726f   mask of the cro
-0002cef0: 7373 2061 7474 656e 7469 6f6e 2077 6974  ss attention wit
-0002cf00: 6820 7368 6170 6520 5b62 6174 6368 2c20  h shape [batch, 
-0002cf10: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
-0002cf20: 2020 2020 2020 2020 2020 2020 2020 7372                sr
-0002cf30: 635f 7365 715f 6c65 6e67 7468 5d0a 2020  c_seq_length].  
-0002cf40: 2020 2020 2020 2020 2020 2020 7768 6572              wher
-0002cf50: 6520 7467 745f 7365 715f 6c65 6e67 7468  e tgt_seq_length
-0002cf60: 2069 7320 7468 6520 6c65 6e67 7468 206f   is the length o
-0002cf70: 6620 7468 6520 6465 636f 6465 722e 2054  f the decoder. T
-0002cf80: 6865 206f 7574 7075 7420 6f66 2074 6865  he output of the
-0002cf90: 2065 6e63 6f64 6572 2077 6974 6820 7368   encoder with sh
-0002cfa0: 6170 6520 5b62 6174 6368 5f73 697a 652c  ape [batch_size,
-0002cfb0: 0a20 2020 2020 2020 2020 2020 2020 2073  .              s
-0002cfc0: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0002cfd0: 6e5f 7369 7a65 5d2c 2074 6869 7320 7368  n_size], this sh
-0002cfe0: 6f75 6c64 2062 6520 6e6f 6e65 2069 6620  ould be none if 
-0002cff0: 7468 6520 6465 636f 6465 7220 6c61 7965  the decoder laye
-0002d000: 7220 6973 2030 206f 7220 7468 6520 7573  r is 0 or the us
-0002d010: 6572 2077 616e 7473 206e 6f20 6d61 736b  er wants no mask
-0002d020: 2e0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
-0002d030: 2a2a 696e 6974 5f72 6573 6574 2a2a 2028  **init_reset** (
-0002d040: 5465 6e73 6f72 2920 2d20 4120 626f 6f6c  Tensor) - A bool
-0002d050: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
-0002d060: 7065 205b 315d 2c20 7573 6564 2074 6f20  pe [1], used to 
-0002d070: 636c 6561 7220 7468 6520 7061 7374 206b  clear the past k
-0002d080: 6579 2070 6172 616d 6574 6572 2061 6e64  ey parameter and
-0002d090: 0a20 2020 2020 2020 2020 2020 2020 2070  .              p
-0002d0a0: 6173 7420 7661 6c75 6520 7061 7261 6d65  ast value parame
-0002d0b0: 7465 7220 7573 6564 2069 6e20 7468 6520  ter used in the 
-0002d0c0: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
-0002d0d0: 6963 7469 6f6e 2e20 4f6e 6c79 2076 616c  iction. Only val
-0002d0e0: 6964 2077 6865 6e20 7573 655f 7061 7374  id when use_past
-0002d0f0: 2069 7320 5472 7565 2e20 4465 6661 756c   is True. Defaul
-0002d100: 7420 5472 7565 2e0a 2020 2020 2020 2020  t True..        
-0002d110: 2020 2020 2d20 2a2a 6261 7463 685f 7661      - **batch_va
-0002d120: 6c69 645f 6c65 6e67 7468 2a2a 2028 5465  lid_length** (Te
-0002d130: 6e73 6f72 2920 2d20 496e 7433 3220 7465  nsor) - Int32 te
-0002d140: 6e73 6f72 2077 6974 6820 7368 6170 6520  nsor with shape 
-0002d150: 5b62 6174 6368 5f73 697a 655d 2074 6865  [batch_size] the
-0002d160: 2070 6173 7420 6361 6c63 756c 6174 6564   past calculated
-0002d170: 2074 6865 2069 6e64 6578 2e0a 2020 2020   the index..    
-0002d180: 2020 2020 2020 2020 2020 5573 6564 2066            Used f
-0002d190: 6f72 2069 6e63 7265 6d65 6e74 616c 2070  or incremental p
-0002d1a0: 7265 6469 6374 696f 6e20 7768 656e 2074  rediction when t
-0002d1b0: 6865 2075 7365 5f70 6173 7420 6973 2054  he use_past is T
-0002d1c0: 7275 652e 2044 6566 6175 6c74 204e 6f6e  rue. Default Non
-0002d1d0: 652e 0a0a 2020 2020 2020 2020 4f75 7470  e...        Outp
-0002d1e0: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-0002d1f0: 2054 7570 6c65 2c20 6120 7475 706c 6520   Tuple, a tuple 
-0002d200: 636f 6e74 6169 6e73 2860 6f75 7470 7574  contains(`output
-0002d210: 602c 2060 656e 636f 6465 725f 6c61 7965  `, `encoder_laye
-0002d220: 725f 7072 6573 656e 7460 2c20 6064 6563  r_present`, `dec
-0002d230: 6f64 6572 5f6c 6179 6572 5f70 7265 7365  oder_layer_prese
-0002d240: 6e74 602c 2060 6163 6375 6d5f 6c6f 7373  nt`, `accum_loss
-0002d250: 6029 0a0a 2020 2020 2020 2020 2020 2020  `)..            
-0002d260: 2d20 2a2a 6f75 7470 7574 2a2a 2028 5465  - **output** (Te
-0002d270: 6e73 6f72 2920 2d20 4966 2074 6865 7265  nsor) - If there
-0002d280: 2069 7320 6f6e 6c79 2065 6e63 6f64 6572   is only encoder
-0002d290: 2c20 7468 6520 6f75 7470 7574 206c 6f67  , the output log
-0002d2a0: 6974 206f 6620 7468 6520 656e 636f 6465  it of the encode
-0002d2b0: 7220 6c61 7965 722e 2054 6865 2073 6861  r layer. The sha
-0002d2c0: 7065 2069 730a 2020 2020 2020 2020 2020  pe is.          
-0002d2d0: 2020 2020 5b62 6174 6368 2c20 7372 635f      [batch, src_
-0002d2e0: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
-0002d2f0: 656e 5f73 697a 655d 206f 7220 5b62 6174  en_size] or [bat
-0002d300: 6368 202a 2073 7263 5f73 6571 5f6c 656e  ch * src_seq_len
-0002d310: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0002d320: 5d2c 2069 6620 7468 6572 6520 6172 6520  ], if there are 
-0002d330: 656e 636f 6465 7220 616e 640a 2020 2020  encoder and.    
-0002d340: 2020 2020 2020 2020 2020 6465 636f 6465            decode
-0002d350: 7273 2c20 7468 6520 6f75 7470 7574 2069  rs, the output i
-0002d360: 7320 6672 6f6d 2074 6865 2064 6563 6f64  s from the decod
-0002d370: 6572 206c 6179 6572 2e20 5468 6520 7368  er layer. The sh
-0002d380: 6170 6520 6973 205b 6261 7463 682c 2074  ape is [batch, t
-0002d390: 6774 5f73 6571 5f6c 656e 6774 682c 2068  gt_seq_length, h
-0002d3a0: 6964 6465 6e5f 7369 7a65 5d20 6f72 0a20  idden_size] or. 
-0002d3b0: 2020 2020 2020 2020 2020 2020 205b 6261               [ba
-0002d3c0: 7463 6820 2a20 7467 745f 7365 715f 6c65  tch * tgt_seq_le
-0002d3d0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
-0002d3e0: 655d 2e0a 2020 2020 2020 2020 2020 2020  e]..            
-0002d3f0: 2d20 2a2a 656e 636f 6465 725f 6c61 7965  - **encoder_laye
-0002d400: 725f 7072 6573 656e 742a 2a20 2854 7570  r_present** (Tup
-0002d410: 6c65 2920 2d20 4120 7475 706c 6520 7769  le) - A tuple wi
-0002d420: 7468 2073 697a 6520 6f66 206e 756d 5f6c  th size of num_l
-0002d430: 6179 6572 732c 2077 6865 7265 2065 6163  ayers, where eac
-0002d440: 6820 7475 706c 6520 6973 2074 6865 2074  h tuple is the t
-0002d450: 656e 736f 7220 7468 650a 2020 2020 2020  ensor the.      
-0002d460: 2020 2020 2020 2020 7072 6f6a 6563 7465          projecte
-0002d470: 6420 6b65 7920 616e 6420 7661 6c75 6520  d key and value 
-0002d480: 7665 6374 6f72 2069 6e20 7365 6c66 2061  vector in self a
-0002d490: 7474 656e 7469 6f6e 2077 6974 6820 7368  ttention with sh
-0002d4a0: 6170 6520 2828 6261 7463 685f 7369 7a65  ape ((batch_size
-0002d4b0: 2c20 6e75 6d5f 6865 6164 732c 2073 697a  , num_heads, siz
-0002d4c0: 655f 7065 725f 6865 6164 2c0a 2020 2020  e_per_head,.    
-0002d4d0: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-0002d4e0: 715f 6c65 6e67 7468 292c 2028 6261 7463  q_length), (batc
-0002d4f0: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-0002d500: 732c 2073 7263 5f73 6571 5f6c 656e 6774  s, src_seq_lengt
-0002d510: 682c 2073 697a 655f 7065 725f 6865 6164  h, size_per_head
-0002d520: 2929 2e0a 2020 2020 2020 2020 2020 2020  ))..            
-0002d530: 2d20 2a2a 6465 636f 6465 725f 6c61 7965  - **decoder_laye
-0002d540: 725f 7072 6573 656e 742a 2a20 2854 7570  r_present** (Tup
-0002d550: 6c65 2920 2d20 4120 7475 706c 6520 7769  le) - A tuple wi
-0002d560: 7468 2073 697a 6520 6f66 206e 756d 5f6c  th size of num_l
-0002d570: 6179 6572 732c 2077 6865 7265 2065 6163  ayers, where eac
-0002d580: 6820 7475 706c 6520 6973 2074 6865 2074  h tuple is the t
-0002d590: 656e 736f 720a 2020 2020 2020 2020 2020  ensor.          
-0002d5a0: 2020 2020 6f66 2074 6865 2070 726f 6a65      of the proje
-0002d5b0: 6374 6564 206b 6579 2061 6e64 2076 616c  cted key and val
-0002d5c0: 7565 2076 6563 746f 7220 696e 2073 656c  ue vector in sel
-0002d5d0: 6620 6174 7465 6e74 696f 6e20 7769 7468  f attention with
-0002d5e0: 2073 6861 7065 2028 2862 6174 6368 5f73   shape ((batch_s
-0002d5f0: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
-0002d600: 7369 7a65 5f70 6572 5f68 6561 642c 0a20  size_per_head,. 
-0002d610: 2020 2020 2020 2020 2020 2020 2074 6774               tgt
-0002d620: 5f73 6571 5f6c 656e 6774 6829 2c20 2862  _seq_length), (b
-0002d630: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
-0002d640: 6561 6473 2c20 7467 745f 7365 715f 6c65  eads, tgt_seq_le
-0002d650: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
-0002d660: 6561 6429 292c 2061 6e64 2074 6865 0a20  ead)), and the. 
-0002d670: 2020 2020 2020 2020 2020 2020 2070 726f               pro
-0002d680: 6a65 6374 6564 206b 6579 2061 6e64 2076  jected key and v
-0002d690: 616c 7565 2076 6563 746f 7220 696e 2063  alue vector in c
-0002d6a0: 726f 7373 2061 7474 656e 7469 6f6e 2077  ross attention w
-0002d6b0: 6974 6820 7368 6170 650a 2020 2020 2020  ith shape.      
-0002d6c0: 2020 2020 2020 2020 2828 6261 7463 685f          ((batch_
-0002d6d0: 7369 7a65 2c20 6e75 6d5f 6865 6164 732c  size, num_heads,
-0002d6e0: 2073 697a 655f 7065 725f 6865 6164 2c20   size_per_head, 
-0002d6f0: 7372 635f 7365 715f 6c65 6e67 7468 292c  src_seq_length),
-0002d700: 0a20 2020 2020 2020 2020 2020 2020 2028  .              (
-0002d710: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
-0002d720: 6865 6164 732c 2073 7263 5f73 6571 5f6c  heads, src_seq_l
-0002d730: 656e 6774 682c 2073 697a 655f 7065 725f  ength, size_per_
-0002d740: 6865 6164 2929 2e20 4966 2074 6865 2064  head)). If the d
-0002d750: 6563 6f64 6572 2069 7320 6e6f 7420 7365  ecoder is not se
-0002d760: 742c 2074 6865 0a20 2020 2020 2020 2020  t, the.         
-0002d770: 2020 2020 2072 6574 7572 6e65 6420 7661       returned va
-0002d780: 6c75 6520 7769 6c6c 2062 6520 4e6f 6e65  lue will be None
-0002d790: 2e0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
-0002d7a0: 2a2a 6163 6375 6d5f 6c6f 7373 2a2a 2028  **accum_loss** (
-0002d7b0: 5465 6e73 6f72 2920 2d20 4120 5465 6e73  Tensor) - A Tens
-0002d7c0: 6f72 2069 6e64 6963 6174 6573 2061 6e20  or indicates an 
-0002d7d0: 6175 7869 6c69 6172 7920 6c6f 7373 2074  auxiliary loss t
-0002d7e0: 6f20 6d69 6e69 6d69 7a65 2074 6865 206d  o minimize the m
-0002d7f0: 6561 6e20 7371 7561 7265 206f 6620 7468  ean square of th
-0002d800: 6520 6461 7461 0a20 2020 2020 2020 2020  e data.         
-0002d810: 2020 2020 2070 6172 7420 726f 7574 6564       part routed
-0002d820: 2074 6f20 6561 6368 2065 7870 6572 742c   to each expert,
-0002d830: 2061 6e64 206f 6e6c 7920 7265 7475 726e   and only return
-0002d840: 6564 2069 6620 7468 6520 6e75 6d62 6572  ed if the number
-0002d850: 206f 6620 6578 7065 7274 7320 6973 2067   of experts is g
-0002d860: 7265 6174 6572 2074 6861 6e20 312e 0a0a  reater than 1...
-0002d870: 2020 2020 2020 2020 5375 7070 6f72 7465          Supporte
-0002d880: 6420 506c 6174 666f 726d 733a 0a20 2020  d Platforms:.   
-0002d890: 2020 2020 2020 2020 2060 6041 7363 656e           ``Ascen
-0002d8a0: 6460 6020 6060 4750 5560 600a 0a20 2020  d`` ``GPU``..   
-0002d8b0: 2020 2020 2045 7861 6d70 6c65 733a 0a20       Examples:. 
-0002d8c0: 2020 2020 2020 2020 2020 203e 3e3e 2069             >>> i
-0002d8d0: 6d70 6f72 7420 6e75 6d70 7920 6173 206e  mport numpy as n
-0002d8e0: 700a 2020 2020 2020 2020 2020 2020 3e3e  p.            >>
-0002d8f0: 3e20 6672 6f6d 206d 696e 6473 706f 7265  > from mindspore
-0002d900: 2069 6d70 6f72 7420 6474 7970 6520 6173   import dtype as
-0002d910: 206d 7374 7970 650a 2020 2020 2020 2020   mstype.        
-0002d920: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
-0002d930: 6466 6f72 6d65 7273 2e6d 6f64 756c 6573  dformers.modules
-0002d940: 2e74 7261 6e73 666f 726d 6572 2069 6d70  .transformer imp
-0002d950: 6f72 7420 5472 616e 7366 6f72 6d65 720a  ort Transformer.
-0002d960: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-0002d970: 6672 6f6d 206d 696e 6473 706f 7265 2069  from mindspore i
-0002d980: 6d70 6f72 7420 5465 6e73 6f72 0a20 2020  mport Tensor.   
-0002d990: 2020 2020 2020 2020 203e 3e3e 206d 6f64           >>> mod
-0002d9a0: 656c 203d 2054 7261 6e73 666f 726d 6572  el = Transformer
-0002d9b0: 2862 6174 6368 5f73 697a 653d 322c 2065  (batch_size=2, e
-0002d9c0: 6e63 6f64 6572 5f6c 6179 6572 733d 312c  ncoder_layers=1,
-0002d9d0: 2064 6563 6f64 6572 5f6c 6179 6572 733d   decoder_layers=
-0002d9e0: 322c 2068 6964 6465 6e5f 7369 7a65 3d36  2, hidden_size=6
-0002d9f0: 342c 0a20 2020 2020 2020 2020 2020 202e  4,.            .
-0002da00: 2e2e 2020 2020 2020 2020 2020 2020 2020  ..              
-0002da10: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-0002da20: 6e5f 7369 7a65 3d36 342c 2073 7263 5f73  n_size=64, src_s
-0002da30: 6571 5f6c 656e 6774 683d 3230 2c20 7467  eq_length=20, tg
-0002da40: 745f 7365 715f 6c65 6e67 7468 3d31 3029  t_seq_length=10)
-0002da50: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0002da60: 2065 6e63 6f64 6572 5f69 6e70 7574 5f76   encoder_input_v
-0002da70: 616c 7565 203d 2054 656e 736f 7228 6e70  alue = Tensor(np
-0002da80: 2e6f 6e65 7328 2832 2c20 3230 2c20 3634  .ones((2, 20, 64
-0002da90: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
-0002daa0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
-0002dab0: 3e3e 3e20 656e 636f 6465 725f 696e 7075  >>> encoder_inpu
-0002dac0: 745f 6d61 736b 203d 2054 656e 736f 7228  t_mask = Tensor(
-0002dad0: 6e70 2e6f 6e65 7328 2832 2c20 3230 2c20  np.ones((2, 20, 
-0002dae0: 3230 2929 2c20 6d73 7479 7065 2e66 6c6f  20)), mstype.flo
-0002daf0: 6174 3136 290a 2020 2020 2020 2020 2020  at16).          
-0002db00: 2020 3e3e 3e20 6465 636f 6465 725f 696e    >>> decoder_in
-0002db10: 7075 745f 7661 6c75 6520 3d20 5465 6e73  put_value = Tens
-0002db20: 6f72 286e 702e 6f6e 6573 2828 322c 2031  or(np.ones((2, 1
-0002db30: 302c 2036 3429 292c 206d 7374 7970 652e  0, 64)), mstype.
-0002db40: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-0002db50: 2020 2020 203e 3e3e 2064 6563 6f64 6572       >>> decoder
-0002db60: 5f69 6e70 7574 5f6d 6173 6b20 3d20 5465  _input_mask = Te
-0002db70: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-0002db80: 2031 302c 2031 3029 292c 206d 7374 7970   10, 10)), mstyp
-0002db90: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-0002dba0: 2020 2020 2020 203e 3e3e 206d 656d 6f72         >>> memor
-0002dbb0: 795f 6d61 736b 203d 2054 656e 736f 7228  y_mask = Tensor(
-0002dbc0: 6e70 2e6f 6e65 7328 2832 2c20 3130 2c20  np.ones((2, 10, 
-0002dbd0: 3230 2929 2c20 6d73 7479 7065 2e66 6c6f  20)), mstype.flo
-0002dbe0: 6174 3136 290a 2020 2020 2020 2020 2020  at16).          
-0002dbf0: 2020 3e3e 3e20 6f75 7470 7574 2c20 656e    >>> output, en
-0002dc00: 5f70 6173 742c 2064 655f 7061 7374 203d  _past, de_past =
-0002dc10: 206d 6f64 656c 2865 6e63 6f64 6572 5f69   model(encoder_i
-0002dc20: 6e70 7574 5f76 616c 7565 2c20 656e 636f  nput_value, enco
-0002dc30: 6465 725f 696e 7075 745f 6d61 736b 2c20  der_input_mask, 
-0002dc40: 6465 636f 6465 725f 696e 7075 745f 7661  decoder_input_va
-0002dc50: 6c75 652c 0a20 2020 2020 2020 2020 2020  lue,.           
-0002dc60: 202e 2e2e 2020 2020 2020 2020 2020 2020   ...            
-0002dc70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dc80: 2020 2020 2020 6465 636f 6465 725f 696e        decoder_in
-0002dc90: 7075 745f 6d61 736b 2c20 6d65 6d6f 7279  put_mask, memory
-0002dca0: 5f6d 6173 6b29 0a20 2020 2020 2020 2020  _mask).         
-0002dcb0: 2020 203e 3e3e 2070 7269 6e74 286f 7574     >>> print(out
-0002dcc0: 7075 742e 7368 6170 6529 0a20 2020 2020  put.shape).     
-0002dcd0: 2020 2020 2020 2028 322c 2031 302c 2036         (2, 10, 6
-0002dce0: 3429 0a20 2020 2020 2020 2020 2020 203e  4).            >
-0002dcf0: 3e3e 2070 7269 6e74 286c 656e 2865 6e5f  >> print(len(en_
-0002dd00: 7061 7374 2929 0a20 2020 2020 2020 2020  past)).         
-0002dd10: 2020 2031 0a20 2020 2020 2020 2020 2020     1.           
-0002dd20: 203e 3e3e 2070 7269 6e74 286c 656e 2864   >>> print(len(d
-0002dd30: 655f 7061 7374 2929 0a20 2020 2020 2020  e_past)).       
-0002dd40: 2020 2020 2032 0a20 2020 2020 2020 2020       2.         
-0002dd50: 2020 203e 3e3e 2070 7269 6e74 2865 6e5f     >>> print(en_
-0002dd60: 7061 7374 5b30 5d5b 305d 2e73 6861 7065  past[0][0].shape
-0002dd70: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-0002dd80: 2c20 322c 2033 322c 2032 3029 0a20 2020  , 2, 32, 20).   
-0002dd90: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-0002dda0: 6e74 2865 6e5f 7061 7374 5b30 5d5b 315d  nt(en_past[0][1]
-0002ddb0: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
-0002ddc0: 2020 2020 2832 2c20 322c 2032 302c 2033      (2, 2, 20, 3
-0002ddd0: 3229 0a20 2020 2020 2020 2020 2020 203e  2).            >
-0002dde0: 3e3e 2070 7269 6e74 2864 655f 7061 7374  >> print(de_past
-0002ddf0: 5b30 5d5b 305d 2e73 6861 7065 290a 2020  [0][0].shape).  
-0002de00: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
-0002de10: 2033 322c 2031 3029 0a20 2020 2020 2020   32, 10).       
-0002de20: 2020 2020 203e 3e3e 2070 7269 6e74 2864       >>> print(d
-0002de30: 655f 7061 7374 5b30 5d5b 315d 2e73 6861  e_past[0][1].sha
-0002de40: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-0002de50: 2832 2c20 322c 2031 302c 2033 3229 0a20  (2, 2, 10, 32). 
-0002de60: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
-0002de70: 7269 6e74 2864 655f 7061 7374 5b30 5d5b  rint(de_past[0][
-0002de80: 325d 2e73 6861 7065 290a 2020 2020 2020  2].shape).      
-0002de90: 2020 2020 2020 2832 2c20 322c 2033 322c        (2, 2, 32,
-0002dea0: 2032 3029 0a20 2020 2020 2020 2020 2020   20).           
-0002deb0: 203e 3e3e 2070 7269 6e74 2864 655f 7061   >>> print(de_pa
-0002dec0: 7374 5b30 5d5b 335d 2e73 6861 7065 290a  st[0][3].shape).
-0002ded0: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-0002dee0: 322c 2032 302c 2033 3229 0a20 2020 2022  2, 20, 32).    "
-0002def0: 2222 0a0a 2020 2020 405f 4c6f 6741 6374  ""..    @_LogAct
-0002df00: 696f 6e4f 6e63 6528 6d5f 6c6f 6767 6572  ionOnce(m_logger
-0002df10: 3d6c 6f67 6765 722c 206b 6579 3d27 5472  =logger, key='Tr
-0002df20: 616e 7366 6f72 6d65 7227 2c0a 2020 2020  ansformer',.    
-0002df30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002df40: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
-0002df50: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
-0002df60: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-0002df70: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
-0002df80: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
-0002df90: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
-0002dfa0: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
-0002dfb0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-0002dfc0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-0002dfd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dfe0: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-0002dff0: 6164 733d 5661 6c69 6461 746f 722e 6368  ads=Validator.ch
-0002e000: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-0002e010: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e030: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-0002e040: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
-0002e050: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
-0002e060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e080: 7372 635f 7365 715f 6c65 6e67 7468 3d56  src_seq_length=V
-0002e090: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-0002e0a0: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
-0002e0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e0c0: 2020 2020 2020 2020 2020 2020 2065 6e63               enc
-0002e0d0: 6f64 6572 5f6c 6179 6572 733d 5661 6c69  oder_layers=Vali
-0002e0e0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-0002e0f0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-0002e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e110: 2020 2020 2020 2020 2020 6465 636f 6465            decode
-0002e120: 725f 6c61 7965 7273 3d56 616c 6964 6174  r_layers=Validat
-0002e130: 6f72 2e63 6865 636b 5f6e 6f6e 5f6e 6567  or.check_non_neg
-0002e140: 6174 6976 655f 696e 742c 0a20 2020 2020  ative_int,.     
-0002e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e160: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
-0002e170: 6571 5f6c 656e 6774 683d 5661 6c69 6461  eq_length=Valida
-0002e180: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-0002e190: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
-0002e1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e1b0: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0002e1c0: 6e5f 6472 6f70 6f75 745f 7261 7465 3d56  n_dropout_rate=V
-0002e1d0: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
-0002e1e0: 6f6e 5f6e 6567 6174 6976 655f 666c 6f61  on_negative_floa
-0002e1f0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0002e200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e210: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
-0002e220: 745f 7261 7465 3d56 616c 6964 6174 6f72  t_rate=Validator
-0002e230: 2e63 6865 636b 5f6e 6f6e 5f6e 6567 6174  .check_non_negat
-0002e240: 6976 655f 666c 6f61 742c 0a20 2020 2020  ive_float,.     
-0002e250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e260: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
-0002e270: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0002e280: 616c 3d56 616c 6964 6174 6f72 2e63 6865  al=Validator.che
-0002e290: 636b 5f62 6f6f 6c2c 0a20 2020 2020 2020  ck_bool,.       
-0002e2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e2b0: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-0002e2c0: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
-0002e2d0: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
-0002e2e0: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
-0002e2f0: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
-0002e300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ba50: 2020 2020 2061 7474 656e 7469 6f6e 5f6d       attention_m
+0002ba60: 6173 6b2c 0a20 2020 2020 2020 2020 2020  ask,.           
+0002ba70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ba80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ba90: 2020 2020 2020 2020 2065 6e63 6f64 6572           encoder
+0002baa0: 5f6f 7574 7075 742c 0a20 2020 2020 2020  _output,.       
+0002bab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bad0: 2020 2020 2020 2020 2020 2020 206d 656d               mem
+0002bae0: 6f72 795f 6d61 736b 2c0a 2020 2020 2020  ory_mask,.      
+0002baf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bb10: 2020 2020 2020 2020 2020 2020 2020 696e                in
+0002bb20: 6974 5f72 6573 6574 2c0a 2020 2020 2020  it_reset,.      
+0002bb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bb40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002bb50: 2020 2020 2020 2020 2020 2020 2020 6261                ba
+0002bb60: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+0002bb70: 290a 2020 2020 2020 2020 2020 2020 7072  ).            pr
+0002bb80: 6573 656e 745f 6c61 7965 7220 3d20 7072  esent_layer = pr
+0002bb90: 6573 656e 745f 6c61 7965 7220 2b20 2870  esent_layer + (p
+0002bba0: 7265 7365 6e74 2c29 0a0a 2020 2020 2020  resent,)..      
+0002bbb0: 2020 7265 7475 726e 2068 6964 6465 6e5f    return hidden_
+0002bbc0: 7374 6174 6573 2c20 7072 6573 656e 745f  states, present_
+0002bbd0: 6c61 7965 720a 0a0a 636c 6173 7320 5472  layer...class Tr
+0002bbe0: 616e 7366 6f72 6d65 7228 4365 6c6c 293a  ansformer(Cell):
+0002bbf0: 0a20 2020 2072 2222 220a 2020 2020 2020  .    r""".      
+0002bc00: 2020 5472 616e 7366 6f72 6d65 7220 6d6f    Transformer mo
+0002bc10: 6475 6c65 2069 6e63 6c75 6469 6e67 2065  dule including e
+0002bc20: 6e63 6f64 6572 2061 6e64 2064 6563 6f64  ncoder and decod
+0002bc30: 6572 2e20 5468 6520 6469 6666 6572 656e  er. The differen
+0002bc40: 6365 2077 6974 6820 7468 6520 6f72 6967  ce with the orig
+0002bc50: 696e 616c 2069 6d70 6c65 6d65 6e74 7320  inal implements 
+0002bc60: 6973 2074 6865 206d 6f64 756c 6520 7573  is the module us
+0002bc70: 650a 2020 2020 2020 2020 7468 6520 7265  e.        the re
+0002bc80: 7369 6475 616c 2061 6464 6974 696f 6e20  sidual addition 
+0002bc90: 6265 666f 7265 2074 6865 206c 6179 6572  before the layer
+0002bca0: 206e 6f72 6d61 6c69 7a61 7469 6f6e 2e20   normalization. 
+0002bcb0: 416e 6420 7468 6520 6465 6661 756c 7420  And the default 
+0002bcc0: 6869 6464 656e 2061 6374 2069 7320 6067  hidden act is `g
+0002bcd0: 656c 7560 2e0a 2020 2020 2020 2020 5468  elu`..        Th
+0002bce0: 6520 6465 7461 696c 7320 6361 6e20 6265  e details can be
+0002bcf0: 2066 6f75 6e64 2069 6e20 6041 7474 656e   found in `Atten
+0002bd00: 7469 6f6e 2069 7320 616c 6c20 796f 7520  tion is all you 
+0002bd10: 6e65 6564 203c 6874 7470 733a 2f2f 6172  need <https://ar
+0002bd20: 7869 762e 6f72 672f 7064 662f 3137 3036  xiv.org/pdf/1706
+0002bd30: 2e30 3337 3632 7635 2e70 6466 3e60 5f2e  .03762v5.pdf>`_.
+0002bd40: 0a0a 2020 2020 2020 2020 4e6f 7465 3a0a  ..        Note:.
+0002bd50: 2020 2020 2020 2020 2020 2020 5468 6973              This
+0002bd60: 2069 7320 616e 2065 7870 6572 696d 656e   is an experimen
+0002bd70: 7461 6c20 696e 7465 7266 6163 6520 7468  tal interface th
+0002bd80: 6174 2069 7320 7375 626a 6563 7420 746f  at is subject to
+0002bd90: 2063 6861 6e67 6520 6f72 2064 656c 6574   change or delet
+0002bda0: 696f 6e2e 0a0a 2020 2020 2020 2020 4172  ion...        Ar
+0002bdb0: 6773 3a0a 2020 2020 2020 2020 2020 2020  gs:.            
+0002bdc0: 6869 6464 656e 5f73 697a 6528 696e 7429  hidden_size(int)
+0002bdd0: 3a20 5468 6520 6869 6464 656e 2073 697a  : The hidden siz
+0002bde0: 6520 6f66 2074 6865 2069 6e70 7574 2e0a  e of the input..
+0002bdf0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
+0002be00: 685f 7369 7a65 2869 6e74 293a 2054 6865  h_size(int): The
+0002be10: 2062 6174 6368 2073 697a 6520 6f66 2074   batch size of t
+0002be20: 6865 2069 6e70 7574 2074 656e 736f 7220  he input tensor 
+0002be30: 7768 656e 2064 6f20 696e 6372 656e 6d65  when do increnme
+0002be40: 6e74 616c 2070 7265 6469 6374 696f 6e2e  ntal prediction.
+0002be50: 2053 686f 756c 6420 6265 2061 2070 6f73   Should be a pos
+0002be60: 6974 6976 650a 2020 2020 2020 2020 2020  itive.          
+0002be70: 2020 2020 2020 7661 6c75 652e 2057 6865        value. Whe
+0002be80: 6e20 646f 2074 7261 696e 696e 6720 6f72  n do training or
+0002be90: 2070 7265 6469 6374 696f 6e2c 2074 6865   prediction, the
+0002bea0: 2061 7267 756d 656e 7420 7769 6c6c 206e   argument will n
+0002beb0: 6f74 2077 6f72 6b20 616e 6420 7468 6520  ot work and the 
+0002bec0: 7573 6572 2063 616e 206a 7573 7420 7061  user can just pa
+0002bed0: 7373 204e 6f6e 6520 746f 0a20 2020 2020  ss None to.     
+0002bee0: 2020 2020 2020 2020 2020 2074 6865 2061             the a
+0002bef0: 7267 756d 656e 742e 0a20 2020 2020 2020  rgument..       
+0002bf00: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+0002bf10: 7369 7a65 2869 6e74 293a 2054 6865 2068  size(int): The h
+0002bf20: 6964 6465 6e20 7369 7a65 206f 6620 626f  idden size of bo
+0002bf30: 7474 6c65 6e65 636b 2069 6e20 7468 6520  ttleneck in the 
+0002bf40: 6665 6564 666f 7277 6172 6420 6c61 7965  feedforward laye
+0002bf50: 722e 0a20 2020 2020 2020 2020 2020 2073  r..            s
+0002bf60: 7263 5f73 6571 5f6c 656e 6774 6828 696e  rc_seq_length(in
+0002bf70: 7429 3a20 5468 6520 7365 715f 6c65 6e67  t): The seq_leng
+0002bf80: 7468 206f 6620 7468 6520 656e 636f 6465  th of the encode
+0002bf90: 7227 7320 696e 7075 7420 7465 6e73 6f72  r's input tensor
+0002bfa0: 2e0a 2020 2020 2020 2020 2020 2020 7467  ..            tg
+0002bfb0: 745f 7365 715f 6c65 6e67 7468 2869 6e74  t_seq_length(int
+0002bfc0: 293a 2054 6865 2073 6571 5f6c 656e 6774  ): The seq_lengt
+0002bfd0: 6820 6f66 2074 6865 2064 6563 6f64 6572  h of the decoder
+0002bfe0: 2773 2069 6e70 7574 2074 656e 736f 722e  's input tensor.
+0002bff0: 0a20 2020 2020 2020 2020 2020 2065 6e63  .            enc
+0002c000: 6f64 6572 5f6c 6179 6572 7328 696e 7429  oder_layers(int)
+0002c010: 3a20 5468 6520 6c61 7965 7273 206f 6620  : The layers of 
+0002c020: 7468 6520 6054 7261 6e73 666f 726d 6572  the `Transformer
+0002c030: 456e 636f 6465 724c 6179 6572 602e 2044  EncoderLayer`. D
+0002c040: 6566 6175 6c74 2033 2e0a 2020 2020 2020  efault 3..      
+0002c050: 2020 2020 2020 6465 636f 6465 725f 6c61        decoder_la
+0002c060: 7965 7273 2869 6e74 293a 2054 6865 206c  yers(int): The l
+0002c070: 6179 6572 7320 6f66 2074 6865 2060 5472  ayers of the `Tr
+0002c080: 616e 7366 6f72 6d65 7244 6563 6f64 6572  ansformerDecoder
+0002c090: 4c61 7965 7260 2e20 4465 6661 756c 7420  Layer`. Default 
+0002c0a0: 332e 0a20 2020 2020 2020 2020 2020 206e  3..            n
+0002c0b0: 756d 5f68 6561 6473 2869 6e74 293a 2054  um_heads(int): T
+0002c0c0: 6865 206e 756d 6265 7220 6f66 2074 6865  he number of the
+0002c0d0: 2068 6561 6473 2e20 4465 6661 756c 743a   heads. Default:
+0002c0e0: 2032 2e0a 2020 2020 2020 2020 2020 2020   2..            
+0002c0f0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+0002c100: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
+0002c110: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
+0002c120: 6f66 2074 6865 2061 7474 656e 7469 6f6e  of the attention
+0002c130: 2073 636f 7265 732e 2044 6566 6175 6c74   scores. Default
+0002c140: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
+0002c150: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
+0002c160: 5f72 6174 6528 666c 6f61 7429 3a20 5468  _rate(float): Th
+0002c170: 6520 6472 6f70 6f75 7420 7261 7465 206f  e dropout rate o
+0002c180: 6620 7468 6520 6669 6e61 6c20 6f75 7470  f the final outp
+0002c190: 7574 206f 6620 7468 6520 6c61 7965 722e  ut of the layer.
+0002c1a0: 2044 6566 6175 6c74 3a30 2e31 2e0a 2020   Default:0.1..  
+0002c1b0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0002c1c0: 5f61 6374 2028 7374 722c 206e 6e2e 4365  _act (str, nn.Ce
+0002c1d0: 6c6c 293a 2054 6865 2061 6374 6976 6174  ll): The activat
+0002c1e0: 696f 6e20 6f66 2074 6865 2069 6e74 6572  ion of the inter
+0002c1f0: 6e61 6c20 6665 6564 666f 7277 6172 6420  nal feedforward 
+0002c200: 6c61 7965 722e 2053 7570 706f 7274 7320  layer. Supports 
+0002c210: 2772 656c 7527 2c0a 2020 2020 2020 2020  'relu',.        
+0002c220: 2020 2020 2020 2020 2772 656c 7536 272c          'relu6',
+0002c230: 2027 7461 6e68 272c 2027 6765 6c75 272c   'tanh', 'gelu',
+0002c240: 2027 6661 7374 5f67 656c 7527 2c20 2765   'fast_gelu', 'e
+0002c250: 6c75 272c 2027 7369 676d 6f69 6427 2c20  lu', 'sigmoid', 
+0002c260: 2770 7265 6c75 272c 2027 6c65 616b 7972  'prelu', 'leakyr
+0002c270: 656c 7527 2c20 2768 7377 6973 6827 2c0a  elu', 'hswish',.
+0002c280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c290: 2768 7369 676d 6f69 6427 2c20 276c 6f67  'hsigmoid', 'log
+0002c2a0: 7369 676d 6f69 6427 2061 6e64 2073 6f20  sigmoid' and so 
+0002c2b0: 6f6e 2e20 5573 6572 2063 616e 2070 726f  on. User can pro
+0002c2c0: 7669 6465 2063 7573 746f 6d20 6163 7469  vide custom acti
+0002c2d0: 7669 7469 6f6e 2074 6f20 7468 6520 6172  vition to the ar
+0002c2e0: 6775 6d65 6e74 2e0a 2020 2020 2020 2020  gument..        
+0002c2f0: 2020 2020 2020 2020 4966 2075 7365 7220          If user 
+0002c300: 7761 6e74 7320 746f 2072 756e 2074 6865  wants to run the
+0002c310: 206e 6574 2069 6e20 7468 6520 7061 7261   net in the para
+0002c320: 6c6c 656c 206d 6f64 652c 2074 6865 2063  llel mode, the c
+0002c330: 7573 746f 6d20 6163 7469 7661 7469 6f6e  ustom activation
+0002c340: 206d 7573 7420 616c 736f 2070 726f 7669   must also provi
+0002c350: 6465 0a20 2020 2020 2020 2020 2020 2020  de.             
+0002c360: 2020 2074 6865 2060 6163 7469 7661 7469     the `activati
+0002c370: 6f6e 5f73 6861 7264 6020 6675 6e63 7469  on_shard` functi
+0002c380: 6f6e 2e20 506c 6561 7365 2073 6565 2074  on. Please see t
+0002c390: 6865 2065 7861 6d70 6c65 7320 6f66 2074  he examples of t
+0002c3a0: 6865 0a20 2020 2020 2020 2020 2020 2020  he.             
+0002c3b0: 2020 2063 6c61 7373 3a60 6d69 6e64 666f     class:`mindfo
+0002c3c0: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
+0002c3d0: 616e 7366 6f72 6d65 722e 4665 6564 466f  ansformer.FeedFo
+0002c3e0: 7277 6172 6460 2e20 4465 6661 756c 743a  rward`. Default:
+0002c3f0: 2067 656c 752e 0a20 2020 2020 2020 2020   gelu..         
+0002c400: 2020 2070 6f73 745f 6c61 7965 726e 6f72     post_layernor
+0002c410: 6d5f 7265 7369 6475 616c 2862 6f6f 6c29  m_residual(bool)
+0002c420: 3a20 446f 2072 6573 6964 7561 6c73 2061  : Do residuals a
+0002c430: 6464 7320 6265 666f 7265 2074 6865 206c  dds before the l
+0002c440: 6179 6572 6e6f 726d 2e20 4465 6661 756c  ayernorm. Defaul
+0002c450: 7420 4661 6c73 652e 0a20 2020 2020 2020  t False..       
+0002c460: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
+0002c470: 6f6d 7075 7465 5f74 7970 6528 6474 7970  ompute_type(dtyp
+0002c480: 652e 4e75 6d62 6572 293a 2054 6865 2063  e.Number): The c
+0002c490: 6f6d 7075 7461 7469 6f6e 2074 7970 6520  omputation type 
+0002c4a0: 6f66 2074 6865 206c 6179 6572 6e6f 726d  of the layernorm
+0002c4b0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0002c4c0: 2020 5368 6f75 6c64 2062 6520 6474 7970    Should be dtyp
+0002c4d0: 652e 666c 6f61 7433 3220 6f72 2064 7479  e.float32 or dty
+0002c4e0: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
+0002c4f0: 756c 7420 6474 7970 652e 666c 6f61 7433  ult dtype.float3
+0002c500: 322e 0a20 2020 2020 2020 2020 2020 2073  2..            s
+0002c510: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+0002c520: 7970 6528 6474 7970 652e 4e75 6d62 6572  ype(dtype.Number
+0002c530: 293a 2054 6865 2063 6f6d 7075 7461 7469  ): The computati
+0002c540: 6f6e 2074 7970 6520 6f66 2074 6865 2073  on type of the s
+0002c550: 6f66 746d 6178 2069 6e20 7468 6520 6174  oftmax in the at
+0002c560: 7465 6e74 696f 6e2e 0a20 2020 2020 2020  tention..       
+0002c570: 2020 2020 2020 2020 2053 686f 756c 6420           Should 
+0002c580: 6265 2064 7479 7065 2e66 6c6f 6174 3332  be dtype.float32
+0002c590: 206f 7220 6474 7970 652e 666c 6f61 7431   or dtype.float1
+0002c5a0: 362e 2044 6566 6175 6c74 206d 7374 7970  6. Default mstyp
+0002c5b0: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
+0002c5c0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+0002c5d0: 745f 7479 7065 2864 7479 7065 2e4e 756d  t_type(dtype.Num
+0002c5e0: 6265 7229 3a20 5468 6520 7061 7261 6d65  ber): The parame
+0002c5f0: 7465 7220 696e 6974 6961 6c69 7a61 7469  ter initializati
+0002c600: 6f6e 2074 7970 6520 6f66 2074 6865 206d  on type of the m
+0002c610: 6f64 756c 652e 0a20 2020 2020 2020 2020  odule..         
+0002c620: 2020 2020 2020 2053 686f 756c 6420 6265         Should be
+0002c630: 2064 7479 7065 2e66 6c6f 6174 3332 206f   dtype.float32 o
+0002c640: 7220 6474 7970 652e 666c 6f61 7431 362e  r dtype.float16.
+0002c650: 2044 6566 6175 6c74 2064 7479 7065 2e66   Default dtype.f
+0002c660: 6c6f 6174 3332 2e0a 2020 2020 2020 2020  loat32..        
+0002c670: 2020 2020 6c61 6d62 6461 5f66 756e 633a      lambda_func:
+0002c680: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
+0002c690: 6465 7465 726d 696e 6520 7468 6520 6675  determine the fu
+0002c6a0: 7369 6f6e 2069 6e64 6578 2c20 7069 7065  sion index, pipe
+0002c6b0: 6c69 6e65 2073 7461 6765 7320 616e 6420  line stages and 
+0002c6c0: 7265 636f 6d70 7574 6520 6174 7472 6962  recompute attrib
+0002c6d0: 7574 652e 2049 6620 7468 6520 7573 6572  ute. If the user
+0002c6e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002c6f0: 2077 616e 7473 2074 6f20 6465 7465 726d   wants to determ
+0002c700: 696e 6520 7468 6520 7069 7065 6c69 6e65  ine the pipeline
+0002c710: 2073 7461 6765 2061 6e64 2067 7261 6469   stage and gradi
+0002c720: 656e 7420 6167 6772 6567 6174 696f 6e20  ent aggregation 
+0002c730: 6675 7369 6f6e 2c20 7468 6520 7573 6572  fusion, the user
+0002c740: 2063 616e 2070 6173 7320 6120 6675 6e63   can pass a func
+0002c750: 7469 6f6e 0a20 2020 2020 2020 2020 2020  tion.           
+0002c760: 2020 2020 2074 6861 7420 6163 6365 7074       that accept
+0002c770: 7320 606e 6574 776f 726b 602c 2060 6c61  s `network`, `la
+0002c780: 7965 725f 6964 602c 2060 6f66 6673 6574  yer_id`, `offset
+0002c790: 602c 2060 7061 7261 6c6c 656c 5f63 6f6e  `, `parallel_con
+0002c7a0: 6669 6760 2c20 606c 6179 6572 7360 2e20  fig`, `layers`. 
+0002c7b0: 5468 6520 606e 6574 776f 726b 2843 656c  The `network(Cel
+0002c7c0: 6c29 600a 2020 2020 2020 2020 2020 2020  l)`.            
+0002c7d0: 2020 2020 7265 7072 6573 656e 7473 2074      represents t
+0002c7e0: 6865 2074 7261 6e73 666f 726d 6572 2062  he transformer b
+0002c7f0: 6c6f 636b 2c20 606c 6179 6572 5f69 6428  lock, `layer_id(
+0002c800: 696e 7429 6020 6d65 616e 7320 7468 6520  int)` means the 
+0002c810: 6c61 7965 7220 696e 6465 7820 666f 7220  layer index for 
+0002c820: 7468 6520 6375 7272 656e 7420 6d6f 6475  the current modu
+0002c830: 6c65 2c20 636f 756e 7473 0a20 2020 2020  le, counts.     
+0002c840: 2020 2020 2020 2020 2020 2066 726f 6d20             from 
+0002c850: 7a65 726f 2c20 606f 6666 7365 7428 696e  zero, `offset(in
+0002c860: 7429 6020 6d65 616e 7320 7468 6520 6c61  t)` means the la
+0002c870: 7965 725f 696e 6465 7820 6e65 6564 7320  yer_index needs 
+0002c880: 616e 206f 6666 7365 742c 2069 6620 7468  an offset, if th
+0002c890: 6572 6520 6172 6520 6f74 6865 7220 6d6f  ere are other mo
+0002c8a0: 6475 6c65 7320 696e 2074 6865 206e 6574  dules in the net
+0002c8b0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0002c8c0: 2020 5468 6520 6465 6661 756c 7420 7365    The default se
+0002c8d0: 7474 696e 6720 666f 7220 7468 6520 7069  tting for the pi
+0002c8e0: 7065 6c69 6e65 2069 733a 2060 286c 6179  peline is: `(lay
+0002c8f0: 6572 5f69 6420 2b20 6f66 6673 6574 2920  er_id + offset) 
+0002c900: 2f2f 2028 2865 6e63 6f64 6572 5f6c 6179  // ((encoder_lay
+0002c910: 6572 7320 2b20 6465 636f 6465 725f 6c61  ers + decoder_la
+0002c920: 7965 7273 290a 2020 2020 2020 2020 2020  yers).          
+0002c930: 2020 2020 2020 2f20 7069 7065 6c69 6e65        / pipeline
+0002c940: 5f73 7461 6765 2960 2e20 4465 6661 756c  _stage)`. Defaul
+0002c950: 7420 4e6f 6e65 2e0a 2020 2020 2020 2020  t None..        
+0002c960: 2020 2020 7573 655f 7061 7374 2862 6f6f      use_past(boo
+0002c970: 6c29 3a20 5573 6520 7468 6520 7061 7374  l): Use the past
+0002c980: 2073 7461 7465 2074 6f20 636f 6d70 7574   state to comput
+0002c990: 652c 2075 7365 6420 666f 7220 696e 6372  e, used for incr
+0002c9a0: 656d 656e 7461 6c20 7072 6564 6963 7469  emental predicti
+0002c9b0: 6f6e 2e20 4465 6661 756c 7420 4661 6c73  on. Default Fals
+0002c9c0: 652e 0a20 2020 2020 2020 2020 2020 206d  e..            m
+0002c9d0: 6f65 5f63 6f6e 6669 6728 4d6f 4543 6f6e  oe_config(MoECon
+0002c9e0: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
+0002c9f0: 7572 6174 696f 6e20 6f66 204d 6f45 2028  uration of MoE (
+0002ca00: 4d69 7874 7572 6520 6f66 2045 7870 6572  Mixture of Exper
+0002ca10: 7429 2e20 4465 6661 756c 7420 6973 2061  t). Default is a
+0002ca20: 6e20 696e 7374 616e 6365 206f 6620 4d6f  n instance of Mo
+0002ca30: 4543 6f6e 6669 670a 2020 2020 2020 2020  EConfig.        
+0002ca40: 2020 2020 2020 2020 7769 7468 2064 6566          with def
+0002ca50: 6175 6c74 2076 616c 7565 732e 2050 6c65  ault values. Ple
+0002ca60: 6173 6520 7365 6520 604d 6f45 436f 6e66  ase see `MoEConf
+0002ca70: 6967 602e 0a20 2020 2020 2020 2020 2020  ig`..           
+0002ca80: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0002ca90: 2854 7261 6e73 666f 726d 6572 4f70 5061  (TransformerOpPa
+0002caa0: 7261 6c6c 656c 436f 6e66 6967 293a 2054  rallelConfig): T
+0002cab0: 6865 2070 6172 616c 6c65 6c20 636f 6e66  he parallel conf
+0002cac0: 6967 7572 652e 2044 6566 6175 6c74 2060  igure. Default `
+0002cad0: 6465 6661 756c 745f 7472 616e 7366 6f72  default_transfor
+0002cae0: 6d65 725f 636f 6e66 6967 602c 0a20 2020  mer_config`,.   
+0002caf0: 2020 2020 2020 2020 2020 2020 2061 6e20               an 
+0002cb00: 696e 7374 616e 6365 206f 6620 6054 7261  instance of `Tra
+0002cb10: 6e73 666f 726d 6572 4f70 5061 7261 6c6c  nsformerOpParall
+0002cb20: 656c 436f 6e66 6967 6020 7769 7468 2064  elConfig` with d
+0002cb30: 6566 6175 6c74 2061 7267 732e 0a0a 2020  efault args...  
+0002cb40: 2020 2020 2020 496e 7075 7473 3a0a 2020        Inputs:.  
+0002cb50: 2020 2020 2020 2020 2020 2d20 2a2a 656e            - **en
+0002cb60: 636f 6465 725f 696e 7075 7473 2a2a 2028  coder_inputs** (
+0002cb70: 5465 6e73 6f72 2920 2d20 5468 6520 696e  Tensor) - The in
+0002cb80: 7075 7420 7465 6e73 6f72 2077 6974 6820  put tensor with 
+0002cb90: 7368 6170 6520 5b62 6174 6368 5f73 697a  shape [batch_siz
+0002cba0: 652c 2073 6571 5f6c 656e 6774 682c 2068  e, seq_length, h
+0002cbb0: 6964 6465 6e5f 7369 7a65 5d20 6f72 0a20  idden_size] or. 
+0002cbc0: 2020 2020 2020 2020 2020 2020 205b 6261               [ba
+0002cbd0: 7463 685f 7369 7a65 202a 2073 6571 5f6c  tch_size * seq_l
+0002cbe0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0002cbf0: 7a65 5d2e 0a20 2020 2020 2020 2020 2020  ze]..           
+0002cc00: 202d 202a 2a65 6e63 6f64 6572 5f6d 6173   - **encoder_mas
+0002cc10: 6b73 2a2a 2028 5465 6e73 6f72 2920 2d20  ks** (Tensor) - 
+0002cc20: 5468 6520 6174 7465 6e74 696f 6e20 6d61  The attention ma
+0002cc30: 736b 2066 6f72 2064 6563 6f64 6572 2077  sk for decoder w
+0002cc40: 6974 6820 7368 6170 650a 2020 2020 2020  ith shape.      
+0002cc50: 2020 2020 2020 2020 5b62 6174 6368 5f73          [batch_s
+0002cc60: 697a 652c 2073 6571 5f6c 656e 6774 682c  ize, seq_length,
+0002cc70: 2073 6571 5f6c 656e 6774 685d 206f 7220   seq_length] or 
+0002cc80: 4e6f 6e65 2e20 4e6f 6e65 206d 6561 6e73  None. None means
+0002cc90: 2074 6865 7265 2077 696c 6c20 6265 206e   there will be n
+0002cca0: 6f20 6d61 736b 2069 6e20 736f 6674 6d61  o mask in softma
+0002ccb0: 7820 636f 6d70 7574 6174 696f 6e0a 2020  x computation.  
+0002ccc0: 2020 2020 2020 2020 2020 2020 696e 2073              in s
+0002ccd0: 656c 6620 6174 7465 6e74 696f 6e20 6f66  elf attention of
+0002cce0: 2074 6865 2065 6e63 6f64 6572 206d 6f64   the encoder mod
+0002ccf0: 756c 652e 0a20 2020 2020 2020 2020 2020  ule..           
+0002cd00: 202d 202a 2a64 6563 6f64 6572 5f69 6e70   - **decoder_inp
+0002cd10: 7574 732a 2a20 2854 656e 736f 7229 202d  uts** (Tensor) -
+0002cd20: 2054 6865 206f 7574 7075 7420 6f66 2074   The output of t
+0002cd30: 6865 2065 6e63 6f64 6572 2077 6974 6820  he encoder with 
+0002cd40: 7368 6170 6520 5b62 6174 6368 5f73 697a  shape [batch_siz
+0002cd50: 652c 2073 6571 5f6c 656e 6774 682c 2068  e, seq_length, h
+0002cd60: 6964 6465 6e5f 7369 7a65 5d0a 2020 2020  idden_size].    
+0002cd70: 2020 2020 2020 2020 2020 6f72 205b 6261            or [ba
+0002cd80: 7463 685f 7369 7a65 202a 2073 6571 5f6c  tch_size * seq_l
+0002cd90: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0002cda0: 7a65 5d2c 2074 6869 7320 7368 6f75 6c64  ze], this should
+0002cdb0: 2062 6520 6e6f 6e65 2069 6620 7468 6520   be none if the 
+0002cdc0: 6465 636f 6465 7220 6c61 7965 7220 6973  decoder layer is
+0002cdd0: 2030 2e0a 2020 2020 2020 2020 2020 2020   0..            
+0002cde0: 2d20 2a2a 6465 636f 6465 725f 6d61 736b  - **decoder_mask
+0002cdf0: 732a 2a20 2854 656e 736f 7229 202d 2054  s** (Tensor) - T
+0002ce00: 6865 2061 7474 656e 7469 6f6e 206d 6173  he attention mas
+0002ce10: 6b20 666f 7220 6465 636f 6465 7220 7769  k for decoder wi
+0002ce20: 7468 2073 6861 7065 0a20 2020 2020 2020  th shape.       
+0002ce30: 2020 2020 2020 205b 6261 7463 685f 7369         [batch_si
+0002ce40: 7a65 2c20 7365 715f 6c65 6e67 7468 2c20  ze, seq_length, 
+0002ce50: 7365 715f 6c65 6e67 7468 5d20 6f72 204e  seq_length] or N
+0002ce60: 6f6e 652e 204e 6f6e 6520 6d65 616e 7320  one. None means 
+0002ce70: 7468 6572 6520 7769 6c6c 2062 6520 6e6f  there will be no
+0002ce80: 206d 6173 6b20 696e 2073 6f66 746d 6178   mask in softmax
+0002ce90: 2063 6f6d 7075 7461 7469 6f6e 0a20 2020   computation.   
+0002cea0: 2020 2020 2020 2020 2020 2069 6e20 7365             in se
+0002ceb0: 6c66 2061 7474 656e 7469 6f6e 206f 6620  lf attention of 
+0002cec0: 7468 6520 6465 636f 6465 7220 6d6f 6475  the decoder modu
+0002ced0: 6c65 2e0a 2020 2020 2020 2020 2020 2020  le..            
+0002cee0: 2d20 2a2a 6d65 6d6f 7279 5f6d 6173 6b2a  - **memory_mask*
+0002cef0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+0002cf00: 206d 656d 6f72 7920 6d61 736b 206f 6620   memory mask of 
+0002cf10: 7468 6520 6372 6f73 7320 6174 7465 6e74  the cross attent
+0002cf20: 696f 6e20 7769 7468 2073 6861 7065 205b  ion with shape [
+0002cf30: 6261 7463 682c 2074 6774 5f73 6571 5f6c  batch, tgt_seq_l
+0002cf40: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+0002cf50: 2020 2020 2073 7263 5f73 6571 5f6c 656e       src_seq_len
+0002cf60: 6774 685d 0a20 2020 2020 2020 2020 2020  gth].           
+0002cf70: 2020 2077 6865 7265 2074 6774 5f73 6571     where tgt_seq
+0002cf80: 5f6c 656e 6774 6820 6973 2074 6865 206c  _length is the l
+0002cf90: 656e 6774 6820 6f66 2074 6865 2064 6563  ength of the dec
+0002cfa0: 6f64 6572 2e20 5468 6520 6f75 7470 7574  oder. The output
+0002cfb0: 206f 6620 7468 6520 656e 636f 6465 7220   of the encoder 
+0002cfc0: 7769 7468 2073 6861 7065 205b 6261 7463  with shape [batc
+0002cfd0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
+0002cfe0: 2020 2020 2020 7365 715f 6c65 6e67 7468        seq_length
+0002cff0: 2c20 6869 6464 656e 5f73 697a 655d 2c20  , hidden_size], 
+0002d000: 7468 6973 2073 686f 756c 6420 6265 206e  this should be n
+0002d010: 6f6e 6520 6966 2074 6865 2064 6563 6f64  one if the decod
+0002d020: 6572 206c 6179 6572 2069 7320 3020 6f72  er layer is 0 or
+0002d030: 2074 6865 2075 7365 7220 7761 6e74 7320   the user wants 
+0002d040: 6e6f 206d 6173 6b2e 0a20 2020 2020 2020  no mask..       
+0002d050: 2020 2020 202d 202a 2a69 6e69 745f 7265       - **init_re
+0002d060: 7365 742a 2a20 2854 656e 736f 7229 202d  set** (Tensor) -
+0002d070: 2041 2062 6f6f 6c20 7465 6e73 6f72 2077   A bool tensor w
+0002d080: 6974 6820 7368 6170 6520 5b31 5d2c 2075  ith shape [1], u
+0002d090: 7365 6420 746f 2063 6c65 6172 2074 6865  sed to clear the
+0002d0a0: 2070 6173 7420 6b65 7920 7061 7261 6d65   past key parame
+0002d0b0: 7465 7220 616e 640a 2020 2020 2020 2020  ter and.        
+0002d0c0: 2020 2020 2020 7061 7374 2076 616c 7565        past value
+0002d0d0: 2070 6172 616d 6574 6572 2075 7365 6420   parameter used 
+0002d0e0: 696e 2074 6865 2069 6e63 7265 6d65 6e74  in the increment
+0002d0f0: 616c 2070 7265 6469 6374 696f 6e2e 204f  al prediction. O
+0002d100: 6e6c 7920 7661 6c69 6420 7768 656e 2075  nly valid when u
+0002d110: 7365 5f70 6173 7420 6973 2054 7275 652e  se_past is True.
+0002d120: 2044 6566 6175 6c74 2054 7275 652e 0a20   Default True.. 
+0002d130: 2020 2020 2020 2020 2020 202d 202a 2a62             - **b
+0002d140: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
+0002d150: 682a 2a20 2854 656e 736f 7229 202d 2049  h** (Tensor) - I
+0002d160: 6e74 3332 2074 656e 736f 7220 7769 7468  nt32 tensor with
+0002d170: 2073 6861 7065 205b 6261 7463 685f 7369   shape [batch_si
+0002d180: 7a65 5d20 7468 6520 7061 7374 2063 616c  ze] the past cal
+0002d190: 6375 6c61 7465 6420 7468 6520 696e 6465  culated the inde
+0002d1a0: 782e 0a20 2020 2020 2020 2020 2020 2020  x..             
+0002d1b0: 2055 7365 6420 666f 7220 696e 6372 656d   Used for increm
+0002d1c0: 656e 7461 6c20 7072 6564 6963 7469 6f6e  ental prediction
+0002d1d0: 2077 6865 6e20 7468 6520 7573 655f 7061   when the use_pa
+0002d1e0: 7374 2069 7320 5472 7565 2e20 4465 6661  st is True. Defa
+0002d1f0: 756c 7420 4e6f 6e65 2e0a 0a20 2020 2020  ult None...     
+0002d200: 2020 204f 7574 7075 7473 3a0a 2020 2020     Outputs:.    
+0002d210: 2020 2020 2020 2020 5475 706c 652c 2061          Tuple, a
+0002d220: 2074 7570 6c65 2063 6f6e 7461 696e 7328   tuple contains(
+0002d230: 606f 7574 7075 7460 2c20 6065 6e63 6f64  `output`, `encod
+0002d240: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+0002d250: 602c 2060 6465 636f 6465 725f 6c61 7965  `, `decoder_laye
+0002d260: 725f 7072 6573 656e 7460 2c20 6061 6363  r_present`, `acc
+0002d270: 756d 5f6c 6f73 7360 290a 0a20 2020 2020  um_loss`)..     
+0002d280: 2020 2020 2020 202d 202a 2a6f 7574 7075         - **outpu
+0002d290: 742a 2a20 2854 656e 736f 7229 202d 2049  t** (Tensor) - I
+0002d2a0: 6620 7468 6572 6520 6973 206f 6e6c 7920  f there is only 
+0002d2b0: 656e 636f 6465 722c 2074 6865 206f 7574  encoder, the out
+0002d2c0: 7075 7420 6c6f 6769 7420 6f66 2074 6865  put logit of the
+0002d2d0: 2065 6e63 6f64 6572 206c 6179 6572 2e20   encoder layer. 
+0002d2e0: 5468 6520 7368 6170 6520 6973 0a20 2020  The shape is.   
+0002d2f0: 2020 2020 2020 2020 2020 205b 6261 7463             [batc
+0002d300: 682c 2073 7263 5f73 6571 5f6c 656e 6774  h, src_seq_lengt
+0002d310: 682c 2068 6964 6465 6e5f 7369 7a65 5d20  h, hidden_size] 
+0002d320: 6f72 205b 6261 7463 6820 2a20 7372 635f  or [batch * src_
+0002d330: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+0002d340: 656e 5f73 697a 655d 2c20 6966 2074 6865  en_size], if the
+0002d350: 7265 2061 7265 2065 6e63 6f64 6572 2061  re are encoder a
+0002d360: 6e64 0a20 2020 2020 2020 2020 2020 2020  nd.             
+0002d370: 2064 6563 6f64 6572 732c 2074 6865 206f   decoders, the o
+0002d380: 7574 7075 7420 6973 2066 726f 6d20 7468  utput is from th
+0002d390: 6520 6465 636f 6465 7220 6c61 7965 722e  e decoder layer.
+0002d3a0: 2054 6865 2073 6861 7065 2069 7320 5b62   The shape is [b
+0002d3b0: 6174 6368 2c20 7467 745f 7365 715f 6c65  atch, tgt_seq_le
+0002d3c0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+0002d3d0: 655d 206f 720a 2020 2020 2020 2020 2020  e] or.          
+0002d3e0: 2020 2020 5b62 6174 6368 202a 2074 6774      [batch * tgt
+0002d3f0: 5f73 6571 5f6c 656e 6774 682c 2068 6964  _seq_length, hid
+0002d400: 6465 6e5f 7369 7a65 5d2e 0a20 2020 2020  den_size]..     
+0002d410: 2020 2020 2020 202d 202a 2a65 6e63 6f64         - **encod
+0002d420: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+0002d430: 2a2a 2028 5475 706c 6529 202d 2041 2074  ** (Tuple) - A t
+0002d440: 7570 6c65 2077 6974 6820 7369 7a65 206f  uple with size o
+0002d450: 6620 6e75 6d5f 6c61 7965 7273 2c20 7768  f num_layers, wh
+0002d460: 6572 6520 6561 6368 2074 7570 6c65 2069  ere each tuple i
+0002d470: 7320 7468 6520 7465 6e73 6f72 2074 6865  s the tensor the
+0002d480: 0a20 2020 2020 2020 2020 2020 2020 2070  .              p
+0002d490: 726f 6a65 6374 6564 206b 6579 2061 6e64  rojected key and
+0002d4a0: 2076 616c 7565 2076 6563 746f 7220 696e   value vector in
+0002d4b0: 2073 656c 6620 6174 7465 6e74 696f 6e20   self attention 
+0002d4c0: 7769 7468 2073 6861 7065 2028 2862 6174  with shape ((bat
+0002d4d0: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
+0002d4e0: 6473 2c20 7369 7a65 5f70 6572 5f68 6561  ds, size_per_hea
+0002d4f0: 642c 0a20 2020 2020 2020 2020 2020 2020  d,.             
+0002d500: 2073 7263 5f73 6571 5f6c 656e 6774 6829   src_seq_length)
+0002d510: 2c20 2862 6174 6368 5f73 697a 652c 206e  , (batch_size, n
+0002d520: 756d 5f68 6561 6473 2c20 7372 635f 7365  um_heads, src_se
+0002d530: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
+0002d540: 6572 5f68 6561 6429 292e 0a20 2020 2020  er_head))..     
+0002d550: 2020 2020 2020 202d 202a 2a64 6563 6f64         - **decod
+0002d560: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+0002d570: 2a2a 2028 5475 706c 6529 202d 2041 2074  ** (Tuple) - A t
+0002d580: 7570 6c65 2077 6974 6820 7369 7a65 206f  uple with size o
+0002d590: 6620 6e75 6d5f 6c61 7965 7273 2c20 7768  f num_layers, wh
+0002d5a0: 6572 6520 6561 6368 2074 7570 6c65 2069  ere each tuple i
+0002d5b0: 7320 7468 6520 7465 6e73 6f72 0a20 2020  s the tensor.   
+0002d5c0: 2020 2020 2020 2020 2020 206f 6620 7468             of th
+0002d5d0: 6520 7072 6f6a 6563 7465 6420 6b65 7920  e projected key 
+0002d5e0: 616e 6420 7661 6c75 6520 7665 6374 6f72  and value vector
+0002d5f0: 2069 6e20 7365 6c66 2061 7474 656e 7469   in self attenti
+0002d600: 6f6e 2077 6974 6820 7368 6170 6520 2828  on with shape ((
+0002d610: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
+0002d620: 6865 6164 732c 2073 697a 655f 7065 725f  heads, size_per_
+0002d630: 6865 6164 2c0a 2020 2020 2020 2020 2020  head,.          
+0002d640: 2020 2020 7467 745f 7365 715f 6c65 6e67      tgt_seq_leng
+0002d650: 7468 292c 2028 6261 7463 685f 7369 7a65  th), (batch_size
+0002d660: 2c20 6e75 6d5f 6865 6164 732c 2074 6774  , num_heads, tgt
+0002d670: 5f73 6571 5f6c 656e 6774 682c 2073 697a  _seq_length, siz
+0002d680: 655f 7065 725f 6865 6164 2929 2c20 616e  e_per_head)), an
+0002d690: 6420 7468 650a 2020 2020 2020 2020 2020  d the.          
+0002d6a0: 2020 2020 7072 6f6a 6563 7465 6420 6b65      projected ke
+0002d6b0: 7920 616e 6420 7661 6c75 6520 7665 6374  y and value vect
+0002d6c0: 6f72 2069 6e20 6372 6f73 7320 6174 7465  or in cross atte
+0002d6d0: 6e74 696f 6e20 7769 7468 2073 6861 7065  ntion with shape
+0002d6e0: 0a20 2020 2020 2020 2020 2020 2020 2028  .              (
+0002d6f0: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
+0002d700: 5f68 6561 6473 2c20 7369 7a65 5f70 6572  _heads, size_per
+0002d710: 5f68 6561 642c 2073 7263 5f73 6571 5f6c  _head, src_seq_l
+0002d720: 656e 6774 6829 2c0a 2020 2020 2020 2020  ength),.        
+0002d730: 2020 2020 2020 2862 6174 6368 5f73 697a        (batch_siz
+0002d740: 652c 206e 756d 5f68 6561 6473 2c20 7372  e, num_heads, sr
+0002d750: 635f 7365 715f 6c65 6e67 7468 2c20 7369  c_seq_length, si
+0002d760: 7a65 5f70 6572 5f68 6561 6429 292e 2049  ze_per_head)). I
+0002d770: 6620 7468 6520 6465 636f 6465 7220 6973  f the decoder is
+0002d780: 206e 6f74 2073 6574 2c20 7468 650a 2020   not set, the.  
+0002d790: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+0002d7a0: 726e 6564 2076 616c 7565 2077 696c 6c20  rned value will 
+0002d7b0: 6265 204e 6f6e 652e 0a20 2020 2020 2020  be None..       
+0002d7c0: 2020 2020 202d 202a 2a61 6363 756d 5f6c       - **accum_l
+0002d7d0: 6f73 732a 2a20 2854 656e 736f 7229 202d  oss** (Tensor) -
+0002d7e0: 2041 2054 656e 736f 7220 696e 6469 6361   A Tensor indica
+0002d7f0: 7465 7320 616e 2061 7578 696c 6961 7279  tes an auxiliary
+0002d800: 206c 6f73 7320 746f 206d 696e 696d 697a   loss to minimiz
+0002d810: 6520 7468 6520 6d65 616e 2073 7175 6172  e the mean squar
+0002d820: 6520 6f66 2074 6865 2064 6174 610a 2020  e of the data.  
+0002d830: 2020 2020 2020 2020 2020 2020 7061 7274              part
+0002d840: 2072 6f75 7465 6420 746f 2065 6163 6820   routed to each 
+0002d850: 6578 7065 7274 2c20 616e 6420 6f6e 6c79  expert, and only
+0002d860: 2072 6574 7572 6e65 6420 6966 2074 6865   returned if the
+0002d870: 206e 756d 6265 7220 6f66 2065 7870 6572   number of exper
+0002d880: 7473 2069 7320 6772 6561 7465 7220 7468  ts is greater th
+0002d890: 616e 2031 2e0a 0a20 2020 2020 2020 2053  an 1...        S
+0002d8a0: 7570 706f 7274 6564 2050 6c61 7466 6f72  upported Platfor
+0002d8b0: 6d73 3a0a 2020 2020 2020 2020 2020 2020  ms:.            
+0002d8c0: 6060 4173 6365 6e64 6060 2060 6047 5055  ``Ascend`` ``GPU
+0002d8d0: 6060 0a0a 2020 2020 2020 2020 4578 616d  ``..        Exam
+0002d8e0: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
+0002d8f0: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
+0002d900: 7079 2061 7320 6e70 0a20 2020 2020 2020  py as np.       
+0002d910: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
+0002d920: 6e64 7370 6f72 6520 696d 706f 7274 2064  ndspore import d
+0002d930: 7479 7065 2061 7320 6d73 7479 7065 0a20  type as mstype. 
+0002d940: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
+0002d950: 726f 6d20 6d69 6e64 666f 726d 6572 732e  rom mindformers.
+0002d960: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
+0002d970: 6d65 7220 696d 706f 7274 2054 7261 6e73  mer import Trans
+0002d980: 666f 726d 6572 0a20 2020 2020 2020 2020  former.         
+0002d990: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
+0002d9a0: 7370 6f72 6520 696d 706f 7274 2054 656e  spore import Ten
+0002d9b0: 736f 720a 2020 2020 2020 2020 2020 2020  sor.            
+0002d9c0: 3e3e 3e20 6d6f 6465 6c20 3d20 5472 616e  >>> model = Tran
+0002d9d0: 7366 6f72 6d65 7228 6261 7463 685f 7369  sformer(batch_si
+0002d9e0: 7a65 3d32 2c20 656e 636f 6465 725f 6c61  ze=2, encoder_la
+0002d9f0: 7965 7273 3d31 2c20 6465 636f 6465 725f  yers=1, decoder_
+0002da00: 6c61 7965 7273 3d32 2c20 6869 6464 656e  layers=2, hidden
+0002da10: 5f73 697a 653d 3634 2c0a 2020 2020 2020  _size=64,.      
+0002da20: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
+0002da30: 2020 2020 2020 2020 2020 2020 2020 6666                ff
+0002da40: 6e5f 6869 6464 656e 5f73 697a 653d 3634  n_hidden_size=64
+0002da50: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
+0002da60: 3d32 302c 2074 6774 5f73 6571 5f6c 656e  =20, tgt_seq_len
+0002da70: 6774 683d 3130 290a 2020 2020 2020 2020  gth=10).        
+0002da80: 2020 2020 3e3e 3e20 656e 636f 6465 725f      >>> encoder_
+0002da90: 696e 7075 745f 7661 6c75 6520 3d20 5465  input_value = Te
+0002daa0: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
+0002dab0: 2032 302c 2036 3429 292c 206d 7374 7970   20, 64)), mstyp
+0002dac0: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
+0002dad0: 2020 2020 2020 203e 3e3e 2065 6e63 6f64         >>> encod
+0002dae0: 6572 5f69 6e70 7574 5f6d 6173 6b20 3d20  er_input_mask = 
+0002daf0: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
+0002db00: 322c 2032 302c 2032 3029 292c 206d 7374  2, 20, 20)), mst
+0002db10: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
+0002db20: 2020 2020 2020 2020 203e 3e3e 2064 6563           >>> dec
+0002db30: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+0002db40: 203d 2054 656e 736f 7228 6e70 2e6f 6e65   = Tensor(np.one
+0002db50: 7328 2832 2c20 3130 2c20 3634 2929 2c20  s((2, 10, 64)), 
+0002db60: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+0002db70: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0002db80: 6465 636f 6465 725f 696e 7075 745f 6d61  decoder_input_ma
+0002db90: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
+0002dba0: 6e65 7328 2832 2c20 3130 2c20 3130 2929  nes((2, 10, 10))
+0002dbb0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+0002dbc0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+0002dbd0: 3e20 6d65 6d6f 7279 5f6d 6173 6b20 3d20  > memory_mask = 
+0002dbe0: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
+0002dbf0: 322c 2031 302c 2032 3029 292c 206d 7374  2, 10, 20)), mst
+0002dc00: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
+0002dc10: 2020 2020 2020 2020 203e 3e3e 206f 7574           >>> out
+0002dc20: 7075 742c 2065 6e5f 7061 7374 2c20 6465  put, en_past, de
+0002dc30: 5f70 6173 7420 3d20 6d6f 6465 6c28 656e  _past = model(en
+0002dc40: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
+0002dc50: 652c 2065 6e63 6f64 6572 5f69 6e70 7574  e, encoder_input
+0002dc60: 5f6d 6173 6b2c 2064 6563 6f64 6572 5f69  _mask, decoder_i
+0002dc70: 6e70 7574 5f76 616c 7565 2c0a 2020 2020  nput_value,.    
+0002dc80: 2020 2020 2020 2020 2e2e 2e20 2020 2020          ...     
+0002dc90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dca0: 2020 2020 2020 2020 2020 2020 2064 6563               dec
+0002dcb0: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b2c  oder_input_mask,
+0002dcc0: 206d 656d 6f72 795f 6d61 736b 290a 2020   memory_mask).  
+0002dcd0: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+0002dce0: 696e 7428 6f75 7470 7574 2e73 6861 7065  int(output.shape
+0002dcf0: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+0002dd00: 2c20 3130 2c20 3634 290a 2020 2020 2020  , 10, 64).      
+0002dd10: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+0002dd20: 6c65 6e28 656e 5f70 6173 7429 290a 2020  len(en_past)).  
+0002dd30: 2020 2020 2020 2020 2020 310a 2020 2020            1.    
+0002dd40: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+0002dd50: 7428 6c65 6e28 6465 5f70 6173 7429 290a  t(len(de_past)).
+0002dd60: 2020 2020 2020 2020 2020 2020 320a 2020              2.  
+0002dd70: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+0002dd80: 696e 7428 656e 5f70 6173 745b 305d 5b30  int(en_past[0][0
+0002dd90: 5d2e 7368 6170 6529 0a20 2020 2020 2020  ].shape).       
+0002dda0: 2020 2020 2028 322c 2032 2c20 3332 2c20       (2, 2, 32, 
+0002ddb0: 3230 290a 2020 2020 2020 2020 2020 2020  20).            
+0002ddc0: 3e3e 3e20 7072 696e 7428 656e 5f70 6173  >>> print(en_pas
+0002ddd0: 745b 305d 5b31 5d2e 7368 6170 6529 0a20  t[0][1].shape). 
+0002dde0: 2020 2020 2020 2020 2020 2028 322c 2032             (2, 2
+0002ddf0: 2c20 3230 2c20 3332 290a 2020 2020 2020  , 20, 32).      
+0002de00: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+0002de10: 6465 5f70 6173 745b 305d 5b30 5d2e 7368  de_past[0][0].sh
+0002de20: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0002de30: 2028 322c 2032 2c20 3332 2c20 3130 290a   (2, 2, 32, 10).
+0002de40: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0002de50: 7072 696e 7428 6465 5f70 6173 745b 305d  print(de_past[0]
+0002de60: 5b31 5d2e 7368 6170 6529 0a20 2020 2020  [1].shape).     
+0002de70: 2020 2020 2020 2028 322c 2032 2c20 3130         (2, 2, 10
+0002de80: 2c20 3332 290a 2020 2020 2020 2020 2020  , 32).          
+0002de90: 2020 3e3e 3e20 7072 696e 7428 6465 5f70    >>> print(de_p
+0002dea0: 6173 745b 305d 5b32 5d2e 7368 6170 6529  ast[0][2].shape)
+0002deb0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
+0002dec0: 2032 2c20 3332 2c20 3230 290a 2020 2020   2, 32, 20).    
+0002ded0: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+0002dee0: 7428 6465 5f70 6173 745b 305d 5b33 5d2e  t(de_past[0][3].
+0002def0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0002df00: 2020 2028 322c 2032 2c20 3230 2c20 3332     (2, 2, 20, 32
+0002df10: 290a 2020 2020 2222 220a 0a20 2020 2040  ).    """..    @
+0002df20: 5f4c 6f67 4163 7469 6f6e 4f6e 6365 286d  _LogActionOnce(m
+0002df30: 5f6c 6f67 6765 723d 6c6f 6767 6572 2c20  _logger=logger, 
+0002df40: 6b65 793d 2754 7261 6e73 666f 726d 6572  key='Transformer
+0002df50: 272c 0a20 2020 2020 2020 2020 2020 2020  ',.             
+0002df60: 2020 2020 2020 206e 6f5f 7761 726e 696e         no_warnin
+0002df70: 673d 5f67 6574 5f70 6172 616c 6c65 6c5f  g=_get_parallel_
+0002df80: 6d6f 6465 2829 2069 6e20 2850 6172 616c  mode() in (Paral
+0002df90: 6c65 6c4d 6f64 652e 5354 414e 445f 414c  lelMode.STAND_AL
+0002dfa0: 4f4e 452c 2929 0a20 2020 2040 5f61 7267  ONE,)).    @_arg
+0002dfb0: 735f 7479 7065 5f76 616c 6964 6174 6f72  s_type_validator
+0002dfc0: 5f63 6865 636b 2868 6964 6465 6e5f 7369  _check(hidden_si
+0002dfd0: 7a65 3d56 616c 6964 6174 6f72 2e63 6865  ze=Validator.che
+0002dfe0: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+0002dff0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002e000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e010: 206e 756d 5f68 6561 6473 3d56 616c 6964   num_heads=Valid
+0002e020: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
+0002e030: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
+0002e040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e050: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+0002e060: 6465 6e5f 7369 7a65 3d56 616c 6964 6174  den_size=Validat
+0002e070: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
+0002e080: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
+0002e090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e0a0: 2020 2020 2020 2073 7263 5f73 6571 5f6c         src_seq_l
+0002e0b0: 656e 6774 683d 5661 6c69 6461 746f 722e  ength=Validator.
+0002e0c0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+0002e0d0: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
+0002e0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e0f0: 2020 2020 656e 636f 6465 725f 6c61 7965      encoder_laye
+0002e100: 7273 3d56 616c 6964 6174 6f72 2e63 6865  rs=Validator.che
+0002e110: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+0002e120: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002e130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e140: 2064 6563 6f64 6572 5f6c 6179 6572 733d   decoder_layers=
+0002e150: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+0002e160: 6e6f 6e5f 6e65 6761 7469 7665 5f69 6e74  non_negative_int
+0002e170: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002e180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e190: 2020 7467 745f 7365 715f 6c65 6e67 7468    tgt_seq_length
+0002e1a0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+0002e1b0: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
+0002e1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e1d0: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+0002e1e0: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
+0002e1f0: 5f72 6174 653d 5661 6c69 6461 746f 722e  _rate=Validator.
+0002e200: 6368 6563 6b5f 6e6f 6e5f 6e65 6761 7469  check_non_negati
+0002e210: 7665 5f66 6c6f 6174 2c0a 2020 2020 2020  ve_float,.      
+0002e220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e230: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0002e240: 5f64 726f 706f 7574 5f72 6174 653d 5661  _dropout_rate=Va
+0002e250: 6c69 6461 746f 722e 6368 6563 6b5f 6e6f  lidator.check_no
+0002e260: 6e5f 6e65 6761 7469 7665 5f66 6c6f 6174  n_negative_float
+0002e270: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002e280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e290: 2020 706f 7374 5f6c 6179 6572 6e6f 726d    post_layernorm
+0002e2a0: 5f72 6573 6964 7561 6c3d 5661 6c69 6461  _residual=Valida
+0002e2b0: 746f 722e 6368 6563 6b5f 626f 6f6c 2c0a  tor.check_bool,.
+0002e2c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e2e0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+0002e2f0: 655f 7479 7065 3d5f 7661 6c69 645f 7661  e_type=_valid_va
+0002e300: 6c75 655f 6368 6563 6b73 285b 6d73 7479  lue_checks([msty
+0002e310: 7065 2e66 6c6f 6174 3332 2c0a 2020 2020  pe.float32,.    
 0002e320: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e340: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-0002e350: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-0002e360: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-0002e370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e360: 2020 2020 2020 2020 6d73 7479 7065 2e66          mstype.f
+0002e370: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
+0002e380: 666c 6f61 7431 365d 2c0a 2020 2020 2020  float16],.      
 0002e390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3a0: 2020 2020 2020 2020 2020 2020 2020 2254                "T
-0002e3b0: 7261 6e73 666f 726d 6572 2229 2c0a 2020  ransformer"),.  
+0002e3a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3d0: 2020 2020 2020 2020 2020 2020 2020 736f                so
-0002e3e0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-0002e3f0: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
-0002e400: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
-0002e410: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
-0002e420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e3d0: 2020 2020 2022 5472 616e 7366 6f72 6d65       "Transforme
+0002e3e0: 7222 292c 0a20 2020 2020 2020 2020 2020  r"),.           
+0002e3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e400: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
+0002e410: 7075 7465 5f74 7970 653d 5f76 616c 6964  pute_type=_valid
+0002e420: 5f76 616c 7565 5f63 6865 636b 7328 5b6d  _value_checks([m
+0002e430: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
 0002e440: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e450: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e460: 2020 6d73 7479 7065 2e66 6c6f 6174 3136    mstype.float16
-0002e470: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-0002e480: 365d 2c0a 2020 2020 2020 2020 2020 2020  6],.            
-0002e490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e480: 2020 2020 2020 2020 206d 7374 7970 652e           mstype.
+0002e490: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+0002e4a0: 6266 6c6f 6174 3136 5d2c 0a20 2020 2020  bfloat16],.     
 0002e4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e4c0: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-0002e4d0: 616e 7366 6f72 6d65 7222 292c 0a20 2020  ansformer"),.   
+0002e4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e4f0: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0002e500: 616d 5f69 6e69 745f 7479 7065 3d5f 7661  am_init_type=_va
-0002e510: 6c69 645f 7661 6c75 655f 6368 6563 6b73  lid_value_checks
-0002e520: 285b 6d73 7479 7065 2e66 6c6f 6174 3332  ([mstype.float32
-0002e530: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e4f0: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
+0002e500: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
+0002e510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e520: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+0002e530: 7970 653d 5f76 616c 6964 5f76 616c 7565  ype=_valid_value
+0002e540: 5f63 6865 636b 7328 5b6d 7374 7970 652e  _checks([mstype.
+0002e550: 666c 6f61 7433 322c 0a20 2020 2020 2020  float32,.       
 0002e560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e570: 2020 2020 2020 206d 7374 7970 652e 666c         mstype.fl
-0002e580: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-0002e590: 6c6f 6174 3136 5d2c 2022 5472 616e 7366  loat16], "Transf
-0002e5a0: 6f72 6d65 7222 292c 0a20 2020 2020 2020  ormer"),.       
-0002e5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e5c0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-0002e5d0: 6c5f 636f 6e66 6967 3d5f 7661 6c69 645f  l_config=_valid_
-0002e5e0: 7479 7065 5f63 6865 636b 7328 5b54 7261  type_checks([Tra
-0002e5f0: 6e73 666f 726d 6572 4f70 5061 7261 6c6c  nsformerOpParall
-0002e600: 656c 436f 6e66 6967 5d2c 2022 5472 616e  elConfig], "Tran
-0002e610: 7366 6f72 6d65 7222 292c 0a20 2020 2020  sformer"),.     
-0002e620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e630: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-0002e640: 6173 743d 5661 6c69 6461 746f 722e 6368  ast=Validator.ch
-0002e650: 6563 6b5f 626f 6f6c 290a 2020 2020 6465  eck_bool).    de
-0002e660: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
-0002e670: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002e680: 2020 6869 6464 656e 5f73 697a 652c 0a20    hidden_size,. 
-0002e690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e6a0: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
-0002e6b0: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-0002e6c0: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
-0002e6d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0002e6e0: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
-0002e6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e700: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
-0002e710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e720: 2065 6e63 6f64 6572 5f6c 6179 6572 733d   encoder_layers=
-0002e730: 332c 0a20 2020 2020 2020 2020 2020 2020  3,.             
-0002e740: 2020 2020 6465 636f 6465 725f 6c61 7965      decoder_laye
-0002e750: 7273 3d33 2c0a 2020 2020 2020 2020 2020  rs=3,.          
-0002e760: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
-0002e770: 3d32 2c0a 2020 2020 2020 2020 2020 2020  =2,.            
-0002e780: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-0002e790: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
-0002e7a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002e7b0: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
-0002e7c0: 5f72 6174 653d 302e 312c 0a20 2020 2020  _rate=0.1,.     
-0002e7d0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-0002e7e0: 656e 5f61 6374 3d27 6765 6c75 272c 0a20  en_act='gelu',. 
-0002e7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e800: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-0002e810: 6573 6964 7561 6c3d 4661 6c73 652c 0a20  esidual=False,. 
-0002e820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e830: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-0002e840: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
-0002e850: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-0002e860: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
-0002e870: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
-0002e880: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
-0002e890: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0002e8a0: 7261 6d5f 696e 6974 5f74 7970 653d 6d73  ram_init_type=ms
-0002e8b0: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
-0002e8c0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
-0002e8d0: 616d 6264 615f 6675 6e63 3d4e 6f6e 652c  ambda_func=None,
-0002e8e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002e8f0: 2020 7573 655f 7061 7374 3d46 616c 7365    use_past=False
-0002e900: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e910: 2020 206d 6f65 5f63 6f6e 6669 673d 6465     moe_config=de
-0002e920: 6661 756c 745f 6d6f 655f 636f 6e66 6967  fault_moe_config
-0002e930: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e940: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
-0002e950: 6967 3d64 6566 6175 6c74 5f74 7261 6e73  ig=default_trans
-0002e960: 666f 726d 6572 5f63 6f6e 6669 6729 3a0a  former_config):.
-0002e970: 2020 2020 2020 2020 7375 7065 7228 5472          super(Tr
-0002e980: 616e 7366 6f72 6d65 722c 2073 656c 6629  ansformer, self)
-0002e990: 2e5f 5f69 6e69 745f 5f28 290a 2020 2020  .__init__().    
-0002e9a0: 2020 2020 6966 2062 6174 6368 5f73 697a      if batch_siz
-0002e9b0: 6520 6f72 2075 7365 5f70 6173 743a 0a20  e or use_past:. 
-0002e9c0: 2020 2020 2020 2020 2020 2056 616c 6964             Valid
-0002e9d0: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-0002e9e0: 6976 655f 696e 7428 6261 7463 685f 7369  ive_int(batch_si
-0002e9f0: 7a65 290a 2020 2020 2020 2020 6966 205f  ze).        if _
-0002ea00: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
-0002ea10: 6528 2920 696e 2028 5061 7261 6c6c 656c  e() in (Parallel
-0002ea20: 4d6f 6465 2e41 5554 4f5f 5041 5241 4c4c  Mode.AUTO_PARALL
-0002ea30: 454c 2c29 3a0a 2020 2020 2020 2020 2020  EL,):.          
-0002ea40: 2020 5f63 6865 636b 5f63 6f6e 6669 6728    _check_config(
-0002ea50: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-0002ea60: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0002ea70: 662e 6261 7463 685f 7369 7a65 203d 2062  f.batch_size = b
-0002ea80: 6174 6368 5f73 697a 650a 2020 2020 2020  atch_size.      
-0002ea90: 2020 2020 2020 7365 6c66 2e68 6964 6465        self.hidde
-0002eaa0: 6e5f 7369 7a65 203d 2068 6964 6465 6e5f  n_size = hidden_
-0002eab0: 7369 7a65 0a20 2020 2020 2020 2020 2020  size.           
-0002eac0: 2073 656c 662e 7372 635f 7365 715f 6c65   self.src_seq_le
-0002ead0: 6e67 7468 203d 2073 7263 5f73 6571 5f6c  ngth = src_seq_l
-0002eae0: 656e 6774 680a 2020 2020 2020 2020 2020  ength.          
-0002eaf0: 2020 7365 6c66 2e74 6774 5f73 6571 5f6c    self.tgt_seq_l
-0002eb00: 656e 6774 6820 3d20 7467 745f 7365 715f  ength = tgt_seq_
-0002eb10: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
-0002eb20: 2020 2073 656c 662e 7573 655f 7061 7374     self.use_past
-0002eb30: 203d 2075 7365 5f70 6173 740a 2020 2020   = use_past.    
-0002eb40: 2020 2020 2020 2020 6966 2065 6e63 6f64          if encod
-0002eb50: 6572 5f6c 6179 6572 7320 3c3d 2030 203c  er_layers <= 0 <
-0002eb60: 2064 6563 6f64 6572 5f6c 6179 6572 733a   decoder_layers:
-0002eb70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002eb80: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-0002eb90: 7228 6622 5472 616e 7366 6f72 6d65 7220  r(f"Transformer 
-0002eba0: 646f 6573 7420 7375 7070 6f72 7420 656e  doest support en
-0002ebb0: 636f 6465 7220 6c61 7965 7220 7b65 6e63  coder layer {enc
-0002ebc0: 6f64 6572 5f6c 6179 6572 737d 2061 6e64  oder_layers} and
-0002ebd0: 2064 6563 6f64 6572 220a 2020 2020 2020   decoder".      
-0002ebe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ebf0: 2020 2020 2020 2020 2020 2066 226c 6179             f"lay
-0002ec00: 6572 207b 6465 636f 6465 725f 6c61 7965  er {decoder_laye
-0002ec10: 7273 7d2c 2070 6c65 6173 6520 7573 6520  rs}, please use 
-0002ec20: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
-0002ec30: 6572 2229 0a20 2020 2020 2020 2020 2020  er").           
-0002ec40: 2069 6620 656e 636f 6465 725f 6c61 7965   if encoder_laye
-0002ec50: 7273 203e 2030 2061 6e64 2064 6563 6f64  rs > 0 and decod
-0002ec60: 6572 5f6c 6179 6572 7320 3e20 3020 616e  er_layers > 0 an
-0002ec70: 6420 7573 655f 7061 7374 3a0a 2020 2020  d use_past:.    
-0002ec80: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-0002ec90: 6520 5661 6c75 6545 7272 6f72 2866 2254  e ValueError(f"T
-0002eca0: 6865 207b 7365 6c66 2e63 6c73 5f6e 616d  he {self.cls_nam
-0002ecb0: 657d 2077 6974 6820 656e 636f 6465 7220  e} with encoder 
-0002ecc0: 616e 6420 6465 636f 6465 7220 646f 6573  and decoder does
-0002ecd0: 206e 6f74 2073 7570 706f 7274 2075 7365   not support use
-0002ece0: 5f70 6173 743d 5472 7565 2e22 290a 2020  _past=True.").  
-0002ecf0: 2020 2020 2020 2020 2020 2320 5468 6520            # The 
-0002ed00: 7368 6172 6420 7365 7474 696e 6720 6f66  shard setting of
-0002ed10: 2054 7261 6e73 666f 726d 6572 2069 7320   Transformer is 
-0002ed20: 7365 7420 7769 7468 696e 2074 6865 2054  set within the T
-0002ed30: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-0002ed40: 724c 6179 6572 0a20 2020 2020 2020 2020  rLayer.         
-0002ed50: 2020 2069 6620 6e6f 7420 6c61 6d62 6461     if not lambda
-0002ed60: 5f66 756e 633a 0a20 2020 2020 2020 2020  _func:.         
-0002ed70: 2020 2020 2020 206c 616d 6264 615f 6675         lambda_fu
-0002ed80: 6e63 203d 205f 6765 745f 6c61 6d62 6461  nc = _get_lambda
-0002ed90: 5f66 756e 6328 746f 7461 6c5f 6c61 7965  _func(total_laye
-0002eda0: 723d 656e 636f 6465 725f 6c61 7965 7273  r=encoder_layers
-0002edb0: 202b 2064 6563 6f64 6572 5f6c 6179 6572   + decoder_layer
-0002edc0: 7329 0a20 2020 2020 2020 2020 2020 205f  s).            _
-0002edd0: 6368 6563 6b5f 6d6f 655f 636f 6e66 6967  check_moe_config
-0002ede0: 286d 6f65 5f63 6f6e 6669 672c 2070 6172  (moe_config, par
-0002edf0: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-0002ee00: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
-0002ee10: 7365 5f6d 6f65 203d 2028 6d6f 655f 636f  se_moe = (moe_co
-0002ee20: 6e66 6967 2e65 7870 6572 745f 6e75 6d20  nfig.expert_num 
-0002ee30: 3e20 3129 0a20 2020 2020 2020 2020 2020  > 1).           
-0002ee40: 2073 656c 662e 6164 6420 3d20 502e 4164   self.add = P.Ad
-0002ee50: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
-0002ee60: 7365 6c66 2e61 7578 5f6c 6f73 7320 3d20  self.aux_loss = 
-0002ee70: 5465 6e73 6f72 2830 2e30 2c20 6d73 7479  Tensor(0.0, msty
-0002ee80: 7065 2e66 6c6f 6174 3332 290a 2020 2020  pe.float32).    
-0002ee90: 2020 2020 2020 2020 6966 2065 6e63 6f64          if encod
-0002eea0: 6572 5f6c 6179 6572 7320 3e20 303a 0a20  er_layers > 0:. 
-0002eeb0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0002eec0: 656c 662e 656e 636f 6465 7220 3d20 5472  elf.encoder = Tr
-0002eed0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-0002eee0: 286e 756d 5f6c 6179 6572 733d 656e 636f  (num_layers=enco
-0002eef0: 6465 725f 6c61 7965 7273 2c0a 2020 2020  der_layers,.    
-0002ef00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ef10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ef20: 2020 2020 2020 2020 2020 2020 2020 6261                ba
-0002ef30: 7463 685f 7369 7a65 3d62 6174 6368 5f73  tch_size=batch_s
-0002ef40: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0002ef50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ef60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ef70: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
-0002ef80: 7a65 3d68 6964 6465 6e5f 7369 7a65 2c0a  ze=hidden_size,.
-0002ef90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002efa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002efb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002efc0: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-0002efd0: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
-0002efe0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002eff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f010: 2020 2020 206e 756d 5f68 6561 6473 3d6e       num_heads=n
-0002f020: 756d 5f68 6561 6473 2c0a 2020 2020 2020  um_heads,.      
-0002f030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f050: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
-0002f060: 6c65 6e67 7468 3d73 7263 5f73 6571 5f6c  length=src_seq_l
-0002f070: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
-0002f080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f0a0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-0002f0b0: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
-0002f0c0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-0002f0d0: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
-0002f0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f0f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f100: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-0002f110: 5f64 726f 706f 7574 5f72 6174 653d 6869  _dropout_rate=hi
-0002f120: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-0002f130: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002f140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f160: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
-0002f170: 6869 6464 656e 5f61 6374 2c0a 2020 2020  hidden_act,.    
-0002f180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f1a0: 2020 2020 2020 2020 2020 2020 2020 6c61                la
-0002f1b0: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-0002f1c0: 7479 7065 3d6c 6179 6572 6e6f 726d 5f63  type=layernorm_c
-0002f1d0: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
-0002f1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f200: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0002f210: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-0002f220: 7970 653d 736f 6674 6d61 785f 636f 6d70  ype=softmax_comp
-0002f230: 7574 655f 7479 7065 2c0a 2020 2020 2020  ute_type,.      
-0002f240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f260: 2020 2020 2020 2020 2020 2020 706f 7374              post
-0002f270: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0002f280: 7561 6c3d 706f 7374 5f6c 6179 6572 6e6f  ual=post_layerno
-0002f290: 726d 5f72 6573 6964 7561 6c2c 0a20 2020  rm_residual,.   
-0002f2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f2b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f2c0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0002f2d0: 6172 616d 5f69 6e69 745f 7479 7065 3d70  aram_init_type=p
-0002f2e0: 6172 616d 5f69 6e69 745f 7479 7065 2c0a  aram_init_type,.
-0002f2f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f320: 2020 6c61 6d62 6461 5f66 756e 633d 6c61    lambda_func=la
-0002f330: 6d62 6461 5f66 756e 632c 0a20 2020 2020  mbda_func,.     
-0002f340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f360: 2020 2020 2020 2020 2020 2020 2075 7365               use
-0002f370: 5f70 6173 743d 7573 655f 7061 7374 2c0a  _past=use_past,.
+0002e570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e590: 2020 2020 2020 2020 2020 2020 2020 6d73                ms
+0002e5a0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+0002e5b0: 7479 7065 2e62 666c 6f61 7431 365d 2c20  type.bfloat16], 
+0002e5c0: 2254 7261 6e73 666f 726d 6572 2229 2c0a  "Transformer"),.
+0002e5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+0002e600: 5f76 616c 6964 5f74 7970 655f 6368 6563  _valid_type_chec
+0002e610: 6b73 285b 5472 616e 7366 6f72 6d65 724f  ks([TransformerO
+0002e620: 7050 6172 616c 6c65 6c43 6f6e 6669 675d  pParallelConfig]
+0002e630: 2c20 2254 7261 6e73 666f 726d 6572 2229  , "Transformer")
+0002e640: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002e650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e660: 2020 7573 655f 7061 7374 3d56 616c 6964    use_past=Valid
+0002e670: 6174 6f72 2e63 6865 636b 5f62 6f6f 6c29  ator.check_bool)
+0002e680: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
+0002e690: 5f28 7365 6c66 2c0a 2020 2020 2020 2020  _(self,.        
+0002e6a0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002e6b0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+0002e6c0: 2020 2020 2020 2062 6174 6368 5f73 697a         batch_siz
+0002e6d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002e6e0: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+0002e6f0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+0002e700: 2020 2020 2020 7372 635f 7365 715f 6c65        src_seq_le
+0002e710: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+0002e720: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+0002e730: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+0002e740: 2020 2020 2020 2020 656e 636f 6465 725f          encoder_
+0002e750: 6c61 7965 7273 3d33 2c0a 2020 2020 2020  layers=3,.      
+0002e760: 2020 2020 2020 2020 2020 2064 6563 6f64             decod
+0002e770: 6572 5f6c 6179 6572 733d 332c 0a20 2020  er_layers=3,.   
+0002e780: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
+0002e790: 6d5f 6865 6164 733d 322c 0a20 2020 2020  m_heads=2,.     
+0002e7a0: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+0002e7b0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+0002e7c0: 7465 3d30 2e31 2c0a 2020 2020 2020 2020  te=0.1,.        
+0002e7d0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002e7e0: 6472 6f70 6f75 745f 7261 7465 3d30 2e31  dropout_rate=0.1
+0002e7f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002e800: 2020 2068 6964 6465 6e5f 6163 743d 2767     hidden_act='g
+0002e810: 656c 7527 2c0a 2020 2020 2020 2020 2020  elu',.          
+0002e820: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
+0002e830: 726e 6f72 6d5f 7265 7369 6475 616c 3d46  rnorm_residual=F
+0002e840: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
+0002e850: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
+0002e860: 5f63 6f6d 7075 7465 5f74 7970 653d 6d73  _compute_type=ms
+0002e870: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+0002e880: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0002e890: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+0002e8a0: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
+0002e8b0: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+0002e8c0: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+0002e8d0: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
+0002e8e0: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
+0002e8f0: 2020 2020 2020 6c61 6d62 6461 5f66 756e        lambda_fun
+0002e900: 633d 4e6f 6e65 2c0a 2020 2020 2020 2020  c=None,.        
+0002e910: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
+0002e920: 743d 4661 6c73 652c 0a20 2020 2020 2020  t=False,.       
+0002e930: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
+0002e940: 6e66 6967 3d64 6566 6175 6c74 5f6d 6f65  nfig=default_moe
+0002e950: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
+0002e960: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+0002e970: 656c 5f63 6f6e 6669 673d 6465 6661 756c  el_config=defaul
+0002e980: 745f 7472 616e 7366 6f72 6d65 725f 636f  t_transformer_co
+0002e990: 6e66 6967 293a 0a20 2020 2020 2020 2073  nfig):.        s
+0002e9a0: 7570 6572 2854 7261 6e73 666f 726d 6572  uper(Transformer
+0002e9b0: 2c20 7365 6c66 292e 5f5f 696e 6974 5f5f  , self).__init__
+0002e9c0: 2829 0a20 2020 2020 2020 2069 6620 6261  ().        if ba
+0002e9d0: 7463 685f 7369 7a65 206f 7220 7573 655f  tch_size or use_
+0002e9e0: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+0002e9f0: 2020 5661 6c69 6461 746f 722e 6368 6563    Validator.chec
+0002ea00: 6b5f 706f 7369 7469 7665 5f69 6e74 2862  k_positive_int(b
+0002ea10: 6174 6368 5f73 697a 6529 0a20 2020 2020  atch_size).     
+0002ea20: 2020 2069 6620 5f67 6574 5f70 6172 616c     if _get_paral
+0002ea30: 6c65 6c5f 6d6f 6465 2829 2069 6e20 2850  lel_mode() in (P
+0002ea40: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
+0002ea50: 5f50 4152 414c 4c45 4c2c 293a 0a20 2020  _PARALLEL,):.   
+0002ea60: 2020 2020 2020 2020 205f 6368 6563 6b5f           _check_
+0002ea70: 636f 6e66 6967 2870 6172 616c 6c65 6c5f  config(parallel_
+0002ea80: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+0002ea90: 2020 2020 7365 6c66 2e62 6174 6368 5f73      self.batch_s
+0002eaa0: 697a 6520 3d20 6261 7463 685f 7369 7a65  ize = batch_size
+0002eab0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0002eac0: 662e 6869 6464 656e 5f73 697a 6520 3d20  f.hidden_size = 
+0002ead0: 6869 6464 656e 5f73 697a 650a 2020 2020  hidden_size.    
+0002eae0: 2020 2020 2020 2020 7365 6c66 2e73 7263          self.src
+0002eaf0: 5f73 6571 5f6c 656e 6774 6820 3d20 7372  _seq_length = sr
+0002eb00: 635f 7365 715f 6c65 6e67 7468 0a20 2020  c_seq_length.   
+0002eb10: 2020 2020 2020 2020 2073 656c 662e 7467           self.tg
+0002eb20: 745f 7365 715f 6c65 6e67 7468 203d 2074  t_seq_length = t
+0002eb30: 6774 5f73 6571 5f6c 656e 6774 680a 2020  gt_seq_length.  
+0002eb40: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
+0002eb50: 7365 5f70 6173 7420 3d20 7573 655f 7061  se_past = use_pa
+0002eb60: 7374 0a20 2020 2020 2020 2020 2020 2069  st.            i
+0002eb70: 6620 656e 636f 6465 725f 6c61 7965 7273  f encoder_layers
+0002eb80: 203c 3d20 3020 3c20 6465 636f 6465 725f   <= 0 < decoder_
+0002eb90: 6c61 7965 7273 3a0a 2020 2020 2020 2020  layers:.        
+0002eba0: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
+0002ebb0: 6c75 6545 7272 6f72 2866 2254 7261 6e73  lueError(f"Trans
+0002ebc0: 666f 726d 6572 2064 6f65 7374 2073 7570  former doest sup
+0002ebd0: 706f 7274 2065 6e63 6f64 6572 206c 6179  port encoder lay
+0002ebe0: 6572 207b 656e 636f 6465 725f 6c61 7965  er {encoder_laye
+0002ebf0: 7273 7d20 616e 6420 6465 636f 6465 7222  rs} and decoder"
+0002ec00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002ec10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ec20: 2020 6622 6c61 7965 7220 7b64 6563 6f64    f"layer {decod
+0002ec30: 6572 5f6c 6179 6572 737d 2c20 706c 6561  er_layers}, plea
+0002ec40: 7365 2075 7365 2054 7261 6e73 666f 726d  se use Transform
+0002ec50: 6572 4465 636f 6465 7222 290a 2020 2020  erDecoder").    
+0002ec60: 2020 2020 2020 2020 6966 2065 6e63 6f64          if encod
+0002ec70: 6572 5f6c 6179 6572 7320 3e20 3020 616e  er_layers > 0 an
+0002ec80: 6420 6465 636f 6465 725f 6c61 7965 7273  d decoder_layers
+0002ec90: 203e 2030 2061 6e64 2075 7365 5f70 6173   > 0 and use_pas
+0002eca0: 743a 0a20 2020 2020 2020 2020 2020 2020  t:.             
+0002ecb0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0002ecc0: 726f 7228 6622 5468 6520 7b73 656c 662e  ror(f"The {self.
+0002ecd0: 636c 735f 6e61 6d65 7d20 7769 7468 2065  cls_name} with e
+0002ece0: 6e63 6f64 6572 2061 6e64 2064 6563 6f64  ncoder and decod
+0002ecf0: 6572 2064 6f65 7320 6e6f 7420 7375 7070  er does not supp
+0002ed00: 6f72 7420 7573 655f 7061 7374 3d54 7275  ort use_past=Tru
+0002ed10: 652e 2229 0a20 2020 2020 2020 2020 2020  e.").           
+0002ed20: 2023 2054 6865 2073 6861 7264 2073 6574   # The shard set
+0002ed30: 7469 6e67 206f 6620 5472 616e 7366 6f72  ting of Transfor
+0002ed40: 6d65 7220 6973 2073 6574 2077 6974 6869  mer is set withi
+0002ed50: 6e20 7468 6520 5472 616e 7366 6f72 6d65  n the Transforme
+0002ed60: 7245 6e63 6f64 6572 4c61 7965 720a 2020  rEncoderLayer.  
+0002ed70: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
+0002ed80: 206c 616d 6264 615f 6675 6e63 3a0a 2020   lambda_func:.  
+0002ed90: 2020 2020 2020 2020 2020 2020 2020 6c61                la
+0002eda0: 6d62 6461 5f66 756e 6320 3d20 5f67 6574  mbda_func = _get
+0002edb0: 5f6c 616d 6264 615f 6675 6e63 2874 6f74  _lambda_func(tot
+0002edc0: 616c 5f6c 6179 6572 3d65 6e63 6f64 6572  al_layer=encoder
+0002edd0: 5f6c 6179 6572 7320 2b20 6465 636f 6465  _layers + decode
+0002ede0: 725f 6c61 7965 7273 290a 2020 2020 2020  r_layers).      
+0002edf0: 2020 2020 2020 5f63 6865 636b 5f6d 6f65        _check_moe
+0002ee00: 5f63 6f6e 6669 6728 6d6f 655f 636f 6e66  _config(moe_conf
+0002ee10: 6967 2c20 7061 7261 6c6c 656c 5f63 6f6e  ig, parallel_con
+0002ee20: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
+0002ee30: 2073 656c 662e 7573 655f 6d6f 6520 3d20   self.use_moe = 
+0002ee40: 286d 6f65 5f63 6f6e 6669 672e 6578 7065  (moe_config.expe
+0002ee50: 7274 5f6e 756d 203e 2031 290a 2020 2020  rt_num > 1).    
+0002ee60: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+0002ee70: 203d 2050 2e41 6464 2829 0a20 2020 2020   = P.Add().     
+0002ee80: 2020 2020 2020 2073 656c 662e 6175 785f         self.aux_
+0002ee90: 6c6f 7373 203d 2054 656e 736f 7228 302e  loss = Tensor(0.
+0002eea0: 302c 206d 7374 7970 652e 666c 6f61 7433  0, mstype.float3
+0002eeb0: 3229 0a20 2020 2020 2020 2020 2020 2069  2).            i
+0002eec0: 6620 656e 636f 6465 725f 6c61 7965 7273  f encoder_layers
+0002eed0: 203e 2030 3a0a 2020 2020 2020 2020 2020   > 0:.          
+0002eee0: 2020 2020 2020 7365 6c66 2e65 6e63 6f64        self.encod
+0002eef0: 6572 203d 2054 7261 6e73 666f 726d 6572  er = Transformer
+0002ef00: 456e 636f 6465 7228 6e75 6d5f 6c61 7965  Encoder(num_laye
+0002ef10: 7273 3d65 6e63 6f64 6572 5f6c 6179 6572  rs=encoder_layer
+0002ef20: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+0002ef30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ef40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ef50: 2020 2020 2062 6174 6368 5f73 697a 653d       batch_size=
+0002ef60: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
+0002ef70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ef80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ef90: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+0002efa0: 6464 656e 5f73 697a 653d 6869 6464 656e  dden_size=hidden
+0002efb0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+0002efc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002efd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002efe0: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+0002eff0: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+0002f000: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0002f010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f030: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+0002f040: 6865 6164 733d 6e75 6d5f 6865 6164 732c  heads=num_heads,
+0002f050: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002f060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f080: 2020 2073 6571 5f6c 656e 6774 683d 7372     seq_length=sr
+0002f090: 635f 7365 715f 6c65 6e67 7468 2c0a 2020  c_seq_length,.  
+0002f0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f0d0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+0002f0e0: 745f 7261 7465 3d61 7474 656e 7469 6f6e  t_rate=attention
+0002f0f0: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
+0002f100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f120: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f130: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+0002f140: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
+0002f150: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+0002f160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f180: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+0002f190: 656e 5f61 6374 3d68 6964 6465 6e5f 6163  en_act=hidden_ac
+0002f1a0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+0002f1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f1d0: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
+0002f1e0: 6f6d 7075 7465 5f74 7970 653d 6c61 7965  ompute_type=laye
+0002f1f0: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
+0002f200: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+0002f210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f230: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
+0002f240: 6d70 7574 655f 7479 7065 3d73 6f66 746d  mpute_type=softm
+0002f250: 6178 5f63 6f6d 7075 7465 5f74 7970 652c  ax_compute_type,
+0002f260: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002f270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f290: 2020 2070 6f73 745f 6c61 7965 726e 6f72     post_layernor
+0002f2a0: 6d5f 7265 7369 6475 616c 3d70 6f73 745f  m_residual=post_
+0002f2b0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+0002f2c0: 616c 2c0a 2020 2020 2020 2020 2020 2020  al,.            
+0002f2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f2f0: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+0002f300: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+0002f310: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
+0002f320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f340: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
+0002f350: 6675 6e63 3d6c 616d 6264 615f 6675 6e63  func=lambda_func
+0002f360: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002f370: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002f380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f3a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f3b0: 2020 6d6f 655f 636f 6e66 6967 3d6d 6f65    moe_config=moe
-0002f3c0: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
-0002f3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f3e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f3f0: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0002f400: 6c65 6c5f 636f 6e66 6967 3d70 6172 616c  lel_config=paral
-0002f410: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
-0002f420: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-0002f430: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0002f440: 6c66 2e65 6e63 6f64 6572 203d 204e 6f6e  lf.encoder = Non
-0002f450: 650a 0a20 2020 2020 2020 2020 2020 2023  e..            #
-0002f460: 204f 6666 7365 7420 6973 206e 6565 6465   Offset is neede
-0002f470: 6420 6173 2074 6865 2065 6e63 6f64 6572  d as the encoder
-0002f480: 2068 6173 2063 6f6e 7375 6d65 6420 736f   has consumed so
-0002f490: 6d65 2066 6c61 6773 2e0a 2020 2020 2020  me flags..      
-0002f4a0: 2020 2020 2020 2320 736f 2074 6865 2064        # so the d
-0002f4b0: 6563 6f64 6572 206e 6565 6420 746f 2069  ecoder need to i
-0002f4c0: 6e63 7265 6173 6520 7468 6520 666c 6167  ncrease the flag
-0002f4d0: 7320 6261 7365 6420 6f6e 2074 6865 2065  s based on the e
-0002f4e0: 6e63 6f64 6572 206c 6179 6572 0a20 2020  ncoder layer.   
-0002f4f0: 2020 2020 2020 2020 2073 656c 662e 6465           self.de
-0002f500: 636f 6465 7220 3d20 4e6f 6e65 0a20 2020  coder = None.   
-0002f510: 2020 2020 2020 2020 2069 6620 6465 636f           if deco
-0002f520: 6465 725f 6c61 7965 7273 203e 2030 3a0a  der_layers > 0:.
-0002f530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f540: 7365 6c66 2e64 6563 6f64 6572 203d 2054  self.decoder = T
-0002f550: 7261 6e73 666f 726d 6572 4465 636f 6465  ransformerDecode
-0002f560: 7228 6e75 6d5f 6c61 7965 7273 3d64 6563  r(num_layers=dec
-0002f570: 6f64 6572 5f6c 6179 6572 732c 0a20 2020  oder_layers,.   
-0002f580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f5a0: 2020 2020 2020 2020 2020 2020 2020 2062                 b
-0002f5b0: 6174 6368 5f73 697a 653d 6261 7463 685f  atch_size=batch_
-0002f5c0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-0002f5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f5f0: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-0002f600: 697a 653d 6869 6464 656e 5f73 697a 652c  ize=hidden_size,
-0002f610: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002f620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f640: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-0002f650: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
-0002f660: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-0002f670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f690: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-0002f6a0: 6e75 6d5f 6865 6164 732c 0a20 2020 2020  num_heads,.     
-0002f6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f6c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f6d0: 2020 2020 2020 2020 2020 2020 2073 7263               src
-0002f6e0: 5f73 6571 5f6c 656e 6774 683d 7372 635f  _seq_length=src_
-0002f6f0: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
-0002f700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f720: 2020 2020 2020 2020 2020 2020 2020 7467                tg
-0002f730: 745f 7365 715f 6c65 6e67 7468 3d74 6774  t_seq_length=tgt
-0002f740: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
-0002f750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f770: 2020 2020 2020 2020 2020 2020 2020 2061                 a
-0002f780: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
-0002f790: 5f72 6174 653d 6174 7465 6e74 696f 6e5f  _rate=attention_
-0002f7a0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-0002f7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f7e0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-0002f7f0: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
-0002f800: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
-0002f810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f830: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0002f840: 6e5f 6163 743d 6869 6464 656e 5f61 6374  n_act=hidden_act
-0002f850: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002f860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f880: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
-0002f890: 726d 5f72 6573 6964 7561 6c3d 706f 7374  rm_residual=post
-0002f8a0: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0002f8b0: 7561 6c2c 0a20 2020 2020 2020 2020 2020  ual,.           
-0002f8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f8e0: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
-0002f8f0: 5f63 6f6d 7075 7465 5f74 7970 653d 6c61  _compute_type=la
-0002f900: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-0002f910: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-0002f920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f940: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
-0002f950: 636f 6d70 7574 655f 7479 7065 3d73 6f66  compute_type=sof
-0002f960: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
-0002f970: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002f980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f9a0: 2020 2020 206c 616d 6264 615f 6675 6e63       lambda_func
-0002f9b0: 3d6c 616d 6264 615f 6675 6e63 2c0a 2020  =lambda_func,.  
-0002f9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f9f0: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
-0002fa00: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0002fa10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fa20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fa30: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
-0002fa40: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
-0002fa50: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-0002fa60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fa70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fa80: 2020 2020 2020 2020 6f66 6673 6574 3d65          offset=e
-0002fa90: 6e63 6f64 6572 5f6c 6179 6572 732c 0a20  ncoder_layers,. 
-0002faa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fad0: 206d 6f65 5f63 6f6e 6669 673d 6d6f 655f   moe_config=moe_
-0002fae0: 636f 6e66 6967 2c0a 2020 2020 2020 2020  config,.        
-0002faf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fb10: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-0002fb20: 656c 5f63 6f6e 6669 673d 7061 7261 6c6c  el_config=parall
-0002fb30: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
-0002fb40: 2020 2065 6c69 6620 5f67 6574 5f70 6172     elif _get_par
-0002fb50: 616c 6c65 6c5f 6d6f 6465 2829 206e 6f74  allel_mode() not
-0002fb60: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
-0002fb70: 652e 4155 544f 5f50 4152 414c 4c45 4c2c  e.AUTO_PARALLEL,
-0002fb80: 293a 0a20 2020 2020 2020 2020 2020 205f  ):.            _
-0002fb90: 6368 6563 6b5f 636f 6e66 6967 2870 6172  check_config(par
-0002fba0: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-0002fbb0: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
-0002fbc0: 6174 6368 5f73 697a 6520 3d20 6261 7463  atch_size = batc
-0002fbd0: 685f 7369 7a65 0a20 2020 2020 2020 2020  h_size.         
-0002fbe0: 2020 2073 656c 662e 6869 6464 656e 5f73     self.hidden_s
-0002fbf0: 697a 6520 3d20 6869 6464 656e 5f73 697a  ize = hidden_siz
-0002fc00: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
-0002fc10: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
-0002fc20: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
-0002fc30: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
-0002fc40: 656c 662e 7467 745f 7365 715f 6c65 6e67  elf.tgt_seq_leng
-0002fc50: 7468 203d 2074 6774 5f73 6571 5f6c 656e  th = tgt_seq_len
-0002fc60: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
-0002fc70: 7365 6c66 2e75 7365 5f70 6173 7420 3d20  self.use_past = 
-0002fc80: 7573 655f 7061 7374 0a20 2020 2020 2020  use_past.       
-0002fc90: 2020 2020 2069 6620 656e 636f 6465 725f       if encoder_
-0002fca0: 6c61 7965 7273 203c 3d20 3020 3c20 6465  layers <= 0 < de
-0002fcb0: 636f 6465 725f 6c61 7965 7273 3a0a 2020  coder_layers:.  
-0002fcc0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-0002fcd0: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-0002fce0: 2254 7261 6e73 666f 726d 6572 2064 6f65  "Transformer doe
-0002fcf0: 7374 2073 7570 706f 7274 2065 6e63 6f64  st support encod
-0002fd00: 6572 206c 6179 6572 207b 656e 636f 6465  er layer {encode
-0002fd10: 725f 6c61 7965 7273 7d20 616e 6420 6465  r_layers} and de
-0002fd20: 636f 6465 7222 0a20 2020 2020 2020 2020  coder".         
-0002fd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fd40: 2020 2020 2020 2020 6622 6c61 7965 7220          f"layer 
-0002fd50: 7b64 6563 6f64 6572 5f6c 6179 6572 737d  {decoder_layers}
-0002fd60: 2c20 706c 6561 7365 2075 7365 2054 7261  , please use Tra
-0002fd70: 6e73 666f 726d 6572 4465 636f 6465 7222  nsformerDecoder"
-0002fd80: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
-0002fd90: 2065 6e63 6f64 6572 5f6c 6179 6572 7320   encoder_layers 
-0002fda0: 3e20 3020 616e 6420 6465 636f 6465 725f  > 0 and decoder_
-0002fdb0: 6c61 7965 7273 203e 2030 2061 6e64 2075  layers > 0 and u
-0002fdc0: 7365 5f70 6173 743a 0a20 2020 2020 2020  se_past:.       
-0002fdd0: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-0002fde0: 616c 7565 4572 726f 7228 6622 5468 6520  alueError(f"The 
-0002fdf0: 7b73 656c 662e 636c 735f 6e61 6d65 7d20  {self.cls_name} 
-0002fe00: 7769 7468 2065 6e63 6f64 6572 2061 6e64  with encoder and
-0002fe10: 2064 6563 6f64 6572 2064 6f65 7320 6e6f   decoder does no
-0002fe20: 7420 7375 7070 6f72 7420 7573 655f 7061  t support use_pa
-0002fe30: 7374 3d54 7275 652e 2229 0a20 2020 2020  st=True.").     
-0002fe40: 2020 2020 2020 206c 6f67 6765 722e 7761         logger.wa
-0002fe50: 726e 696e 6728 2246 6f72 2070 6172 616c  rning("For paral
-0002fe60: 6c65 6c20 6d6f 6465 2c20 7368 6172 6469  lel mode, shardi
-0002fe70: 6e67 2070 726f 7061 6761 7469 6f6e 2069  ng propagation i
-0002fe80: 7320 7265 636f 6d6d 656e 6465 642c 2079  s recommended, y
-0002fe90: 6f75 2063 616e 2075 7365 2069 7420 6279  ou can use it by
-0002fea0: 2073 6574 7469 6e67 2022 0a20 2020 2020   setting ".     
-0002feb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002fec0: 2020 2020 2020 2227 7365 745f 6175 746f        "'set_auto
-0002fed0: 5f70 6172 616c 6c65 6c5f 636f 6e74 6578  _parallel_contex
-0002fee0: 7428 7061 7261 6c6c 656c 5f6d 6f64 653d  t(parallel_mode=
-0002fef0: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
-0002ff00: 4f5f 5041 5241 4c4c 454c 2c20 220a 2020  O_PARALLEL, ".  
-0002ff10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ff20: 2020 2020 2020 2020 2022 7365 6172 6368           "search
-0002ff30: 5f6d 6f64 653d 5c22 7368 6172 6469 6e67  _mode=\"sharding
-0002ff40: 5f70 726f 7061 6761 7469 6f6e 5c22 2927  _propagation\")'
-0002ff50: 2061 6e64 2022 0a20 2020 2020 2020 2020   and ".         
-0002ff60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ff70: 2020 2227 7365 745f 616c 676f 5f70 6172    "'set_algo_par
-0002ff80: 616d 6574 6572 7328 656c 656d 656e 7477  ameters(elementw
-0002ff90: 6973 655f 6f70 5f73 7472 6174 6567 795f  ise_op_strategy_
-0002ffa0: 666f 6c6c 6f77 3d46 616c 7365 2c20 6675  follow=False, fu
-0002ffb0: 6c6c 795f 7573 655f 6465 7669 6365 733d  lly_use_devices=
-0002ffc0: 4661 6c73 6529 2722 290a 2020 2020 2020  False)'").      
-0002ffd0: 2020 2020 2020 2320 5468 6520 7368 6172        # The shar
-0002ffe0: 6420 7365 7474 696e 6720 6f66 2054 7261  d setting of Tra
-0002fff0: 6e73 666f 726d 6572 2069 7320 7365 7420  nsformer is set 
-00030000: 7769 7468 696e 2074 6865 2054 7261 6e73  within the Trans
-00030010: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
-00030020: 6572 0a20 2020 2020 2020 2020 2020 2069  er.            i
-00030030: 6620 6e6f 7420 6c61 6d62 6461 5f66 756e  f not lambda_fun
-00030040: 633a 0a20 2020 2020 2020 2020 2020 2020  c:.             
-00030050: 2020 206c 616d 6264 615f 6675 6e63 203d     lambda_func =
-00030060: 205f 6765 745f 6c61 6d62 6461 5f66 756e   _get_lambda_fun
-00030070: 6328 746f 7461 6c5f 6c61 7965 723d 656e  c(total_layer=en
-00030080: 636f 6465 725f 6c61 7965 7273 202b 2064  coder_layers + d
-00030090: 6563 6f64 6572 5f6c 6179 6572 7329 0a20  ecoder_layers). 
-000300a0: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
-000300b0: 6b5f 6d6f 655f 636f 6e66 6967 286d 6f65  k_moe_config(moe
-000300c0: 5f63 6f6e 6669 672c 2070 6172 616c 6c65  _config, paralle
-000300d0: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
-000300e0: 2020 2020 2020 7365 6c66 2e75 7365 5f6d        self.use_m
-000300f0: 6f65 203d 2028 6d6f 655f 636f 6e66 6967  oe = (moe_config
-00030100: 2e65 7870 6572 745f 6e75 6d20 3e20 3129  .expert_num > 1)
-00030110: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00030120: 662e 6164 6420 3d20 502e 4164 6428 292e  f.add = P.Add().
-00030130: 7368 6172 6428 2828 292c 2028 2929 290a  shard(((), ())).
-00030140: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00030150: 2e61 7578 5f6c 6f73 7320 3d20 5465 6e73  .aux_loss = Tens
-00030160: 6f72 2830 2e30 2c20 6d73 7479 7065 2e66  or(0.0, mstype.f
-00030170: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
-00030180: 2020 2020 6966 2065 6e63 6f64 6572 5f6c      if encoder_l
-00030190: 6179 6572 7320 3e20 303a 0a20 2020 2020  ayers > 0:.     
-000301a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000301b0: 656e 636f 6465 7220 3d20 5472 616e 7366  encoder = Transf
-000301c0: 6f72 6d65 7245 6e63 6f64 6572 286e 756d  ormerEncoder(num
-000301d0: 5f6c 6179 6572 733d 656e 636f 6465 725f  _layers=encoder_
-000301e0: 6c61 7965 7273 2c0a 2020 2020 2020 2020  layers,.        
-000301f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030210: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
-00030220: 7369 7a65 3d62 6174 6368 5f73 697a 652c  size=batch_size,
-00030230: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00030240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030260: 2020 2068 6964 6465 6e5f 7369 7a65 3d68     hidden_size=h
-00030270: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00030280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000302a0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000302b0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-000302c0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-000302d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000302e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000302f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030300: 206e 756d 5f68 6561 6473 3d6e 756d 5f68   num_heads=num_h
-00030310: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
-00030320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030340: 2020 2020 2020 2020 7365 715f 6c65 6e67          seq_leng
-00030350: 7468 3d73 7263 5f73 6571 5f6c 656e 6774  th=src_seq_lengt
-00030360: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
-00030370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030390: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-000303a0: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
-000303b0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-000303c0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-000303d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000303e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000303f0: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
-00030400: 706f 7574 5f72 6174 653d 6869 6464 656e  pout_rate=hidden
-00030410: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
-00030420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030450: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-00030460: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-00030470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030490: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
-000304a0: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-000304b0: 3d6c 6179 6572 6e6f 726d 5f63 6f6d 7075  =layernorm_compu
-000304c0: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
-000304d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000304e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000304f0: 2020 2020 2020 2020 2020 2073 6f66 746d             softm
-00030500: 6178 5f63 6f6d 7075 7465 5f74 7970 653d  ax_compute_type=
-00030510: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
-00030520: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-00030530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030550: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-00030560: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-00030570: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-00030580: 6573 6964 7561 6c2c 0a20 2020 2020 2020  esidual,.       
-00030590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000305a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000305b0: 2020 2020 2020 2020 2020 2070 6172 616d             param
-000305c0: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
-000305d0: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
-000305e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000305f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030600: 2020 2020 2020 2020 2020 2020 2020 6c61                la
-00030610: 6d62 6461 5f66 756e 633d 6c61 6d62 6461  mbda_func=lambda
-00030620: 5f66 756e 632c 0a20 2020 2020 2020 2020  _func,.         
-00030630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030650: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-00030660: 743d 7573 655f 7061 7374 2c0a 2020 2020  t=use_past,.    
+0002f390: 2020 2020 7573 655f 7061 7374 3d75 7365      use_past=use
+0002f3a0: 5f70 6173 742c 0a20 2020 2020 2020 2020  _past,.         
+0002f3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f3d0: 2020 2020 2020 2020 206d 6f65 5f63 6f6e           moe_con
+0002f3e0: 6669 673d 6d6f 655f 636f 6e66 6967 2c0a  fig=moe_config,.
+0002f3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f410: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f420: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0002f430: 673d 7061 7261 6c6c 656c 5f63 6f6e 6669  g=parallel_confi
+0002f440: 6729 0a20 2020 2020 2020 2020 2020 2065  g).            e
+0002f450: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+0002f460: 2020 2020 2073 656c 662e 656e 636f 6465       self.encode
+0002f470: 7220 3d20 4e6f 6e65 0a0a 2020 2020 2020  r = None..      
+0002f480: 2020 2020 2020 2320 4f66 6673 6574 2069        # Offset i
+0002f490: 7320 6e65 6564 6564 2061 7320 7468 6520  s needed as the 
+0002f4a0: 656e 636f 6465 7220 6861 7320 636f 6e73  encoder has cons
+0002f4b0: 756d 6564 2073 6f6d 6520 666c 6167 732e  umed some flags.
+0002f4c0: 0a20 2020 2020 2020 2020 2020 2023 2073  .            # s
+0002f4d0: 6f20 7468 6520 6465 636f 6465 7220 6e65  o the decoder ne
+0002f4e0: 6564 2074 6f20 696e 6372 6561 7365 2074  ed to increase t
+0002f4f0: 6865 2066 6c61 6773 2062 6173 6564 206f  he flags based o
+0002f500: 6e20 7468 6520 656e 636f 6465 7220 6c61  n the encoder la
+0002f510: 7965 720a 2020 2020 2020 2020 2020 2020  yer.            
+0002f520: 7365 6c66 2e64 6563 6f64 6572 203d 204e  self.decoder = N
+0002f530: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
+0002f540: 6966 2064 6563 6f64 6572 5f6c 6179 6572  if decoder_layer
+0002f550: 7320 3e20 303a 0a20 2020 2020 2020 2020  s > 0:.         
+0002f560: 2020 2020 2020 2073 656c 662e 6465 636f         self.deco
+0002f570: 6465 7220 3d20 5472 616e 7366 6f72 6d65  der = Transforme
+0002f580: 7244 6563 6f64 6572 286e 756d 5f6c 6179  rDecoder(num_lay
+0002f590: 6572 733d 6465 636f 6465 725f 6c61 7965  ers=decoder_laye
+0002f5a0: 7273 2c0a 2020 2020 2020 2020 2020 2020  rs,.            
+0002f5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f5d0: 2020 2020 2020 6261 7463 685f 7369 7a65        batch_size
+0002f5e0: 3d62 6174 6368 5f73 697a 652c 0a20 2020  =batch_size,.   
+0002f5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f610: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0002f620: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
+0002f630: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+0002f640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f660: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+0002f670: 6464 656e 5f73 697a 653d 6666 6e5f 6869  dden_size=ffn_hi
+0002f680: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0002f690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f6b0: 2020 2020 2020 2020 2020 2020 206e 756d               num
+0002f6c0: 5f68 6561 6473 3d6e 756d 5f68 6561 6473  _heads=num_heads
+0002f6d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002f6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f700: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
+0002f710: 7468 3d73 7263 5f73 6571 5f6c 656e 6774  th=src_seq_lengt
+0002f720: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
+0002f730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f750: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
+0002f760: 6774 683d 7467 745f 7365 715f 6c65 6e67  gth=tgt_seq_leng
+0002f770: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
+0002f780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f7a0: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
+0002f7b0: 6472 6f70 6f75 745f 7261 7465 3d61 7474  dropout_rate=att
+0002f7c0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0002f7d0: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
+0002f7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f800: 2020 2020 2020 2068 6964 6465 6e5f 6472         hidden_dr
+0002f810: 6f70 6f75 745f 7261 7465 3d68 6964 6465  opout_rate=hidde
+0002f820: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+0002f830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f860: 2020 6869 6464 656e 5f61 6374 3d68 6964    hidden_act=hid
+0002f870: 6465 6e5f 6163 742c 0a20 2020 2020 2020  den_act,.       
+0002f880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f8a0: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
+0002f8b0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+0002f8c0: 616c 3d70 6f73 745f 6c61 7965 726e 6f72  al=post_layernor
+0002f8d0: 6d5f 7265 7369 6475 616c 2c0a 2020 2020  m_residual,.    
+0002f8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f900: 2020 2020 2020 2020 2020 2020 2020 6c61                la
+0002f910: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
+0002f920: 7479 7065 3d6c 6179 6572 6e6f 726d 5f63  type=layernorm_c
+0002f930: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
+0002f940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f960: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0002f970: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+0002f980: 7970 653d 736f 6674 6d61 785f 636f 6d70  ype=softmax_comp
+0002f990: 7574 655f 7479 7065 2c0a 2020 2020 2020  ute_type,.      
+0002f9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f9c0: 2020 2020 2020 2020 2020 2020 6c61 6d62              lamb
+0002f9d0: 6461 5f66 756e 633d 6c61 6d62 6461 5f66  da_func=lambda_f
+0002f9e0: 756e 632c 0a20 2020 2020 2020 2020 2020  unc,.           
+0002f9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fa00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fa10: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+0002fa20: 7573 655f 7061 7374 2c0a 2020 2020 2020  use_past,.      
+0002fa30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fa40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fa50: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0002fa60: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
+0002fa70: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
+0002fa80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fa90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002faa0: 2020 2020 2020 2020 2020 2020 2020 206f                 o
+0002fab0: 6666 7365 743d 656e 636f 6465 725f 6c61  ffset=encoder_la
+0002fac0: 7965 7273 2c0a 2020 2020 2020 2020 2020  yers,.          
+0002fad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002faf0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+0002fb00: 6967 3d6d 6f65 5f63 6f6e 6669 672c 0a20  ig=moe_config,. 
+0002fb10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fb20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fb40: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0002fb50: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
+0002fb60: 290a 2020 2020 2020 2020 656c 6966 205f  ).        elif _
+0002fb70: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
+0002fb80: 6528 2920 6e6f 7420 696e 2028 5061 7261  e() not in (Para
+0002fb90: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
+0002fba0: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
+0002fbb0: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
+0002fbc0: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
+0002fbd0: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
+0002fbe0: 2073 656c 662e 6261 7463 685f 7369 7a65   self.batch_size
+0002fbf0: 203d 2062 6174 6368 5f73 697a 650a 2020   = batch_size.  
+0002fc00: 2020 2020 2020 2020 2020 7365 6c66 2e68            self.h
+0002fc10: 6964 6465 6e5f 7369 7a65 203d 2068 6964  idden_size = hid
+0002fc20: 6465 6e5f 7369 7a65 0a20 2020 2020 2020  den_size.       
+0002fc30: 2020 2020 2073 656c 662e 7372 635f 7365       self.src_se
+0002fc40: 715f 6c65 6e67 7468 203d 2073 7263 5f73  q_length = src_s
+0002fc50: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
+0002fc60: 2020 2020 2020 7365 6c66 2e74 6774 5f73        self.tgt_s
+0002fc70: 6571 5f6c 656e 6774 6820 3d20 7467 745f  eq_length = tgt_
+0002fc80: 7365 715f 6c65 6e67 7468 0a20 2020 2020  seq_length.     
+0002fc90: 2020 2020 2020 2073 656c 662e 7573 655f         self.use_
+0002fca0: 7061 7374 203d 2075 7365 5f70 6173 740a  past = use_past.
+0002fcb0: 2020 2020 2020 2020 2020 2020 6966 2065              if e
+0002fcc0: 6e63 6f64 6572 5f6c 6179 6572 7320 3c3d  ncoder_layers <=
+0002fcd0: 2030 203c 2064 6563 6f64 6572 5f6c 6179   0 < decoder_lay
+0002fce0: 6572 733a 0a20 2020 2020 2020 2020 2020  ers:.           
+0002fcf0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0002fd00: 4572 726f 7228 6622 5472 616e 7366 6f72  Error(f"Transfor
+0002fd10: 6d65 7220 646f 6573 7420 7375 7070 6f72  mer doest suppor
+0002fd20: 7420 656e 636f 6465 7220 6c61 7965 7220  t encoder layer 
+0002fd30: 7b65 6e63 6f64 6572 5f6c 6179 6572 737d  {encoder_layers}
+0002fd40: 2061 6e64 2064 6563 6f64 6572 220a 2020   and decoder".  
+0002fd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fd60: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+0002fd70: 226c 6179 6572 207b 6465 636f 6465 725f  "layer {decoder_
+0002fd80: 6c61 7965 7273 7d2c 2070 6c65 6173 6520  layers}, please 
+0002fd90: 7573 6520 5472 616e 7366 6f72 6d65 7244  use TransformerD
+0002fda0: 6563 6f64 6572 2229 0a20 2020 2020 2020  ecoder").       
+0002fdb0: 2020 2020 2069 6620 656e 636f 6465 725f       if encoder_
+0002fdc0: 6c61 7965 7273 203e 2030 2061 6e64 2064  layers > 0 and d
+0002fdd0: 6563 6f64 6572 5f6c 6179 6572 7320 3e20  ecoder_layers > 
+0002fde0: 3020 616e 6420 7573 655f 7061 7374 3a0a  0 and use_past:.
+0002fdf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002fe00: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0002fe10: 2866 2254 6865 207b 7365 6c66 2e63 6c73  (f"The {self.cls
+0002fe20: 5f6e 616d 657d 2077 6974 6820 656e 636f  _name} with enco
+0002fe30: 6465 7220 616e 6420 6465 636f 6465 7220  der and decoder 
+0002fe40: 646f 6573 206e 6f74 2073 7570 706f 7274  does not support
+0002fe50: 2075 7365 5f70 6173 743d 5472 7565 2e22   use_past=True."
+0002fe60: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
+0002fe70: 6767 6572 2e77 6172 6e69 6e67 2822 466f  gger.warning("Fo
+0002fe80: 7220 7061 7261 6c6c 656c 206d 6f64 652c  r parallel mode,
+0002fe90: 2073 6861 7264 696e 6720 7072 6f70 6167   sharding propag
+0002fea0: 6174 696f 6e20 6973 2072 6563 6f6d 6d65  ation is recomme
+0002feb0: 6e64 6564 2c20 796f 7520 6361 6e20 7573  nded, you can us
+0002fec0: 6520 6974 2062 7920 7365 7474 696e 6720  e it by setting 
+0002fed0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0002fee0: 2020 2020 2020 2020 2020 2020 2022 2773               "'s
+0002fef0: 6574 5f61 7574 6f5f 7061 7261 6c6c 656c  et_auto_parallel
+0002ff00: 5f63 6f6e 7465 7874 2870 6172 616c 6c65  _context(paralle
+0002ff10: 6c5f 6d6f 6465 3d50 6172 616c 6c65 6c4d  l_mode=ParallelM
+0002ff20: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
+0002ff30: 4c2c 2022 0a20 2020 2020 2020 2020 2020  L, ".           
+0002ff40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ff50: 2273 6561 7263 685f 6d6f 6465 3d5c 2273  "search_mode=\"s
+0002ff60: 6861 7264 696e 675f 7072 6f70 6167 6174  harding_propagat
+0002ff70: 696f 6e5c 2229 2720 616e 6420 220a 2020  ion\")' and ".  
+0002ff80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ff90: 2020 2020 2020 2020 2022 2773 6574 5f61           "'set_a
+0002ffa0: 6c67 6f5f 7061 7261 6d65 7465 7273 2865  lgo_parameters(e
+0002ffb0: 6c65 6d65 6e74 7769 7365 5f6f 705f 7374  lementwise_op_st
+0002ffc0: 7261 7465 6779 5f66 6f6c 6c6f 773d 4661  rategy_follow=Fa
+0002ffd0: 6c73 652c 2066 756c 6c79 5f75 7365 5f64  lse, fully_use_d
+0002ffe0: 6576 6963 6573 3d46 616c 7365 2927 2229  evices=False)'")
+0002fff0: 0a20 2020 2020 2020 2020 2020 2023 2054  .            # T
+00030000: 6865 2073 6861 7264 2073 6574 7469 6e67  he shard setting
+00030010: 206f 6620 5472 616e 7366 6f72 6d65 7220   of Transformer 
+00030020: 6973 2073 6574 2077 6974 6869 6e20 7468  is set within th
+00030030: 6520 5472 616e 7366 6f72 6d65 7245 6e63  e TransformerEnc
+00030040: 6f64 6572 4c61 7965 720a 2020 2020 2020  oderLayer.      
+00030050: 2020 2020 2020 6966 206e 6f74 206c 616d        if not lam
+00030060: 6264 615f 6675 6e63 3a0a 2020 2020 2020  bda_func:.      
+00030070: 2020 2020 2020 2020 2020 6c61 6d62 6461            lambda
+00030080: 5f66 756e 6320 3d20 5f67 6574 5f6c 616d  _func = _get_lam
+00030090: 6264 615f 6675 6e63 2874 6f74 616c 5f6c  bda_func(total_l
+000300a0: 6179 6572 3d65 6e63 6f64 6572 5f6c 6179  ayer=encoder_lay
+000300b0: 6572 7320 2b20 6465 636f 6465 725f 6c61  ers + decoder_la
+000300c0: 7965 7273 290a 2020 2020 2020 2020 2020  yers).          
+000300d0: 2020 5f63 6865 636b 5f6d 6f65 5f63 6f6e    _check_moe_con
+000300e0: 6669 6728 6d6f 655f 636f 6e66 6967 2c20  fig(moe_config, 
+000300f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+00030100: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00030110: 662e 7573 655f 6d6f 6520 3d20 286d 6f65  f.use_moe = (moe
+00030120: 5f63 6f6e 6669 672e 6578 7065 7274 5f6e  _config.expert_n
+00030130: 756d 203e 2031 290a 2020 2020 2020 2020  um > 1).        
+00030140: 2020 2020 7365 6c66 2e61 6464 203d 2050      self.add = P
+00030150: 2e41 6464 2829 2e73 6861 7264 2828 2829  .Add().shard((()
+00030160: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
+00030170: 2020 2073 656c 662e 6175 785f 6c6f 7373     self.aux_loss
+00030180: 203d 2054 656e 736f 7228 302e 302c 206d   = Tensor(0.0, m
+00030190: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
+000301a0: 2020 2020 2020 2020 2020 2069 6620 656e             if en
+000301b0: 636f 6465 725f 6c61 7965 7273 203e 2030  coder_layers > 0
+000301c0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000301d0: 2020 7365 6c66 2e65 6e63 6f64 6572 203d    self.encoder =
+000301e0: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
+000301f0: 6465 7228 6e75 6d5f 6c61 7965 7273 3d65  der(num_layers=e
+00030200: 6e63 6f64 6572 5f6c 6179 6572 732c 0a20  ncoder_layers,. 
+00030210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030240: 2062 6174 6368 5f73 697a 653d 6261 7463   batch_size=batc
+00030250: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
+00030260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030280: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00030290: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+000302a0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000302b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000302c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000302d0: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+000302e0: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+000302f0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00030300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030320: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+00030330: 733d 6e75 6d5f 6865 6164 732c 0a20 2020  s=num_heads,.   
+00030340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030360: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00030370: 6571 5f6c 656e 6774 683d 7372 635f 7365  eq_length=src_se
+00030380: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
+00030390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000303a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000303b0: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+000303c0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+000303d0: 7465 3d61 7474 656e 7469 6f6e 5f64 726f  te=attention_dro
+000303e0: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+000303f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030410: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00030420: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+00030430: 3d68 6964 6465 6e5f 6472 6f70 6f75 745f  =hidden_dropout_
+00030440: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
+00030450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030470: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+00030480: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+00030490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000304a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000304b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000304c0: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
+000304d0: 7465 5f74 7970 653d 6c61 7965 726e 6f72  te_type=layernor
+000304e0: 6d5f 636f 6d70 7574 655f 7479 7065 2c0a  m_compute_type,.
+000304f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030520: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
+00030530: 655f 7479 7065 3d73 6f66 746d 6178 5f63  e_type=softmax_c
+00030540: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
+00030550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030570: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00030580: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+00030590: 7369 6475 616c 3d70 6f73 745f 6c61 7965  sidual=post_laye
+000305a0: 726e 6f72 6d5f 7265 7369 6475 616c 2c0a  rnorm_residual,.
+000305b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000305c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000305d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000305e0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+000305f0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
+00030600: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00030610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030620: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030630: 2020 2020 206c 616d 6264 615f 6675 6e63       lambda_func
+00030640: 3d6c 616d 6264 615f 6675 6e63 2c0a 2020  =lambda_func,.  
+00030650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030660: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00030670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030690: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-000306a0: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
-000306b0: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
-000306c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000306d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000306e0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-000306f0: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
-00030700: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
-00030710: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-00030720: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00030730: 6e63 6f64 6572 203d 204e 6f6e 650a 0a20  ncoder = None.. 
-00030740: 2020 2020 2020 2020 2020 2023 204f 6666             # Off
-00030750: 7365 7420 6973 206e 6565 6465 6420 6173  set is needed as
-00030760: 2074 6865 2065 6e63 6f64 6572 2068 6173   the encoder has
-00030770: 2063 6f6e 7375 6d65 6420 736f 6d65 2066   consumed some f
-00030780: 6c61 6773 2e0a 2020 2020 2020 2020 2020  lags..          
-00030790: 2020 2320 736f 2074 6865 2064 6563 6f64    # so the decod
-000307a0: 6572 206e 6565 6420 746f 2069 6e63 7265  er need to incre
-000307b0: 6173 6520 7468 6520 666c 6167 7320 6261  ase the flags ba
-000307c0: 7365 6420 6f6e 2074 6865 2065 6e63 6f64  sed on the encod
-000307d0: 6572 206c 6179 6572 0a20 2020 2020 2020  er layer.       
-000307e0: 2020 2020 2073 656c 662e 6465 636f 6465       self.decode
-000307f0: 7220 3d20 4e6f 6e65 0a20 2020 2020 2020  r = None.       
-00030800: 2020 2020 2069 6620 6465 636f 6465 725f       if decoder_
-00030810: 6c61 7965 7273 203e 2030 3a0a 2020 2020  layers > 0:.    
-00030820: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00030830: 2e64 6563 6f64 6572 203d 2054 7261 6e73  .decoder = Trans
-00030840: 666f 726d 6572 4465 636f 6465 7228 6e75  formerDecoder(nu
-00030850: 6d5f 6c61 7965 7273 3d64 6563 6f64 6572  m_layers=decoder
-00030860: 5f6c 6179 6572 732c 0a20 2020 2020 2020  _layers,.       
-00030870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030890: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-000308a0: 5f73 697a 653d 6261 7463 685f 7369 7a65  _size=batch_size
-000308b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000308c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000308d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000308e0: 2020 2020 6869 6464 656e 5f73 697a 653d      hidden_size=
-000308f0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-00030900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030920: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-00030930: 666e 5f68 6964 6465 6e5f 7369 7a65 3d66  fn_hidden_size=f
-00030940: 666e 5f68 6964 6465 6e5f 7369 7a65 2c0a  fn_hidden_size,.
-00030950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030980: 2020 6e75 6d5f 6865 6164 733d 6e75 6d5f    num_heads=num_
-00030990: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
-000309a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000309b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000309c0: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-000309d0: 5f6c 656e 6774 683d 7372 635f 7365 715f  _length=src_seq_
-000309e0: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
-000309f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a10: 2020 2020 2020 2020 2020 7467 745f 7365            tgt_se
-00030a20: 715f 6c65 6e67 7468 3d74 6774 5f73 6571  q_length=tgt_seq
-00030a30: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-00030a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a60: 2020 2020 2020 2020 2020 2061 7474 656e             atten
-00030a70: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00030a80: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
-00030a90: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-00030aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ac0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-00030ad0: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
-00030ae0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-00030af0: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
-00030b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b20: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
-00030b30: 743d 6869 6464 656e 5f61 6374 2c0a 2020  t=hidden_act,.  
-00030b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b70: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-00030b80: 6573 6964 7561 6c3d 706f 7374 5f6c 6179  esidual=post_lay
-00030b90: 6572 6e6f 726d 5f72 6573 6964 7561 6c2c  ernorm_residual,
-00030ba0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00030bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030bd0: 2020 206c 6179 6572 6e6f 726d 5f63 6f6d     layernorm_com
-00030be0: 7075 7465 5f74 7970 653d 6c61 7965 726e  pute_type=layern
-00030bf0: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-00030c00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00030c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c30: 2020 2020 736f 6674 6d61 785f 636f 6d70      softmax_comp
-00030c40: 7574 655f 7479 7065 3d73 6f66 746d 6178  ute_type=softmax
-00030c50: 5f63 6f6d 7075 7465 5f74 7970 652c 0a20  _compute_type,. 
-00030c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c90: 206c 616d 6264 615f 6675 6e63 3d6c 616d   lambda_func=lam
-00030ca0: 6264 615f 6675 6e63 2c0a 2020 2020 2020  bda_func,.      
-00030cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030cd0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
-00030ce0: 7061 7374 3d75 7365 5f70 6173 742c 0a20  past=use_past,. 
+00030680: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
+00030690: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000306a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000306b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000306c0: 2020 2020 206d 6f65 5f63 6f6e 6669 673d       moe_config=
+000306d0: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
+000306e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000306f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030700: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00030710: 7261 6c6c 656c 5f63 6f6e 6669 673d 7061  rallel_config=pa
+00030720: 7261 6c6c 656c 5f63 6f6e 6669 6729 0a20  rallel_config). 
+00030730: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00030740: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00030750: 2073 656c 662e 656e 636f 6465 7220 3d20   self.encoder = 
+00030760: 4e6f 6e65 0a0a 2020 2020 2020 2020 2020  None..          
+00030770: 2020 2320 4f66 6673 6574 2069 7320 6e65    # Offset is ne
+00030780: 6564 6564 2061 7320 7468 6520 656e 636f  eded as the enco
+00030790: 6465 7220 6861 7320 636f 6e73 756d 6564  der has consumed
+000307a0: 2073 6f6d 6520 666c 6167 732e 0a20 2020   some flags..   
+000307b0: 2020 2020 2020 2020 2023 2073 6f20 7468           # so th
+000307c0: 6520 6465 636f 6465 7220 6e65 6564 2074  e decoder need t
+000307d0: 6f20 696e 6372 6561 7365 2074 6865 2066  o increase the f
+000307e0: 6c61 6773 2062 6173 6564 206f 6e20 7468  lags based on th
+000307f0: 6520 656e 636f 6465 7220 6c61 7965 720a  e encoder layer.
+00030800: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00030810: 2e64 6563 6f64 6572 203d 204e 6f6e 650a  .decoder = None.
+00030820: 2020 2020 2020 2020 2020 2020 6966 2064              if d
+00030830: 6563 6f64 6572 5f6c 6179 6572 7320 3e20  ecoder_layers > 
+00030840: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
+00030850: 2020 2073 656c 662e 6465 636f 6465 7220     self.decoder 
+00030860: 3d20 5472 616e 7366 6f72 6d65 7244 6563  = TransformerDec
+00030870: 6f64 6572 286e 756d 5f6c 6179 6572 733d  oder(num_layers=
+00030880: 6465 636f 6465 725f 6c61 7965 7273 2c0a  decoder_layers,.
+00030890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000308a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000308b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000308c0: 2020 6261 7463 685f 7369 7a65 3d62 6174    batch_size=bat
+000308d0: 6368 5f73 697a 652c 0a20 2020 2020 2020  ch_size,.       
+000308e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000308f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030900: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+00030910: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+00030920: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00030930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030950: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
+00030960: 5f73 697a 653d 6666 6e5f 6869 6464 656e  _size=ffn_hidden
+00030970: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+00030980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000309a0: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+000309b0: 6473 3d6e 756d 5f68 6561 6473 2c0a 2020  ds=num_heads,.  
+000309c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000309d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000309e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000309f0: 7372 635f 7365 715f 6c65 6e67 7468 3d73  src_seq_length=s
+00030a00: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
+00030a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a40: 2074 6774 5f73 6571 5f6c 656e 6774 683d   tgt_seq_length=
+00030a50: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
+00030a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030a90: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+00030aa0: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
+00030ab0: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
+00030ac0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00030ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030af0: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
+00030b00: 745f 7261 7465 3d68 6964 6465 6e5f 6472  t_rate=hidden_dr
+00030b10: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
+00030b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030b40: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+00030b50: 6464 656e 5f61 6374 3d68 6964 6465 6e5f  dden_act=hidden_
+00030b60: 6163 742c 0a20 2020 2020 2020 2020 2020  act,.           
+00030b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030b90: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
+00030ba0: 726e 6f72 6d5f 7265 7369 6475 616c 3d70  rnorm_residual=p
+00030bb0: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+00030bc0: 7369 6475 616c 2c0a 2020 2020 2020 2020  sidual,.        
+00030bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030bf0: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
+00030c00: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
+00030c10: 3d6c 6179 6572 6e6f 726d 5f63 6f6d 7075  =layernorm_compu
+00030c20: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+00030c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030c50: 2020 2020 2020 2020 2020 2073 6f66 746d             softm
+00030c60: 6178 5f63 6f6d 7075 7465 5f74 7970 653d  ax_compute_type=
+00030c70: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+00030c80: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+00030c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030cb0: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+00030cc0: 756e 633d 6c61 6d62 6461 5f66 756e 632c  unc=lambda_func,
+00030cd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00030ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00030cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d20: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00030d30: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00030d40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00030d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d70: 2020 2020 6f66 6673 6574 3d65 6e63 6f64      offset=encod
-00030d80: 6572 5f6c 6179 6572 732c 0a20 2020 2020  er_layers,.     
-00030d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030db0: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-00030dc0: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-00030dd0: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-00030de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030e00: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-00030e10: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
-00030e20: 6f6e 6669 6729 0a20 2020 2020 2020 2065  onfig).        e
-00030e30: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-00030e40: 2072 6169 7365 2052 756e 7469 6d65 4572   raise RuntimeEr
-00030e50: 726f 7228 6622 5468 6520 7b73 656c 662e  ror(f"The {self.
-00030e60: 636c 735f 6e61 6d65 7d20 6f6e 6c79 2073  cls_name} only s
-00030e70: 7570 706f 7274 2073 6861 7264 696e 6720  upport sharding 
-00030e80: 7072 6f70 6167 6174 696f 6e20 6f72 2022  propagation or "
-00030e90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00030ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030eb0: 6622 7365 6d69 2d61 7574 6f20 7061 7261  f"semi-auto para
-00030ec0: 6c6c 656c 206d 6f64 6520 6e6f 772e 2229  llel mode now.")
-00030ed0: 0a0a 2020 2020 6465 6620 636f 6e73 7472  ..    def constr
-00030ee0: 7563 7428 7365 6c66 2c20 656e 636f 6465  uct(self, encode
-00030ef0: 725f 696e 7075 7473 2c0a 2020 2020 2020  r_inputs,.      
-00030f00: 2020 2020 2020 2020 2020 2020 656e 636f              enco
-00030f10: 6465 725f 6d61 736b 732c 0a20 2020 2020  der_masks,.     
-00030f20: 2020 2020 2020 2020 2020 2020 2064 6563               dec
-00030f30: 6f64 6572 5f69 6e70 7574 733d 4e6f 6e65  oder_inputs=None
+00030d00: 2020 2075 7365 5f70 6173 743d 7573 655f     use_past=use_
+00030d10: 7061 7374 2c0a 2020 2020 2020 2020 2020  past,.          
+00030d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030d40: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+00030d50: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
+00030d60: 6974 5f74 7970 652c 0a20 2020 2020 2020  it_type,.       
+00030d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030d90: 2020 2020 2020 2020 2020 206f 6666 7365             offse
+00030da0: 743d 656e 636f 6465 725f 6c61 7965 7273  t=encoder_layers
+00030db0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00030dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030de0: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+00030df0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+00030e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030e20: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00030e30: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
+00030e40: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+00030e50: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+00030e60: 2020 2020 2020 2020 7261 6973 6520 5275          raise Ru
+00030e70: 6e74 696d 6545 7272 6f72 2866 2254 6865  ntimeError(f"The
+00030e80: 207b 7365 6c66 2e63 6c73 5f6e 616d 657d   {self.cls_name}
+00030e90: 206f 6e6c 7920 7375 7070 6f72 7420 7368   only support sh
+00030ea0: 6172 6469 6e67 2070 726f 7061 6761 7469  arding propagati
+00030eb0: 6f6e 206f 7220 220a 2020 2020 2020 2020  on or ".        
+00030ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030ed0: 2020 2020 2020 2066 2273 656d 692d 6175         f"semi-au
+00030ee0: 746f 2070 6172 616c 6c65 6c20 6d6f 6465  to parallel mode
+00030ef0: 206e 6f77 2e22 290a 0a20 2020 2064 6566   now.")..    def
+00030f00: 2063 6f6e 7374 7275 6374 2873 656c 662c   construct(self,
+00030f10: 2065 6e63 6f64 6572 5f69 6e70 7574 732c   encoder_inputs,
+00030f20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00030f30: 2020 2065 6e63 6f64 6572 5f6d 6173 6b73     encoder_masks
 00030f40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00030f50: 2020 2020 6465 636f 6465 725f 6d61 736b      decoder_mask
-00030f60: 733d 4e6f 6e65 2c0a 2020 2020 2020 2020  s=None,.        
-00030f70: 2020 2020 2020 2020 2020 6d65 6d6f 7279            memory
-00030f80: 5f6d 6173 6b3d 4e6f 6e65 2c0a 2020 2020  _mask=None,.    
-00030f90: 2020 2020 2020 2020 2020 2020 2020 696e                in
-00030fa0: 6974 5f72 6573 6574 3d54 7275 652c 0a20  it_reset=True,. 
-00030fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030fc0: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-00030fd0: 6774 683d 4e6f 6e65 293a 0a20 2020 2020  gth=None):.     
-00030fe0: 2020 2022 2222 7072 6f63 6573 7320 7072     """process pr
-00030ff0: 6f63 6573 7322 2222 0a20 2020 2020 2020  ocess""".       
-00031000: 2065 6e63 6f64 6572 5f6f 7574 7075 7420   encoder_output 
-00031010: 3d20 4e6f 6e65 0a20 2020 2020 2020 206f  = None.        o
-00031020: 7574 7075 7420 3d20 4e6f 6e65 0a20 2020  utput = None.   
-00031030: 2020 2020 2065 6e63 6f64 6572 5f6c 6179       encoder_lay
-00031040: 6572 5f70 7265 7365 6e74 203d 204e 6f6e  er_present = Non
-00031050: 650a 2020 2020 2020 2020 6465 636f 6465  e.        decode
-00031060: 725f 6c61 7965 725f 7072 6573 656e 7420  r_layer_present 
-00031070: 3d20 4e6f 6e65 0a20 2020 2020 2020 2061  = None.        a
-00031080: 6363 756d 5f6c 6f73 7320 3d20 7365 6c66  ccum_loss = self
-00031090: 2e61 7578 5f6c 6f73 730a 2020 2020 2020  .aux_loss.      
-000310a0: 2020 6966 2073 656c 662e 656e 636f 6465    if self.encode
-000310b0: 7220 6973 206e 6f74 204e 6f6e 653a 0a20  r is not None:. 
-000310c0: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-000310d0: 6c66 2e75 7365 5f6d 6f65 3a0a 2020 2020  lf.use_moe:.    
-000310e0: 2020 2020 2020 2020 2020 2020 656e 636f              enco
-000310f0: 6465 725f 6f75 7470 7574 2c20 656e 636f  der_output, enco
-00031100: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
-00031110: 742c 2065 6e63 6f64 6572 5f61 7578 5f6c  t, encoder_aux_l
-00031120: 6f73 7320 3d20 7365 6c66 2e65 6e63 6f64  oss = self.encod
-00031130: 6572 2865 6e63 6f64 6572 5f69 6e70 7574  er(encoder_input
-00031140: 732c 2065 6e63 6f64 6572 5f6d 6173 6b73  s, encoder_masks
-00031150: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00031160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030f50: 2020 2020 6465 636f 6465 725f 696e 7075      decoder_inpu
+00030f60: 7473 3d4e 6f6e 652c 0a20 2020 2020 2020  ts=None,.       
+00030f70: 2020 2020 2020 2020 2020 2064 6563 6f64             decod
+00030f80: 6572 5f6d 6173 6b73 3d4e 6f6e 652c 0a20  er_masks=None,. 
+00030f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030fa0: 206d 656d 6f72 795f 6d61 736b 3d4e 6f6e   memory_mask=Non
+00030fb0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00030fc0: 2020 2020 2069 6e69 745f 7265 7365 743d       init_reset=
+00030fd0: 5472 7565 2c0a 2020 2020 2020 2020 2020  True,.          
+00030fe0: 2020 2020 2020 2020 6261 7463 685f 7661          batch_va
+00030ff0: 6c69 645f 6c65 6e67 7468 3d4e 6f6e 6529  lid_length=None)
+00031000: 3a0a 2020 2020 2020 2020 2222 2270 726f  :.        """pro
+00031010: 6365 7373 2070 726f 6365 7373 2222 220a  cess process""".
+00031020: 2020 2020 2020 2020 656e 636f 6465 725f          encoder_
+00031030: 6f75 7470 7574 203d 204e 6f6e 650a 2020  output = None.  
+00031040: 2020 2020 2020 6f75 7470 7574 203d 204e        output = N
+00031050: 6f6e 650a 2020 2020 2020 2020 656e 636f  one.        enco
+00031060: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
+00031070: 7420 3d20 4e6f 6e65 0a20 2020 2020 2020  t = None.       
+00031080: 2064 6563 6f64 6572 5f6c 6179 6572 5f70   decoder_layer_p
+00031090: 7265 7365 6e74 203d 204e 6f6e 650a 2020  resent = None.  
+000310a0: 2020 2020 2020 6163 6375 6d5f 6c6f 7373        accum_loss
+000310b0: 203d 2073 656c 662e 6175 785f 6c6f 7373   = self.aux_loss
+000310c0: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+000310d0: 2e65 6e63 6f64 6572 2069 7320 6e6f 7420  .encoder is not 
+000310e0: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
+000310f0: 2020 6966 2073 656c 662e 7573 655f 6d6f    if self.use_mo
+00031100: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+00031110: 2020 2065 6e63 6f64 6572 5f6f 7574 7075     encoder_outpu
+00031120: 742c 2065 6e63 6f64 6572 5f6c 6179 6572  t, encoder_layer
+00031130: 5f70 7265 7365 6e74 2c20 656e 636f 6465  _present, encode
+00031140: 725f 6175 785f 6c6f 7373 203d 2073 656c  r_aux_loss = sel
+00031150: 662e 656e 636f 6465 7228 656e 636f 6465  f.encoder(encode
+00031160: 725f 696e 7075 7473 2c20 656e 636f 6465  r_inputs, encode
+00031170: 725f 6d61 736b 732c 0a20 2020 2020 2020  r_masks,.       
 00031180: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00031190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000311a0: 2020 2020 2020 2020 2069 6e69 745f 7265           init_re
-000311b0: 7365 742c 2062 6174 6368 5f76 616c 6964  set, batch_valid
-000311c0: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-000311d0: 2020 2020 2020 2020 2061 6363 756d 5f6c           accum_l
-000311e0: 6f73 7320 3d20 7365 6c66 2e61 6464 2861  oss = self.add(a
-000311f0: 6363 756d 5f6c 6f73 732c 2065 6e63 6f64  ccum_loss, encod
-00031200: 6572 5f61 7578 5f6c 6f73 7329 0a20 2020  er_aux_loss).   
-00031210: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-00031220: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-00031230: 6e63 6f64 6572 5f6f 7574 7075 742c 2065  ncoder_output, e
-00031240: 6e63 6f64 6572 5f6c 6179 6572 5f70 7265  ncoder_layer_pre
-00031250: 7365 6e74 203d 2073 656c 662e 656e 636f  sent = self.enco
-00031260: 6465 7228 656e 636f 6465 725f 696e 7075  der(encoder_inpu
-00031270: 7473 2c20 656e 636f 6465 725f 6d61 736b  ts, encoder_mask
-00031280: 732c 2069 6e69 745f 7265 7365 742c 0a20  s, init_reset,. 
-00031290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000312a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000312b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311d0: 696e 6974 5f72 6573 6574 2c20 6261 7463  init_reset, batc
+000311e0: 685f 7661 6c69 645f 6c65 6e67 7468 290a  h_valid_length).
+000311f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031200: 6163 6375 6d5f 6c6f 7373 203d 2073 656c  accum_loss = sel
+00031210: 662e 6164 6428 6163 6375 6d5f 6c6f 7373  f.add(accum_loss
+00031220: 2c20 656e 636f 6465 725f 6175 785f 6c6f  , encoder_aux_lo
+00031230: 7373 290a 2020 2020 2020 2020 2020 2020  ss).            
+00031240: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00031250: 2020 2020 2020 656e 636f 6465 725f 6f75        encoder_ou
+00031260: 7470 7574 2c20 656e 636f 6465 725f 6c61  tput, encoder_la
+00031270: 7965 725f 7072 6573 656e 7420 3d20 7365  yer_present = se
+00031280: 6c66 2e65 6e63 6f64 6572 2865 6e63 6f64  lf.encoder(encod
+00031290: 6572 5f69 6e70 7574 732c 2065 6e63 6f64  er_inputs, encod
+000312a0: 6572 5f6d 6173 6b73 2c20 696e 6974 5f72  er_masks, init_r
+000312b0: 6573 6574 2c0a 2020 2020 2020 2020 2020  eset,.          
 000312c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000312d0: 2020 2020 6261 7463 685f 7661 6c69 645f      batch_valid_
-000312e0: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
-000312f0: 2020 2020 6f75 7470 7574 203d 2065 6e63      output = enc
-00031300: 6f64 6572 5f6f 7574 7075 740a 0a20 2020  oder_output..   
-00031310: 2020 2020 2069 6620 7365 6c66 2e64 6563       if self.dec
-00031320: 6f64 6572 2069 7320 6e6f 7420 4e6f 6e65  oder is not None
-00031330: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
-00031340: 6465 636f 6465 7220 6d61 736b 2073 686f  decoder mask sho
-00031350: 756c 6420 6265 2063 7265 6174 6564 206f  uld be created o
-00031360: 7574 7369 6465 206f 6620 7468 6520 6d6f  utside of the mo
-00031370: 6465 6c0a 2020 2020 2020 2020 2020 2020  del.            
-00031380: 6966 2073 656c 662e 7573 655f 6d6f 653a  if self.use_moe:
-00031390: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000313a0: 2064 6563 6f64 6572 5f6f 7574 7075 742c   decoder_output,
-000313b0: 2064 6563 6f64 6572 5f6c 6179 6572 5f70   decoder_layer_p
-000313c0: 7265 7365 6e74 2c20 6465 636f 6465 725f  resent, decoder_
-000313d0: 6175 785f 6c6f 7373 203d 2073 656c 662e  aux_loss = self.
-000313e0: 6465 636f 6465 7228 6465 636f 6465 725f  decoder(decoder_
-000313f0: 696e 7075 7473 2c20 6465 636f 6465 725f  inputs, decoder_
-00031400: 6d61 736b 732c 0a20 2020 2020 2020 2020  masks,.         
-00031410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312f0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
+00031300: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
+00031310: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
+00031320: 7420 3d20 656e 636f 6465 725f 6f75 7470  t = encoder_outp
+00031330: 7574 0a0a 2020 2020 2020 2020 6966 2073  ut..        if s
+00031340: 656c 662e 6465 636f 6465 7220 6973 206e  elf.decoder is n
+00031350: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
+00031360: 2020 2020 2023 2064 6563 6f64 6572 206d       # decoder m
+00031370: 6173 6b20 7368 6f75 6c64 2062 6520 6372  ask should be cr
+00031380: 6561 7465 6420 6f75 7473 6964 6520 6f66  eated outside of
+00031390: 2074 6865 206d 6f64 656c 0a20 2020 2020   the model.     
+000313a0: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+000313b0: 7365 5f6d 6f65 3a0a 2020 2020 2020 2020  se_moe:.        
+000313c0: 2020 2020 2020 2020 6465 636f 6465 725f          decoder_
+000313d0: 6f75 7470 7574 2c20 6465 636f 6465 725f  output, decoder_
+000313e0: 6c61 7965 725f 7072 6573 656e 742c 2064  layer_present, d
+000313f0: 6563 6f64 6572 5f61 7578 5f6c 6f73 7320  ecoder_aux_loss 
+00031400: 3d20 7365 6c66 2e64 6563 6f64 6572 2864  = self.decoder(d
+00031410: 6563 6f64 6572 5f69 6e70 7574 732c 2064  ecoder_inputs, d
+00031420: 6563 6f64 6572 5f6d 6173 6b73 2c0a 2020  ecoder_masks,.  
 00031430: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00031440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031450: 2020 2020 2020 2020 2020 2020 2020 656e                en
-00031460: 636f 6465 725f 6f75 7470 7574 2c20 6d65  coder_output, me
-00031470: 6d6f 7279 5f6d 6173 6b2c 0a20 2020 2020  mory_mask,.     
-00031480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000314a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031480: 2020 2020 2065 6e63 6f64 6572 5f6f 7574       encoder_out
+00031490: 7075 742c 206d 656d 6f72 795f 6d61 736b  put, memory_mask
+000314a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 000314b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000314c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000314d0: 2020 696e 6974 5f72 6573 6574 2c20 6261    init_reset, ba
-000314e0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-000314f0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00031500: 2020 6163 6375 6d5f 6c6f 7373 203d 2073    accum_loss = s
-00031510: 656c 662e 6164 6428 6163 6375 6d5f 6c6f  elf.add(accum_lo
-00031520: 7373 2c20 6465 636f 6465 725f 6175 785f  ss, decoder_aux_
-00031530: 6c6f 7373 290a 2020 2020 2020 2020 2020  loss).          
-00031540: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-00031550: 2020 2020 2020 2020 6465 636f 6465 725f          decoder_
-00031560: 6f75 7470 7574 2c20 6465 636f 6465 725f  output, decoder_
-00031570: 6c61 7965 725f 7072 6573 656e 7420 3d20  layer_present = 
-00031580: 7365 6c66 2e64 6563 6f64 6572 2864 6563  self.decoder(dec
-00031590: 6f64 6572 5f69 6e70 7574 732c 0a20 2020  oder_inputs,.   
-000315a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000315b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000315c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000314d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000314e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000314f0: 2020 2020 2020 2020 2069 6e69 745f 7265           init_re
+00031500: 7365 742c 2062 6174 6368 5f76 616c 6964  set, batch_valid
+00031510: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
+00031520: 2020 2020 2020 2020 2061 6363 756d 5f6c           accum_l
+00031530: 6f73 7320 3d20 7365 6c66 2e61 6464 2861  oss = self.add(a
+00031540: 6363 756d 5f6c 6f73 732c 2064 6563 6f64  ccum_loss, decod
+00031550: 6572 5f61 7578 5f6c 6f73 7329 0a20 2020  er_aux_loss).   
+00031560: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+00031570: 2020 2020 2020 2020 2020 2020 2020 2064                 d
+00031580: 6563 6f64 6572 5f6f 7574 7075 742c 2064  ecoder_output, d
+00031590: 6563 6f64 6572 5f6c 6179 6572 5f70 7265  ecoder_layer_pre
+000315a0: 7365 6e74 203d 2073 656c 662e 6465 636f  sent = self.deco
+000315b0: 6465 7228 6465 636f 6465 725f 696e 7075  der(decoder_inpu
+000315c0: 7473 2c0a 2020 2020 2020 2020 2020 2020  ts,.            
 000315d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000315e0: 2020 6465 636f 6465 725f 6d61 736b 732c    decoder_masks,
-000315f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00031600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000315e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000315f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031600: 2020 2020 2020 2020 2064 6563 6f64 6572           decoder
+00031610: 5f6d 6173 6b73 2c0a 2020 2020 2020 2020  _masks,.        
 00031620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031630: 2020 2020 2020 656e 636f 6465 725f 6f75        encoder_ou
-00031640: 7470 7574 2c0a 2020 2020 2020 2020 2020  tput,.          
-00031650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031650: 2020 2020 2020 2020 2020 2020 2065 6e63               enc
+00031660: 6f64 6572 5f6f 7574 7075 742c 0a20 2020  oder_output,.   
 00031670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031680: 2020 2020 2020 2020 2020 206d 656d 6f72             memor
-00031690: 795f 6d61 736b 2c20 696e 6974 5f72 6573  y_mask, init_res
-000316a0: 6574 2c0a 2020 2020 2020 2020 2020 2020  et,.            
-000316b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000316c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000316a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000316b0: 2020 6d65 6d6f 7279 5f6d 6173 6b2c 2069    memory_mask, i
+000316c0: 6e69 745f 7265 7365 742c 0a20 2020 2020  nit_reset,.     
 000316d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000316e0: 2020 2020 2020 2020 2062 6174 6368 5f76           batch_v
-000316f0: 616c 6964 5f6c 656e 6774 6829 0a20 2020  alid_length).   
-00031700: 2020 2020 2020 2020 206f 7574 7075 7420           output 
-00031710: 3d20 6465 636f 6465 725f 6f75 7470 7574  = decoder_output
-00031720: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
-00031730: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-00031740: 2020 2020 2020 7265 7475 726e 206f 7574        return out
-00031750: 7075 742c 2065 6e63 6f64 6572 5f6c 6179  put, encoder_lay
-00031760: 6572 5f70 7265 7365 6e74 2c20 6465 636f  er_present, deco
-00031770: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
-00031780: 742c 2061 6363 756d 5f6c 6f73 730a 2020  t, accum_loss.  
-00031790: 2020 2020 2020 7265 7475 726e 206f 7574        return out
-000317a0: 7075 742c 2065 6e63 6f64 6572 5f6c 6179  put, encoder_lay
-000317b0: 6572 5f70 7265 7365 6e74 2c20 6465 636f  er_present, deco
-000317c0: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
-000317d0: 740a                                     t.
+000316e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000316f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031710: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+00031720: 7468 290a 2020 2020 2020 2020 2020 2020  th).            
+00031730: 6f75 7470 7574 203d 2064 6563 6f64 6572  output = decoder
+00031740: 5f6f 7574 7075 740a 2020 2020 2020 2020  _output.        
+00031750: 6966 2073 656c 662e 7573 655f 6d6f 653a  if self.use_moe:
+00031760: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret
+00031770: 7572 6e20 6f75 7470 7574 2c20 656e 636f  urn output, enco
+00031780: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
+00031790: 742c 2064 6563 6f64 6572 5f6c 6179 6572  t, decoder_layer
+000317a0: 5f70 7265 7365 6e74 2c20 6163 6375 6d5f  _present, accum_
+000317b0: 6c6f 7373 0a20 2020 2020 2020 2072 6574  loss.        ret
+000317c0: 7572 6e20 6f75 7470 7574 2c20 656e 636f  urn output, enco
+000317d0: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
+000317e0: 742c 2064 6563 6f64 6572 5f6c 6179 6572  t, decoder_layer
+000317f0: 5f70 7265 7365 6e74 0a                   _present.
```

## mindformers/tools/check_rules.py

```diff
@@ -106,15 +106,15 @@
                        f"disable use_flash_attention in {mode} mode.")
 
 
 def _rule_pp_only_for_train(config, mode):
     """pp only support training for now"""
     _, _, pp = get_parallel_strategy(config)
     if pp > 1:
-        raise ValueError(f"pp only support training process for now, set pp=1 to {mode} ")
+        raise ValueError(f"pipeline stage only support training process for now, set pipeline stage=1 to {mode} ")
 
 
 def _check_full_batch():
     """check full_batch"""
     parallel_mode = ms.get_auto_parallel_context("parallel_mode")
     full_batch = ms.get_auto_parallel_context("full_batch")
     if parallel_mode not in ["semi_auto_parallel", "auto_parallel"] and full_batch:
```

## mindformers/tools/logger.py

```diff
@@ -174,14 +174,38 @@
             self.latest_log = log_message
             self._emit(record)
 
     def _emit(self, record):
         super().emit(record)
 
 
+class LimitedRepeatFileHandler(logging.handlers.RotatingFileHandler):
+    """Limited Repeat File Handler"""
+    def __init__(self, max_repeats=10, **kwargs):
+        super().__init__(**kwargs)
+        self.max_repeats = max_repeats
+        self.latest_log = ''
+        self.count = 1
+
+    def emit(self, record):
+        """emit"""
+        log_message = record.getMessage()
+        if log_message == self.latest_log:
+            self.count += 1
+            if self.count <= self.max_repeats:
+                self._emit(record)
+        else:
+            self.count = 1
+            self.latest_log = log_message
+            self._emit(record)
+
+    def _emit(self, record):
+        super().emit(record)
+
+
 class StreamRedirector:
     """Stream Re-director for Log."""
 
     def __init__(self, source_stream, target_stream):
         """Redirects the source stream to the target stream.
 
         Args:
@@ -526,17 +550,17 @@
             os.makedirs(base_dir, exist_ok=True)
         file_path.append(path)
 
     max_file_size = max_file_size * 1024 * 1024
 
     file_formatter = _DataFormatter(DEFAULT_FILEHANDLER_FORMAT)
     for i, level in enumerate(file_level):
-        file_handler = logging.handlers.RotatingFileHandler(filename=file_path[i],
-                                                            maxBytes=max_file_size,
-                                                            backupCount=max_num_of_files)
+        file_handler = LimitedRepeatFileHandler(filename=file_path[i],
+                                                maxBytes=max_file_size,
+                                                backupCount=max_num_of_files)
         file_handler.setLevel(level)
         file_handler.setFormatter(file_formatter)
         mf_logger.addHandler(file_handler)
 
     mf_logger.setLevel(_convert_level('INFO'))
 
     mf_logger.propagate = False
```

## mindformers/trainer/base_trainer.py

```diff
@@ -625,15 +625,18 @@
             logger.info(".............Start load resume context from checkpoint..................")
             load_resume_context_from_checkpoint(config, dataset)
             resume_dict = {
                 "step_num": config.runner_config.initial_step,
                 "epoch_num": config.runner_config.initial_epoch,
             }
             if config.runner_wrapper.scale_sense is not None:
-                resume_dict["loss_scale"] = config.runner_wrapper.scale_sense.loss_scale_value
+                if hasattr(config.runner_wrapper.scale_sense, 'loss_scale_value'):
+                    resume_dict["loss_scale"] = config.runner_wrapper.scale_sense.loss_scale_value
+                else:
+                    resume_dict["loss_scale"] = config.runner_wrapper.scale_sense
             logger.info("initial epoch: %d", config.runner_config.initial_epoch)
             logger.info("initial step: %d", config.runner_config.initial_step)
             append_info = [resume_dict]
             dataset.set_init_step(config.runner_config.initial_step)
         else:
             config.runner_config.initial_epoch = 0
             config.runner_config.initial_step = 0
```

## mindformers/trainer/utils.py

```diff
@@ -235,45 +235,47 @@
     for key, value in config.items():
         if isinstance(value, MindFormerConfig):
             value = config2dict(value)
         new_dict.setdefault(key, value)
     return new_dict
 
 
-def load_distributed_checkpoint(checkpoint_dir, specify_prefix=None):
+def load_distributed_checkpoint(checkpoint_dir, choice_func=None):
     """Load Checkpoint in Parallel Mode."""
     if os.path.isdir(checkpoint_dir):
         logger.info(
             "When distributed loads are sliced weights,"
             "load_checkpoint should be a checkpoint directory containing the directory of rank_{0-*},"
             "The directory structure is as follows: **checkpoint_root_dir/rank_{0-*}/**.ckpt")
         distribute_checkpoint_dir = os.path.join(
             checkpoint_dir, "rank_{}".format(get_real_rank()))
         distribute_checkpoint_path = get_last_checkpoint(distribute_checkpoint_dir)
     elif os.path.isfile(checkpoint_dir):
         logger.info("Your load_checkpoint is file, it will be load in network.")
         distribute_checkpoint_path = checkpoint_dir
     else:
         raise FileNotFoundError(f"{checkpoint_dir} is not found.")
-    checkpoint_dict = load_checkpoint(distribute_checkpoint_path, specify_prefix=specify_prefix)
+    checkpoint_dict = load_checkpoint(distribute_checkpoint_path, choice_func=choice_func)
     logger.info("Distribute load is success.")
     return checkpoint_dict
 
 
 def load_resume_context_from_checkpoint(config, dataset):
     """resume training, load training info from checkpoint to config"""
     if not os.path.realpath(config.load_checkpoint) or \
             not os.path.exists(config.load_checkpoint):
         raise FileNotFoundError(f"The load_checkpoint must be correct, "
                                 f"but get {config.load_checkpoint}")
 
     if os.path.isdir(config.load_checkpoint):
-        resume_dict = load_distributed_checkpoint(config.load_checkpoint, ["loss_scale", "epoch_num", "step_num"])
+        resume_dict = load_distributed_checkpoint(config.load_checkpoint,
+                                                  choice_func=lambda x: x in ["loss_scale", "epoch_num", "step_num"])
     else:
-        resume_dict = load_checkpoint(config.load_checkpoint, specify_prefix=["loss_scale", "epoch_num", "step_num"])
+        resume_dict = load_checkpoint(config.load_checkpoint,
+                                      choice_func=lambda x: x in ["loss_scale", "epoch_num", "step_num"])
 
     if "step_num" in resume_dict:
         config.runner_config.initial_step = int(resume_dict["step_num"])
     else:
         config.runner_config.initial_step = 0
 
     if "epoch_num" in resume_dict:
@@ -285,15 +287,18 @@
             config.runner_config.initial_epoch = int(resume_dict["epoch_num"]) - not_last_step_in_epoch
     else:
         config.runner_config.initial_epoch = 0
 
     for callback in config.callbacks:
         if "type" in callback and callback["type"] == "CheckpointMointor":
             if config.runner_wrapper.scale_sense is not None and "loss_scale" in resume_dict:
-                config.runner_wrapper.scale_sense.loss_scale_value = resume_dict["loss_scale"]
+                if hasattr(config.runner_wrapper.scale_sense, "loss_scale_value"):
+                    config.runner_wrapper.scale_sense.loss_scale_value = resume_dict["loss_scale"]
+                else:
+                    config.runner_wrapper.scale_sense = resume_dict["loss_scale"]
             break
 
 
 def transform_and_load_checkpoint(config, model, network, dataset, optimizer=None, do_eval=False, do_predict=False):
     """
     load checkpoint into net, transform checkpoint if transform is True
     1. build net if parallel mode is auto_parallel
@@ -303,16 +308,16 @@
     5. load ckpt
     """
     if not config.only_save_strategy and (not os.path.realpath(config.load_checkpoint) or
                                           not os.path.exists(config.load_checkpoint)):
         raise FileNotFoundError(f"The load_checkpoint must be correct, "
                                 f"but get {config.load_checkpoint}")
 
-    if check_path_include_total_ckpt(config.load_checkpoint) and not config.auto_trans_ckpt and \
-                                                                 not config.only_save_strategy:
+    if not config.auto_trans_ckpt and not config.only_save_strategy and \
+        check_path_include_total_ckpt(config.load_checkpoint):
         load_ckpt(config, network, optimizer=optimizer)
         return
 
     if context.get_auto_parallel_context('parallel_mode') in ['semi_auto_parallel', 'auto_parallel',
                                                               'hybrid_parallel']:
         # 1. build net if parallel mode is auto_parallel
         logger.info(".........Building model.........")
@@ -348,14 +353,22 @@
     load_ckpt(config, network, optimizer=optimizer)
 
 
 def check_ckpt_for_transform(ckpt_dir):
     """check input ckpt_dir and transform it by using softlink"""
     soft_link_dir = os.path.join(get_output_root_path(), "softlink_ckpt")
     rank_id = get_real_rank()
+
+    if os.path.isdir(ckpt_dir) and not check_rank_folders(ckpt_dir, 0) and \
+        not check_ckpt_file_exist(ckpt_dir):
+        raise ValueError(f"No rank_0 folder or ckpt files are found under {ckpt_dir}.")
+    if os.path.isfile(ckpt_dir) and not ckpt_dir.endswith('.ckpt'):
+        raise ValueError(f"The value of load_checkpoint must be a folder or a file with suffix '.ckpt', "
+                         f"but got {ckpt_dir}")
+
     if (not rank_id) or (rank_id % 8 == 0 and check_in_modelarts()):
         if os.path.exists(soft_link_dir):
             shutil.rmtree(soft_link_dir)
             logger.info("Find exist softlink dir %s and delete it.", os.path.join(os.getcwd(), soft_link_dir))
         if os.path.isdir(ckpt_dir):
             if check_rank_folders(ckpt_dir, 0):
                 if check_ckpt_file_exist(ckpt_dir):
@@ -367,30 +380,24 @@
                 soft_link = os.path.join(soft_link_dir, os.path.basename(ckpt_dir))
                 logger.info("Make soft link of checkpoint file from %s to %s", ckpt_dir, soft_link)
                 if not os.path.exists(soft_link):
                     os.symlink(ckpt_dir, soft_link)
                 else:
                     os.remove(soft_link)
                     os.symlink(ckpt_dir, soft_link)
-            elif check_ckpt_file_exist(ckpt_dir):
+            else:
                 for ckpt_file in os.listdir(ckpt_dir):
                     if ckpt_file.endswith('.ckpt'):
                         soft_link = os.path.join(soft_link_dir, os.path.splitext(ckpt_file)[0])
                         ckpt_file = os.path.join(ckpt_dir, ckpt_file)
                         make_softlink(soft_link, ckpt_file)
-            else:
-                raise ValueError(f"No rank_0 folder or ckpt files are found under {ckpt_dir}.")
         else:
-            if ckpt_dir.endswith('.ckpt'):
-                ckpt_file = ckpt_dir
-                soft_link = os.path.join(soft_link_dir, os.path.splitext(os.path.basename(ckpt_file))[0])
-                make_softlink(soft_link, ckpt_file)
-            else:
-                raise ValueError(f"The value of load_checkpoint must be a folder or a file with suffix '.ckpt', "
-                                 f"but got {ckpt_dir}")
+            ckpt_file = ckpt_dir
+            soft_link = os.path.join(soft_link_dir, os.path.splitext(os.path.basename(ckpt_file))[0])
+            make_softlink(soft_link, ckpt_file)
 
     wait_create_softlink(soft_link_dir)
 
     return soft_link_dir
 
 
 def check_rank_folders(path, rank_id):
@@ -407,14 +414,16 @@
         if file_name.endswith('.ckpt'):
             return True
     return False
 
 
 def check_path_include_total_ckpt(path):
     """check if the input path is total, not split."""
+    if path is None:
+        return False
     if os.path.isdir(path):
         if check_ckpt_file_exist(path):
             return True
     elif path.endswith('.ckpt'):
         return True
     return False
 
@@ -445,25 +454,21 @@
 
 
 def get_src_and_dst_strategy(config):
     """get strategy"""
     rank_id = get_real_rank()
     world_size = get_real_group_size()
 
+    if config.src_strategy_path_or_dir:
+        assert os.path.exists(config.src_strategy_path_or_dir), \
+            f'{config.src_strategy_path_or_dir} not found!'
+
     dst_strategy_path = None
     if (not rank_id) or (rank_id % 8 == 0 and check_in_modelarts()):
-        if config.src_strategy_path_or_dir and os.path.isdir(config.src_strategy_path_or_dir):
-            if config.parallel_config.pipeline_stage > 1:
-                src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
-            elif config.parallel_config.pipeline_stage == 1:
-                src_strategy_paths = glob(os.path.join(config.src_strategy_path_or_dir, "*_rank_*.ckpt"))
-                src_strategy_paths.sort()
-                src_strategy_path = src_strategy_paths[0]
-        else:
-            src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
+        src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
     else:
         src_strategy_path = None
 
     if world_size == 1:
         return src_strategy_path, dst_strategy_path
 
     if check_in_modelarts():
@@ -569,16 +574,16 @@
                                      src_ckpt_strategy,
                                      dst_ckpt_strategy)
             logger.info(".........Transform succeed!.........")
             transform_succeed_txt = os.path.join(transformed_ckpt_dir,
                                                  f'transform_succeed_rank_{rank_id}.txt')
             f = open(transform_succeed_txt, 'w')
             f.close()
-        except RuntimeError:
-            logger.error(".........Transform failed!.........")
+        except (NotADirectoryError, TypeError, ValueError, NotImplementedError, RuntimeError) as e:
+            logger.error(f".........Transform failed due to: {str(e)}.........")
             transform_failed_txt = os.path.join(transformed_ckpt_dir,
                                                 f'transform_failed_rank_{rank_id}.txt')
             f = open(transform_failed_txt, 'w')
             f.close()
 
         if check_in_modelarts():
             transformed_ckpt_dir_obs = os.path.join(config.remote_save_url, "transformed_checkpoint", ckpt_dir)
```

## mindformers/wrapper/wrapper.py

```diff
@@ -84,14 +84,16 @@
     def __init__(self,
                  network,
                  optimizer,
                  use_clip_grad=False,
                  max_grad_norm=1.0,
                  scale_sense=1.0,
                  **kwargs):
+        if isinstance(scale_sense, (int, float)):
+            scale_sense = Tensor(scale_sense)
         super(MFTrainOneStepCell, self).__init__(network, optimizer, scale_sense)
         self.use_clip_grad = use_clip_grad
         if isinstance(optimizer, FusedCastAdamWeightDecay):
             self.use_grad_norm = True
         else:
             self.use_grad_norm = False
         self.clip_grad_norm = ClipGradNorm(max_norm=max_grad_norm)
@@ -185,14 +187,16 @@
     Raises:
         TypeError: If `scale_sense` is neither Cell nor Tensor.
         ValueError: If shape of `scale_sense` is neither (1,) nor ().
     """
 
     def __init__(self, network, optimizer, use_clip_grad=True, max_grad_norm=1.0,
                  scale_sense=1.0, micro_batch_num=1, **kwargs):
+        if isinstance(scale_sense, (int, float)):
+            scale_sense = Tensor(scale_sense)
         super(MFPipelineWithLossScaleCell, self).__init__(network, optimizer, scale_sense)
         self.network = network
         self.network.add_flags(defer_inline=True)
         self.weights = optimizer.parameters
         self.accu_grads = self.weights.clone(prefix="accu_grads", init="zeros")
         self.optimizer = optimizer
         self.grad = C.GradOperation(get_by_list=True, sens_param=True)
```

## Comparing `mindformers-1.0.1.dist-info/LICENSE` & `mindformers-1.0.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mindformers-1.0.1.dist-info/METADATA` & `mindformers-1.0.2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mindformers
-Version: 1.0.1
+Version: 1.0.2
 Summary: mindformers platform: linux, cpu: x86_64
 Home-page: https://www.mindspore.cn
 Download-URL: https://gitee.com/mindspore/mindformers/tags
 Author: The MindSpore Authors
 Author-email: contact@mindspore.cn
 License: Apache 2.0
 Project-URL: Sources, https://gitee.com/mindspore/mindformers
@@ -90,14 +90,15 @@
 |           [MAE](docs/model_cards/mae.md)           | mae_vit_base_p16                                                   |
 |           [VIT](docs/model_cards/vit.md)           | vit_base_p16                                                       |
 |          [Swin](docs/model_cards/swin.md)          | swin_base_p4w7                                                     |
 |       [skywork](research/skywork/skywork.md)       | skywork_13b                                                        |
 |    [Baichuan2](research/baichuan2/baichuan2.md)    | baichuan2_7b, baichuan2_13b, baichuan2_7b_lora, baichuan2_13b_lora |
 |     [Baichuan](research/baichuan/baichuan.md)      | baichuan_7b, baichuan_13b                                          |
 |           [Qwen](research/qwen/qwen.md)            | qwen_7b, qwen_14b, qwen_7b_lora, qwen_14b_lora                     |
+|        [Qwen1_5](research/qwen1_5/qwen1_5.md)         | qwen1_5_72b                                                        |
 | [Wizardcoder](research/wizardcoder/wizardcoder.md) | wizardcoder_15b                                                    |
 |     [Internlm](research/internlm/internlm.md)      | internlm_7b, internlm_20b, internlm_7b_lora                        |
 |           [ziya](research/ziya/ziya.md)            | ziya_13b                                                           |
 |    [VisualGLM](research/visualglm/visualglm.md)    | visualglm                                                          |
 |[iFlytekSpark](research/iflytekspark/iflytekspark.md)    | iflytekspark_13b, iflytekspark_13b_lora                                               |
 
 ## 二、mindformers安装
@@ -113,15 +114,15 @@
 ```
 
 - 方式2：镜像
 
 docker下载命令
 
 ```shell
-docker pull swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125
+docker pull swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0.2_mindspore2.2.13:20240416
 ```
 
 创建容器
 
 ```shell
 # --device用于控制指定容器的运行NPU卡号和范围
 # -v 用于映射容器外的目录
@@ -142,27 +143,27 @@
 --device=/dev/devmm_svm \
 --device=/dev/hisi_hdc \
 -v /etc/localtime:/etc/localtime \
 -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
 -v /var/log/npu/:/usr/slog \
 -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
 --name {请手动输入容器名称} \
-swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125 \
+swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0.2_mindspore2.2.13:20240416 \
 /bin/bash
 ```
 
 ## 三、版本匹配关系
 
 当前支持的硬件为Atlas 800训练服务器 与 [Atlas 800T A2](https://www.hiascend.com/hardware/ai-server?tag=900A2)训练服务器。
 
 当前套件建议使用的Python版本为3.9。
 
 | MindFormers | MindPet |                 MindSpore                  |                                                                                                                                     CANN                                                                                                                                     |                               驱动固件                               |                               镜像链接                               | 备注     |
 | :---------: | :-----: | :----------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------: | :------------------------------------------------------------------: | -------- |
-|    r1.0     |  1.0.3  | [2.2.11](https://www.mindspore.cn/install) | 7.0.0.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run) | [链接](https://www.hiascend.com/hardware/firmware-drivers/community) | [链接](http://mirrors.cn-central-221.ovaijisuan.com/detail/118.html) | 版本分支 |
+|    r1.0     |  1.0.3  | [2.2.13](https://www.mindspore.cn/install) | 7.0.0.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run) | [链接](https://www.hiascend.com/hardware/firmware-drivers/community) | [链接](http://mirrors.cn-central-221.ovaijisuan.com/detail/118.html) | 版本分支 |
 
 其中CANN，固件驱动的安装需与使用的机器匹配，请注意识别机器型号，选择对应架构的版本
 
 ## 四、快速使用
 
 MindFormers套件对外提供两种使用和开发形式，为开发者提供灵活且简洁的使用方式和高阶开发接口。
```

## Comparing `mindformers-1.0.1.dist-info/RECORD` & `mindformers-1.0.2.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-configs/README.md,sha256=MbH9Wvik_fIHbH41_l2qWuTlRr5-jh5SKSbgxYEF0vk,14676
+configs/README.md,sha256=ORAUnAO65h1GDXqS7R68MDB6u1uXGxNfrE_0ES3NNKE,14807
 configs/bert/run_bert_base_uncased.yaml,sha256=iWOw49tacAz3fmtLRlHq04Ioo0RZ0ozTLoolf_PwgnY,4241
 configs/bert/run_bert_tiny_uncased.yaml,sha256=A0ccR79xeoDmiVQcfTXJexO_w9QmcgUom101gvm8_Z0,4257
 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml,sha256=dmd4upjSosmlfmDYHpW6NvrKfcG-TyYOJWuIne-SRJA,6074
 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml,sha256=1Bdi0SPLXgSwkSozh3LEAq10nqaiOeNeVNKjKZTrVo4,6139
 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml,sha256=YDNI1hyn6qpKGAa4crFpsdwzxC_ipomfBulS6dW7dM8,4951
 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml,sha256=mpQ2UidIQmeismh7uEq7BitX0lO_tLOCrLcWDYw7wI8,6214
 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml,sha256=joW0qsvJ6oJjL8Ti2Ulrtli6kM6uLsZ014BeUxW4arc,4705
@@ -22,15 +22,15 @@
 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml,sha256=uU7eMPTS6aeZPNCQmXeCrJ2bhkDyFI4c5aDx6AloHLU,4139
 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml,sha256=LHHp3kZa6z_34yTYqANZX1hLML9O6Q-fvgaC_6Birbk,4286
 configs/codegeex2/run_codegeex2_6b.yaml,sha256=xluiExAoTNkKsDHZN6Nq0ijAxezJAc-WI16Elwaq248,6097
 configs/codegeex2/run_codegeex2_6b_eval.yaml,sha256=mz9coO4OY9EvU3qZ8P9mDwP8Ak2pIbPlGcLXCj4_LRE,5811
 configs/codegeex2/run_codegeex2_6b_finetune.yaml,sha256=JLChiOOMBRMAEBt7Ygkqq-oR9y7AZIDye5CYjI9yl2k,5866
 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml,sha256=OXgCP_YX10mJxk9LNIBVY_tA4ptI8QFehlb8B1d_cbA,5870
 configs/codellama/predict_codellama_34b_910b.yaml,sha256=lULuMs3rdRcMG_P9WeGOZVfNOF-8OAscgXRTFV8RZzI,4009
-configs/codellama/run_codellama_34b_910b.yaml,sha256=w9kJcXUJcAse0KTfGKVOpomObojK10jjOAUIDVqPnNA,5330
+configs/codellama/run_codellama_34b_910b.yaml,sha256=8F3IqLj-euTe_NFfGNXLG6MsShq0wf7EpgwQzL-T1Bs,5218
 configs/general/run_general_task.yaml,sha256=U2W8mpKxZeLMNGY-QU3AzR-_sRoA0qt0097x_F9G80E,2637
 configs/glm/run_glm_6b_finetune.yaml,sha256=jvFDWtYM47R0lHv1XnXSL6zmciY95k9cQPyH1AFlKsM,5756
 configs/glm/run_glm_6b_infer.yaml,sha256=LonnLMP4yQo7cvcHOE7yESGnhe0UzhLQrxoJO68GZ7Q,5677
 configs/glm/run_glm_6b_lora.yaml,sha256=0TlyQatKyxosS7PvX6v-Rl1Jxhl4OndMY4jre3OvhWA,5954
 configs/glm/run_glm_6b_lora_infer.yaml,sha256=8eq2ufiwvd1aG9CzlTXbYKOWs6VnvOnLQeL3zABq4FE,5861
 configs/glm2/export_glm2_6b.yaml,sha256=_dPxIJE6FcUwZSMHhuEsNmlgRubV-JiBw4beZ-y9TB0,1564
 configs/glm2/run_glm2_6b.yaml,sha256=QlEk4a4eC7mLfmGOirUcAqeCfdQRKMte09dGrrx4ftY,5954
@@ -42,46 +42,47 @@
 configs/glm2/run_glm2_6b_lora.yaml,sha256=jaEgzrUtCrPpeY4HWsfrVMZ2Gf5MyuWjSpHkWPqJ-bE,6246
 configs/glm2/run_glm2_6b_lora_2k.yaml,sha256=qszl0gA_V4Wayy1zvyFn_APMT-DMzGKYPvQBNGnX_AQ,6254
 configs/glm2/run_glm2_6b_lora_2k_910b.yaml,sha256=nP-8ythz_gFcHmiAEzrIzcIZQJNN3U7OCM7D6aTYu98,6252
 configs/glm2/run_glm2_6b_lora_910b.yaml,sha256=_7NwggaFRl3G6mkFI6ItlP9wQLzlxBkj-xAgM_bjm7Y,6240
 configs/glm2/run_glm2_6b_lora_eval.yaml,sha256=irArX86ds2RJ5EUfvBMyzkXU2UyH9G2rBqTcoZ6NRI4,6146
 configs/glm2/run_glm2_6b_ptuning2.yaml,sha256=EMVyexSnXvITBsjYhMpdSzsogVpRbw217Pn5dDueeAY,6200
 configs/glm3/export_glm3_6b.yaml,sha256=_dO0SLWEjiJIt-sZIO8oYA622nak7SA-8aDYMrmSIDM,1640
+configs/glm3/export_glm3_6b_pa.yaml,sha256=cW920LUBf8IzftH_WsYofLPZTk6kLOgFztiwmT9AlOU,6268
 configs/glm3/run_glm3_6b.yaml,sha256=Nto5SSI6iGgigdRYDxlxGRwnENx2llk6VvPguHBbX1c,5954
 configs/glm3/run_glm3_6b_finetune_2k_910b.yaml,sha256=L4FUj31WasdkyX8o6l6Ni2puxANqfQuNYQgTcuwXjBs,5956
 configs/gpt2/run_gpt2.yaml,sha256=Meenag0SkOVnFK1imE5OxvuCsHKfAtjc6O-p9_UHxBI,4301
 configs/gpt2/run_gpt2_13b.yaml,sha256=5PB34sbNOkZAGNIw9ZFlq8X4q2A_1Xl0357K6nb0Z8A,4678
 configs/gpt2/run_gpt2_13b_910b.yaml,sha256=wbwhge4xhDIKZehAgzkEmr79Xg4hvWD2XqXUrPQPtDk,4822
 configs/gpt2/run_gpt2_52b.yaml,sha256=yHrPKuqOp4mYIPxGb68R3hmxjV3533Abl5i2cjrldzM,4628
 configs/gpt2/run_gpt2_lora.yaml,sha256=5XQX97JyQCy9vwcfBJmOfJXwvbe32wi3oXpHN8Q16Tk,4331
 configs/gpt2/run_gpt2_txtcls.yaml,sha256=iRlbO4-EPpFlRsLNkQBYXCQ1dH4qq4k9LdboHrpv0QE,4261
 configs/gpt2/run_gpt2_xl.yaml,sha256=pEKDfDXlictWLA-yw1yZhc6fY6oDw6RvFP1a1vqlD8A,4631
 configs/gpt2/run_gpt2_xl_lora.yaml,sha256=3WYUKPhtDS4nIcZSOeuCAkgfJcAhgpvZ3qN6FipPOxQ,4824
-configs/llama/run_llama_13b.yaml,sha256=FmiET1N2Dq94NGhLPgp7O2ylji7cfvfvxj-dAclHItM,5173
-configs/llama/run_llama_13b_910b.yaml,sha256=uFDLSPspQxIM8zRUoWulgisCC2PkaxcbDZZ8jWPj4Fs,5176
-configs/llama/run_llama_7b.yaml,sha256=3xCwffaPpiNS45fjWCz40sPYPEYlAf3XN6t7hcn9u6g,5169
-configs/llama/run_llama_7b_910b.yaml,sha256=CGbewna6u1Tq_pUdBBl8LzN-_VOKSnCU063RMamNH00,5167
-configs/llama/run_llama_7b_lora.yaml,sha256=CxhPIfVrqGnkI6sejAtLw4z145pnl8rpPMmAS0ckctM,5625
-configs/llama2/export_llama2_13b.yaml,sha256=etkMnV6erB8s3764A2etAs4lleio2LPXXobGhxALets,2890
-configs/llama2/export_llama2_7b.yaml,sha256=5J0gR3dMzGDvv8vThlCQjw6CV8vNoluNZtqMldygUJg,2721
-configs/llama2/predict_llama2_70b_910b.yaml,sha256=gOc3WXZh8KBJrSvuuRMJqiGDahGPLqNg5sKpa21qmVY,4057
-configs/llama2/run_llama2_13b.yaml,sha256=hbMoLjEj1Su7st_2EXi7-FsvT-15BWUWDEf63JkbmZ0,5424
+configs/llama/run_llama_13b.yaml,sha256=3mHYkplQC5q7oPoAqURd5aSZglcTTRQVUzm80RRSdUI,5024
+configs/llama/run_llama_13b_910b.yaml,sha256=PiG63uT1uXznbjxHqBIFKQiyZ8aBSYsQ6AxxwLxtqpA,5028
+configs/llama/run_llama_7b.yaml,sha256=KKZg1PMXKTWzj3qcjJDHreb8JmFr-ITbHQnCSvps2vo,5020
+configs/llama/run_llama_7b_910b.yaml,sha256=L1d-SqzxSt5gGIvoPfq9YW7CM6dPHEf5Rtv8d-hOHRc,5019
+configs/llama/run_llama_7b_lora.yaml,sha256=AVxIpZtbqLWtsftT19UfQa4O22Chykb2U8-gtiYVUcM,5473
+configs/llama2/export_llama2_13b.yaml,sha256=mAdhsPEyGa8wwu6k6Olkf7J3RGBLnEDSb5rR1PTxCQE,2766
+configs/llama2/export_llama2_7b.yaml,sha256=p3adBqumYBupDO3P6zWC-Ntr94OpU9-Rldsq_MgQfRE,2597
+configs/llama2/predict_llama2_70b_910b.yaml,sha256=ct4G5iHEACnTqgFTC68kQwTuF3x21q0cgE71OXxB-sQ,4056
+configs/llama2/run_llama2_13b.yaml,sha256=knh45sZ6nvyIPd3tdLvr_E1vJY-RX-sfUGdS6eWf8eM,5299
 configs/llama2/run_llama2_13b_910b.yaml,sha256=xkNqDwO85dch6hysr_ie650DOdfKznPnKDBHJB7na4E,5087
-configs/llama2/run_llama2_13b_910b_auto_parallel.yaml,sha256=sfT8kakOTCqYE6PwklYehDKUsEbLeBoZvTgVsWY-IAI,5655
-configs/llama2/run_llama2_13b_910b_finetune.yaml,sha256=vgSM_1k889p7ddfG6nJzbr9dRCorAj8MXlCMV_IQQTQ,5137
-configs/llama2/run_llama2_13b_lora_910b.yaml,sha256=OTnJ2ihN7AMk41BUqJNk3eB2tKziim6UTrJUmTNSNWc,5420
-configs/llama2/run_llama2_70b.yaml,sha256=U-Brmax84WmO_GRm_RLWi4SEvaw2VDWNNvMhFrtSo0U,5473
+configs/llama2/run_llama2_13b_910b_auto_parallel.yaml,sha256=RVTbLG4VRqpmiewARqS5XLACxAGMSoP-rNQeJ5rIW1k,5531
+configs/llama2/run_llama2_13b_910b_finetune.yaml,sha256=J069nZgaI0LdQX1QJn4iO8TXCNHQqqW9VkoTqknmUA0,5137
+configs/llama2/run_llama2_13b_lora_910b.yaml,sha256=gGp5sh_hjfWDxOPVKgENLglgJKa8tbT5qoq10t7W1_Y,5296
+configs/llama2/run_llama2_70b.yaml,sha256=_lPlURiUuhxzhhth_0HTtupXgLgIi_cX1HMsuqm5taM,5373
 configs/llama2/run_llama2_70b_910b.yaml,sha256=KX8l4exgMnQZhEdWMdBwZ6HXn7oV2tPuar4SEVsWtSo,5332
-configs/llama2/run_llama2_70b_910b_auto_parallel.yaml,sha256=IDECQcbQVG28QKz8Vmn5STKHj9BGeEUfVPTczj3qxi0,5897
-configs/llama2/run_llama2_70b_910b_finetune.yaml,sha256=293BdBW9oHgfj7C31aBTmdEwNucfYleCpbPu0aBaTfo,5378
-configs/llama2/run_llama2_7b.yaml,sha256=m_JOfChmRBmUbUF3MnkKQxJIjifGVQRoSbefvnSXn2Y,5414
+configs/llama2/run_llama2_70b_910b_auto_parallel.yaml,sha256=r2SIf7uQryxW_IUDodiyaY6Ge7JUf3HtouPsJgfjghU,5773
+configs/llama2/run_llama2_70b_910b_finetune.yaml,sha256=tx44WaRo7iSOi04Wcq74T45MSjxaXT2U36O_Cvu5bGE,5352
+configs/llama2/run_llama2_7b.yaml,sha256=d-1w0jGnn_Kqu4r_FHNP8KpgEz9aBIUDGy9UwGEgBLc,5289
 configs/llama2/run_llama2_7b_910b.yaml,sha256=7Cprq_WrcAMv1F7Hv0fq_MciBNjvt_JK0B3hDLOg3A0,5100
-configs/llama2/run_llama2_7b_910b_auto_parallel.yaml,sha256=4Pyypi1z7xKHMSvydkG00CJjOzA7N-ITMtXtzRdbHRM,5632
+configs/llama2/run_llama2_7b_910b_auto_parallel.yaml,sha256=yv_O5c7pkuSs9PRsDISnxrVLnniy7zIQStm9_BbRA3Y,5508
 configs/llama2/run_llama2_7b_910b_finetune.yaml,sha256=FYun0XLD1hf0e98fanmRm31rdU4bGkHTp_R81Ccd7dk,5113
-configs/llama2/run_llama2_7b_lora_910b.yaml,sha256=qzasyo7o6YRPonT16bk9D48w7csWK0615irAliO_m-o,5452
+configs/llama2/run_llama2_7b_lora_910b.yaml,sha256=GraKzXT11O7DX6YBW4W6fA_hqz1eZlA9aVXWWLXeZSo,5328
 configs/mae/run_mae_vit_base_p16_224_800ep.yaml,sha256=k_1oK8Z6YjPuhlokUKyneRl-mDP4CV3lQ8_p59l8nUU,4785
 configs/pangualpha/run_pangualpha_13b.yaml,sha256=Njz1B-sB8fJP65eDf1xdVLifVFAnOUgH9g-Odc63hhk,4942
 configs/pangualpha/run_pangualpha_2_6b.yaml,sha256=bJOc2rnSDgNQowI5IcnK8U_yQwYjRmLWa-tkskiacIU,4791
 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml,sha256=w_H7ffl4sEykYtFfky83r-zOCCFTJVCuLklYLELmtHQ,4229
 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml,sha256=vaQlqe28Ne6Qgi6u_Z0a6kO_2FtbDJT5_WTKkMCTSmQ,4495
 configs/qa/run_qa_bert_base_uncased.yaml,sha256=wcKbfloeEEX5v27UYqqyeXJCARhiyUdElAsYaW0pFXc,5531
 configs/sam/run_sam_vit-b.yaml,sha256=R-43lDKxtpEfhWzWJ1xamNozvcDe3A-sXAGr63WigyA,6818
@@ -91,19 +92,19 @@
 configs/t5/run_t5_small_on_wmt16.yaml,sha256=JSE8TBEnCYM6RAZd53vBpIU8HlyisrPmdBLDz0h2DgU,4494
 configs/t5/run_t5_tiny_on_wmt16.yaml,sha256=YcBNgrSl0YWkfXSP9YrQnXIMtY3EB1jqWkhe_rfqLz4,4455
 configs/tokcls/run_tokcls_bert_base_chinese.yaml,sha256=itd5Wk3zGYHunpUVI_oic1SKK1gwe3nwJIa4d8Y_Frs,5772
 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml,sha256=GeRdztBCVDejR5t-lsw7rtqelOB1PHA2rVVygfxHhEE,5788
 configs/txtcls/run_txtcls_bert_base_uncased.yaml,sha256=gnymRAUiIEFLScDD_cwvhuKvBMe7AnYg0eFN4357VOg,4428
 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml,sha256=7ePBhRKSAbhuiVJOSAAhRZWBKuiGcubkO3FKDiZ0lws,4438
 configs/vit/run_vit_base_p16_224_100ep.yaml,sha256=pU6CMikVOpJxeuXHvwzvsqElZPlEbmQr2NB_oPAkZAU,6020
-mindformers/.commit_id,sha256=Q8nt0idOsPFltX9blHR02PF1JQc6XUkN-BCGzmEdE8A,277
+mindformers/.commit_id,sha256=ZY88-wqV3VdPGaNpE-_VRmvTO9gujILy2UiL4lmm_SM,295
 mindformers/__init__.py,sha256=JNBu8QJXpvwn62yczNVecdnNrlj-0ZVQU2cuvjiQ8ZE,1402
 mindformers/auto_class.py,sha256=D2dhoxNbl4g_-6UKhvsvhj0cXevaWaQyxRxYBX69B7s,39712
 mindformers/mindformer_book.py,sha256=12rKSeMC0E1sNrHHmEwHxqXm4hG7OTeuQxnk6UUNy70,67667
-mindformers/version_control.py,sha256=levUodQFe4Vg-dPzI_kkNyl_UXtWVgj6NVIRD_axtWU,11096
+mindformers/version_control.py,sha256=SPJYNnZf5LvbQOmS0L5B2V0WUyCNtqswyzp0hCN3nAs,11659
 mindformers/core/__init__.py,sha256=vrKepDLPXIu_WLypOsUf-bJw4PNZCAASienEdzFPBtk,1297
 mindformers/core/clip_grad.py,sha256=_jkBQhKakCIjZf_LBkzCQnEGJIz6wd8oeoB9l-9t_r4,3956
 mindformers/core/parallel_config.py,sha256=EUIbF8huDgkDgooqV43GLn_mGY5g_bLop6FCy3wlqNU,2995
 mindformers/core/callback/__init__.py,sha256=IldgJVA6wzO2z6K1Ee4ok0nchHEfdWpi1E7AASPt8iQ,809
 mindformers/core/callback/build_callback.py,sha256=yeP_njaaoNXyR1VOSyMbyIbo3f5Dd8_FHfnJ3f4tbFI,3309
 mindformers/core/callback/callback.py,sha256=PpKTi9vLVVjBr3SLI9NRrXtGYE1KOYUA1SD9kzfcSsM,37078
 mindformers/core/context/__init__.py,sha256=f_J_0_HrDgZaeg04q93vN2gSgZWQoTuv6nk3rTYs-4E,833
@@ -175,17 +176,17 @@
 mindformers/inference/__init__.py,sha256=jp5KKZOJp_XZt8Ac2y8LR_df5uK9QL-1Bjbh1QthEKw,862
 mindformers/inference/context.py,sha256=pWsZLvUY5XICQCZmE46F43c_LzQKmQAEgNofVJkVv0s,1406
 mindformers/inference/infer_config.py,sha256=Zfzp8WRVvH-W4Xt3cm_rsYy5DPKZGn70NeGFc1nvCNw,2533
 mindformers/inference/infer_task.py,sha256=z09VDeZ9RxbcxMaxiX4vgsvXgKa8SK9G9KX2DYMNbvw,1930
 mindformers/inference/pipeline.py,sha256=gC5FlTXU2xWPA1zMkcMm9nfSiUntqJYcDQxl5022ZmE,10700
 mindformers/inference/postprocess_sampler.py,sha256=CmjGPt5ofAqjPIIGo3PxNIPJXTOj9KfCdh67Lu7uscA,2561
 mindformers/inference/infers/__init__.py,sha256=bZH2bUaomEs9XwJxjGs6VPC1B1aBADKInSKu90vGmuc,704
-mindformers/inference/infers/base_infer.py,sha256=c6k6fUe_1RiPSksy8fp5IzlYQCPKZ622JkcAxGedutc,8435
+mindformers/inference/infers/base_infer.py,sha256=oZUvqmlJ3Qwik2pBNtEeDgO4SBzj9Vh9raPgJrW-zFk,8464
 mindformers/inference/infers/cache_engine.py,sha256=cDZroa8oifXDBYNigkqcXXJTPfQpZ1r5LGvHBsHwVIA,3234
-mindformers/inference/infers/text_generator_infer.py,sha256=NbJEEYlrpgS1rryHS0wYljaYhGJE6QEtMPVRzGvmDaA,31700
+mindformers/inference/infers/text_generator_infer.py,sha256=kYzFR08uRjRuTaIcq6kTMooqrw-IubtPnUc9BrIcWb4,31813
 mindformers/models/__init__.py,sha256=eR3B5cW2iEXUn56K6C1xeJOXNGgzKY0q8o_2otYcGXM,1912
 mindformers/models/base_config.py,sha256=4XepQsvgGAduTKNclHhnv4fR0eoj26MKLCKMGamy96Q,10530
 mindformers/models/base_fast_tokenizer.py,sha256=TbS6dL9b8dveqXdj8ylMrVyuDrve2VKX39cW1PdtMms,38049
 mindformers/models/base_model.py,sha256=Ak08xuZu6AWiPI151SU15oZE4D9VFzLCDozsys2MJOg,17280
 mindformers/models/base_processor.py,sha256=8cqiSmSbK4L5m4vNH7gkfH7212nQgmpTE9R2YL9Duak,13391
 mindformers/models/base_tokenizer.py,sha256=cMrZIJBchZ1pZ0AwkFlNqVQId9u8yGzVMYrMb4BfXDQ,207274
 mindformers/models/build_config.py,sha256=Q401sHTaRZtWYYnUUZZb88CbWgPedHCqQ-kC1us0F2Y,2681
@@ -220,15 +221,15 @@
 mindformers/models/bloom/bloom.py,sha256=-yrpNGmXR78TzJiKJWzhSNq76HMuUKkcnpXs066t2CM,16962
 mindformers/models/bloom/bloom_config.py,sha256=7ifviDJOjicmcDq6L50IEE7JbMtBoPZqf8EAQKwd8W4,9127
 mindformers/models/bloom/bloom_processor.py,sha256=Em4MFWIg9w4X-JUdiayAGqXpoQOH8tc2c5Y3xpDfkq0,3474
 mindformers/models/bloom/bloom_reward.py,sha256=WK_aZmtvG06dgHIZWsaVWAbCzGxri6-zhocFuwhENrQ,4595
 mindformers/models/bloom/bloom_tokenizer.py,sha256=XvrVbftOUnN4GSxw1JOTmJg244bWfimhE5pl8hNIENI,10501
 mindformers/models/bloom/bloom_tokenizer_fast.py,sha256=eYfDEHnxm9fz8wCe5qLfeAhIhv8PpkcO_K6fnRNq8jk,6806
 mindformers/models/bloom/convert_weight.py,sha256=gToct4sgQNp3qMhzzY7Q-RYJUc5rbRw5qznzZUaVl_I,6527
-mindformers/models/bloom/layers.py,sha256=lKEaTCqhCqkCA3FYxgaLgWbOpVsCqJyLGl0TRZdCKIU,32878
+mindformers/models/bloom/layers.py,sha256=kFqlI1y6nGwkDBLZhpzjaC-jZmpvyokQ0f7m0nIZ8uM,32896
 mindformers/models/clip/__init__.py,sha256=GOAcQFfxjsPryUH79AB_hsv8aMQwIjVC8Abm1UXckfg,1030
 mindformers/models/clip/clip.py,sha256=DVJ-ujj-uZbjDjYGNg4OTU79heeekzuMUpp7qNx9NHA,10123
 mindformers/models/clip/clip_config.py,sha256=y7oq0vsE2BsEIW3LHoKiNqqzKasVZ-ea7OuC_aiBuUU,9082
 mindformers/models/clip/clip_modules.py,sha256=cOl-_wS31h-jq100seS7TOfx5QXTSr9MDS1l94Bay_g,10123
 mindformers/models/clip/clip_processor.py,sha256=dvSN1FKd17mOdGTLi9VLCZPl4mdh57oDD0EA8Rcd3lc,4894
 mindformers/models/clip/clip_tokenizer.py,sha256=-ozqTo-MvPr1I_Lnqc7DX38II9u5_bcstp6NTkQWL58,11864
 mindformers/models/clip/convert_weight.py,sha256=eZKWQfYnZuHVnizddcnOBh8IdJDUo0FBx-8dooCufNw,2989
@@ -237,39 +238,39 @@
 mindformers/models/glm/chatglm_6b_tokenizer.py,sha256=o8h5v4jKunNbMlXVU91aU_34FRzl5Y6KGXnEUql0D-U,16123
 mindformers/models/glm/convert_weight.py,sha256=9--QYldNSj5K97LcvrQdkjqrug7rhMoCxAa-PPEIITU,2224
 mindformers/models/glm/glm.py,sha256=CB8E7LtnI7WgN7sdHcV12qcH2QEX0b7UT09W_uF1WI4,22268
 mindformers/models/glm/glm_config.py,sha256=X-UV_hWKmrsvS4_MjEr3_jxgRC0huLCjQCfQbf58RL0,11300
 mindformers/models/glm/glm_processor.py,sha256=wfdLtaiVZfxUelM8W_bpFL-bNpxxQBMgdCFO2lAXiZ8,4309
 mindformers/models/glm/layers.py,sha256=Pvm-EXqdEaTF0iLKMA_YVpikBaded9BLBQNz0Lx4d9k,13117
 mindformers/models/glm2/__init__.py,sha256=NHnhl0EVXBoTJnEqmi5XyXTPLGRn2keFFzpeztu5uOE,901
-mindformers/models/glm2/glm2.py,sha256=yciNFsulUaDJEiFKU0EbMqLpvG2snGaTkuG6KOObqBI,13135
-mindformers/models/glm2/glm2_config.py,sha256=T7bkKBuT3guzo-jmQbYXQBL98ws1GoBYXej6JqHU4_A,4685
-mindformers/models/glm2/glm2_modules.py,sha256=KovBjp-GJpaKNUmpV5JHgE_MKldHz8YtwpRyX-jh2Pg,6386
+mindformers/models/glm2/glm2.py,sha256=1_MXyw_SaAFqffopmcSOcte6lHupWB4cQKilO7sh3gA,20422
+mindformers/models/glm2/glm2_config.py,sha256=d5Hi23ajY8RGlxubN6DBGdQ9UvTcz9ckZzFPCA0gQyI,7000
+mindformers/models/glm2/glm2_modules.py,sha256=5mD6RGFkdLgNkKCHMpZ43C0X3E4ekLdzlM0okJ1C13w,8791
 mindformers/models/glm2/glm2_tokenizer.py,sha256=ODawegJseQSlafLln9o2sj-0shVYGqDLAlYCTbBgKIU,10928
-mindformers/models/glm2/glm2_transformer.py,sha256=COmvOIeOgpQKH-J_KggGjRq6sT_qZ6uDZ6bAETYOFsw,32231
+mindformers/models/glm2/glm2_transformer.py,sha256=x166Iyn1Pdi9XuLg-JpsAxz6oWd6OoId2scmn_VZS8I,32609
 mindformers/models/glm3/__init__.py,sha256=1aRqJzTEWduImVv9MJd0K17tfUPH9gYvovTRfidr9l4,789
 mindformers/models/glm3/glm3_tokenizer.py,sha256=9qzYY_BVDkvxkD9i-UOTeKj6Y6n6WOO9FyTe45yK6cc,16842
 mindformers/models/gpt2/__init__.py,sha256=J4YrFIEGc2fuFgahxprEQLBy4EuarxG-ikjswTCSCx8,1106
 mindformers/models/gpt2/convert_weight.py,sha256=fDFj8kLY_6rKxqy2iNKjr6-DhAfcn7y2uP1hvmrGRfE,7212
 mindformers/models/gpt2/gpt2.py,sha256=w6xyTcDPPtcGpCVhnD-Z1E4qXDvBrwuWwOdxJoJaMHc,28341
 mindformers/models/gpt2/gpt2_config.py,sha256=zQYjvieIVX7yVuMR-CFlCN8igSPf2xhuB1e4ttSQjAs,8920
 mindformers/models/gpt2/gpt2_processor.py,sha256=eMN6bJFM8IZQ8dl1TVrBEfb6yq-_W5DAsbJLFIj_kwc,3430
 mindformers/models/gpt2/gpt2_tokenizer.py,sha256=ituwMCo-bUGIR4gY32AKJOkqbzURds5fZfN0yIFJrAE,14380
 mindformers/models/gpt2/gpt2_tokenizer_fast.py,sha256=sfHgjgo1_dSAq7ugSbYvB7nJ0zr0N5uPV07G5ot3WRI,9074
 mindformers/models/gpt2/gpt_modules.py,sha256=8Vn28SfRoeoeuzWtITvUkxgcIJqt_hTZPG1B1MIqJRc,11091
 mindformers/models/llama/__init__.py,sha256=5IFM9yzk7bwbZqL8jAeI5JjFV4cDcabf920vNJfqPDs,1061
 mindformers/models/llama/convert_weight.py,sha256=89i6rkYmJj6ya_WfdvfWpJWFNsi-j4kZrLc88ZbH_70,6999
-mindformers/models/llama/llama.py,sha256=_ZDn_BihoOH1zXC4nbqfnBvL4WnC1b4t0PQpLWcr0cc,25605
-mindformers/models/llama/llama_config.py,sha256=BUdu7QwHgwFN0YZkPrREY6RpJwOKLXjFXHmiDaxoyIQ,10886
-mindformers/models/llama/llama_interleave.py,sha256=0pnIqLN2t64tiJJAAFIYe-vO9P3HFlpcDMWo5Z3zNOQ,34676
-mindformers/models/llama/llama_layer.py,sha256=f02O49-6HVMkeCjCDLpkXoa2v8IBPQDa8AHi4d5B23Y,24857
+mindformers/models/llama/llama.py,sha256=mxxHsTncAZS0jNHIShGl9yA9ugnZZnqE5_IyMKs_njU,25687
+mindformers/models/llama/llama_config.py,sha256=dLlObonX5F5qJbqVppcltawf_Fc_VIyQbVECESwZGKE,11014
+mindformers/models/llama/llama_interleave.py,sha256=OBAwaAROkt3eIzBUMMpx68qv22mSs5UD7nKM8th_0C4,34023
+mindformers/models/llama/llama_layer.py,sha256=VwIwA9wr7_XmrJLDa3gWENEmOdXeuWSY_oji-M6FsxI,25605
 mindformers/models/llama/llama_processor.py,sha256=P_cgtSS_6J7irSjeL0NTNzfbA-5WnG4t_D2K4SzooJc,3440
 mindformers/models/llama/llama_tokenizer.py,sha256=6JblvHMlvRktCDqE80kIxmL6PRIsIVm-j6VGm4vmovM,17102
 mindformers/models/llama/llama_tokenizer_fast.py,sha256=o8ahHzZirlO1rI6rWUPf2A6v1N-tcEOCWPOge6Y59Bw,9034
-mindformers/models/llama/llama_transformer.py,sha256=MXyXgdadSABJVJ7SgSmlzQ8USfV0IKgRwWEu6xFkvoo,30964
+mindformers/models/llama/llama_transformer.py,sha256=R8XUjattoXUJ6d8d5YVOn0dxor2kMDZgiAB3DOStDfI,32324
 mindformers/models/mae/__init__.py,sha256=IB4gzMH574udRoFXuX0SZ8G_x9nv-cwE7yE4vaEMYnI,878
 mindformers/models/mae/convert_weight.py,sha256=b4wrOj3NUomuH3pn0EImZkNq4IPZuYA5P208ywL37QE,3416
 mindformers/models/mae/mae.py,sha256=v4Y3utC4Q5iz4keT38ZNqLYy7GqibHk5SymIWzN0XrA,18082
 mindformers/models/mae/mae_config.py,sha256=nxvMd9M_kIl_dd_UgLtSH99kr_GAan1J80fj66b-KiQ,7343
 mindformers/models/mae/mae_modules.py,sha256=DI1ZoWxa2l4-3aVRRsWhRFyBYj1hXmLYNRGDPH6aI-0,36274
 mindformers/models/mae/mae_processor.py,sha256=Mu_c86HleGDfo1N5dONRxsjMHxASauCnnqsNQgPw-Mg,6194
 mindformers/models/pangualpha/__init__.py,sha256=PqPG5THtOAbcpdo-81VvIlVLyI-4Y4CHnbSaJ83S47o,1022
@@ -306,22 +307,22 @@
 mindformers/models/vit/convert_weight.py,sha256=zhmsxx8Unr3iCYo6VjTcKYMKpeteyt_MROnICVm1xfg,3364
 mindformers/models/vit/vit.py,sha256=Zv0fNSe_BgEzF0iQ4LlnLhmaDCZG6LUfD6yObGHiU88,13411
 mindformers/models/vit/vit_config.py,sha256=4VgQBoOXQFh3LWBNFTquJPCAkdSFXMh_D-riCtKnVLk,8231
 mindformers/models/vit/vit_modules.py,sha256=3INeOVqVZH6JcWzJg4ZhRE-isPFoSJbpi1UPP-05aJQ,37548
 mindformers/models/vit/vit_processor.py,sha256=HSc6Cd9XogUlE88IE3vPOfFuW55G8ridwmJ545fwSxc,4925
 mindformers/modules/__init__.py,sha256=AFMSj9ZmfVDqslkGhxADVjJk4qequ-s9cxYQFt7f7Zw,992
 mindformers/modules/activation.py,sha256=KWbuLz7-vJTPisX8tstP9b7ibloEqlcysgX6tki__n0,50360
-mindformers/modules/kvcache_mgr.py,sha256=UBDn1aCujrYninEBHmLPZEvn7xaIIG9k6gMMkxw3hok,13356
+mindformers/modules/kvcache_mgr.py,sha256=VSGnIhdQEGd-QbuyfZd9WlkQM2Vi9AvGFaLmhbzcU6o,13354
 mindformers/modules/layers.py,sha256=LRLjztr2HgYYfzH6DIphUngXKzcC3V9sdK2M9HmLlMs,48060
 mindformers/modules/local_block_sparse_attention.py,sha256=Zc_Q0DMOVjBTKpRbcokuoWwiBG7lrgvXiroIJcUV2ZM,14414
 mindformers/modules/paged_attention_mgr.py,sha256=OYE4BgcJPBGdqnLGRSLakbQMlYZizenhwc352ZKumOY,4545
 mindformers/modules/transformer/__init__.py,sha256=m_AJH_u4x0yS5K9WLAQs739JFzCszpkTdZDE2D-0EF8,1335
 mindformers/modules/transformer/moe.py,sha256=uCBj9A6JzNf6uPs2BfcBmXR9sF1ZGgZLdIAh3RkOJvI,39235
 mindformers/modules/transformer/op_parallel_config.py,sha256=dd72P0OFMvJK6MuFaBZ6NXrMlEMzzvFJGTe3Fw6wiTU,8481
-mindformers/modules/transformer/transformer.py,sha256=lfGHlaK8ulLYZq4lWPCi2U1m0sbZ36di_W1eKS9PhJo,202706
+mindformers/modules/transformer/transformer.py,sha256=mOd2boWj9RUMo3G-T82Y1yHcwG-oOGGbUrVGv-PXHSo,202745
 mindformers/pet/__init__.py,sha256=mRgbD5cOL1zTOdrivVS6xXMjZIOwScMM8PNciYTPcnE,897
 mindformers/pet/constants.py,sha256=XMxgWJ9Ma0aG1woOs_FjL9lnGyscb8iH3kPAO6kekMg,1176
 mindformers/pet/pet_config.py,sha256=jDjGdPJ8RmA0Aw3Gpa5ugC9mYqoaiGbhQdvv6s1JQ-g,4920
 mindformers/pet/pet_model.py,sha256=4nMFbqoYg5Gpaqag2gFgYSf9O33gDBc329lumIjmRr0,3928
 mindformers/pet/utils.py,sha256=j7XZR7qyzEinZj5qbcogvXF9usX6G4WqCeeMj2XCxWQ,1096
 mindformers/pet/models/__init__.py,sha256=p3ZkxxsIlz8MV9Mxgwc1wBP7DpTXbi-Ico33EUTZVeI,696
 mindformers/pet/models/lora.py,sha256=NLxZV-_4ufFFiXkvlKWKjOrY0YyYr2ZK7UhmFYRtRdg,3511
@@ -344,39 +345,39 @@
 mindformers/pipeline/segment_anything_pipeline.py,sha256=grZ3bsxpRKx8vJiTaBvmAGF3X263ALKOS74VuKcN-e0,27170
 mindformers/pipeline/text_classification_pipeline.py,sha256=__1SFkMvGvk6IyZ9jKZv3-ziFoPAM4jPJULkLDCyG0U,10648
 mindformers/pipeline/text_generation_pipeline.py,sha256=UYIA5pBb6X419ZPdQR_0ZJockYj0qUHXP82H8T0IYGU,10971
 mindformers/pipeline/token_classification_pipeline.py,sha256=TU3VoHVvDSVIoYiqALiKPQVexaqKGNe6jhcwfbA1P5M,10121
 mindformers/pipeline/translation_pipeline.py,sha256=ss4nyAzqtEI1pwZHDhbQGfGp5NeNBfP5bHXsMeq0w9c,8167
 mindformers/pipeline/zero_shot_image_classification_pipeline.py,sha256=ePHlHQoJQtT22h07Lrs3eUUHyH4OS2iRAr9ADgHof2k,9276
 mindformers/tools/__init__.py,sha256=ZbtDJmB8EVDBalSdDZKgeS_RpWjGKqXFkNffvBQOmAg,1127
-mindformers/tools/check_rules.py,sha256=uBT9PbICELdFj5qYqdkgeTUzTWoVeuEL-I0HsL_ziV4,10202
+mindformers/tools/check_rules.py,sha256=NgWBgUoEUjbzPEAF26N0cFk9HCuOf4LmoWr6UpHlRlQ,10226
 mindformers/tools/download_tools.py,sha256=P26bZQX101wIOUqm_2k0v_KUa81wfWu0Z7ttSxQS02U,4399
 mindformers/tools/download_tools_multithread.py,sha256=jQWWx4Qgikw5AGWDjSw5bPxvvPtGVGNU5BBpbHlrfic,6243
 mindformers/tools/export.py,sha256=RJWeG7C8oKm1d0ls23VI8vaU1U1-tchHxHeTGqKiM1I,12033
 mindformers/tools/hccl_tools.py,sha256=oKJw__LN1fE29NH76OEdyvnZF_cKxob6EbJxWMz2vrY,6520
 mindformers/tools/image_tools.py,sha256=FAgyl164Emo1V-tAxE0jUXU1OGl9lLwi9-FGbtYAeLY,1925
-mindformers/tools/logger.py,sha256=hQBTXKEkP5syL4f4_LV8g4R8RtZ-FEeIMdbYnhefkLc,22670
+mindformers/tools/logger.py,sha256=y_xevCnbowne5PWHyVzsV9Z8jh6RvbDi3v8QBWh5rmc,23336
 mindformers/tools/merge_hccl.py,sha256=LygpRvgIJz2_WkNVMES5j5JpHGmYZmqZcP1IsA_GbDY,2316
 mindformers/tools/moe_token_distribution_tools.py,sha256=riNrEXrm9zqAzfVfqNsgM2W0apcAFoppBM5kG_2aelo,5598
 mindformers/tools/transform_ckpt.py,sha256=UzPq2hyzu4WWHTi62GBBdFRTA8wFTyFHfQUcYL-kMNM,3164
 mindformers/tools/utils.py,sha256=E37ZGEov2zbIPqwlLKHOe9DSCTGOtT1qJcXVTO1ylJo,13716
 mindformers/tools/cloud_adapter/__init__.py,sha256=Sj46iusOGBVYh5mx1SE8j1JVNYIxj96toskg4QezNi8,842
 mindformers/tools/cloud_adapter/cloud_adapter.py,sha256=URV7BYDEtL0FsCv5UxsLON6CIrVUVdh3ILUfXdwafls,8522
 mindformers/tools/cloud_adapter/cloud_monitor.py,sha256=Bwx9lvzDXMkcHOmizg4AuNrykic7RPUMUJ-g8e4ILMo,3464
 mindformers/tools/register/__init__.py,sha256=04DTJlQ3pYL8VAJWKS9fKKN15JEFkPXk7CiljsnYObU,905
 mindformers/tools/register/config.py,sha256=0ww03GFoeni3DwI5H71eAjPX44pFWF5Y6ZkFv1bXlBw,11037
 mindformers/tools/register/register.py,sha256=QJGTgZaoPB7EMAOYb5d0UIa6mFOYrvEaCGi1ixh62nE,7056
 mindformers/trainer/__init__.py,sha256=uzTXgoCNl08_kxVpUAtZ6xCsLkv4pfBXh17XPOgm3U0,1982
-mindformers/trainer/base_trainer.py,sha256=zmP9YfnGvWs4A31H7eOF_wAXdOBQCDCh5ijmjmdjJNs,52479
+mindformers/trainer/base_trainer.py,sha256=9KiqsUPO2K8RF3O7tN9UJKbZodGU8H6purQU9RkIBiY,52670
 mindformers/trainer/build_trainer.py,sha256=RvcstU-InuwHWkO0c_rV4B08kxH0xlU6frIGxSOC83I,4428
 mindformers/trainer/config_args.py,sha256=DL9aGSp3UCPnQnDyDvLtsz0M29S69r8z38X909hfArg,55851
 mindformers/trainer/optimizer_grouped_parameters.py,sha256=CJ3605L3wWTz41cdPp_GxC0L7SJ4Q6A6xlwMSzNzJas,5925
 mindformers/trainer/trainer.py,sha256=SyKYvMFpTVQl3Mcmr1cCiYWCbupmtR0ySt2pxFlThsc,51978
 mindformers/trainer/training_args.py,sha256=vJYyPFq_bi6oJI_4qrperLd3WGic6mv9XVC33Pyw0KM,15070
-mindformers/trainer/utils.py,sha256=sWBNU1rM9vYaKmJ33Dc536d4LjXA620MpAM3g4g6WjA,35015
+mindformers/trainer/utils.py,sha256=E-5aa3yG8nCIolxDPZURmCLTyZJZYrt2fBT6XvAyo1Y,35089
 mindformers/trainer/causal_language_modeling/__init__.py,sha256=ajvw2Xr-stOAdwVS56K7n1Ctx6iVIv91kktl_AylBts,821
 mindformers/trainer/causal_language_modeling/causal_language_modeling.py,sha256=G7q9RedMhaI8z8Q3QWzUfnTL7GJZ0H8KHORX9uOftBw,20146
 mindformers/trainer/contrastive_language_image_pretrain/__init__.py,sha256=_2QLmjdq0aYkYYeGQQGdrv2UuRNokZaGgPTlkicFXKM,866
 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py,sha256=vWGBhQyGr-fP9tR96GPn0_pXuudwN623hiaLLHRHO8A,4609
 mindformers/trainer/general_task_trainer/__init__.py,sha256=KQhPQnJv5tkVZ7hsfN5JVG7Vdjd3rDrn0xH9y2htnNQ,795
 mindformers/trainer/general_task_trainer/general_task_trainer.py,sha256=AVKoRs49Gdio0O0hks699zxhJvixBpGm2adSBoX-6zw,9462
 mindformers/trainer/image_classification/__init__.py,sha256=6mp76hxxRWCQ9H2YdoVB8FJ-RTbrfFnHb79rd8h5Qr4,928
@@ -400,13 +401,13 @@
 mindformers/trainer/token_classification/__init__.py,sha256=a_5Y4Kk-pNCooniku6ewvhhc8zzzirM3xZJ33-q4lxw,807
 mindformers/trainer/token_classification/token_classification.py,sha256=ptx17at-E7J5xWNCYvfygmszr4t-aU2nCQpeiINe2lg,9500
 mindformers/trainer/translation/__init__.py,sha256=ghZF7nb3MoZBOhBg5WJxhzT5-Xzdp8TTZU4e5cjWww8,795
 mindformers/trainer/translation/translation_finetune.py,sha256=92ovnBd81YaZqeLrSwdz3omItgGkbdQBpfdKq5j_znI,6738
 mindformers/wrapper/__init__.py,sha256=XjjgWv6F36VtMMoD6FQDNPCI1AlGFoZqitVPvDVLh0s,913
 mindformers/wrapper/adaptive_loss_scale.py,sha256=VCQdTjFb5GtkdfUwSMdk6h9dmYVLVQi55FoZowO4qEU,14329
 mindformers/wrapper/build_wrapper.py,sha256=MoEaObzg8VjfaYcZAZp3BwLX9eHLa7COwCrSSpsUJ2U,4117
-mindformers/wrapper/wrapper.py,sha256=RafxPlbwUL4KUmPJMusnsbnSYwEK1pJLiVjDjsCcq90,12204
-mindformers-1.0.1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-mindformers-1.0.1.dist-info/METADATA,sha256=dt5Tka4EbvJlE9CjpNGIP0owKA9ObqIWqMWfdobAXrs,19262
-mindformers-1.0.1.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mindformers-1.0.1.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
-mindformers-1.0.1.dist-info/RECORD,,
+mindformers/wrapper/wrapper.py,sha256=YHezu49QIpojNL0GHCd6Tn4Q8Gpt8PBg8Agr6EnP6lg,12396
+mindformers-1.0.2.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+mindformers-1.0.2.dist-info/METADATA,sha256=zCoS7rPR0mlZJLMuHfX5JsHFNQTfnjFqqt5rVNWpeyU,19381
+mindformers-1.0.2.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mindformers-1.0.2.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
+mindformers-1.0.2.dist-info/RECORD,,
```

